{
  "commit_hash": "baeded25699f9f4851843306f27f685c4d4ee7c5",
  "parent_hash": "3e1c76cf3a87854396d9e86a56a335e7d750c85f",
  "message": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>",
  "author": "Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>",
  "date": "2025-01-31 21:52:51 -0800",
  "files_changed": [
    {
      "file_path": "vllm/attention/backends/mla/utils.py",
      "old_content": "from abc import abstractmethod\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, Generic, List, Optional\n\nimport torch\n\nfrom vllm import _custom_ops as ops\nfrom vllm import envs\nfrom vllm.attention.backends.abstract import (AttentionLayer,\n                                              AttentionMetadata,\n                                              MLAAttentionImpl, T)\nfrom vllm.distributed import get_tensor_model_parallel_world_size\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\nfrom vllm.vllm_flash_attn import flash_attn_varlen_func\n\n\n@dataclass\nclass MLACommonMetadata(AttentionMetadata):\n    # Input positions for rotrary embeddings since for MLA the rotary\n    # position embeddings are applied inside the attention backend\n    input_positions: torch.Tensor\n\n\nclass MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n    \"\"\"\n    Common class for implementing repeated parts \n    \n    Main reference: DeepseekV2 paper, and FlashInfer Implementation\n    (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n    \n    Deepseek's MLA attention works the following way:\n    * Use a single latent vector to represent the entire KV cache.\n    * The attention \"simulates\" a multi-head attention, while the compute is\n      similar to multi-query attention.\n    * The dataflow is as follows,\n\n        * B: batch/sequence length\n        * H: hidden size\n        * N: number of attention heads\n        * Lq: latent dimension for Q\n        * Lkv: latent dimension for K/V\n        * P: nope dimension, P+R is the actual head_dim in common attention.\n        * R: rope dimension, this slide of the head_dim goes through rope.\n        * V: V head dim.\n        * kv_c: latent/compressed KV\n        * q_c: latent/compressed Q\n        \n        #\n        # Outside the MLA attention backend\n        #\n\n        1. The hidden states (B, H) are projected down into cq (B, Lq) and\n           kv_c_k_pe (B, Lkv+R).\n        2. The kv_c_k_pe is split into kv_c (B, Lkv) and k_pe (B, R). cq\n           and kv_c are normalized.\n        \n        #\n        # Inside the MLA attention backend\n        #\n\n        * if prefill:\n        \n        3. The q_c is then projected up into the multi-head version. \n           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope \n             (B, N, P) and q_pe (B, N, R). \n        4. q_pe, k_pe are then passed through rotary embeddings.\n        5. kv_c and k_pe are concatenated and inserted into the cache\n        6. The kv_c is then projected up into the multi-head version. \n           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope \n             dimensions for K and V, which is split into k_nope (B, N, P) \n             and v (B, N, V).\n        7. q (B, N, (P+R)) and k (B, N, (P+R)) matrices are assembled from\n           q_nope, q_pe, k_nope, k_pe.\n        8. Attention is computued with q, k, v.\n        9. The attention computation returns (B, N, V), which is projected back\n           to (B, H) using out projection.\n\n        * if decode:\n\n        3. Here's the change, we do not perform up the full up projection for\n           q_c, and there is no up projection at all for kv_c. This is\n           achieved by the technique of \"weight absorption\". The paper says\n           \"Fortunately, due to the associative law of matrix multiplication,\n           we can absorb WUK into WUQ, and WUV into WO\"\n           * The q up projection turns (B, Lq) into (B, N, (P+R)), we split it\n             into W_UQ (Lq, N, P) and W_QR (Lq, N, R).\n           * The kv_c up projection turns (B, Lkv) into (B, N, (P+V)), we split\n             it into W_UK (Lkv, N, P) and W_UV (Lkv, N, V).\n           * The out projection shape W_O (N*V, H) turns (B, N, V) into (B, H).\n           * We can precompute the product of W_UQ and W_UK into\n             W_UQ_UK (Lq, N, Lkv), which is possible due to QK^T operation in\n             attention.\n           * We can precompute the product of W_UV and W_O into\n             W_UV_O (N, Lkv, H), which is possible due to V@O as the\n             \"epilogue\" of attention\n        4. We still need to compute q_pe (B, N, R) by applying W_QR to q_latent.\n        5. q_pe, k_pe are then passed through rotary embeddings.\n        6. kv_c and k_pe are concatenated and inserted into the cache\n        7. By applying W_UQ_UK to q_latent, we have the new q_nope of shape\n           (B, N, Lkv).\n        8. q (B, N, (Lkv+R)), k (B, (Lkv+R)) are assembled from q_nope, q_pe,\n           kv_a, k_pe. v (B, Lkv) is exactly the same vector as kv_a.\n        9. The attention is computed with q, k, v. Note that we just performed\n           a MQA attention with (LKv+R) as our head dim.\n        10. The KV cache is updated using the new entries k (B, N, (Lkv+R)),\n           which included the v and rope values.\n        11. The attention computation returns (B, N, Lkv), which is projected\n           back to (B, H) using W_UV_O.\n\n    From @tsu-bin's calculation, we only want to use the absorption technique\n    for decode. The prefill algorithm should still use the up-projected MHA\n    for less flops and memory usage.\n    \n    \"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]],\n        logits_soft_cap: Optional[float],\n        attn_type: str,\n        # MLA Specific Arguments\n        q_lora_rank: Optional[int],\n        kv_lora_rank: int,\n        qk_nope_head_dim: int,\n        qk_rope_head_dim: int,\n        qk_head_dim: int,\n        v_head_dim: int,\n        rotary_emb: RotaryEmbedding,\n        # q_proj should be q_b_proj if q_lora_rank is not None, but from an\n        # attention backend perspective we rely on the layer to pass in the\n        # correct matrix\n        q_proj: ColumnParallelLinear,\n        kv_b_proj: ColumnParallelLinear,\n        o_proj: RowParallelLinear,\n    ) -> None:\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        self.kv_cache_dtype = kv_cache_dtype\n\n        self.q_lora_rank = q_lora_rank\n        self.kv_lora_rank = kv_lora_rank\n        self.qk_nope_head_dim = qk_nope_head_dim\n        self.qk_rope_head_dim = qk_rope_head_dim\n        self.qk_head_dim = qk_head_dim\n        self.v_head_dim = v_head_dim\n\n        self.rotary_emb = rotary_emb\n        self.q_proj = q_proj\n        self.kv_b_proj = kv_b_proj\n        self.o_proj = o_proj\n\n    def _v_up_proj_and_o_proj(self, x):\n        if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n            return self.o_proj_absorbed(\n                x.reshape(-1, self.num_heads * self.kv_lora_rank))[0]\n        else:\n            x = torch.einsum(\"bnl,lnv->bnv\", x, self.W_UV)\n            return self.o_proj(x.reshape(-1,\n                                         self.num_heads * self.v_head_dim))[0]\n\n    def _q_proj_and_k_up_proj(self, x):\n        if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n            return torch.matmul(x, self.W_Q_UK)\\\n                .view(-1, self.num_heads, self.kv_lora_rank)\n        else:\n            x = torch.matmul(x, self.W_Q)\\\n                .view(-1, self.num_heads, self.qk_nope_head_dim)\n            return torch.einsum(\"bnp,lnp->bnl\", x, self.W_UK)\\\n                .view(-1, self.num_heads, self.kv_lora_rank)\n\n    def process_weights_after_loading(self):\n        kv_b_proj_weight = self.kv_b_proj.weight.T\n        assert kv_b_proj_weight.shape == (\n            self.kv_lora_rank,\n            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim)), (\n                f\"{kv_b_proj_weight.shape=}, \"\n                f\"{self.kv_lora_rank=}, \"\n                f\"{self.num_heads=}, \"\n                f\"{self.qk_nope_head_dim=}, \"\n                f\"{self.v_head_dim=}\")\n        kv_b_proj_weight = kv_b_proj_weight.view(\n            self.kv_lora_rank,\n            self.num_heads,\n            self.qk_nope_head_dim + self.v_head_dim,\n        )\n\n        W_UK, W_UV = kv_b_proj_weight.split(\n            [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n\n        q_proj = self.q_proj.weight.T\\\n                .view(-1, self.num_heads, self.qk_head_dim)\n\n        # can be W_Q or W_UQ depending q_lora_rank, the former if\n        # q_lora_rank is None, the latter otherwise. From the Attention backend\n        # perspective though we call these both W_Q and rely on the layer\n        # to pass in the correct matrix\n        W_Q = q_proj[..., :self.qk_nope_head_dim]\n        self.W_QR = q_proj[..., self.qk_nope_head_dim:]\\\n            .flatten(start_dim=1).contiguous()\n\n        if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n            #\n            # Perform matrix-absorption following\n            #     https://github.com/flashinfer-ai/flashinfer/pull/551\n            # for decode, as a result we end up with absorbed weights for decode\n            # and another copy of raw weights for prefill.\n            #\n            self.W_UK, self.W_UV = kv_b_proj_weight.split(\n                [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n            # We absorb `W_UK` into `W_Q` resulting in either W_Q_UK or W_UQ_UK\n            # depending q_lora_rank, the former if q_lora_rank is None, the\n            # latter otherwise\n            # basically if q_lora_rank is none we are absorbing into q_proj\n            # instead of UQ\n            self.W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n                .flatten(start_dim=1).contiguous()\n\n            W_O = self.o_proj.weight\\\n                .view(-1, self.num_heads, self.v_head_dim)\n            self.W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n                .flatten(start_dim=0, end_dim=1).contiguous()\n\n            tp_size = get_tensor_model_parallel_world_size()\n            self.o_proj_absorbed = RowParallelLinear(\n                self.W_UV_O.shape[0] * tp_size,\n                self.W_UV_O.shape[1],\n                bias=False,\n                # TODO(lucas) figure out how to properly forward quant_method\n                #quant_config=self.o_proj.quant_method,\n            )\n\n            self.o_proj_absorbed.weight = torch.nn.Parameter(self.W_UV_O.T)\n        else:\n            self.W_UV = W_UV\n            self.W_UK = W_UK\n            self.W_Q = W_Q.flatten(start_dim=1)\n\n    @abstractmethod\n    def _forward_prefill(\n        self,\n        q: torch.Tensor,\n        kv_c_normed: torch.Tensor,\n        k_pe: torch.Tensor,\n        attn_metadata: T,\n    ) -> torch.Tensor:\n        raise NotImplementedError\n\n    @abstractmethod\n    def _forward_decode(\n        self,\n        q_nope: torch.Tensor,\n        q_pe: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: T,\n    ) -> torch.Tensor:\n        raise NotImplementedError\n\n    def forward(\n        self,\n        layer: AttentionLayer,\n        hidden_states_or_q_c: torch.Tensor,  # query in unified attn\n        k_c_normed: torch.Tensor,  # key in unified attn\n        k_pe: torch.Tensor,  # value in unified attn\n        kv_cache: torch.Tensor,\n        attn_metadata: T,\n        output: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        if output is not None:\n            raise NotImplementedError(\n                \"output is not yet supported for MLAImplBase\")\n\n        is_decode = attn_metadata.decode_metadata is not None\n        is_prefill = attn_metadata.prefill_metadata is not None\n\n        if (is_decode and is_prefill):\n            raise NotImplementedError(\n                \"chunked prefill is not supported for MLAImplBase\")\n\n        # Restore head dim (for rotary embedding)\n        k_pe = k_pe.unsqueeze(1)\n        assert hasattr(attn_metadata, \"input_positions\")\n\n        if is_decode:\n            q_nope = self._q_proj_and_k_up_proj(hidden_states_or_q_c)\n            q_pe = torch.matmul(hidden_states_or_q_c, self.W_QR)\\\n                .view(-1, self.num_heads, self.qk_rope_head_dim)\n            q_pe, k_pe = \\\n                self.rotary_emb(attn_metadata.input_positions, q_pe, k_pe)\n        else:\n            assert is_prefill\n            q = self.q_proj(hidden_states_or_q_c)[0]\\\n                .view(-1, self.num_heads, self.qk_head_dim)\n\n            # TODO(lucas): there must be a nicer way to write this line\n            q[..., self.qk_nope_head_dim:], k_pe = \\\n                self.rotary_emb(\n                    attn_metadata.input_positions,\n                    q[..., self.qk_nope_head_dim:], k_pe)\n\n        # write the latent and rope to kv cache\n        if kv_cache.numel() > 0:\n            ops.concat_and_cache_mla(\n                k_c_normed,\n                k_pe.squeeze(1),\n                kv_cache,\n                attn_metadata.slot_mapping.flatten(),\n                kv_cache_dtype=self.kv_cache_dtype,\n                scale=layer._k_scale,\n            )\n\n        if attn_metadata.prefill_metadata is not None:\n            return self._forward_prefill(q, k_c_normed, k_pe, attn_metadata)\n\n        if attn_metadata.decode_metadata is not None:\n            return self._forward_decode(q_nope, q_pe, kv_cache, attn_metadata)\n\n    # Optional common flash-attn based prefill\n    def _forward_prefill_flash(\n        self,\n        q: torch.Tensor,\n        k_c_normed: torch.Tensor,\n        k_pe: torch.Tensor,\n        seq_start_loc: torch.Tensor,\n        max_prefill_seq_len: int,\n    ) -> torch.Tensor:\n\n        kv_nope = self.kv_b_proj(k_c_normed)[0]\\\n            .view(-1, self.num_heads, self.qk_nope_head_dim + self.v_head_dim)\n        k_nope, v = kv_nope\\\n            .split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n\n        k = torch.cat((k_nope, k_pe.expand((*k_nope.shape[:-1], -1))), dim=-1)\n\n        # For MLA the v head dim is smaller than qk head dim so we pad out\n        # v with 0s to match the qk head dim\n        v_padded = torch.nn.functional.pad(v, [0, q.shape[-1] - v.shape[-1]],\n                                           value=0)\n\n        attn_output = flash_attn_varlen_func(\n            q=q,\n            k=k,\n            v=v_padded,\n            cu_seqlens_q=seq_start_loc,\n            cu_seqlens_k=seq_start_loc,\n            max_seqlen_q=max_prefill_seq_len,\n            max_seqlen_k=max_prefill_seq_len,\n            softmax_scale=self.scale,\n            causal=True,\n        )\n        attn_output = attn_output\\\n            .view(-1, self.num_heads, q.shape[-1])[..., :v.shape[-1]]\\\n                .reshape(-1, self.num_heads * v.shape[-1])\n\n        return self.o_proj(attn_output)[0]\n",
      "diff": "diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py\nindex c6c8a6034..e8fec234c 100644\n--- a/vllm/attention/backends/mla/utils.py\n+++ b/vllm/attention/backends/mla/utils.py\n@@ -1,17 +1,29 @@\n from abc import abstractmethod\n from dataclasses import dataclass\n-from typing import Any, Dict, Generic, List, Optional\n+from typing import Any, Dict, Generic, List, Optional, Tuple\n \n import torch\n+from compressed_tensors.quantization import QuantizationStrategy\n \n from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionLayer,\n                                               AttentionMetadata,\n                                               MLAAttentionImpl, T)\n-from vllm.distributed import get_tensor_model_parallel_world_size\n+from vllm.distributed import (get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n-                                               RowParallelLinear)\n+                                               LinearBase, RowParallelLinear,\n+                                               UnquantizedLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501\n+    CompressedTensorsLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.schemes import (\n+    CompressedTensorsW8A8Fp8)\n+from vllm.model_executor.layers.quantization.fp8 import Fp8LinearMethod\n+from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n+    apply_fp8_linear_generic, current_platform_fp8_dtype, is_fp8)\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    scaled_dequantize, scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n from vllm.vllm_flash_attn import flash_attn_varlen_func\n \n@@ -25,11 +37,11 @@ class MLACommonMetadata(AttentionMetadata):\n \n class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     \"\"\"\n-    Common class for implementing repeated parts \n-    \n+    Common class for implementing repeated parts\n+\n     Main reference: DeepseekV2 paper, and FlashInfer Implementation\n     (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n-    \n+\n     Deepseek's MLA attention works the following way:\n     * Use a single latent vector to represent the entire KV cache.\n     * The attention \"simulates\" a multi-head attention, while the compute is\n@@ -46,7 +58,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         * V: V head dim.\n         * kv_c: latent/compressed KV\n         * q_c: latent/compressed Q\n-        \n+\n         #\n         # Outside the MLA attention backend\n         #\n@@ -55,21 +67,21 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n            kv_c_k_pe (B, Lkv+R).\n         2. The kv_c_k_pe is split into kv_c (B, Lkv) and k_pe (B, R). cq\n            and kv_c are normalized.\n-        \n+\n         #\n         # Inside the MLA attention backend\n         #\n \n         * if prefill:\n-        \n-        3. The q_c is then projected up into the multi-head version. \n-           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope \n-             (B, N, P) and q_pe (B, N, R). \n+\n+        3. The q_c is then projected up into the multi-head version.\n+           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope\n+             (B, N, P) and q_pe (B, N, R).\n         4. q_pe, k_pe are then passed through rotary embeddings.\n         5. kv_c and k_pe are concatenated and inserted into the cache\n-        6. The kv_c is then projected up into the multi-head version. \n-           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope \n-             dimensions for K and V, which is split into k_nope (B, N, P) \n+        6. The kv_c is then projected up into the multi-head version.\n+           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope\n+             dimensions for K and V, which is split into k_nope (B, N, P)\n              and v (B, N, V).\n         7. q (B, N, (P+R)) and k (B, N, (P+R)) matrices are assembled from\n            q_nope, q_pe, k_nope, k_pe.\n@@ -112,7 +124,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     From @tsu-bin's calculation, we only want to use the absorption technique\n     for decode. The prefill algorithm should still use the up-projected MHA\n     for less flops and memory usage.\n-    \n+\n     \"\"\"\n \n     def __init__(\n@@ -162,8 +174,19 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _v_up_proj_and_o_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n-            return self.o_proj_absorbed(\n-                x.reshape(-1, self.num_heads * self.kv_lora_rank))[0]\n+            if is_fp8(self.W_UV_O):\n+                output_parallel = apply_fp8_linear_generic(\n+                    x.flatten(start_dim=1), self.W_UV_O, self.W_UV_O_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape)\n+            else:\n+                output_parallel = torch.matmul(x.flatten(start_dim=1),\n+                                               self.W_UV_O)\n+            if self.tp_size > 1:\n+                output = tensor_model_parallel_all_reduce(output_parallel)\n+            else:\n+                output = output_parallel\n+            return output\n         else:\n             x = torch.einsum(\"bnl,lnv->bnv\", x, self.W_UV)\n             return self.o_proj(x.reshape(-1,\n@@ -171,6 +194,12 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _q_proj_and_k_up_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            if is_fp8(self.W_Q_UK):\n+                return apply_fp8_linear_generic(\n+                    x, self.W_Q_UK, self.W_Q_UK_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape).view(\n+                        -1, self.num_heads, self.kv_lora_rank)\n             return torch.matmul(x, self.W_Q_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n         else:\n@@ -179,8 +208,91 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             return torch.einsum(\"bnp,lnp->bnl\", x, self.W_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n \n-    def process_weights_after_loading(self):\n-        kv_b_proj_weight = self.kv_b_proj.weight.T\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n+\n+        def is_layer_fp8(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, Fp8LinearMethod) or\\\n+                (isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8))\n+\n+        def quantization_scheme_supported(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, UnquantizedLinearMethod) or \\\n+                is_layer_fp8(layer)\n+\n+        # TODO(lucas) This is very gross, we need a more wide scale refactor of\n+        # all the FP8 code with a more standard way of\n+        # defining schemes/group-shapes, we should also potentially force\n+        # quant_methods to support a decompress function\n+        #\n+        # returns input_group_shape, weight_group_shape\n+        def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \\\n+            Tuple[Tuple[int, int], Tuple[int, int]]:\n+            if isinstance(layer.quant_method, Fp8LinearMethod):\n+                if layer.quant_method.block_quant is not None:\n+                    weight_block_size = \\\n+                        layer.quant_method.quant_config.weight_block_size\n+                    # per-token-group (1, X), block-quantized (X, Y)\n+                    return (1, weight_block_size[-1]), weight_block_size\n+                else:\n+                    return (-1, -1), (-1, -1)  # per-tensor, per-tensor\n+            elif isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                # this is hacky but we always assume the for\n+                # CompressedTensorsW8A8Fp8 the input is dynamic per-token\n+                # we ignore if it is static-per-tensor since we are going to\n+                # requantize after later anyways\n+                strategy = layer.scheme.strategy\n+                if strategy == QuantizationStrategy.TENSOR:\n+                    return (1, -1), (-1, -1)  # per-token, per-tensor\n+                elif strategy == QuantizationStrategy.CHANNEL:\n+                    return (1, -1), (-1, 1)  # per-token, per-channel\n+                else:\n+                    raise NotImplementedError(\n+                        f\"QuantizationStrategy.{strategy} is not supported for \"\n+                        \"fp8 MLA, please run with VLLM_MLA_DISABLE=1\")\n+            else:\n+                raise NotImplementedError(\n+                    \"Can't determine scale group shapes for \"\n+                    f\"{layer.quant_method}, please run with VLLM_MLA_DISABLE=1\"\n+                )\n+\n+        def get_scales(layer: LinearBase) -> torch.Tensor:\n+            if hasattr(layer, \"weight_scale_inv\"):\n+                return layer.weight_scale_inv\n+            return layer.weight_scale\n+\n+        def get_and_maybe_dequant_weights(layer: LinearBase):\n+            if is_layer_fp8(layer):\n+                if isinstance(layer.quant_method, \\\n+                    CompressedTensorsLinearMethod) and \\\n+                    isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                    # NOTE(lucas): note sure why but `CompressedTensorsW8A8Fp8`\n+                    # seems to store weights as (input, output) instead of\n+                    # (output, input) so we need to transpose\n+                    weight = layer.weight.T  # standardize to (output, input)\n+                else:\n+                    weight = layer.weight\n+                _, weight_scale_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(layer)\n+                scales = get_scales(layer)\n+\n+                return scaled_dequantize(weight, scales,\n+                                         weight_scale_group_shape)\n+            else:\n+                return layer.weight\n+\n+        if not (quantization_scheme_supported(self.kv_b_proj) and\\\n+            quantization_scheme_supported(self.q_proj) and\\\n+                quantization_scheme_supported(self.o_proj)):\n+            raise NotImplementedError(\n+                \"Only FP8 and UnquantizedLinearMethod are supported for MLA\"\n+                \", please run with VLLM_MLA_DISABLE=1\")\n+\n+        weight_dtype = self.kv_b_proj.weight.dtype\n+        assert self.o_proj.weight.dtype == weight_dtype\n+        assert self.q_proj.weight.dtype == weight_dtype\n+\n+        kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T\n         assert kv_b_proj_weight.shape == (\n             self.kv_lora_rank,\n             self.num_heads * (self.qk_nope_head_dim + self.v_head_dim)), (\n@@ -198,18 +310,35 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         W_UK, W_UV = kv_b_proj_weight.split(\n             [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n \n-        q_proj = self.q_proj.weight.T\\\n+        q_proj_weight = get_and_maybe_dequant_weights(self.q_proj).T\\\n                 .view(-1, self.num_heads, self.qk_head_dim)\n \n         # can be W_Q or W_UQ depending q_lora_rank, the former if\n         # q_lora_rank is None, the latter otherwise. From the Attention backend\n         # perspective though we call these both W_Q and rely on the layer\n         # to pass in the correct matrix\n-        W_Q = q_proj[..., :self.qk_nope_head_dim]\n-        self.W_QR = q_proj[..., self.qk_nope_head_dim:]\\\n+        W_Q = q_proj_weight[..., :self.qk_nope_head_dim]\n+        self.W_QR = q_proj_weight[..., self.qk_nope_head_dim:]\\\n             .flatten(start_dim=1).contiguous()\n \n+        # W_QR is small so for simplicity we dont bother requantizing it\n+        self.W_QR = self.W_QR.to(act_dtype)\n+\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            requantization_enabled = not envs.VLLM_MLA_DISABLE_REQUANTIZATION\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                # This assumes it wise to requantize using the same group shapes\n+                # (i.e. strategy, per-tensor, per-channel, block etc.) that the\n+                # weights were originally quantized\n+                requant_input_group_shape, requant_weight_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(self.q_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.kv_b_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.o_proj)\n+                self.reqaunt_input_group_shape = requant_input_group_shape\n+                self.reqaunt_weight_group_shape = requant_weight_group_shape\n+\n             #\n             # Perform matrix-absorption following\n             #     https://github.com/flashinfer-ai/flashinfer/pull/551\n@@ -223,25 +352,44 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             # latter otherwise\n             # basically if q_lora_rank is none we are absorbing into q_proj\n             # instead of UQ\n-            self.W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n+            W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n                 .flatten(start_dim=1).contiguous()\n \n-            W_O = self.o_proj.weight\\\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_Q_UK, W_Q_UK_scales = scaled_quantize(\n+                    W_Q_UK,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_Q_UK = W_Q_UK.T.contiguous()\n+                self.W_Q_UK_scales = W_Q_UK_scales.T.contiguous()\n+            else:\n+                self.W_Q_UK = W_Q_UK.to(act_dtype)\n+\n+            W_O = get_and_maybe_dequant_weights(self.o_proj)\\\n                 .view(-1, self.num_heads, self.v_head_dim)\n-            self.W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n+            W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n                 .flatten(start_dim=0, end_dim=1).contiguous()\n \n-            tp_size = get_tensor_model_parallel_world_size()\n-            self.o_proj_absorbed = RowParallelLinear(\n-                self.W_UV_O.shape[0] * tp_size,\n-                self.W_UV_O.shape[1],\n-                bias=False,\n-                # TODO(lucas) figure out how to properly forward quant_method\n-                #quant_config=self.o_proj.quant_method,\n-            )\n-\n-            self.o_proj_absorbed.weight = torch.nn.Parameter(self.W_UV_O.T)\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_UV_O, W_UV_O_scales = scaled_quantize(\n+                    W_UV_O,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_UV_O = W_UV_O.T.contiguous()\n+                self.W_UV_O_scales = W_UV_O_scales.T.contiguous()\n+            else:\n+                self.W_UV_O = W_UV_O.to(act_dtype)\n+\n+            self.tp_size = get_tensor_model_parallel_world_size()\n         else:\n+            if is_fp8(weight_dtype):\n+                raise NotImplementedError(\n+                    \"Currently fp8 requires matrix absorption\")\n+\n             self.W_UV = W_UV\n             self.W_UK = W_UK\n             self.W_Q = W_Q.flatten(start_dim=1)",
      "change_type": "modified",
      "lines_added": 185,
      "lines_removed": 37
    },
    {
      "file_path": "vllm/attention/backends/triton_mla.py",
      "old_content": "from collections import defaultdict\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom itertools import accumulate\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type\n\nfrom vllm.multimodal import MultiModalPlaceholderMap\n\ntry:\n    from flashinfer import BatchDecodeMlaWithPagedKVCacheWrapper\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024\nexcept ImportError:\n    BatchDecodeMlaWithPagedKVCacheWrapper = None\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 0\n\nimport torch\n\nfrom vllm import _custom_ops as ops\nfrom vllm.attention.backends.abstract import (AttentionBackend,\n                                              AttentionMetadata,\n                                              AttentionMetadataBuilder,\n                                              AttentionState, AttentionType)\nfrom vllm.attention.backends.mla.utils import MLACommonImpl, MLACommonMetadata\nfrom vllm.attention.backends.utils import (PAD_SLOT_ID, compute_slot_mapping,\n                                           compute_slot_mapping_start_idx,\n                                           is_block_tables_empty)\nfrom vllm.attention.ops.paged_attn import PagedAttention\nfrom vllm.attention.ops.triton_decode_attention import decode_attention_fwd\nfrom vllm.utils import async_tensor_h2d, make_tensor_with_pad\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import (ModelInputForGPUBuilder,\n                                          ModelInputForGPUWithSamplingMetadata)\n\n\nclass TritonMLABackend(AttentionBackend):\n\n    @staticmethod\n    def get_name() -> str:\n        return \"TRITON_MLA\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"TritonMLAImpl\"]:\n        return TritonMLAImpl\n\n    @staticmethod\n    def get_metadata_cls() -> Type[\"AttentionMetadata\"]:\n        return TritonMLAMetadata\n\n    @staticmethod\n    def get_builder_cls() -> Type[\"TritonMLAMetadataBuilder\"]:\n        return TritonMLAMetadataBuilder\n\n    @staticmethod\n    def get_state_cls() -> Type[\"TritonMLAState\"]:\n        return TritonMLAState\n\n    @staticmethod\n    def get_kv_cache_shape(\n            num_blocks: int,\n            block_size: int,\n            num_kv_heads: int,  # assumed to be 1 for MLA\n            kv_lora_rank: int,  # passed via head_size\n    ) -> Tuple[int, ...]:\n        # TODO(lucas): remove hardcoding k_pe size as 1/8th of kv_lora_rank\n        k_pe_size = kv_lora_rank // 8\n        return (num_blocks, block_size, kv_lora_rank + k_pe_size)\n\n    @staticmethod\n    def swap_blocks(\n        src_kv_cache: torch.Tensor,\n        dst_kv_cache: torch.Tensor,\n        src_to_dst: torch.Tensor,\n    ) -> None:\n        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)\n\n    @staticmethod\n    def copy_blocks(\n        kv_caches: List[torch.Tensor],\n        src_to_dists: torch.Tensor,\n    ) -> None:\n        PagedAttention.copy_blocks(kv_caches, src_to_dists)\n\n    @staticmethod\n    def get_supported_head_sizes() -> List[int]:\n        return [512]\n\n\nclass TritonMLAState(AttentionState):\n\n    def __init__(self, runner):\n        self.runner = runner\n        self._is_graph_capturing = False\n\n    @contextmanager\n    def graph_capture(self, max_batch_size: int):\n        self._is_graph_capturing = True\n\n        self._graph_slot_mapping = torch.full((max_batch_size, ),\n                                              PAD_SLOT_ID,\n                                              dtype=torch.long,\n                                              device=self.runner.device)\n        self._graph_seq_lens = torch.ones(max_batch_size,\n                                          dtype=torch.int32,\n                                          device=self.runner.device)\n        self._graph_block_tables = torch.from_numpy(\n            self.runner.graph_block_tables).to(device=self.runner.device)\n\n        self._positions = torch.zeros((max_batch_size, ),\n                                      dtype=torch.long,\n                                      device=self.runner.device)\n\n        yield\n\n        self._is_graph_capturing = False\n        del self._graph_slot_mapping\n        del self._graph_seq_lens\n        del self._graph_block_tables\n        del self._positions\n\n    def graph_clone(self, batch_size: int):\n        assert self._is_graph_capturing\n        return self.__class__(self.runner)\n\n    def graph_capture_get_metadata_for_batch(\n            self, batch_size: int, is_encoder_decoder_model: bool = False):\n        assert self._is_graph_capturing\n\n        attn_metadata = self.runner.attn_backend.make_metadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=batch_size,\n            slot_mapping=self._graph_slot_mapping[:batch_size],\n            multi_modal_placeholder_index_maps=None,\n            enable_kv_scales_calculation=True,\n            seq_lens=None,\n            seq_lens_tensor=self._graph_seq_lens[:batch_size],\n            max_query_len=1,\n            max_decode_query_len=1,\n            max_prefill_seq_len=0,\n            max_decode_seq_len=self.runner.max_seq_len_to_capture,\n            query_start_loc=None,\n            seq_start_loc=None,\n            context_lens_tensor=None,\n            block_tables=self._graph_block_tables[:batch_size],\n            use_cuda_graph=True,\n            input_positions=self._positions[:batch_size],\n            head_dim=self.runner.model_config.get_head_size())\n\n        if is_encoder_decoder_model:\n            raise NotImplementedError(\n                \"TritonMLAState does not support encoder/decoder yet\")\n\n        return attn_metadata\n\n    def get_graph_input_buffers(self,\n                                attn_metadata,\n                                is_encoder_decoder_model: bool = False):\n        input_buffers = {\n            \"slot_mapping\": attn_metadata.slot_mapping,\n            \"seq_lens_tensor\": attn_metadata.decode_metadata.seq_lens_tensor,\n            \"block_tables\": attn_metadata.decode_metadata.block_tables,\n            \"input_positions\": attn_metadata.decode_metadata.input_positions,\n        }\n        if is_encoder_decoder_model:\n            raise NotImplementedError(\n                \"TritonMLAState does not support encoder/decoder yet\")\n\n        return input_buffers\n\n    def prepare_graph_input_buffers(self,\n                                    input_buffers,\n                                    attn_metadata,\n                                    is_encoder_decoder_model: bool = False):\n        input_positions = attn_metadata.input_positions\n        num_positions = input_positions.shape[0]\n        input_buffers[\"seq_lens_tensor\"].copy_(\n            attn_metadata.decode_metadata.seq_lens_tensor, non_blocking=True)\n        input_buffers[\"block_tables\"].copy_(\n            attn_metadata.decode_metadata.block_tables, non_blocking=True)\n        # CUDA graph buffer is padded so only perform a partial copy based on\n        # num_positions\n        input_buffers[\"input_positions\"][:num_positions].copy_(\n            input_positions, non_blocking=True)\n        if is_encoder_decoder_model:\n            raise NotImplementedError(\n                \"TritonMLAState does not support encoder/decoder yet\")\n\n    def begin_forward(self, model_input):\n        return\n\n\n@dataclass\nclass TritonMLAMetadata(MLACommonMetadata):\n    \"\"\"Metadata for TritonMLAMetadata.\n\n    NOTE: Any python object stored here is not updated when it is\n    cuda-graph replayed. If you have values that need to be changed\n    dynamically, it should be stored in tensor. The tensor has to be\n    updated from `CUDAGraphRunner.forward` API.\n    \"\"\"\n    # (batch_size,). The sequence length per sequence. Sequence length means\n    # the computed tokens + new tokens None if it is a decoding.\n    seq_lens: Optional[List[int]]\n    # seq_lens stored as a tensor.\n    seq_lens_tensor: Optional[torch.Tensor]\n\n    # NOTE(sang): Definition of context_len, query_len, and seq_len.\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ---------------------|\n    #                                   |-- query_len ---|\n\n    # Maximum sequence length among prefill batch. 0 if there are decoding\n    # requests only.\n    max_prefill_seq_len: int\n    # Maximum sequence length among decode batch. 0 if there are prefill\n    # requests only.\n    max_decode_seq_len: int\n    # (batch_size,) A tensor of context lengths (tokens that are computed\n    # so far).\n    context_lens_tensor: Optional[torch.Tensor]\n\n    # (batch_size, max_blocks_per_seq).\n    # Block addresses per sequence. (Seq id -> list of physical block)\n    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks\n    # in the kv cache. Each block can contain up to block_size tokens.\n    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph\n    # captured.\n    block_tables: Optional[torch.Tensor]\n\n    # Whether or not if cuda graph is enabled.\n    # Cuda-graph is currently enabled for decoding only.\n    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.\n\n    use_cuda_graph: bool\n\n    # Maximum query length in the batch.\n    max_query_len: Optional[int] = None\n\n    # Max number of query tokens among request in the batch.\n    max_decode_query_len: Optional[int] = None\n\n    # (batch_size + 1,). The cumulative subquery lengths of the sequences in\n    # the batch, used to index into subquery. E.g., if the subquery length\n    # is [4, 6], it is [0, 4, 10].\n    query_start_loc: Optional[torch.Tensor] = None\n    # (batch_size + 1,). The cumulative sequence lengths of the sequences in\n    # the batch, used to index into sequence. E.g., if the sequence length is\n    # [4, 6], it is [0, 4, 10].\n    seq_start_loc: Optional[torch.Tensor] = None\n\n    _cached_prefill_metadata: Optional[\"TritonMLAMetadata\"] = None\n    _cached_decode_metadata: Optional[\"TritonMLAMetadata\"] = None\n\n    num_prefill_tokens: int\n\n    num_kv_splits: int = 4  # TODO(lucas) add heuristic\n    attn_logits: Optional[torch.Tensor] = None\n    req_idx: Optional[torch.Tensor] = None\n\n    # The dimension of the attention heads\n    head_dim: Optional[int] = None\n\n    def __post_init__(self):\n        supported_head_sizes = TritonMLABackend.get_supported_head_sizes()\n        if self.head_dim is not None and self.head_dim \\\n                not in supported_head_sizes:\n            raise ValueError(\n                f\"Only {supported_head_sizes} are supported for head_dim,\",\n                f\"received {self.head_dim}.\")\n\n    @property\n    def prefill_metadata(self) -> Optional[\"TritonMLAMetadata\"]:\n        if self.num_prefills == 0:\n            return None\n\n        if self._cached_prefill_metadata is not None:\n            return self._cached_prefill_metadata\n\n        assert self.seq_lens is not None\n        assert self.seq_lens_tensor is not None\n\n        # Compute some attn_metadata fields which default to None\n        query_start_loc = (None if self.query_start_loc is None else\n                           self.query_start_loc[:self.num_prefills + 1])\n        slot_mapping = (None if self.slot_mapping is None else\n                        self.slot_mapping[:self.num_prefill_tokens])\n        seq_lens = (None if self.seq_lens is None else\n                    self.seq_lens[:self.num_prefills])\n        seq_lens_tensor = (None if self.seq_lens_tensor is None else\n                           self.seq_lens_tensor[:self.num_prefills])\n        seq_start_loc = (None if self.seq_start_loc is None else\n                         self.seq_start_loc[:self.num_prefills + 1])\n        context_lens_tensor = (None if self.context_lens_tensor is None else\n                               self.context_lens_tensor[:self.num_prefills])\n        block_tables = (None if self.block_tables is None else\n                        self.block_tables[:self.num_prefills])\n        input_positions = (None if self.input_positions is None else\n                           self.input_positions[:self.num_prefill_tokens])\n\n        self._cached_prefill_metadata = TritonMLAMetadata(\n            num_prefills=self.num_prefills,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=0,\n            slot_mapping=slot_mapping,\n            multi_modal_placeholder_index_maps=self.\n            multi_modal_placeholder_index_maps,\n            enable_kv_scales_calculation=self.enable_kv_scales_calculation,\n            input_positions=input_positions,\n            seq_lens=seq_lens,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=self.max_query_len,\n            max_prefill_seq_len=self.max_prefill_seq_len,\n            max_decode_query_len=0,\n            max_decode_seq_len=0,\n            query_start_loc=query_start_loc,\n            seq_start_loc=seq_start_loc,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=False,\n            head_dim=self.head_dim)\n        return self._cached_prefill_metadata\n\n    @property\n    def decode_metadata(self) -> Optional[\"TritonMLAMetadata\"]:\n        if self.num_decode_tokens == 0:\n            return None\n\n        if self._cached_decode_metadata is not None:\n            return self._cached_decode_metadata\n        assert self.seq_lens_tensor is not None\n\n        # Compute some attn_metadata fields which default to None\n        slot_mapping = (None if self.slot_mapping is None else\n                        self.slot_mapping[self.num_prefill_tokens:])\n        seq_lens_tensor = (None if self.seq_lens_tensor is None else\n                           self.seq_lens_tensor[self.num_prefills:])\n        block_tables = (None if self.block_tables is None else\n                        self.block_tables[self.num_prefills:])\n        input_positions = (None if self.input_positions is None else\n                           self.input_positions[self.num_prefill_tokens:])\n\n        self._cached_decode_metadata = TritonMLAMetadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=self.num_decode_tokens,\n            slot_mapping=slot_mapping,\n            multi_modal_placeholder_index_maps=None,\n            enable_kv_scales_calculation=True,\n            seq_lens=None,\n            seq_lens_tensor=seq_lens_tensor,\n            max_decode_query_len=self.max_decode_query_len,\n            max_query_len=self.max_query_len,\n            max_prefill_seq_len=0,\n            max_decode_seq_len=self.max_decode_seq_len,\n            # Batch may be composed of prefill|decodes, adjust query start\n            # indices to refer to the start of decodes. E.g.\n            # in tokens:[3 prefills|6 decodes], query_start_loc=[3,9] => [0,6].\n            query_start_loc=(self.query_start_loc[self.num_prefills:] -\n                             self.query_start_loc[self.num_prefills])\n            if self.query_start_loc is not None else None,\n            seq_start_loc=self.seq_start_loc[self.num_prefills:]\n            if self.seq_start_loc is not None else None,\n            context_lens_tensor=None,\n            block_tables=block_tables,\n            use_cuda_graph=self.use_cuda_graph,\n            input_positions=input_positions,\n            head_dim=self.head_dim)\n        return self._cached_decode_metadata\n\n    def advance_step(self,\n                     model_input: \"ModelInputForGPUWithSamplingMetadata\",\n                     sampled_token_ids: Optional[torch.Tensor],\n                     block_size: int,\n                     num_seqs: int,\n                     num_queries: int,\n                     turn_prefills_into_decodes: bool = False):\n        \"\"\"\n        Update metadata in-place to advance one decode step.\n        \"\"\"\n        # When using cudagraph, the num_seqs is padded to the next captured\n        # batch sized, but num_queries tracks the actual number of requests in\n        # the batch. For --enforce-eager mode, num_seqs == num_queries\n        if num_seqs != num_queries:\n            assert num_seqs > num_queries\n            assert self.use_cuda_graph\n\n        if turn_prefills_into_decodes:\n            # When Mutli-Step is enabled with Chunked-Prefill, prefills and\n            # decodes are scheduled together. In the first step, all the\n            # prefills turn into decodes. This update reflects that\n            # conversion.\n            assert self.num_decode_tokens + self.num_prefills == num_seqs\n            self.num_decode_tokens += self.num_prefills\n            self.num_prefills = 0\n            self.num_prefill_tokens = 0\n            self.max_prefill_seq_len = 0\n            self.max_query_len = 1\n\n            self.slot_mapping = self.slot_mapping[:num_seqs]\n        else:\n            assert self.seq_lens is not None\n            assert self.max_decode_seq_len == max(self.seq_lens)\n\n        assert self.num_prefills == 0\n        assert self.num_prefill_tokens == 0\n        assert self.num_decode_tokens == num_seqs\n        assert self.slot_mapping.shape == (num_seqs, )\n\n        assert self.seq_lens is not None\n        assert len(self.seq_lens) == num_seqs\n        assert self.seq_lens_tensor is not None\n        assert self.seq_lens_tensor.shape == (num_seqs, )\n        assert self.max_query_len == 1\n        assert self.max_prefill_seq_len == 0\n\n        assert self.query_start_loc is not None\n        assert self.query_start_loc.shape == (num_queries + 1, )\n        assert self.seq_start_loc is not None\n        assert self.seq_start_loc.shape == (num_seqs + 1, )\n\n        assert self.context_lens_tensor is not None\n        assert self.context_lens_tensor.shape == (num_queries, )\n\n        assert self.block_tables is not None\n        assert self.block_tables.shape[0] == num_seqs\n\n        # Update query lengths. Note that we update only queries and not seqs,\n        # since tensors may be padded due to captured cuda graph batch size\n        for i in range(num_queries):\n            self.seq_lens[i] += 1\n        self.max_decode_seq_len = max(self.seq_lens)\n\n        ops.advance_step_flashattn(num_seqs=num_seqs,\n                                   num_queries=num_queries,\n                                   block_size=block_size,\n                                   input_tokens=model_input.input_tokens,\n                                   sampled_token_ids=sampled_token_ids,\n                                   input_positions=model_input.input_positions,\n                                   seq_lens=self.seq_lens_tensor,\n                                   slot_mapping=self.slot_mapping,\n                                   block_tables=self.block_tables)\n\n\nclass TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n\n    def prepare(self):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.input_positions: List[int] = []\n        self.multimodal_placeholder_maps: Dict[\n            str,\n            MultiModalPlaceholderMap] = defaultdict(MultiModalPlaceholderMap)\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n        self.has_prefix_cache_hit = False\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool, prefix_cache_hit: bool):\n        \"\"\"Add a sequence group to the metadata. Specifically update/append\n        1. context length.\n        2. block table.\n        3. slot mapping.\n        \"\"\"\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block, input_positions) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks,\n                 inter_data.input_positions):\n            self.input_positions.extend(input_positions)\n            self.context_lens.append(context_len)\n            if is_prompt:\n                mm_maps = inter_data.multi_modal_placeholder_maps\n                if mm_maps:\n                    for modality, placeholders in mm_maps.items():\n                        self.multimodal_placeholder_maps[modality].extend(\n                            placeholders)\n\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if prefix_cache_hit:\n                # NOTE(woosuk): For flash-attn, the block table should\n                # include the entries for the incoming prefill tokens.\n                block_table = block_tables[seq_id]\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                if curr_sliding_window_block == 0:\n                    block_table = block_tables[seq_id]\n                else:\n                    block_table = block_tables[seq_id][\n                        -curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            # Compute slot mapping.\n            is_profile_run = is_block_tables_empty(block_tables)\n            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n                                                       context_len,\n                                                       self.sliding_window)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n    def _get_graph_runner_block_tables(\n            self, num_seqs: int,\n            block_tables: List[List[int]]) -> torch.Tensor:\n        # The shape of graph_block_tables is\n        # [max batch size, max context len // block size].\n        max_batch_size, max_blocks = self.runner.graph_block_tables.shape\n        assert max_batch_size >= num_seqs\n\n        graph_block_tables = self.runner.graph_block_tables[:num_seqs]\n        for i, block_table in enumerate(block_tables):\n            if block_table:\n                num_blocks = len(block_table)\n                if num_blocks <= max_blocks:\n                    graph_block_tables[i, :num_blocks] = block_table\n                else:\n                    # It may be possible to have more blocks allocated due\n                    # to lookahead slots of multi-step, however, they are\n                    # not used anyway, so can be safely ignored.\n                    graph_block_tables[\n                        i, :max_blocks] = block_table[:max_blocks]\n\n        return torch.from_numpy(graph_block_tables).to(\n            device=self.runner.device, non_blocking=True)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        prefix_cache_hit = any([\n            inter_data.prefix_cache_hit\n            for inter_data in self.input_builder.inter_data_list\n        ])\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled,\n                                prefix_cache_hit)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_query_len = max(query_lens)\n        decode_query_lens = query_lens[self.num_prefills:]\n        if len(decode_query_lens) > 0:\n            max_decode_query_len = max(decode_query_lens)\n        else:\n            max_decode_query_len = 1\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        max_decode_seq_len = max(self.curr_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n        query_start_loc = list(accumulate(query_lens, initial=0))\n        seq_start_loc = list(accumulate(seq_lens, initial=0))\n\n        num_seqs = len(seq_lens)\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size - self.num_prefill_tokens\n            block_tables = self._get_graph_runner_block_tables(\n                num_seqs, self.block_tables)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n        assert max_query_len > 0, (\"query_lens: {}\".format(query_lens))\n\n        assert device is not None\n        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,\n                                               device, self.runner.pin_memory)\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        input_positions = async_tensor_h2d(self.input_positions, torch.long,\n                                           device, self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc_tensor = async_tensor_h2d(query_start_loc, torch.int32,\n                                                  device,\n                                                  self.runner.pin_memory)\n        seq_start_loc_tensor = async_tensor_h2d(seq_start_loc, torch.int32,\n                                                device, self.runner.pin_memory)\n        placeholder_index_maps = {\n            modality: placeholder_map.index_map()\n            for modality, placeholder_map in\n            self.multimodal_placeholder_maps.items()\n        }\n\n        num_kv_splits = 8\n\n        return TritonMLAMetadata(\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            seq_lens=seq_lens,\n            multi_modal_placeholder_index_maps=placeholder_index_maps,\n            enable_kv_scales_calculation=True,\n            input_positions=input_positions,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=max_query_len,\n            max_decode_query_len=max_decode_query_len,\n            max_prefill_seq_len=max_prefill_seq_len,\n            max_decode_seq_len=max_decode_seq_len,\n            query_start_loc=query_start_loc_tensor,\n            seq_start_loc=seq_start_loc_tensor,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n            num_kv_splits=num_kv_splits,\n            head_dim=self.runner.model_config.get_head_size(),\n        )\n\n\nclass TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):\n\n    def __init__(\n            self,\n            num_heads: int,\n            head_size: int,\n            scale: float,\n            num_kv_heads: int,\n            alibi_slopes: Optional[List[float]],\n            sliding_window: Optional[int],\n            kv_cache_dtype: str,\n            blocksparse_params: Optional[Dict[str, Any]],\n            logits_soft_cap: Optional[float],\n            attn_type: str,\n            # MLA Specific Arguments\n            **kwargs) -> None:\n        super().__init__(num_heads, head_size, scale, num_kv_heads,\n                         alibi_slopes, sliding_window, kv_cache_dtype,\n                         blocksparse_params, logits_soft_cap, attn_type,\n                         **kwargs)\n\n        unsupported_features = [\n            alibi_slopes, sliding_window, blocksparse_params, logits_soft_cap\n        ]\n        if any(unsupported_features):\n            raise NotImplementedError(\n                \"TritonMLAImpl does not support one of the following: \"\n                \"alibi_slopes, sliding_window, blocksparse_params, \"\n                \"logits_soft_cap\")\n\n        if attn_type != AttentionType.DECODER:\n            raise NotImplementedError(\"Encoder self-attention and \"\n                                      \"encoder/decoder cross-attention \"\n                                      \"are not implemented for \"\n                                      \"TritonMLAImpl\")\n\n    def _forward_prefill(\n        self,\n        q: torch.Tensor,\n        kv_c_normed: torch.Tensor,\n        k_pe: torch.Tensor,\n        attn_metadata: TritonMLAMetadata,\n    ) -> torch.Tensor:\n        assert isinstance(attn_metadata, TritonMLAMetadata)\n        return self._forward_prefill_flash(q, kv_c_normed, k_pe,\n                                           attn_metadata.seq_start_loc,\n                                           attn_metadata.max_prefill_seq_len)\n\n    def _forward_decode(\n        self,\n        q_nope: torch.Tensor,\n        q_pe: torch.Tensor,\n        kv_c_and_k_pe_cache: torch.Tensor,\n        attn_metadata: TritonMLAMetadata,\n    ) -> torch.Tensor:\n        assert kv_c_and_k_pe_cache.numel() > 0\n        if self.kv_cache_dtype.startswith(\"fp8\"):\n            raise NotImplementedError(\"FP8 Triton MLA not yet supported\")\n\n        decode_meta = attn_metadata.decode_metadata\n        assert decode_meta is not None\n        B = q_nope.shape[0]\n\n        q = torch.cat([q_nope, q_pe], dim=-1)\n        o = torch.zeros(B,\n                        self.num_heads,\n                        self.kv_lora_rank,\n                        dtype=q.dtype,\n                        device=q.device)\n\n        # TODO(lucas) Allocate ahead of time\n        attn_logits = torch.empty(\n            (\n                B,\n                self.num_heads,\n                attn_metadata.num_kv_splits,\n                # NOTE(lucas) idk why the +1 is here but sglang has it so we\n                # just mirror that\n                self.kv_lora_rank + 1,\n            ),\n            dtype=torch.float32,\n            device=q.device,\n        )\n\n        # Add a head dim of 1\n        kv_c_and_k_pe_cache = kv_c_and_k_pe_cache.unsqueeze(2)\n        kv_c_cache = kv_c_and_k_pe_cache[..., :self.kv_lora_rank]\n        PAGE_SIZE = kv_c_and_k_pe_cache.size(1)\n\n        # Run MQA\n        decode_attention_fwd(q, kv_c_and_k_pe_cache, kv_c_cache, o,\n                             decode_meta.block_tables,\n                             decode_meta.seq_lens_tensor, attn_logits,\n                             attn_metadata.num_kv_splits, self.scale,\n                             PAGE_SIZE)\n\n        return self._v_up_proj_and_o_proj(o)\n",
      "diff": "diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex da09bb70b..95dc119a4 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -57,14 +57,12 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_kv_cache_shape(\n-            num_blocks: int,\n-            block_size: int,\n-            num_kv_heads: int,  # assumed to be 1 for MLA\n-            kv_lora_rank: int,  # passed via head_size\n+        num_blocks: int,\n+        block_size: int,\n+        num_kv_heads: int,  # assumed to be 1 for MLA\n+        head_size: int,\n     ) -> Tuple[int, ...]:\n-        # TODO(lucas): remove hardcoding k_pe size as 1/8th of kv_lora_rank\n-        k_pe_size = kv_lora_rank // 8\n-        return (num_blocks, block_size, kv_lora_rank + k_pe_size)\n+        return (num_blocks, block_size, head_size)\n \n     @staticmethod\n     def swap_blocks(\n@@ -83,7 +81,7 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_supported_head_sizes() -> List[int]:\n-        return [512]\n+        return [576]\n \n \n class TritonMLAState(AttentionState):\n@@ -624,8 +622,6 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             self.multimodal_placeholder_maps.items()\n         }\n \n-        num_kv_splits = 8\n-\n         return TritonMLAMetadata(\n             num_prefills=self.num_prefills,\n             slot_mapping=slot_mapping_tensor,\n@@ -645,7 +641,7 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             context_lens_tensor=context_lens_tensor,\n             block_tables=block_tables,\n             use_cuda_graph=use_captured_graph,\n-            num_kv_splits=num_kv_splits,\n+            num_kv_splits=4,  # TODO(lucas) add heuristic\n             head_dim=self.runner.model_config.get_head_size(),\n         )",
      "change_type": "modified",
      "lines_added": 8,
      "lines_removed": 12
    },
    {
      "file_path": "vllm/attention/layer.py",
      "old_content": "\"\"\"Attention layer.\"\"\"\nfrom typing import Any, Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, AttentionType\nfrom vllm.attention.selector import backend_name_to_enum, get_attn_backend\nfrom vllm.config import CacheConfig, get_current_vllm_config\nfrom vllm.forward_context import ForwardContext, get_forward_context\nfrom vllm.model_executor.layers.quantization.base_config import (\n    QuantizationConfig)\nfrom vllm.model_executor.layers.quantization.kv_cache import BaseKVCacheMethod\nfrom vllm.platforms import _Backend, current_platform\nfrom vllm.utils import direct_register_custom_op\n\n\nclass Attention(nn.Module):\n    \"\"\"Attention layer.\n\n    This class takes query, key, and value tensors as input. The input tensors\n    can either contain prompt tokens or generation tokens.\n    The class does the following:\n\n    1. Store the input key and value tensors in the KV cache.\n    2. Perform (multi-head/multi-query/grouped-query) attention.\n    3. Return the output tensor.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: Optional[int] = None,\n        alibi_slopes: Optional[List[float]] = None,\n        cache_config: Optional[CacheConfig] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n        logits_soft_cap: Optional[float] = None,\n        per_layer_sliding_window: Optional[int] = None,\n        use_mla: bool = False,\n        prefix: str = \"\",\n        attn_type: str = AttentionType.DECODER,\n        **extra_impl_args,\n    ) -> None:\n        super().__init__()\n        if per_layer_sliding_window is not None:\n            # per-layer sliding window\n            sliding_window = per_layer_sliding_window\n        elif cache_config is not None:\n            # model-level sliding window\n            sliding_window = cache_config.sliding_window\n        else:\n            sliding_window = None\n\n        if cache_config is not None:\n            kv_cache_dtype = cache_config.cache_dtype\n            block_size = cache_config.block_size\n            is_attention_free = cache_config.is_attention_free\n            calculate_kv_scales = cache_config.calculate_kv_scales\n        else:\n            kv_cache_dtype = \"auto\"\n            block_size = 16\n            is_attention_free = False\n            calculate_kv_scales = False\n        if num_kv_heads is None:\n            num_kv_heads = num_heads\n\n        # The default k/v_scale is set to 1.0. This is ignored\n        # when kv-cache is not fp8, and should be used with\n        # kv-cache in fp8_e5m2. For kv-cache in fp8_e4m3, we\n        # expect the pre-quantized k/v_scale to be loaded along\n        # with the model weights.\n        self.kv_cache_dtype = kv_cache_dtype\n        self.calculate_kv_scales = calculate_kv_scales\n        self._k_scale = torch.tensor(1.0, dtype=torch.float32)\n        self._v_scale = torch.tensor(1.0, dtype=torch.float32)\n\n        # We also keep the float32 versions of k/v_scale for attention\n        # backends that don't support tensors (Flashinfer)\n        self._k_scale_float = 1.0\n        self._v_scale_float = 1.0\n\n        quant_method = quant_config.get_quant_method(\n            self, prefix=prefix) if quant_config else None\n        if quant_method is not None:\n            assert isinstance(quant_method, BaseKVCacheMethod)\n            # TODO (mgoin): kv cache dtype should be specified in the FP8\n            # checkpoint config and become the \"auto\" behavior\n            if self.kv_cache_dtype == \"fp8_e5m2\":\n                raise ValueError(\"fp8_e5m2 kv-cache is not supported with \"\n                                 \"fp8 checkpoints.\")\n            # If quantization is enabled, we make \"k_scale\" and \"v_scale\"\n            # parameters so that it can be loaded from the model checkpoint.\n            # The k/v_scale will then be converted back to native float32\n            # values after weight loading.\n            self.quant_method = quant_method\n            self.quant_method.create_weights(self)\n\n        # During model initialization, the default dtype is set as the model\n        # weight and activation dtype.\n        dtype = torch.get_default_dtype()\n        attn_backend = get_attn_backend(head_size,\n                                        dtype,\n                                        kv_cache_dtype,\n                                        block_size,\n                                        is_attention_free,\n                                        blocksparse_params is not None,\n                                        use_mla=use_mla)\n        impl_cls = attn_backend.get_impl_cls()\n        self.impl = impl_cls(num_heads, head_size, scale, num_kv_heads,\n                             alibi_slopes, sliding_window, kv_cache_dtype,\n                             blocksparse_params, logits_soft_cap, attn_type,\n                             **extra_impl_args)\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.num_kv_heads = num_kv_heads\n        self.sliding_window = sliding_window\n        self.backend = backend_name_to_enum(attn_backend.get_name())\n        self.dtype = dtype\n\n        # For cuda-alike (CUDA and ROCM) and cpu platforms, we control how\n        # torch.compile works by registering the attention as one giant\n        # opaque custom op. For other platforms, we directly call them\n        # and let torch.compile handle them.\n        self.use_direct_call = not current_platform.is_cuda_alike(\n        ) and not current_platform.is_cpu()\n\n        self.use_output = attn_backend.accept_output_buffer\n        compilation_config = get_current_vllm_config().compilation_config\n        if prefix in compilation_config.static_forward_context:\n            raise ValueError(f\"Duplicate layer name: {prefix}\")\n        compilation_config.static_forward_context[prefix] = self\n        self.layer_name = prefix\n        self.attn_type = attn_type\n        # use a placeholder kv cache tensor during init, which will be replaced\n        # by bind_kv_cache\n        # this variable will not be accessed if use_direct_call is True\n        self.kv_cache = [\n            torch.tensor([]) for _ in range(get_current_vllm_config(\n            ).parallel_config.pipeline_parallel_size)\n        ]\n\n        self.k_range = torch.tensor(envs.K_SCALE_CONSTANT, dtype=torch.float32)\n        self.v_range = torch.tensor(envs.V_SCALE_CONSTANT, dtype=torch.float32)\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        if self.calculate_kv_scales and \\\n            attn_metadata.enable_kv_scales_calculation:\n            self.calc_kv_scales(key, value)\n        if self.use_output:\n            output = torch.empty_like(query)\n            hidden_size = query.size(-1)\n            # Reshape the query, key, and value tensors.\n            # NOTE(woosuk): We do this outside the custom op to minimize the\n            # CPU overheads from the non-CUDA-graph regions.\n            query = query.view(-1, self.num_heads, self.head_size)\n            output = output.view(-1, self.num_heads, self.head_size)\n            if key is not None:\n                key = key.view(-1, self.num_kv_heads, self.head_size)\n            if value is not None:\n                value = value.view(-1, self.num_kv_heads, self.head_size)\n            if self.use_direct_call:\n                unified_attention_with_output(query, key, value, output,\n                                              self.layer_name)\n            else:\n                torch.ops.vllm.unified_attention_with_output(\n                    query, key, value, output, self.layer_name)\n            return output.view(-1, hidden_size)\n        else:\n            if self.use_direct_call:\n                return unified_attention(query, key, value, self.layer_name)\n            else:\n                return torch.ops.vllm.unified_attention(\n                    query, key, value, self.layer_name)\n\n    def calc_kv_scales(self, key, value):\n        self._k_scale.copy_(torch.abs(key).max() / self.k_range)\n        self._v_scale.copy_(torch.abs(value).max() / self.v_range)\n        self._k_scale_float = self._k_scale.item()\n        self._v_scale_float = self._v_scale.item()\n        # We only calculate the scales once\n        self.calculate_kv_scales = False\n\n    def extra_repr(self) -> str:\n        s = f\"head_size={self.impl.head_size}\"  # type: ignore\n        s += f\", num_heads={self.impl.num_heads}\"  # type: ignore\n        s += f\", num_kv_heads={self.impl.num_kv_heads}\"  # type: ignore\n        s += f\", scale={self.impl.scale}\"  # type: ignore\n        s += f\", backend={self.impl.__class__.__name__}\"\n        return s\n\n    def process_weights_after_loading(self):\n        if hasattr(self.impl, \"process_weights_after_loading\"):\n            self.impl.process_weights_after_loading()\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-headed attention without any cache, used for ViT.\"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: Optional[int] = None,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = scale\n        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        dtype = torch.get_default_dtype()\n        attn_backend = get_attn_backend(head_size,\n                                        dtype,\n                                        kv_cache_dtype=None,\n                                        block_size=16,\n                                        is_attention_free=False)\n        backend = backend_name_to_enum(attn_backend.get_name())\n        if backend in {_Backend.FLASH_ATTN, _Backend.FLASH_ATTN_VLLM_V1}:\n            backend = _Backend.XFORMERS\n\n        self.attn_backend = backend if backend in {\n            _Backend.TORCH_SDPA,\n            _Backend.XFORMERS,\n        } else _Backend.TORCH_SDPA\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Input shape: batch_size x seq_len x hidden_size\"\"\"\n        # TODO(Isotr0py): Use existing backend implementations and support FA3\n        bsz, q_len, _ = query.size()\n        kv_len = key.size(1)\n\n        query = query.view(bsz, q_len, self.num_heads, self.head_size)\n        key = key.view(bsz, kv_len, self.num_kv_heads, self.head_size)\n        value = value.view(bsz, kv_len, self.num_kv_heads, self.head_size)\n\n        if (num_repeat := self.num_queries_per_kv) > 1:\n            # Handle MQA and GQA\n            key = torch.repeat_interleave(key, num_repeat, dim=2)\n            value = torch.repeat_interleave(value, num_repeat, dim=2)\n\n        if self.attn_backend == _Backend.XFORMERS:\n            from xformers import ops as xops\n\n            out = xops.memory_efficient_attention_forward(query,\n                                                          key,\n                                                          value,\n                                                          scale=self.scale)\n        elif self.attn_backend == _Backend.TORCH_SDPA:\n            query, key, value = (x.transpose(1, 2)\n                                 for x in (query, key, value))\n            out = F.scaled_dot_product_attention(query,\n                                                 key,\n                                                 value,\n                                                 scale=self.scale)\n            out = out.transpose(1, 2)\n        return out.reshape(bsz, q_len, -1)\n\n\ndef unified_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    layer_name: str,\n) -> torch.Tensor:\n    forward_context: ForwardContext = get_forward_context()\n    attn_metadata = forward_context.attn_metadata\n    self = forward_context.attn_layers[layer_name]\n    kv_cache = self.kv_cache[forward_context.virtual_engine]\n    return self.impl.forward(self, query, key, value, kv_cache, attn_metadata)\n\n\ndef unified_attention_fake(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    layer_name: str,\n) -> torch.Tensor:\n    return torch.empty_like(query).contiguous()\n\n\ndirect_register_custom_op(\n    op_name=\"unified_attention\",\n    op_func=unified_attention,\n    mutates_args=[],\n    fake_impl=unified_attention_fake,\n    dispatch_key=current_platform.dispatch_key,\n)\n\n\ndef unified_attention_with_output(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    output: torch.Tensor,\n    layer_name: str,\n) -> None:\n    forward_context: ForwardContext = get_forward_context()\n    attn_metadata = forward_context.attn_metadata\n    self = forward_context.attn_layers[layer_name]\n    kv_cache = self.kv_cache[forward_context.virtual_engine]\n    self.impl.forward(self,\n                      query,\n                      key,\n                      value,\n                      kv_cache,\n                      attn_metadata,\n                      output=output)\n\n\ndef unified_attention_with_output_fake(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    output: torch.Tensor,\n    layer_name: str,\n) -> None:\n    return\n\n\ndirect_register_custom_op(\n    op_name=\"unified_attention_with_output\",\n    op_func=unified_attention_with_output,\n    mutates_args=[\"output\"],\n    fake_impl=unified_attention_with_output_fake,\n    dispatch_key=current_platform.dispatch_key,\n)\n",
      "diff": "diff --git a/vllm/attention/layer.py b/vllm/attention/layer.py\nindex 9b804a29a..b97165f62 100644\n--- a/vllm/attention/layer.py\n+++ b/vllm/attention/layer.py\n@@ -200,9 +200,9 @@ class Attention(nn.Module):\n         s += f\", backend={self.impl.__class__.__name__}\"\n         return s\n \n-    def process_weights_after_loading(self):\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n         if hasattr(self.impl, \"process_weights_after_loading\"):\n-            self.impl.process_weights_after_loading()\n+            self.impl.process_weights_after_loading(act_dtype)\n \n \n class MultiHeadAttention(nn.Module):",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/config.py",
      "old_content": "import ast\nimport copy\nimport enum\nimport hashlib\nimport json\nimport sys\nimport warnings\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field, replace\nfrom pathlib import Path\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Counter, Dict,\n                    Final, List, Literal, Mapping, Optional, Protocol, Set,\n                    Tuple, Type, Union)\n\nimport torch\nfrom pydantic import BaseModel, Field, PrivateAttr\nfrom transformers import PretrainedConfig\n\nimport vllm.envs as envs\nfrom vllm.compilation.inductor_pass import CallableInductorPass, InductorPass\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n                                                     get_quantization_config)\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import CpuArchEnum\nfrom vllm.tracing import is_otel_available, otel_import_error_traceback\nfrom vllm.transformers_utils.config import (\n    ConfigFormat, get_config, get_hf_image_processor_config,\n    get_hf_text_config, get_pooling_config,\n    get_sentence_transformer_tokenizer_config, is_encoder_decoder,\n    try_get_generation_config, uses_mrope)\nfrom vllm.transformers_utils.s3_utils import S3Model\nfrom vllm.transformers_utils.utils import is_s3\nfrom vllm.utils import (GiB_bytes, LayerBlockType, cuda_device_count_stateless,\n                        get_cpu_memory, random_uuid, resolve_obj_by_qualname)\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.layers.quantization.base_config import (\n        QuantizationConfig)\n    from vllm.model_executor.model_loader.loader import BaseModelLoader\n    from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n        BaseTokenizerGroup)\nelse:\n    QuantizationConfig = None\n\nlogger = init_logger(__name__)\n\n_POOLING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n_MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 5120\n\nTaskOption = Literal[\"auto\", \"generate\", \"embedding\", \"embed\", \"classify\",\n                     \"score\", \"reward\"]\n\n_ResolvedTask = Literal[\"generate\", \"embed\", \"classify\", \"score\", \"reward\",\n                        \"draft\"]\n\nRunnerType = Literal[\"generate\", \"pooling\", \"draft\"]\n\n_RUNNER_TASKS: Dict[RunnerType, List[_ResolvedTask]] = {\n    \"generate\": [\"generate\"],\n    \"pooling\": [\"embed\", \"classify\", \"score\", \"reward\"],\n    \"draft\": [\"draft\"],\n}\n\n_TASK_RUNNER: Dict[_ResolvedTask, RunnerType] = {\n    task: runner\n    for runner, tasks in _RUNNER_TASKS.items()\n    for task in tasks\n}\n\nHfOverrides = Union[Dict[str, Any], Callable[[PretrainedConfig],\n                                             PretrainedConfig]]\n\n\nclass SupportsHash(Protocol):\n\n    def compute_hash(self) -> str:\n        ...\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n            It is also used as the content for `model_name` tag in metrics\n            output when `served_model_name` is not specified.\n        task: The task to use the model for. Each vLLM instance only supports\n            one task, even if the same model can be used for multiple tasks.\n            When the model only supports one task, \"auto\" can be used to select\n            it; otherwise, you must specify explicitly which task to use.\n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, \"slow\" will always use the slow tokenizer, and\n            \"mistral\" will always use the tokenizer from `mistral_common`.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        allowed_local_media_path: Allowing API requests to read local images or\n            videos from directories specified by the server file system.\n            This is a security risk. Should only be enabled in trusted\n            environments.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        code_revision: The specific revision to use for the model code on\n            Hugging Face Hub. It can be a branch name, a tag name, or a\n            commit id. If unspecified, will use the default version.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        spec_target_max_model_len: Specify the the maximum length for spec\n            decoding draft models.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n            If None, the user did not specify, so default to False.\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode. Additionally for encoder-decoder models, if the\n            sequence length of the encoder input is larger than this, we fall\n            back to the eager mode.\n        max_logprobs: Maximum number of log probabilities. Defaults to 20.\n        disable_sliding_window: Whether to disable sliding window. If True,\n            we will disable the sliding window functionality of the model.\n            If the model does not support sliding window, this argument is\n            ignored.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer.\n        served_model_name: The model name used in metrics tag `model_name`,\n            matches the model name exposed via the APIs. If multiple model\n            names provided, the first name will be used. If not specified,\n            the model name will be the same as `model`.\n        limit_mm_per_prompt: Maximum number of data items per modality\n            per prompt. Only applicable for multimodal models.\n        use_async_output_proc: Whether to use async output processor.\n            Defaults to True.\n        config_format: The config format which shall be loaded.\n            Defaults to 'auto' which defaults to 'hf'.\n        hf_overrides: If a dictionary, contains arguments to be forwarded to the\n            HuggingFace config. If a callable, it is called to update the\n            HuggingFace config.\n        mm_processor_kwargs: Arguments to be forwarded to the model's processor\n            for multi-modal data, e.g., image processor.\n        disable_mm_preprocessor_cache: If true, then disables caching of the\n            multi-modal preprocessor/mapper. (not recommended)\n        override_neuron_config: Initialize non default neuron config or\n            override default neuron config that are specific to Neuron devices,\n            this argument will be used to configure the neuron config that\n            can not be gathered from the vllm arguments.\n        override_pooler_config: Initialize non default pooling config or\n            override default pooling config for the pooling model.\n        logits_processor_pattern: Optional regex pattern specifying valid\n            logits processor qualified names that can be passed with the\n            `logits_processors` extra completion argument. Defaults to None,\n            which allows no processors.\n        generation_config: Configuration parameter file for generation.\n        override_generation_config: Override the generation config with the\n            given config.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: List[Any] = []\n        factors.append(self.model)\n        factors.append(self.dtype)\n        factors.append(self.quantization)\n        factors.append(self.revision)\n        factors.append(self.code_revision)\n        factors.append(self.trust_remote_code)\n        factors.append(self.rope_scaling)\n        factors.append(self.rope_theta)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __init__(\n        self,\n        model: str,\n        task: Union[TaskOption, Literal[\"draft\"]],\n        tokenizer: str,\n        tokenizer_mode: str,\n        trust_remote_code: bool,\n        dtype: Union[str, torch.dtype],\n        seed: int,\n        allowed_local_media_path: str = \"\",\n        revision: Optional[str] = None,\n        code_revision: Optional[str] = None,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        rope_theta: Optional[float] = None,\n        tokenizer_revision: Optional[str] = None,\n        max_model_len: Optional[int] = None,\n        spec_target_max_model_len: Optional[int] = None,\n        quantization: Optional[str] = None,\n        enforce_eager: Optional[bool] = None,\n        max_seq_len_to_capture: Optional[int] = None,\n        max_logprobs: int = 20,\n        disable_sliding_window: bool = False,\n        skip_tokenizer_init: bool = False,\n        served_model_name: Optional[Union[str, List[str]]] = None,\n        limit_mm_per_prompt: Optional[Mapping[str, int]] = None,\n        use_async_output_proc: bool = True,\n        config_format: ConfigFormat = ConfigFormat.AUTO,\n        hf_overrides: Optional[HfOverrides] = None,\n        mm_processor_kwargs: Optional[Dict[str, Any]] = None,\n        disable_mm_preprocessor_cache: bool = False,\n        override_neuron_config: Optional[Dict[str, Any]] = None,\n        override_pooler_config: Optional[\"PoolerConfig\"] = None,\n        logits_processor_pattern: Optional[str] = None,\n        generation_config: Optional[str] = None,\n        enable_sleep_mode: bool = False,\n        override_generation_config: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.allowed_local_media_path = allowed_local_media_path\n        self.seed = seed\n        self.revision = revision\n        self.code_revision = code_revision\n        self.rope_scaling = rope_scaling\n        self.rope_theta = rope_theta\n\n        if hf_overrides is None:\n            hf_overrides = {}\n\n        if callable(hf_overrides):\n            hf_overrides_kw = {}\n            hf_overrides_fn = hf_overrides\n        else:\n            hf_overrides_kw = hf_overrides\n            hf_overrides_fn = None\n\n        if rope_scaling is not None:\n            hf_override: Dict[str, Any] = {\"rope_scaling\": rope_scaling}\n            hf_overrides_kw.update(hf_override)\n            msg = (\"`--rope-scaling` will be removed in a future release. \"\n                   f\"'Please instead use `--hf-overrides '{hf_override!r}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n        if rope_theta is not None:\n            hf_override = {\"rope_theta\": rope_theta}\n            hf_overrides_kw.update(hf_override)\n            msg = (\"`--rope-theta` will be removed in a future release. \"\n                   f\"'Please instead use `--hf-overrides '{hf_override!r}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n\n        self.maybe_pull_model_tokenizer_for_s3(model, tokenizer)\n\n        # The tokenizer version is consistent with the model version by default.\n        if tokenizer_revision is None:\n            self.tokenizer_revision = revision\n        else:\n            self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n        self.enforce_eager = enforce_eager\n        self.max_seq_len_to_capture = max_seq_len_to_capture\n        self.max_logprobs = max_logprobs\n        self.disable_sliding_window = disable_sliding_window\n        self.skip_tokenizer_init = skip_tokenizer_init\n        self.enable_sleep_mode = enable_sleep_mode\n\n        from vllm.platforms import current_platform\n\n        if self.enable_sleep_mode and not current_platform.is_cuda():\n            raise ValueError(\"Sleep mode is only supported on CUDA devices.\")\n\n        hf_config = get_config(self.model, trust_remote_code, revision,\n                               code_revision, config_format)\n\n        if hf_overrides_kw:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_kw)\n            hf_config.update(hf_overrides_kw)\n        if hf_overrides_fn:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_fn)\n            hf_config = hf_overrides_fn(hf_config)\n\n        self.hf_config = hf_config\n\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.encoder_config = self._get_encoder_config()\n        self.hf_image_processor_config = get_hf_image_processor_config(\n            self.model, revision)\n        self.dtype = _get_and_verify_dtype(self.hf_text_config, dtype)\n        self.use_async_output_proc = use_async_output_proc\n        self.mm_processor_kwargs = mm_processor_kwargs\n        self.disable_mm_preprocessor_cache = disable_mm_preprocessor_cache\n\n        # Set enforce_eager to False if the value is unset.\n        if self.enforce_eager is None:\n            self.enforce_eager = False\n\n        sliding_window = getattr(self.hf_text_config, \"sliding_window\", None)\n        has_interleaved_attention = (sliding_window is not None) and (\n            isinstance(sliding_window, list) or\n            (self.hf_text_config.model_type in [\"gemma2\", \"cohere2\"]))\n\n        if (not self.disable_sliding_window and has_interleaved_attention):\n            if (backend :=\n                    envs.VLLM_ATTENTION_BACKEND) in (\"XFORMERS\", \"FLASHINFER\"):\n                sliding_window_len_min = get_min_sliding_window(\n                    self.hf_text_config.sliding_window)\n\n                logger.warning_once(\n                    f\"{self.hf_text_config.model_type} has interleaved \"\n                    \"attention, which is currently not supported by the \"\n                    f\"{backend} backend. Disabling sliding window and capping \"\n                    \"the max length to the sliding window size \"\n                    f\"({sliding_window_len_min}).\")\n                self.disable_sliding_window = True\n            else:\n                # for a model with interleaved attention,\n                # the scheduler and the model treat it as full attention\n                # (i.e., not dropping any tokens outside the window).\n                # only the attention layer itself is aware of the sliding\n                # window, and use the window size to compute the attention.\n                self.hf_text_config.interleaved_sliding_window = sliding_window\n                delattr(self.hf_text_config, \"sliding_window\")\n                sliding_window = None\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window(),\n            spec_target_max_model_len=spec_target_max_model_len,\n            encoder_config=self.encoder_config)\n        self.served_model_name = get_served_model_name(model,\n                                                       served_model_name)\n        self.multimodal_config = self._init_multimodal_config(\n            limit_mm_per_prompt)\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n\n        self.is_attention_free = self._init_attention_free()\n        self.is_hybrid = self._init_is_hybrid()\n        self.has_inner_state = self._init_has_inner_state()\n\n        if current_platform.is_neuron():\n            self.override_neuron_config = override_neuron_config\n        else:\n            self.override_neuron_config = None\n\n        supported_tasks, task = self._resolve_task(task, self.hf_config)\n        self.supported_tasks = supported_tasks\n        self.task: Final = task\n        if self.task in (\"draft\", \"generate\"):\n            self.truncation_side = \"left\"\n        else:\n            self.truncation_side = \"right\"\n\n        self.pooler_config = self._init_pooler_config(override_pooler_config)\n        self.logits_processor_pattern = logits_processor_pattern\n\n        self.generation_config = generation_config\n        self.override_generation_config = override_generation_config or {}\n\n        self._verify_quantization()\n        self._verify_cuda_graph()\n        self._verify_bnb_config()\n\n    def maybe_pull_model_tokenizer_for_s3(self, model: str,\n                                          tokenizer: str) -> None:\n        \"\"\"\n        Pull the model config or tokenizer to a temporary\n        directory in case of S3.\n\n        Args:\n            model: The model name or path.\n            tokenizer: The tokenizer name or path.\n\n        \"\"\"\n        if is_s3(model) or is_s3(tokenizer):\n            if is_s3(model):\n                s3_model = S3Model()\n                s3_model.pull_files(model, allow_pattern=[\"*config.json\"])\n                self.model_weights = self.model\n                self.model = s3_model.dir\n\n            if is_s3(tokenizer):\n                s3_tokenizer = S3Model()\n                s3_tokenizer.pull_files(\n                    model, ignore_pattern=[\"*.pt\", \"*.safetensors\", \"*.bin\"])\n                self.tokenizer = s3_tokenizer.dir\n\n    def _init_multimodal_config(\n        self, limit_mm_per_prompt: Optional[Mapping[str, int]]\n    ) -> Optional[\"MultiModalConfig\"]:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if ModelRegistry.is_multimodal_model(architectures):\n            return MultiModalConfig(limit_per_prompt=limit_mm_per_prompt or {})\n\n        if limit_mm_per_prompt:\n            raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\n                             \"multimodal models.\")\n\n        return None\n\n    def _get_encoder_config(self):\n        return get_sentence_transformer_tokenizer_config(\n            self.model, self.revision)\n\n    def _init_pooler_config(\n        self,\n        override_pooler_config: Optional[\"PoolerConfig\"],\n    ) -> Optional[\"PoolerConfig\"]:\n\n        if self.runner_type == \"pooling\":\n            user_config = override_pooler_config or PoolerConfig()\n\n            base_config = get_pooling_config(self.model, self.revision)\n            if base_config is not None:\n                # Only set values that are not overridden by the user\n                for k, v in base_config.items():\n                    if getattr(user_config, k) is None:\n                        setattr(user_config, k, v)\n\n            return user_config\n\n        return None\n\n    def _init_attention_free(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_attention_free_model(architectures)\n\n    def _init_is_hybrid(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_hybrid_model(architectures)\n\n    def _init_has_inner_state(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.model_has_inner_state(architectures)\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\", \"mistral\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto', 'slow' or 'mistral'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _get_preferred_task(\n        self,\n        architectures: List[str],\n        supported_tasks: Set[_ResolvedTask],\n    ) -> Optional[_ResolvedTask]:\n        model_id = self.model\n        if get_pooling_config(model_id, self.revision):\n            return \"embed\"\n        if ModelRegistry.is_cross_encoder_model(architectures):\n            return \"score\"\n\n        suffix_to_preferred_task: List[Tuple[str, _ResolvedTask]] = [\n            # Other models follow this pattern\n            (\"ForCausalLM\", \"generate\"),\n            (\"ForConditionalGeneration\", \"generate\"),\n            (\"ForSequenceClassification\", \"classify\"),\n            (\"ChatModel\", \"generate\"),\n            (\"LMHeadModel\", \"generate\"),\n            (\"EmbeddingModel\", \"embed\"),\n            (\"RewardModel\", \"reward\"),\n        ]\n        _, arch = ModelRegistry.inspect_model_cls(architectures)\n\n        for suffix, pref_task in suffix_to_preferred_task:\n            if arch.endswith(suffix) and pref_task in supported_tasks:\n                return pref_task\n\n        return None\n\n    def _resolve_task(\n        self,\n        task_option: Union[TaskOption, Literal[\"draft\"]],\n        hf_config: PretrainedConfig,\n    ) -> Tuple[Set[_ResolvedTask], _ResolvedTask]:\n        if task_option == \"draft\":\n            return {\"draft\"}, \"draft\"\n\n        architectures = getattr(hf_config, \"architectures\", [])\n\n        runner_support: Dict[RunnerType, bool] = {\n            # NOTE: Listed from highest to lowest priority,\n            # in case the model supports multiple of them\n            \"generate\": ModelRegistry.is_text_generation_model(architectures),\n            \"pooling\": ModelRegistry.is_pooling_model(architectures),\n        }\n        supported_runner_types_lst: List[RunnerType] = [\n            runner_type\n            for runner_type, is_supported in runner_support.items()\n            if is_supported\n        ]\n\n        supported_tasks_lst: List[_ResolvedTask] = [\n            task for runner_type in supported_runner_types_lst\n            for task in _RUNNER_TASKS[runner_type]\n        ]\n        supported_tasks = set(supported_tasks_lst)\n\n        if task_option == \"auto\":\n            selected_task = next(iter(supported_tasks_lst))\n\n            if len(supported_tasks_lst) > 1:\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task is not None:\n                    selected_task = preferred_task\n\n                logger.info(\n                    \"This model supports multiple tasks: %s. \"\n                    \"Defaulting to '%s'.\", supported_tasks, selected_task)\n        else:\n            # Aliases\n            if task_option == \"embedding\":\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task != \"embed\":\n                    msg = (\"The 'embedding' task will be restricted to \"\n                           \"embedding models in a future release. Please \"\n                           \"pass `--task classify`, `--task score`, or \"\n                           \"`--task reward` explicitly for other pooling \"\n                           \"models.\")\n                    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n                task_option = preferred_task or \"embed\"\n\n            if task_option not in supported_tasks:\n                msg = (\n                    f\"This model does not support the '{task_option}' task. \"\n                    f\"Supported tasks: {supported_tasks}\")\n                raise ValueError(msg)\n\n            selected_task = task_option\n\n        return supported_tasks, selected_task\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = QUANTIZATION_METHODS\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"modelopt\", \"gptq_marlin_24\", \"gptq_marlin\",\n            \"awq_marlin\", \"fbgemm_fp8\", \"compressed_tensors\",\n            \"compressed-tensors\", \"experts_int8\", \"quark\"\n        ]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n\n            # Detect which checkpoint is it\n            for name in QUANTIZATION_METHODS:\n                method = get_quantization_config(name)\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override:\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            from vllm.platforms import current_platform\n            current_platform.verify_quantization(self.quantization)\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n\n    def _verify_cuda_graph(self) -> None:\n        if self.max_seq_len_to_capture is None:\n            self.max_seq_len_to_capture = self.max_model_len\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n\n        MODEL_NOT_SUPPORT_CUDA_GRAPH = ['mllama']\n        if (self.hf_config.model_type in MODEL_NOT_SUPPORT_CUDA_GRAPH\n                and not self.enforce_eager):\n            logger.warning(\n                \"CUDA graph is not supported for %s yet, fallback to the eager \"\n                \"mode.\", self.hf_config.model_type)\n            self.enforce_eager = True\n\n    def _verify_bnb_config(self) -> None:\n        \"\"\"\n        The current version of bitsandbytes (0.44.0) with 8-bit models does not\n        yet support CUDA graph.\n        \"\"\"\n        is_bitsandbytes = self.quantization == \"bitsandbytes\"\n        has_quantization_config = (getattr(self.hf_config,\n                                           \"quantization_config\", None)\n                                   is not None)\n        is_8bit = (self.hf_config.quantization_config.get(\n            \"load_in_8bit\", False) if has_quantization_config else False)\n        if all([\n                is_bitsandbytes,\n                has_quantization_config,\n                is_8bit,\n                not self.enforce_eager,\n        ]):\n            logger.warning(\n                \"CUDA graph is not supported on BitAndBytes 8bit yet, \"\n                \"fallback to the eager mode.\")\n            self.enforce_eager = True\n\n    def verify_async_output_proc(self, parallel_config, speculative_config,\n                                 device_config) -> None:\n        if not self.use_async_output_proc:\n            # Nothing to check\n            return\n\n        if parallel_config.pipeline_parallel_size > 1:\n            logger.warning(\"Async output processing can not be enabled \"\n                           \"with pipeline parallel\")\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        from vllm.platforms import current_platform\n        if not current_platform.is_async_output_supported(self.enforce_eager):\n            logger.warning(\n                \"Async output processing is not supported on the \"\n                \"current platform type %s.\", current_platform.device_type)\n            self.use_async_output_proc = False\n            return\n\n        if envs.VLLM_USE_RAY_SPMD_WORKER:\n            logger.warning(\n                \"Async output processing can not be enabled with ray spmd\")\n            self.use_async_output_proc = False\n            return\n\n        # Async postprocessor is not necessary for pooling models\n        # since there is no token generation\n        if self.runner_type == \"pooling\":\n            self.use_async_output_proc = False\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        if speculative_config:\n            logger.warning(\"Async output processing is not supported with\"\n                           \" speculative decoding currently.\")\n            self.use_async_output_proc = False\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if pipeline_parallel_size > 1:\n            architectures = getattr(self.hf_config, \"architectures\", [])\n            if not ModelRegistry.is_pp_supported_model(architectures):\n                raise NotImplementedError(\n                    \"Pipeline parallelism is not supported for this model. \"\n                    \"Supported models implement the `SupportsPP` interface.\")\n\n            if self.use_async_output_proc:\n                logger.warning(\"Async output processor is not supported with \"\n                               \"pipeline parallelism currently. Disabling it.\")\n                self.use_async_output_proc = False\n\n    def get_hf_config_sliding_window(\n            self) -> Union[Optional[int], List[Optional[int]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n    def get_sliding_window(self) -> Optional[Union[int, List[Optional[int]]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n        # If user disables sliding window, return None.\n        if self.disable_sliding_window:\n            return None\n        # Otherwise get the value from the hf config.\n        return self.get_hf_config_sliding_window()\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    @property\n    def is_deepseek_mla(self) -> bool:\n        # TODO add deepseek_v3\n        return hasattr(self.hf_text_config,\n                       \"model_type\") and (self.hf_text_config.model_type\n                                          in ('deepseek_v2'))\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if self.is_deepseek_mla:\n            if self.use_mla:\n                return self.hf_text_config.kv_lora_rank\n            else:\n                qk_rope_head_dim = getattr(self.hf_text_config,\n                                           \"qk_rope_head_dim\", 0)\n                qk_nope_head_dim = getattr(self.hf_text_config,\n                                           \"qk_nope_head_dim\", 0)\n                if qk_rope_head_dim and qk_nope_head_dim:\n                    return qk_rope_head_dim + qk_nope_head_dim\n\n        if self.is_attention_free:\n            return 0\n\n        if hasattr(self.hf_text_config, \"head_dim\"):\n            return self.hf_text_config.head_dim\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        if self.is_attention_free:\n            return 0\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        if self.use_mla:\n            # When using MLA during decode it becomes MQA\n            return 1\n\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_layers_start_end_indices(\n            self, parallel_config: \"ParallelConfig\") -> Tuple[int, int]:\n        from vllm.distributed.utils import get_pp_indices\n        total_num_hidden_layers = getattr(self.hf_text_config,\n                                          \"num_hidden_layers\", 0)\n        pp_rank = parallel_config.rank // parallel_config.tensor_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return start, end\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        start, end = self.get_layers_start_end_indices(parallel_config)\n        return end - start\n\n    def get_num_layers_by_block_type(\n        self,\n        parallel_config: \"ParallelConfig\",\n        block_type: LayerBlockType = LayerBlockType.attention,\n    ) -> int:\n        # This function relies on 'layers_block_type' in hf_config,\n        # for w/o this attribute, we will need to have workarounds like so\n        attn_block_type = block_type == LayerBlockType.attention\n        is_transformer = not self.is_hybrid and not self.is_attention_free\n        start, end = self.get_layers_start_end_indices(parallel_config)\n\n        if is_transformer:\n            # Handle the basic case first\n            return end - start if attn_block_type else 0\n        elif self.is_attention_free:\n            # Attention free\n            # Note that this code assumes there\n            # is only one type of attention-free block type.\n            return 0 if attn_block_type else end - start\n        else:\n            # Hybrid model\n            layers_block_type_value = getattr(self.hf_config,\n                                              \"layers_block_type\", None)\n            if layers_block_type_value is None:\n                raise ValueError(\"The model is an hybrid without a\"\n                                 \"layers_block_type in the hf_config,\"\n                                 \"cannot determine the num of \"\n                                 f\"{block_type.value} layers\")\n\n            return sum(t == block_type.value\n                       for t in layers_block_type_value[start:end])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    def try_get_generation_config(self) -> Dict[str, Any]:\n        if self.generation_config is None or self.generation_config == \"auto\":\n            config = try_get_generation_config(\n                self.model,\n                trust_remote_code=self.trust_remote_code,\n                revision=self.revision,\n            )\n        else:\n            config = try_get_generation_config(\n                self.generation_config,\n                trust_remote_code=self.trust_remote_code,\n            )\n\n        if config is None:\n            return {}\n\n        return config.to_diff_dict()\n\n    def get_diff_sampling_param(self) -> Dict[str, Any]:\n        \"\"\"\n        This method returns a dictionary containing the parameters\n        that differ from the default sampling parameters, but only\n        if `generation_config` is set. If `generation_config` is not\n        set, an empty dictionary is returned.\n\n        Returns:\n            Dict[str, Any]: A dictionary with the differing sampling\n            parameters if `generation_config` is set, otherwise an\n            empty dictionary.\n        \"\"\"\n        if self.generation_config is None:\n            # When generation_config is not set\n            config = {}\n        else:\n            config = self.try_get_generation_config()\n\n        # Overriding with given generation config\n        config.update(self.override_generation_config)\n\n        available_params = [\n            \"repetition_penalty\",\n            \"temperature\",\n            \"top_k\",\n            \"top_p\",\n            \"min_p\",\n            \"max_new_tokens\",\n        ]\n        if any(p in config for p in available_params):\n            diff_sampling_param = {\n                p: config.get(p)\n                for p in available_params if config.get(p) is not None\n            }\n            # Huggingface definition of max_new_tokens is equivalent\n            # to vLLM's max_tokens\n            if \"max_new_tokens\" in diff_sampling_param:\n                diff_sampling_param[\"max_tokens\"] = diff_sampling_param.pop(\n                    \"max_new_tokens\")\n        else:\n            diff_sampling_param = {}\n        return diff_sampling_param\n\n    @property\n    def is_encoder_decoder(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return is_encoder_decoder(self.hf_config)\n\n    @property\n    def uses_mrope(self) -> bool:\n        return uses_mrope(self.hf_config)\n\n    @property\n    def is_multimodal_model(self) -> bool:\n        return self.multimodal_config is not None\n\n    @property\n    def is_cross_encoder(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_cross_encoder_model(architectures)\n\n    @property\n    def use_mla(self) -> bool:\n        use_mla = (self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE)\n        return use_mla\n\n    @property\n    def supported_runner_types(self) -> Set[RunnerType]:\n        return {_TASK_RUNNER[task] for task in self.supported_tasks}\n\n    @property\n    def runner_type(self) -> RunnerType:\n        return _TASK_RUNNER[self.task]\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n        cache_dtype: Data type for kv cache storage.\n        is_attention_free: Whether the model is attention-free.\n        num_gpu_blocks_override: Number of GPU blocks to use. This overrides the\n            profiled num_gpu_blocks if specified. Does nothing if None.\n        sliding_window: Sliding window size for the KV cache. Can not work with\n            prefix caching enabled.\n        enable_prefix_caching: Whether to enable prefix caching.\n        cpu_offload_gb: Size of the CPU offload buffer in GiB.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: List[Any] = []\n        factors.append(self.cache_dtype)\n        # `cpu_offload_gb` does not use `torch.compile` yet.\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: float,\n        cache_dtype: str,\n        is_attention_free: bool = False,\n        num_gpu_blocks_override: Optional[int] = None,\n        sliding_window: Optional[int] = None,\n        enable_prefix_caching: bool = False,\n        cpu_offload_gb: float = 0,\n        calculate_kv_scales: Optional[bool] = None,\n    ) -> None:\n        self.block_size = block_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.swap_space_bytes = swap_space * GiB_bytes\n        self.num_gpu_blocks_override = num_gpu_blocks_override\n        self.cache_dtype = cache_dtype\n        self.is_attention_free = is_attention_free\n        self.sliding_window = sliding_window\n        self.enable_prefix_caching = enable_prefix_caching\n        self.cpu_offload_gb = cpu_offload_gb\n        self.calculate_kv_scales = calculate_kv_scales\n        self._verify_args()\n        self._verify_cache_dtype()\n        self._verify_prefix_caching()\n\n        # Will be set after profiling.\n        self.num_gpu_blocks: Optional[int] = None\n        self.num_cpu_blocks: Optional[int] = None\n\n        # Set calculate_kv_scales to False if the value is unset.\n        if self.calculate_kv_scales is None:\n            self.calculate_kv_scales = False\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype in (\"fp8\", \"fp8_e4m3\", \"fp8_e5m2\"):\n            logger.info(\n                \"Using fp8 data type to store kv cache. It reduces the GPU \"\n                \"memory footprint and boosts the performance. \"\n                \"Meanwhile, it may cause accuracy drop without a proper \"\n                \"scaling factor\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def _verify_prefix_caching(self) -> None:\n        if not self.enable_prefix_caching:\n            return\n\n        if self.sliding_window is not None:\n            raise NotImplementedError(\n                \"Prefix caching is not supported with sliding window. \"\n                \"Run with --disable-sliding-window to use prefix caching.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / GiB_bytes:.2f} GiB out of the \"\n               f\"{total_cpu_memory / GiB_bytes:.2f} GiB total CPU memory \"\n               \"is allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. %s\", msg)\n\n\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"Configuration for the tokenizer pool.\n\n    Args:\n        pool_size: Number of tokenizer workers in the pool.\n        pool_type: Type of the pool.\n        extra_config: Additional config for the pool.\n            The way the config will be used depends on the\n            pool type.\n    \"\"\"\n    pool_size: int\n    pool_type: Union[str, Type[\"BaseTokenizerGroup\"]]\n    extra_config: dict\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if self.pool_type not in (\"ray\", ) and not isinstance(\n                self.pool_type, type):\n            raise ValueError(f\"Unknown pool type: {self.pool_type}\")\n        if not isinstance(self.extra_config, dict):\n            raise ValueError(\"extra_config must be a dictionary.\")\n\n    @classmethod\n    def create_config(\n        cls, tokenizer_pool_size: int,\n        tokenizer_pool_type: Union[str, Type[\"BaseTokenizerGroup\"]],\n        tokenizer_pool_extra_config: Optional[Union[str, dict]]\n    ) -> Optional[\"TokenizerPoolConfig\"]:\n        \"\"\"Create a TokenizerPoolConfig from the given parameters.\n\n        If tokenizer_pool_size is 0, return None.\n\n        Args:\n            tokenizer_pool_size: Number of tokenizer workers in the pool.\n            tokenizer_pool_type: Type of the pool.\n            tokenizer_pool_extra_config: Additional config for the pool.\n                The way the config will be used depends on the\n                pool type. This can be a JSON string (will be parsed).\n        \"\"\"\n        if tokenizer_pool_size:\n            if isinstance(tokenizer_pool_extra_config, str):\n                tokenizer_pool_extra_config_parsed = json.loads(\n                    tokenizer_pool_extra_config)\n            else:\n                tokenizer_pool_extra_config_parsed = (\n                    tokenizer_pool_extra_config or {})\n            tokenizer_pool_config = cls(tokenizer_pool_size,\n                                        tokenizer_pool_type,\n                                        tokenizer_pool_extra_config_parsed)\n        else:\n            tokenizer_pool_config = None\n        return tokenizer_pool_config\n\n\nclass LoadFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    PT = \"pt\"\n    SAFETENSORS = \"safetensors\"\n    NPCACHE = \"npcache\"\n    DUMMY = \"dummy\"\n    TENSORIZER = \"tensorizer\"\n    SHARDED_STATE = \"sharded_state\"\n    GGUF = \"gguf\"\n    BITSANDBYTES = \"bitsandbytes\"\n    MISTRAL = \"mistral\"\n    RUNAI_STREAMER = \"runai_streamer\"\n\n\n@dataclass\nclass LoadConfig:\n    \"\"\"\n        download_dir: Directory to download and load the weights, default to the\n            default cache directory of huggingface.\n        load_format: The format of the model weights to load:\n            \"auto\" will try to load the weights in the safetensors format and\n                fall back to the pytorch bin format if safetensors format is\n                not available.\n            \"pt\" will load the weights in the pytorch bin format.\n            \"safetensors\" will load the weights in the safetensors format.\n            \"npcache\" will load the weights in pytorch format and store\n                a numpy cache to speed up the loading.\n            \"dummy\" will initialize the weights with random values, which is\n                mainly for profiling.\n            \"tensorizer\" will use CoreWeave's tensorizer library for\n                fast weight loading.\n            \"bitsandbytes\" will load nf4 type weights.\n        model_loader_extra_config: The extra config for the model loader.\n        ignore_patterns: The list of patterns to ignore when loading the model.\n            Default to \"original/**/*\" to avoid repeated loading of llama's\n            checkpoints.\n    \"\"\"\n\n    load_format: Union[str, LoadFormat, \"BaseModelLoader\"] = LoadFormat.AUTO\n    download_dir: Optional[str] = None\n    model_loader_extra_config: Optional[Union[str, dict]] = field(\n        default_factory=dict)\n    ignore_patterns: Optional[Union[List[str], str]] = None\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        model_loader_extra_config = self.model_loader_extra_config or {}\n        if isinstance(model_loader_extra_config, str):\n            self.model_loader_extra_config = json.loads(\n                model_loader_extra_config)\n        if isinstance(self.load_format, str):\n            load_format = self.load_format.lower()\n            self.load_format = LoadFormat(load_format)\n\n        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:\n            logger.info(\n                \"Ignoring the following patterns when downloading weights: %s\",\n                self.ignore_patterns)\n        else:\n            self.ignore_patterns = [\"original/**/*\"]\n\n\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1  # Number of pipeline parallel groups.\n    tensor_parallel_size: int = 1  # Number of tensor parallel groups.\n\n    # Maximum number of multiple batches\n    # when load model sequentially. To avoid RAM OOM when using tensor\n    # parallel and large models.\n    max_parallel_loading_workers: Optional[int] = None\n\n    # Disable the custom all-reduce kernel and fall back to NCCL.\n    disable_custom_all_reduce: bool = False\n\n    # Config for the tokenizer pool. If None, will use synchronous tokenization.\n    tokenizer_pool_config: Optional[TokenizerPoolConfig] = None\n\n    # Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\n    ray_workers_use_nsight: bool = False\n\n    # ray distributed model workers placement group.\n    placement_group: Optional[\"PlacementGroup\"] = None\n\n    # Backend to use for distributed model\n    # workers, either \"ray\" or \"mp\" (multiprocessing). If the product\n    # of pipeline_parallel_size and tensor_parallel_size is less than\n    # or equal to the number of GPUs available, \"mp\" will be used to\n    # keep processing on a single host. Otherwise, this will default\n    # to \"ray\" if Ray is installed and fail otherwise. Note that tpu\n    # and hpu only support Ray for distributed inference.\n    distributed_executor_backend: Optional[Union[str,\n                                                 Type[\"ExecutorBase\"]]] = None\n\n    # the full name of the worker class to use. If \"auto\", the worker class\n    # will be determined based on the platform.\n    worker_cls: str = \"auto\"\n    sd_worker_cls: str = \"auto\"\n\n    world_size: int = field(init=False)\n\n    rank: int = 0\n\n    def compute_hash(self):\n        \"\"\"\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: List[Any] = []\n        factors.append(self.pipeline_parallel_size)\n        factors.append(self.tensor_parallel_size)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __post_init__(self) -> None:\n        self.world_size = self.pipeline_parallel_size * \\\n            self.tensor_parallel_size\n\n        ray_only_devices = [\"tpu\"]\n        from vllm.platforms import current_platform\n        if (current_platform.device_type in ray_only_devices\n                and self.world_size > 1):\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            if self.distributed_executor_backend != \"ray\":\n                raise ValueError(\n                    f\"{current_platform.device_type.upper()} backend only \"\n                    \"supports Ray for distributed inference.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.executor import ray_utils\n            backend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if current_platform.is_neuron():\n                # neuron uses single process to control multiple devices\n                backend = \"uni\"\n            elif (current_platform.is_cuda()\n                  and cuda_device_count_stateless() < self.world_size):\n                if not ray_found:\n                    raise ValueError(\"Unable to load Ray which is \"\n                                     \"required for multi-node inference, \"\n                                     \"please install Ray with `pip install \"\n                                     \"ray`.\") from ray_utils.ray_import_err\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.info(\"Defaulting to use %s for distributed inference\",\n                        backend)\n\n        self._verify_args()\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and self.distributed_executor_backend.uses_ray)\n\n    def _verify_args(self) -> None:\n        # Lazy import to avoid circular import\n        from vllm.executor.executor_base import ExecutorBase\n        from vllm.platforms import current_platform\n        if self.distributed_executor_backend not in (\n                \"ray\", \"mp\", \"uni\",\n                \"external_launcher\", None) and not (isinstance(\n                    self.distributed_executor_backend, type) and issubclass(\n                        self.distributed_executor_backend, ExecutorBase)):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' 'uni', 'external_launcher' or\"\n                \" custom ExecutorBase subclass.\")\n        if self.use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n        if current_platform.is_rocm():\n            self.disable_custom_all_reduce = True\n            logger.info(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on AMD GPUs.\")\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n\n@dataclass\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\"\"\"\n\n    runner_type: str = \"generate\"  # The runner type to launch for the model.\n\n    # Maximum number of tokens to be processed in a single iteration.\n    max_num_batched_tokens: int = field(default=None)  # type: ignore\n\n    # Maximum number of sequences to be processed in a single iteration.\n    max_num_seqs: int = 128\n\n    # Maximum length of a sequence (including prompt and generated text).\n    max_model_len: int = 8192\n\n    # The number of slots to allocate per sequence per\n    # step, beyond the known token ids. This is used in speculative\n    # decoding to store KV activations of tokens which may or may not be\n    # accepted.\n    num_lookahead_slots: int = 0\n\n    # Apply a delay (of delay factor multiplied by previous\n    # prompt latency) before scheduling next prompt.\n    delay_factor: float = 0.0\n\n    # If True, prefill requests can be chunked based\n    # on the remaining max_num_batched_tokens.\n    enable_chunked_prefill: bool = False\n\n    is_multimodal_model: bool = False\n\n    # NOTE: The following multimodal encoder budget will be initialized to\n    # max_num_batched_tokens and overridden in case max multimodal embedding\n    # size is larger.\n    # TODO (ywang96): Make these configurable.\n    # Multimodal encoder compute budget, only used in V1\n    max_num_encoder_input_tokens: int = field(default=None)  # type: ignore\n\n    # Multimodal encoder cache size, only used in V1\n    encoder_cache_size: int = field(default=None)  # type: ignore\n\n    # Whether to perform preemption by swapping or\n    # recomputation. If not specified, we determine the mode as follows:\n    # We use recomputation by default since it incurs lower overhead than\n    # swapping. However, when the sequence group has multiple sequences\n    # (e.g., beam search), recomputation is not currently supported. In\n    # such a case, we use swapping instead.\n    preemption_mode: Optional[str] = None\n\n    num_scheduler_steps: int = 1\n\n    multi_step_stream_outputs: bool = False\n\n    # Private API. If used, scheduler sends delta data to\n    # workers instead of an entire data. It should be enabled only\n    # when SPMD worker architecture is enabled. I.e.,\n    # VLLM_USE_RAY_SPMD_WORKER=1\n    send_delta_data: bool = False\n\n    # The scheduling policy to use. \"fcfs\" (default) or \"priority\".\n    policy: str = \"fcfs\"\n\n    chunked_prefill_enabled: bool = field(init=False)\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        if self.max_num_batched_tokens is None:\n            if self.enable_chunked_prefill:\n                if self.num_scheduler_steps > 1:\n                    # Multi-step Chunked-Prefill doesn't allow prompt-chunking\n                    # for now. Have max_num_batched_tokens set to max_model_len\n                    # so we don't reject sequences on account of a short\n                    # max_num_batched_tokens.\n                    self.max_num_batched_tokens = max(self.max_model_len, 2048)\n                else:\n                    # This value is chosen to have a balance between ITL\n                    # and TTFT. Note it is not optimized for throughput.\n                    self.max_num_batched_tokens = 2048\n            else:\n                # If max_model_len is too short, use 2048 as the default value\n                # for higher throughput.\n                self.max_num_batched_tokens = max(self.max_model_len, 2048)\n\n            if self.runner_type == \"pooling\":\n                # Choose specific value for higher throughput\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _POOLING_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n            if self.is_multimodal_model:\n                # The value needs to be at least the number of multimodal tokens\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n\n        self.max_num_encoder_input_tokens = self.max_num_batched_tokens\n        self.encoder_cache_size = self.max_num_batched_tokens\n\n        if self.enable_chunked_prefill:\n            logger.info(\n                \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                self.max_num_batched_tokens)\n\n        self.chunked_prefill_enabled = self.enable_chunked_prefill\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if (self.max_num_batched_tokens < self.max_model_len\n                and not self.chunked_prefill_enabled):\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n        if self.num_lookahead_slots < 0:\n            raise ValueError(\n                \"num_lookahead_slots \"\n                f\"({self.num_lookahead_slots}) must be greater than or \"\n                \"equal to 0.\")\n\n        if self.num_scheduler_steps < 1:\n            raise ValueError(\n                \"num_scheduler_steps \"\n                f\"({self.num_scheduler_steps}) must be greater than or \"\n                \"equal to 1.\")\n\n    @property\n    def is_multi_step(self) -> bool:\n        return self.num_scheduler_steps > 1\n\n\nclass DeviceConfig:\n    device: Optional[torch.device]\n    device_type: str\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # the device/platform information will be summarized\n        # by torch/vllm automatically.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __init__(self, device: str = \"auto\") -> None:\n        if device == \"auto\":\n            # Automated device type detection\n            from vllm.platforms import current_platform\n            self.device_type = current_platform.device_type\n            if not self.device_type:\n                raise RuntimeError(\"Failed to infer device type\")\n        else:\n            # Device type is assigned explicitly\n            self.device_type = device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\", \"openvino\"]:\n            self.device = torch.device(\"cpu\")\n        elif self.device_type in [\"tpu\"]:\n            self.device = None\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\nclass SpeculativeConfig:\n    \"\"\"Configuration for speculative decoding.\n\n    The configuration is currently specialized to draft-model speculative\n    decoding with top-1 proposals.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # spec decode does not use `torch.compile` yet.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    @staticmethod\n    def maybe_create_spec_config(\n        target_model_config: ModelConfig,\n        target_parallel_config: ParallelConfig,\n        target_dtype: str,\n        speculative_model: Optional[str],\n        speculative_model_quantization: Optional[str],\n        speculative_draft_tensor_parallel_size: Optional[int],\n        num_speculative_tokens: Optional[int],\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_max_model_len: Optional[int],\n        enable_chunked_prefill: bool,\n        disable_log_stats: bool,\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: Optional[float],\n        typical_acceptance_sampler_posterior_alpha: Optional[float],\n        disable_logprobs: Optional[bool],\n    ) -> Optional[\"SpeculativeConfig\"]:\n        \"\"\"Create a SpeculativeConfig if possible, else return None.\n\n        This function attempts to create a SpeculativeConfig object based on the\n        provided parameters. If the necessary conditions are met, it returns an\n        instance of SpeculativeConfig. Otherwise, it returns None.\n\n        Args:\n            target_model_config (ModelConfig): The configuration of the target\n                model.\n            target_parallel_config (ParallelConfig): The parallel configuration\n                for the target model.\n            target_dtype (str): The data type used for the target model.\n            speculative_model (Optional[str]): The name of the speculative\n                model, if provided.\n            speculative_model_quantization (Optional[str]): Quantization method\n                that was used to quantize the speculative model weights. If\n                None, we assume the model weights are not quantized.\n            speculative_draft_tensor_parallel_size (Optional[int]): The degree\n                of the tensor parallelism for the draft model.\n            num_speculative_tokens (Optional[int]): The number of speculative\n                tokens, if provided. Will default to the number in the draft\n                model config if present, otherwise is required.\n            speculative_disable_mqa_scorer (Optional[bool]): Disable the MQA\n                scorer for the speculative model and fall back to batch\n                expansion for scoring.\n            speculative_max_model_len (Optional[int]): The maximum model len of\n                the speculative model. Used when testing the ability to skip\n                speculation for some sequences.\n            enable_chunked_prefill (bool): Whether vLLM is configured to use\n                chunked prefill or not. Used for raising an error since its not\n                yet compatible with spec decode.\n            speculative_disable_by_batch_size (Optional[int]): Disable\n                speculative decoding for new incoming requests when the number\n                of enqueue requests  is larger than this value, if provided.\n            ngram_prompt_lookup_max (Optional[int]): Max size of ngram token\n                window, if provided.\n            ngram_prompt_lookup_min (Optional[int]): Min size of ngram token\n                window, if provided.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the\n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs (Optional[bool]): If set to True, token log\n                probabilities are not returned during speculative decoding.\n                If set to False, token log probabilities are returned\n                according to the log probability settings in SamplingParams.\n                If not specified, it defaults to True.\n\n        Returns:\n            Optional[\"SpeculativeConfig\"]: An instance of SpeculativeConfig if\n                the necessary conditions are met, else None.\n        \"\"\"\n\n        if speculative_model is None:\n            if num_speculative_tokens is not None:\n                raise ValueError(\"num_speculative_tokens was provided without \"\n                                 \"speculative_model.\")\n            return None\n\n        if (speculative_disable_by_batch_size is not None\n                and speculative_disable_by_batch_size < 2):\n            raise ValueError(\"Expect the batch size threshold of disabling \"\n                             \"speculative decoding is > 1, but got \"\n                             f\"{speculative_disable_by_batch_size=}\")\n        if (enable_chunked_prefill and speculative_model == \"eagle\"):\n            raise ValueError(\"Chunked prefill and EAGLE are not compatible.\")\n        # TODO: The user should be able to specify revision/max model len\n        # for the draft model. It is not currently supported.\n        draft_revision = None\n        draft_code_revision = None\n        draft_quantization = speculative_model_quantization\n\n        if speculative_model == \"[ngram]\":\n            if ngram_prompt_lookup_min is None:\n                ngram_prompt_lookup_min = 1\n            if ngram_prompt_lookup_max is None or ngram_prompt_lookup_max < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_max=} must be > 0\")\n            if ngram_prompt_lookup_min < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} must be > 0\")\n            if ngram_prompt_lookup_min > ngram_prompt_lookup_max:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} cannot be \"\n                                 f\"larger than {ngram_prompt_lookup_max=}\")\n\n            # TODO: current we still need extract vocab_size from target model\n            # config, in future, we may try refactor it out, and set\n            # draft related config as None here.\n            draft_model_config = target_model_config\n            draft_parallel_config = target_parallel_config\n        else:\n            ngram_prompt_lookup_max = 0\n            ngram_prompt_lookup_min = 0\n            draft_model_config = ModelConfig(\n                model=speculative_model,\n                task=\"draft\",\n                tokenizer=target_model_config.tokenizer,\n                tokenizer_mode=target_model_config.tokenizer_mode,\n                trust_remote_code=target_model_config.trust_remote_code,\n                allowed_local_media_path=target_model_config.\n                allowed_local_media_path,\n                dtype=target_model_config.dtype,\n                seed=target_model_config.seed,\n                revision=draft_revision,\n                code_revision=draft_code_revision,\n                tokenizer_revision=target_model_config.tokenizer_revision,\n                max_model_len=None,\n                spec_target_max_model_len=target_model_config.max_model_len,\n                quantization=draft_quantization,\n                enforce_eager=target_model_config.enforce_eager,\n                max_seq_len_to_capture=target_model_config.\n                max_seq_len_to_capture,\n                max_logprobs=target_model_config.max_logprobs,\n            )\n\n            draft_hf_config = draft_model_config.hf_config\n\n            if (num_speculative_tokens is not None\n                    and hasattr(draft_hf_config, \"num_lookahead_tokens\")):\n                draft_hf_config.num_lookahead_tokens = num_speculative_tokens\n\n            n_predict = getattr(draft_hf_config, \"n_predict\", None)\n            if n_predict is not None:\n                if num_speculative_tokens is None:\n                    # Default to max value defined in draft model config.\n                    num_speculative_tokens = n_predict\n                elif num_speculative_tokens > n_predict:\n                    # Verify provided value doesn't exceed the maximum\n                    # supported by the draft model.\n                    raise ValueError(\n                        \"This speculative model supports a maximum of \"\n                        f\"num_speculative_tokens={n_predict}, but \"\n                        f\"{num_speculative_tokens=} was provided.\")\n\n            speculative_draft_tensor_parallel_size = \\\n                SpeculativeConfig._verify_and_get_draft_model_tensor_parallel_size(\n                    target_parallel_config,\n                    speculative_draft_tensor_parallel_size,\n                    draft_hf_config\n            )\n\n            draft_model_config.max_model_len = (\n                SpeculativeConfig._maybe_override_draft_max_model_len(\n                    speculative_max_model_len,\n                    draft_model_config.max_model_len,\n                    target_model_config.max_model_len,\n                ))\n\n            draft_parallel_config = (\n                SpeculativeConfig.create_draft_parallel_config(\n                    target_parallel_config,\n                    speculative_draft_tensor_parallel_size, draft_hf_config))\n\n        if num_speculative_tokens is None:\n            raise ValueError(\n                \"num_speculative_tokens must be provided with \"\n                \"speculative_model unless the draft model config contains an \"\n                \"n_predict parameter.\")\n\n        if typical_acceptance_sampler_posterior_threshold is None:\n            typical_acceptance_sampler_posterior_threshold = 0.09\n        if typical_acceptance_sampler_posterior_alpha is None:\n            typical_acceptance_sampler_posterior_alpha = 0.3\n        if disable_logprobs is None:\n            disable_logprobs = True\n\n        return SpeculativeConfig(\n            draft_model_config,\n            draft_parallel_config,\n            num_speculative_tokens,\n            speculative_disable_mqa_scorer,\n            speculative_disable_by_batch_size,\n            ngram_prompt_lookup_max,\n            ngram_prompt_lookup_min,\n            draft_token_acceptance_method=draft_token_acceptance_method,\n            typical_acceptance_sampler_posterior_threshold=\\\n                typical_acceptance_sampler_posterior_threshold,\n            typical_acceptance_sampler_posterior_alpha=\\\n                typical_acceptance_sampler_posterior_alpha,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n        )\n\n    @staticmethod\n    def _maybe_override_draft_max_model_len(\n        speculative_max_model_len: Optional[int],\n        draft_max_model_len: int,\n        target_max_model_len: int,\n    ) -> int:\n        \"\"\"Determine the max sequence len for the draft model. This is usually\n        the draft_max_model_len, but may be the target_max_model_len if it is\n        less than the draft_max_model_len, or may be speculative_max_model_len\n        if it is specified.\n\n        This is necessary so that sequences do not exceed the capacity of the\n        draft model or the target model.\n\n        speculative_max_model_len is mainly used for testing that sequences can\n        skip speculation.\n        \"\"\"\n\n        if speculative_max_model_len is not None:\n\n            if speculative_max_model_len > draft_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {draft_max_model_len=}\")\n\n            if speculative_max_model_len > target_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {target_max_model_len=}\")\n\n            return speculative_max_model_len\n\n        return min(\n            draft_max_model_len,\n            target_max_model_len,\n        )\n\n    @staticmethod\n    def _verify_and_get_draft_model_tensor_parallel_size(\n            target_parallel_config: ParallelConfig,\n            speculative_draft_tensor_parallel_size: Optional[int],\n            draft_hf_config: PretrainedConfig) -> int:\n        \"\"\"\n        Verifies and adjusts the tensor parallel size for a draft model\n        specified using speculative_draft_tensor_parallel_size.\n        \"\"\"\n        # If speculative_draft_tensor_parallel_size is unset then set it\n        # appropriately else verify that it is set correctly.\n        if speculative_draft_tensor_parallel_size is None:\n            if draft_hf_config.model_type == \"mlp_speculator\":\n                speculative_draft_tensor_parallel_size = 1\n                if target_parallel_config.tensor_parallel_size > 1:\n                    logger.warning(\n                        \"MLPSpeculator cannot currently be run with tp>1; \"\n                        \"setting speculative_draft_tensor_parallel_size=1\")\n            else:\n                speculative_draft_tensor_parallel_size = \\\n                    target_parallel_config.tensor_parallel_size\n        elif speculative_draft_tensor_parallel_size not in (\n                1, target_parallel_config.tensor_parallel_size):\n            raise ValueError(\n                f\"{speculative_draft_tensor_parallel_size=} cannot be \"\n                f\"other value than 1 or target model tensor_parallel_size\")\n        return speculative_draft_tensor_parallel_size\n\n    @staticmethod\n    def create_draft_parallel_config(\n        target_parallel_config: ParallelConfig,\n        speculative_draft_tensor_parallel_size: int,\n        draft_hf_config: PretrainedConfig,\n    ) -> ParallelConfig:\n        \"\"\"Create a parallel config for use by the draft worker.\n\n        This is mostly a copy of the target parallel config, except the tp_size.\n        \"\"\"\n        draft_parallel_config = ParallelConfig(\n            pipeline_parallel_size=target_parallel_config.\n            pipeline_parallel_size,\n            tensor_parallel_size=speculative_draft_tensor_parallel_size,\n            distributed_executor_backend=target_parallel_config.\n            distributed_executor_backend,\n            max_parallel_loading_workers=target_parallel_config.\n            max_parallel_loading_workers,\n            disable_custom_all_reduce=target_parallel_config.\n            disable_custom_all_reduce,\n            tokenizer_pool_config=target_parallel_config.tokenizer_pool_config,\n            ray_workers_use_nsight=target_parallel_config.\n            ray_workers_use_nsight,\n            placement_group=target_parallel_config.placement_group,\n        )\n\n        return draft_parallel_config\n\n    def __init__(\n        self,\n        draft_model_config: ModelConfig,\n        draft_parallel_config: ParallelConfig,\n        num_speculative_tokens: int,\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ):\n        \"\"\"Create a SpeculativeConfig object.\n\n        Args:\n            draft_model_config: ModelConfig for the draft model.\n            draft_parallel_config: ParallelConfig for the draft model.\n            num_speculative_tokens: The number of tokens to sample from the\n                draft model before scoring with the target model.\n            speculative_disable_by_batch_size: Disable speculative\n                decoding for new incoming requests when the number of\n                enqueue requests is larger than this value.\n            ngram_prompt_lookup_max: Max size of ngram token window.\n            ngram_prompt_lookup_min: Min size of ngram token window.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the\n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will not\n                be returned even if requested by sampling parameters. This\n                reduces latency by skipping logprob calculation in proposal\n                sampling, target sampling, and after accepted tokens are\n                determined. If set to False, log probabilities will be\n                returned.\n            disable_log_stats: Whether to disable periodic printing of stage\n                times in speculative decoding.\n        \"\"\"\n        self.draft_model_config = draft_model_config\n        self.draft_parallel_config = draft_parallel_config\n        self.num_speculative_tokens = num_speculative_tokens\n        self.speculative_disable_mqa_scorer = speculative_disable_mqa_scorer\n        self.speculative_disable_by_batch_size = \\\n            speculative_disable_by_batch_size\n        self.ngram_prompt_lookup_max = ngram_prompt_lookup_max or 0\n        self.ngram_prompt_lookup_min = ngram_prompt_lookup_min or 0\n        self.draft_token_acceptance_method = draft_token_acceptance_method\n        self.typical_acceptance_sampler_posterior_threshold = \\\n            typical_acceptance_sampler_posterior_threshold\n        self.typical_acceptance_sampler_posterior_alpha = \\\n            typical_acceptance_sampler_posterior_alpha\n        self.disable_logprobs = disable_logprobs\n        self.disable_log_stats = disable_log_stats\n\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.num_speculative_tokens <= 0:\n            raise ValueError(\"Expected num_speculative_tokens to be greater \"\n                             f\"than zero ({self.num_speculative_tokens}).\")\n\n        if self.draft_model_config:\n            self.draft_model_config.verify_with_parallel_config(\n                self.draft_parallel_config)\n            # Validate and set draft token acceptance related settings.\n\n        if (self.draft_token_acceptance_method is None):\n            raise ValueError(\"draft_token_acceptance_method is not set. \"\n                             \"Expected values are rejection_sampler or \"\n                             \"typical_acceptance_sampler.\")\n\n        if (self.draft_token_acceptance_method != 'rejection_sampler'\n                and self.draft_token_acceptance_method\n                != 'typical_acceptance_sampler'):\n            raise ValueError(\n                \"Expected draft_token_acceptance_method to be either \"\n                \"rejection_sampler or typical_acceptance_sampler. Instead it \"\n                f\"is {self.draft_token_acceptance_method}\")\n\n        if (self.typical_acceptance_sampler_posterior_threshold < 0\n                or self.typical_acceptance_sampler_posterior_alpha < 0):\n            raise ValueError(\n                \"Expected typical_acceptance_sampler_posterior_threshold \"\n                \"and typical_acceptance_sampler_posterior_alpha to be > 0. \"\n                \"Instead found \"\n                f\"typical_acceptance_sampler_posterior_threshold = \"\n                f\"{self.typical_acceptance_sampler_posterior_threshold} and \"\n                f\"typical_acceptance_sampler_posterior_alpha = \"\n                f\"{self.typical_acceptance_sampler_posterior_alpha}\")\n\n    @property\n    def num_lookahead_slots(self) -> int:\n        \"\"\"The number of additional slots the scheduler should allocate per\n        step, in addition to the slots allocated for each known token.\n\n        This is equal to the number of speculative tokens, as each speculative\n        token must be scored.\n        \"\"\"\n        return self.num_speculative_tokens\n\n    def __repr__(self) -> str:\n        if self.ngram_prompt_lookup_max > 0:\n            draft_model = \"[ngram]\"\n        else:\n            draft_model = self.draft_model_config.model\n        num_spec_tokens = self.num_speculative_tokens\n        return f\"SpeculativeConfig({draft_model=}, {num_spec_tokens=})\"\n\n\n@dataclass\nclass LoRAConfig:\n    max_lora_rank: int\n    max_loras: int\n    fully_sharded_loras: bool = False\n    max_cpu_loras: Optional[int] = None\n    lora_dtype: Optional[Union[torch.dtype, str]] = None\n    lora_extra_vocab_size: int = 256\n    # This is a constant.\n    lora_vocab_padding_size: ClassVar[int] = 256\n    long_lora_scaling_factors: Optional[Tuple[float]] = None\n    bias_enabled: bool = False\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # LoRA is not compatible with `torch.compile` .\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        # Setting the maximum rank to 256 should be able to satisfy the vast\n        # majority of applications.\n        possible_max_ranks = (8, 16, 32, 64, 128, 256)\n        possible_lora_extra_vocab_size = (0, 256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_cache_config(self, cache_config: CacheConfig):\n        # TODO LoRA supports CPU offload.\n        if cache_config.cpu_offload_gb > 0:\n            raise ValueError(\"CPU offload is not supported with LoRA yet.\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n        if model_config.quantization and model_config.quantization not in [\n                \"awq\", \"gptq\"\n        ]:\n            # TODO support marlin\n            logger.warning(\"%s quantization is not tested with LoRA yet.\",\n                           model_config.quantization)\n\n    def verify_with_scheduler_config(self, scheduler_config: SchedulerConfig):\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        if scheduler_config.chunked_prefill_enabled:\n            logger.warning(\"LoRA with chunked prefill is still experimental \"\n                           \"and may be unstable.\")\n\n\n@dataclass\nclass PromptAdapterConfig:\n    max_prompt_adapters: int\n    max_prompt_adapter_token: int\n    max_cpu_prompt_adapters: Optional[int] = None\n    prompt_adapter_dtype: Optional[torch.dtype] = None\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n\n        if self.max_prompt_adapters < 1:\n            raise ValueError(f\"max_prompt_adapters \"\n                             f\"({self.max_prompt_adapters}) must be >= 1.\")\n        if self.max_prompt_adapter_token == 0:\n            raise ValueError(\"max_prompt_adapter_token must be set.\")\n        if self.max_cpu_prompt_adapters is None:\n            self.max_cpu_prompt_adapters = self.max_prompt_adapters\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.prompt_adapter_dtype in (None, \"auto\"):\n            self.prompt_adapter_dtype = model_config.dtype\n        elif isinstance(self.prompt_adapter_dtype, str):\n            self.prompt_adapter_dtype = getattr(torch,\n                                                self.prompt_adapter_dtype)\n\n\n@dataclass\nclass MultiModalConfig:\n    \"\"\"Controls the behavior of multimodal models.\"\"\"\n\n    limit_per_prompt: Mapping[str, int] = field(default_factory=dict)\n    \"\"\"\n    The maximum number of input items allowed per prompt for each modality.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    # TODO: Add configs to init vision tower or not.\n\n\n@dataclass\nclass PoolerConfig:\n    \"\"\"Controls the behavior of output pooling in pooling models.\"\"\"\n\n    pooling_type: Optional[str] = None\n    \"\"\"\n    The pooling method of the pooling model. This should be a key in\n    :class:`vllm.model_executor.layers.pooler.PoolingType`.\n    \"\"\"\n\n    normalize: Optional[bool] = None\n    \"\"\"\n    Whether to normalize the pooled outputs. Usually, this should be set to\n    ``True`` for embedding outputs.\n    \"\"\"\n\n    softmax: Optional[bool] = None\n    \"\"\"\n    Whether to apply softmax to the pooled outputs. Usually, this should be set\n    to ``True`` for classification outputs.\n    \"\"\"\n\n    step_tag_id: Optional[int] = None\n    \"\"\"\n    If set, only the score corresponding to the ``step_tag_id`` in the\n    generated sentence should be returned. Otherwise, the scores for all tokens\n    are returned.\n    \"\"\"\n\n    returned_token_ids: Optional[List[int]] = None\n    \"\"\"\n    A list of indices for the vocabulary dimensions to be extracted,\n    such as the token IDs of ``good_token`` and ``bad_token`` in the\n    ``math-shepherd-mistral-7b-prm`` model.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    @staticmethod\n    def from_json(json_str: str) -> \"PoolerConfig\":\n        return PoolerConfig(**json.loads(json_str))\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE: List[str] = []  #\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            if config_dtype == torch.float32:\n                if config.model_type == \"gemma2\":\n                    logger.info(\n                        \"For Gemma 2, we downcast float32 to bfloat16 instead \"\n                        \"of float16 by default. Please specify `dtype` if you \"\n                        \"want to use float16.\")\n                    torch_dtype = torch.bfloat16\n                else:\n                    # Following the common practice, we use float16 for float32\n                    # models.\n                    torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n\n            from vllm.platforms import current_platform\n            if (current_platform.is_cpu()\n                    and current_platform.get_cpu_architecture()\n                    == CpuArchEnum.POWERPC\n                    and (config_dtype == torch.float16\n                         or config_dtype == torch.float32)):\n                logger.info(\n                    \"For POWERPC, we cast models to bfloat16 instead of \"\n                    \"using float16 by default. Float16 is not currently \"\n                    \"supported for POWERPC.\")\n                torch_dtype = torch.bfloat16\n\n            # TODO: change this condition to check if the platform support bf16\n            # instead of checking the OS. For instance M2 shall supports bf16\n            # already. But we need to modify `cpu_extension.cmake` to activate\n            # the feature in the build.\n            if (current_platform.is_cpu() and sys.platform.startswith(\"darwin\")\n                    and current_platform.get_cpu_architecture()\n                    == CpuArchEnum.ARM and config_dtype == torch.bfloat16):\n                logger.info(\"For macOS with Apple Silicon, currently bfloat16 \"\n                            \"is not supported. Setting dtype to float16.\")\n                torch_dtype = torch.float16\n\n            if current_platform.is_hpu() and config_dtype == torch.float16:\n                logger.info(\n                    \"For HPU, we cast models to bfloat16 instead of\"\n                    \"using float16 by default. Please specify `dtype` if you \"\n                    \"want to use float16.\")\n                torch_dtype = torch.bfloat16\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            logger.info(\"Upcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            logger.info(\"Downcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(\"Casting %s to %s.\", config_dtype, torch_dtype)\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n    disable_sliding_window: bool,\n    sliding_window_len: Optional[Union[int, List[Optional[int]]]],\n    spec_target_max_model_len: Optional[int] = None,\n    encoder_config: Optional[Any] = None,\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Command-R\n        \"model_max_length\",\n        # Whisper\n        \"max_target_positions\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    # Choose the smallest \"max_length\" from the possible keys.\n    max_len_key = None\n    for key in possible_keys:\n        max_len = getattr(hf_config, key, None)\n        if max_len is not None:\n            max_len_key = key if max_len < derived_max_model_len \\\n                else max_len_key\n            derived_max_model_len = min(derived_max_model_len, max_len)\n\n    # If sliding window is manually disabled, max_length should be less\n    # than the sliding window length in the model config.\n    if disable_sliding_window and sliding_window_len is not None:\n\n        sliding_window_len_min = get_min_sliding_window(sliding_window_len)\n        max_len_key = \"sliding_window\" \\\n            if sliding_window_len_min < derived_max_model_len else max_len_key\n        derived_max_model_len = min(derived_max_model_len,\n                                    sliding_window_len_min)\n\n    # If none of the keys were found in the config, use a default and\n    # log a warning.\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        if spec_target_max_model_len is not None:\n            # If this is a speculative draft model, we use the max model len\n            # from the target model.\n            return spec_target_max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            \"%s. Assuming the model's maximum length is %d.\", possible_keys,\n            default_max_len)\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        # No need to consider \"type\" key because of patch_rope_scaling when\n        # loading HF config\n        rope_type = rope_scaling[\"rope_type\"]\n\n        if rope_type not in (\"su\", \"longrope\", \"llama3\"):\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that supports rope_scaling\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"with rope_scaling. Please raise an issue so we can \"\n                    \"investigate.\")\n\n            # NOTE: rope_type == \"default\" does not define factor\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/modeling_rope_utils.py\n            scaling_factor = rope_scaling.get(\"factor\", 1.0)\n\n            if rope_type == \"yarn\":\n                derived_max_model_len = rope_scaling[\n                    \"original_max_position_embeddings\"]\n            derived_max_model_len *= scaling_factor\n\n    if encoder_config and \"max_seq_length\" in encoder_config:\n        derived_max_model_len = encoder_config[\"max_seq_length\"]\n\n    # If the user specified a max length, make sure it is smaller than the\n    # derived length from the HF model config.\n    if max_model_len is None:\n        max_model_len = int(derived_max_model_len)\n    elif max_model_len > derived_max_model_len:\n        # Some models might have a separate key for specifying model_max_length\n        # that will be bigger than derived_max_model_len. We compare user input\n        # with model_max_length and allow this override when it's smaller.\n        model_max_length = getattr(hf_config, \"model_max_length\", None)\n        if model_max_length is not None and max_model_len <= model_max_length:\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that has model_max_length\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"model_max_length in the config. Please raise an issue \"\n                    \"so we can investigate.\")\n        else:\n            msg = (\n                f\"User-specified max_model_len ({max_model_len}) is greater \"\n                f\"than the derived max_model_len ({max_len_key}=\"\n                f\"{derived_max_model_len} or model_max_length=\"\n                f\"{model_max_length} in model's config.json). This may lead \"\n                \"to incorrect model outputs or CUDA errors.\")\n            if envs.VLLM_ALLOW_LONG_MAX_MODEL_LEN:\n                logger.warning(\n                    \"%s Make sure the value is correct and within the \"\n                    \"model context size.\", msg)\n            else:\n                raise ValueError(\n                    f\"{msg} To allow overriding this maximum, set \"\n                    \"the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\")\n    return int(max_model_len)\n\n\ndef get_min_sliding_window(\n        sliding_window: Union[int, List[Optional[int]]]) -> int:\n    if isinstance(sliding_window, list):\n        return min(s for s in sliding_window if s is not None)\n\n    return sliding_window\n\n\ndef get_served_model_name(model: str,\n                          served_model_name: Optional[Union[str, List[str]]]):\n    \"\"\"\n    If the input is a non-empty list, the first model_name in\n    `served_model_name` is taken.\n    If the input is a non-empty string, it is used directly.\n    For cases where the input is either an empty string or an\n    empty list, the fallback is to use `self.model`.\n    \"\"\"\n    if not served_model_name:\n        return model\n    if isinstance(served_model_name, list):\n        return served_model_name[0]\n    return served_model_name\n\n\n@dataclass\nclass DecodingConfig:\n    \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n\n    # Which guided decoding algo to use.\n    # 'outlines' / 'lm-format-enforcer' / 'xgrammar'\n    guided_decoding_backend: str = 'xgrammar'\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        valid_guided_backends = ['outlines', 'lm-format-enforcer', 'xgrammar']\n        backend = self.guided_decoding_backend\n        if backend not in valid_guided_backends:\n            raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\n                             f\"must be one of {valid_guided_backends}\")\n\n\n@dataclass\nclass ObservabilityConfig:\n    \"\"\"Configuration for observability.\"\"\"\n    otlp_traces_endpoint: Optional[str] = None\n\n    # Collecting detailed timing information for each request can be expensive.\n\n    # If set, collects the model forward time for the request.\n    collect_model_forward_time: bool = False\n\n    # If set, collects the model execute time for the request.\n    collect_model_execute_time: bool = False\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if not is_otel_available() and self.otlp_traces_endpoint is not None:\n            raise ValueError(\n                \"OpenTelemetry is not available. Unable to configure \"\n                \"'otlp_traces_endpoint'. Ensure OpenTelemetry packages are \"\n                f\"installed. Original error:\\n{otel_import_error_traceback}\")\n\n\nclass KVTransferConfig(BaseModel):\n    \"\"\"Configuration for distributed KV cache transfer.\"\"\"\n\n    # The KV connector for vLLM to transmit KV caches between vLLM instances.\n    kv_connector: Optional[str] = None\n\n    # The device used by kv connector to buffer the KV cache.\n    # Currently only support 'cuda'.\n    kv_buffer_device: Optional[str] = \"cuda\"\n\n    # The buffer size for TorchDistributedConnector. Measured in number of\n    # bytes. Recommended value: 1e9 (about 1GB).\n    kv_buffer_size: float = 1e9\n\n    # Whether this vLLM instance produces, consumes KV cache, or both. Choices\n    # are 'kv_producer', 'kv_consumer', and 'both'.\n    kv_role: Optional[str] = None\n\n    # The rank of this vLLM instance in the KV cache transfer. Typical value:\n    # 0 for prefill instance, 1 for decode instance.\n    # Currently only 1P1D is supported.\n    kv_rank: Optional[int] = None\n\n    # The number of parallel instances for KV cache transfer. For\n    # PyNcclConnector, this should be 2.\n    kv_parallel_size: int = 1\n\n    # The KV connector ip, used to build distributed connection\n    kv_ip: str = \"127.0.0.1\"\n\n    # The KV connector port, used to build distributed connection\n    kv_port: int = 14579\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: List[Any] = []\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()\n        return hash_str\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"KVTransferConfig\":\n        \"\"\"Parse the CLI value for the kv cache transfer config.\"\"\"\n        return KVTransferConfig.model_validate_json(cli_value)\n\n    def model_post_init(self, __context: Any) -> None:\n\n        if self.kv_role is not None and self.kv_role not in [\n                \"kv_producer\", \"kv_consumer\", \"kv_both\"\n        ]:\n            raise ValueError(\n                f\"Unsupported kv_role: {self.kv_role}. \"\n                f\"Supported roles are `kv_producer`, `kv_consumer`, \"\n                f\"and `kv_both`\")\n\n        if self.kv_connector is not None and self.kv_role is None:\n            raise ValueError(\"Please specify kv_disagg_role when kv_connector \"\n                             \"is set, supported roles are `kv_producer`, \"\n                             \"`kv_consumer`, and `kv_both`\")\n\n    @property\n    def is_kv_transfer_instance(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_producer\", \"kv_consumer\", \"kv_both\"]\n\n    @property\n    def need_kv_parallel_group(self) -> bool:\n        # for those database-based connector, vLLM does not need to create\n        # parallel group, and in that case the kv parallel size will be 1.\n        return self.kv_connector is not None and self.kv_parallel_size > 1\n\n    @property\n    def is_kv_producer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_producer\", \"kv_both\"]\n\n    @property\n    def is_kv_consumer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_consumer\", \"kv_both\"]\n\n\nclass CompilationLevel:\n    # constants for the levels of the compilation process\n    NO_COMPILATION = 0\n    DYNAMO_AS_IS = 1\n    DYNAMO_ONCE = 2\n    PIECEWISE = 3\n\n\nclass CompilationConfig(BaseModel):\n    \"\"\"\n    Configuration for compilation.\n    It has three parts:\n    - Top-level Compilation control:\n        - level: the level of compilation.\n            - 0: no compilation.\n            - 1: dynamo as is.\n            - 2: dynamo once.\n            - 3: piecewise compilation.\n        - debug_dump_path: the path to dump the debug information.\n        - cache_dir: the directory to store the compiled graph, to\n            accelerate Inductor compilation. By default, it will use\n            model-related information to generate a cache directory.\n        - backend: the backend for compilation. It needs to be a string.\n            - \"\" (empty string): use the default backend.\n            - \"eager\"/\"openxla\"/...: use the specified backend registered in PyTorch.\n            - \"full.module.name\": a qualified name which can be used to import the backend function.\n            We use string to avoid serialization issues when using compilation in a distributed setting.\n            When the compilation level is 1 or 2, the backend is used for the compilation directly (it sees the whole graph).\n            When the compilation level is 3, the backend is used for the piecewise compilation (it sees a part of the graph).\n        - custom_ops: fine-grained control over which custom ops to enable/disable.\n            Use 'all' to enable all, 'none' to disable all.\n            Also specify a list of custom op names to enable (prefixed with a '+'),\n            or disable (prefixed with a '-').\n            Examples:\n                - 'all,-op1' to enable all except op1\n                - 'none,+op1,+op2' to enable only op1 and op2\n            By default, all custom ops are enabled when running without Inductor\n                and disabled when running with Inductor (compile_level >= Inductor).\n        - splitting_ops: a list of ops to split the full graph into subgraphs, used in piecewise compilation.\n    - CudaGraph capture:\n        - use_cudagraph: whether to use cudagraph inside compilation.\n            - False: cudagraph inside compilation is not used.\n            - True: cudagraph inside compilation is used. It requires\n                that all input buffers have fixed addresses, and all\n                splitting ops write their outputs to input buffers.\n            Note that this is orthogonal to the cudagraph capture logic\n            outside of compilation.\n            TODO: move outside cudagraph logic into compilation.\n            torch.compile will handle cudagraph capture logic in the future.\n        - cudagraph_capture_sizes: sizes to capture cudagraph.\n            - None (default): capture sizes are inferred from vllm config.\n            - List[int]: capture sizes are specified as given.\n        - cudagraph_num_of_warmups: number of warmup runs for cudagraph.\n            It means the first several runs will be treated as warmup runs.\n            Only after that, the execution will be recorded, and the recorded\n            cudagraph will be used for subsequent runs.\n        - cudagraph_copy_inputs: whether to copy input tensors for\n            cudagraph. If the caller can guarantee that the same input buffers\n            are always used, it can set this to False. Otherwise, it should\n            set this to True, and the compiler will copy the input to an\n            internally managed buffer. Default is False.\n    - Inductor compilation:\n        - use_inductor: whether to use inductor compilation.\n            - False: inductor compilation is not used. graph runs in eager.\n            - True: inductor compilation is used. one graph for symbolic shape\n                is compiled. In addition, compile for compile_sizes,\n                using configurations in inductor_compile_config.\n        - compile_sizes: sizes to compile for inductor. In addition\n            to integers, it also supports \"cudagraph_capture_sizes\" to\n            specify the sizes for cudagraph capture.\n        - inductor_compile_config: additional configurations for inductor.\n            - None: use default configurations.\n        - inductor_passes: additional passes for inductor. It is a dictionary\n            from pass name to pass function qualified name. We use function\n            name because the config uses json format. If we pass the config\n            from Python, functions can also be passed directly via Python object\n            constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`\n        - custom inductor passes: see PassConfig for more details\n\n    Why we have different sizes for cudagraph and inductor:\n    - cudagraph: a cudagraph captured for a specific size can only be used\n        for the same size. We need to capture all the sizes we want to use.\n    - inductor: a graph compiled by inductor for a general shape can be used\n        for different sizes. Inductor can also compile for specific sizes,\n        where it can have more information to optimize the graph with fully\n        static shapes. However, we find the general shape compilation is\n        sufficient for most cases. It might be beneficial to compile for\n        certain small batchsizes, where inductor is good at optimizing.\n    \"\"\" # noqa\n    level: int = 0\n    debug_dump_path: str = \"\"\n    cache_dir: str = \"\"\n    backend: str = \"\"\n    custom_ops: List[str] = Field(default_factory=list)\n    splitting_ops: List[str] = Field(default=None)  # type: ignore\n\n    use_inductor: bool = True\n    compile_sizes: Optional[List[Union[int, str]]] = Field(default=None)\n    inductor_compile_config: Dict = Field(default_factory=dict)\n    inductor_passes: Dict[str, str] = Field(default_factory=dict)\n\n    use_cudagraph: bool = False\n    cudagraph_num_of_warmups: int = 0\n    cudagraph_capture_sizes: Optional[List[int]] = None\n    cudagraph_copy_inputs: bool = False\n\n    class PassConfig(BaseModel):\n        \"\"\"\n        Configuration for custom Inductor passes.\n        This is separate from general CompilationConfig so that inductor passes\n        don't all have access to full configuration - that would create a cycle\n        as the PassManager is set as a property of config.\n        - dump_graph_stages: list of stages for which we want to dump the graph.\n            Each pass defines its own stages (before, after, maybe in-between).\n        - dump_graph_dir: directory to dump the graphs. Default is .\n        - enable_fusion: whether to enable the custom fusion pass.\n        - enable_reshape: whether to enable the custom reshape elimination pass.\n            TODO better pass enabling system.\n        \"\"\"\n        dump_graph_stages: List[str] = Field(default_factory=list)\n        dump_graph_dir: Path = Field(default=Path(\".\"))\n        enable_fusion: bool = True\n        enable_reshape: bool = True\n\n        def uuid(self):\n            \"\"\"\n            Produces a hash unique to the pass configuration.\n            Any new fields that affect compilation should be added to the hash.\n            Do not include dump_graph_* in the hash - they don't affect\n            compilation.\n            \"\"\"\n            dict_ = self.model_dump(\n                include={\"enable_fusion\", \"enable_reshape\"})\n            encoded = json.dumps(dict_, sort_keys=True).encode(\"utf-8\")\n            return hashlib.sha256(encoded).digest()\n\n        def model_post_init(self, __context: Any) -> None:\n            if not self.enable_reshape and self.enable_fusion:\n                logger.warning_once(\n                    \"Fusion enabled but reshape elimination disabled.\"\n                    \"RMSNorm + quant (fp8) fusion might not work\")\n\n    pass_config: PassConfig = Field(default_factory=PassConfig)\n\n    # not configurable, computed after init\n    max_capture_size: int = PrivateAttr\n    local_cache_dir: str = PrivateAttr  # local cache dir for each rank\n    # optimization:\n    # Intuitively, bs_to_padded_graph_size should be Dict[int, int].\n    # since we know all keys are in a range [0, max_capture_size],\n    # we can optimize it to List[int] for better lookup performance.\n    bs_to_padded_graph_size: List[int] = PrivateAttr\n\n    # keep track of enabled and disabled custom ops\n    enabled_custom_ops: Counter[str] = PrivateAttr\n    disabled_custom_ops: Counter[str] = PrivateAttr\n    traced_files: Set[str] = PrivateAttr\n    compilation_time: float = PrivateAttr\n\n    # Per-model forward context\n    # Map from layer name to the attention cls\n    static_forward_context: Dict[str, Any] = PrivateAttr\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: List[Any] = []\n        factors.append(self.level)\n        factors.append(self.backend)\n        factors.append(self.custom_ops)\n        factors.append(self.splitting_ops)\n        factors.append(self.use_inductor)\n        factors.append(self.inductor_compile_config)\n        factors.append(self.inductor_passes)\n        factors.append(self.pass_config.uuid())\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __repr__(self) -> str:\n        exclude = {\n            \"static_forward_context\",\n            \"enabled_custom_ops\",\n            \"disabled_custom_ops\",\n            \"compilation_time\",\n            \"bs_to_padded_graph_size\",\n            \"pass_config\",\n            \"traced_files\",\n        }\n        return self.model_dump_json(exclude=exclude, exclude_unset=True)\n\n    __str__ = __repr__\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"CompilationConfig\":\n        \"\"\"Parse the CLI value for the compilation config.\"\"\"\n        if cli_value in [\"0\", \"1\", \"2\", \"3\"]:\n            return cls(level=int(cli_value))\n        # do not use `eval`, it is dangerous and can execute arbitrary code\n        dict_value = ast.literal_eval(cli_value)\n        return CompilationConfig.model_validate(dict_value)\n\n    def model_post_init(self, __context: Any) -> None:\n\n        count_none = self.custom_ops.count(\"none\")\n        count_all = self.custom_ops.count(\"all\")\n        assert count_none + count_all <= 1, \"Can only specify 'none' or 'all'\"\n\n        if self.splitting_ops is None:\n            if envs.VLLM_USE_V1:\n                # v1 must split the graph on attention ops\n                # for piecewise cudagraph\n                self.splitting_ops = [\n                    \"vllm.unified_attention\",\n                    \"vllm.unified_attention_with_output\",\n                ]\n            else:\n                # v0 uses full graph compilation\n                self.splitting_ops = []\n\n        for k, v in self.inductor_passes.items():\n            if not isinstance(v, str):\n                assert callable(v), (\n                    f\"pass {k} should be callable or a qualified name\")\n                self.inductor_compile_config[k] = v if isinstance(\n                    v, InductorPass) else CallableInductorPass(v)\n                continue\n\n            # resolve function from qualified name\n            names = v.split(\".\")\n            module = \".\".join(names[:-1])\n            func_name = names[-1]\n            func = __import__(module).__dict__[func_name]\n            self.inductor_compile_config[k] = func if isinstance(\n                func, InductorPass) else CallableInductorPass(func)\n\n        self.enabled_custom_ops = Counter()\n        self.disabled_custom_ops = Counter()\n        self.traced_files = set()\n        self.static_forward_context = {}\n        self.compilation_time = 0.0\n\n    def init_backend(self, vllm_config: \"VllmConfig\") -> Union[str, Callable]:\n        if self.level == CompilationLevel.NO_COMPILATION:\n            raise ValueError(\"No compilation level is set.\")\n\n        from torch._dynamo.backends.registry import list_backends\n        torch_backends = list_backends(exclude_tags=tuple())\n        if self.level in [\n                CompilationLevel.DYNAMO_AS_IS, CompilationLevel.DYNAMO_ONCE\n        ]:\n            if self.backend == \"\":\n                return \"eager\"\n            if self.backend in torch_backends:\n                return self.backend\n            return resolve_obj_by_qualname(self.backend)\n\n        # TODO: pass user-specified backend to piecewise compilation\n        # merge with the config use_inductor\n        assert self.level == CompilationLevel.PIECEWISE\n\n        from vllm.compilation.backends import VllmBackend\n        return VllmBackend(vllm_config)\n\n    def init_with_cudagraph_sizes(self,\n                                  cudagraph_capture_sizes: List[int]) -> None:\n        \"\"\"To complete the initialization of config,\n        we need to know the cudagraph sizes.\"\"\"\n\n        if self.cudagraph_capture_sizes is None:\n            self.cudagraph_capture_sizes = cudagraph_capture_sizes\n        else:\n            # de-duplicate the sizes provided by the config\n            self.cudagraph_capture_sizes = list(\n                set(self.cudagraph_capture_sizes))\n            logger.info((\"cudagraph sizes specified by model runner\"\n                         \" %s is overridden by config %s\"),\n                        cudagraph_capture_sizes, self.cudagraph_capture_sizes)\n\n        computed_compile_sizes = []\n        if self.compile_sizes is not None:\n            # de-duplicate the sizes provided by the config\n            self.compile_sizes = list(set(self.compile_sizes))\n            for x in self.compile_sizes:\n                if isinstance(x, str):\n                    assert x == \"cudagraph_capture_sizes\", \\\n                    \"Unrecognized size type in compile_sizes, \" \\\n                    f\"expect 'cudagraph_capture_sizes', got {x}\"\n                    computed_compile_sizes.extend(self.cudagraph_capture_sizes)\n                else:\n                    assert isinstance(x, int)\n                    computed_compile_sizes.append(x)\n        self.compile_sizes = computed_compile_sizes  # type: ignore\n\n        # sort to make sure cudagraph capture sizes are in descending order\n        self.cudagraph_capture_sizes.sort(reverse=True)\n        self.max_capture_size = self.cudagraph_capture_sizes[\n            0] if self.cudagraph_capture_sizes else 0\n\n        # pre-compute the mapping from batch size to padded graph size\n        self.bs_to_padded_graph_size = [\n            0 for i in range(self.max_capture_size + 1)\n        ]\n        for end, start in zip(self.cudagraph_capture_sizes,\n                              self.cudagraph_capture_sizes[1:] + [0]):\n            for bs in range(start, end):\n                if bs == start:\n                    self.bs_to_padded_graph_size[bs] = start\n                else:\n                    self.bs_to_padded_graph_size[bs] = end\n        self.bs_to_padded_graph_size[\n            self.max_capture_size] = self.max_capture_size\n\n\n@dataclass\nclass VllmConfig:\n    \"\"\"Dataclass which contains all vllm-related configuration. This\n    simplifies passing around the distinct configurations in the codebase.\n    \"\"\"\n\n    model_config: ModelConfig = field(default=None, init=True)  # type: ignore\n    cache_config: CacheConfig = field(default=None, init=True)  # type: ignore\n    parallel_config: ParallelConfig = field(default_factory=ParallelConfig,\n                                            init=True)\n    scheduler_config: SchedulerConfig = field(default_factory=SchedulerConfig,\n                                              init=True)\n    device_config: DeviceConfig = field(default=None,\n                                        init=True)  # type: ignore\n    load_config: LoadConfig = field(default=None, init=True)  # type: ignore\n    lora_config: Optional[LoRAConfig] = None\n    speculative_config: Optional[SpeculativeConfig] = None\n    decoding_config: Optional[DecodingConfig] = None\n    observability_config: Optional[ObservabilityConfig] = None\n    prompt_adapter_config: Optional[PromptAdapterConfig] = None\n    quant_config: Optional[QuantizationConfig] = None\n    compilation_config: CompilationConfig = field(default=None,\n                                                  init=True)  # type: ignore\n    kv_transfer_config: KVTransferConfig = field(default=None,\n                                                 init=True)  # type: ignore\n    # some opaque config, only used to provide additional information\n    # for the hash computation, mainly used for testing and debugging.\n    additional_config: SupportsHash = field(default=None,\n                                            init=True)  # type: ignore\n    instance_id: str = \"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: List[Any] = []\n        # summarize system state\n        from torch._inductor.codecache import CacheBase\n        system_factors = CacheBase.get_system()\n        factors.append(system_factors)\n\n        # summarize pytorch state\n        from torch._inductor.codecache import torch_key\n        torch_factors = torch_key()\n        factors.append(torch_factors)\n\n        # summarize vllm config\n        vllm_factors: List[Any] = []\n        from vllm import __version__\n        vllm_factors.append(__version__)\n        if self.model_config:\n            vllm_factors.append(self.model_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.cache_config:\n            vllm_factors.append(self.cache_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.parallel_config:\n            vllm_factors.append(self.parallel_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.scheduler_config:\n            vllm_factors.append(self.scheduler_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.device_config:\n            vllm_factors.append(self.device_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.load_config:\n            vllm_factors.append(self.load_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.lora_config:\n            vllm_factors.append(self.lora_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.speculative_config:\n            vllm_factors.append(self.speculative_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.decoding_config:\n            vllm_factors.append(self.decoding_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.observability_config:\n            vllm_factors.append(self.observability_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.prompt_adapter_config:\n            vllm_factors.append(self.prompt_adapter_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.quant_config:\n            pass  # should be captured by model_config.quantization\n        if self.compilation_config:\n            vllm_factors.append(self.compilation_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.kv_transfer_config:\n            vllm_factors.append(self.kv_transfer_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.additional_config:\n            vllm_factors.append(self.additional_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        factors.append(vllm_factors)\n\n        hash_str = hashlib.md5(str(factors).encode()).hexdigest()[:10]\n        return hash_str\n\n    def pad_for_cudagraph(self, batch_size: int) -> int:\n        # if batch_size > self.compilation_config.max_capture_size,\n        # it should raise an IndexError.\n        # the caller should make sure the batch_size is within the range,\n        # i.e., batch_size <= self.compilation_config.max_capture_size\n        return self.compilation_config.bs_to_padded_graph_size[batch_size]\n\n    @staticmethod\n    def _get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        \"\"\"Get the quantization config.\"\"\"\n        from vllm.platforms import current_platform\n        if model_config.quantization is not None:\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_quant_config)\n            quant_config = get_quant_config(model_config, load_config)\n            capability_tuple = current_platform.get_device_capability()\n\n            if capability_tuple is not None:\n                capability = capability_tuple.to_int()\n                if capability < quant_config.get_min_capability():\n                    raise ValueError(\n                        f\"The quantization method {model_config.quantization} \"\n                        \"is not supported for the current GPU. Minimum \"\n                        f\"capability: {quant_config.get_min_capability()}. \"\n                        f\"Current capability: {capability}.\")\n            supported_dtypes = quant_config.get_supported_act_dtypes()\n            if model_config.dtype not in supported_dtypes:\n                raise ValueError(\n                    f\"{model_config.dtype} is not supported for quantization \"\n                    f\"method {model_config.quantization}. Supported dtypes: \"\n                    f\"{supported_dtypes}\")\n            return quant_config\n        return None\n\n    def with_hf_config(\n        self,\n        hf_config: PretrainedConfig,\n        architectures: Optional[list[str]] = None,\n    ) -> \"VllmConfig\":\n        if architectures is not None:\n            hf_config = copy.deepcopy(hf_config)\n            hf_config.architectures = architectures\n\n        model_config = copy.deepcopy(self.model_config)\n        model_config.hf_config = hf_config\n\n        return replace(self, model_config=model_config)\n\n    def __post_init__(self):\n        \"\"\"Verify configs are valid & consistent with each other.\n        \"\"\"\n        if self.model_config is not None:\n            self.model_config.verify_async_output_proc(self.parallel_config,\n                                                       self.speculative_config,\n                                                       self.device_config)\n            self.model_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.cache_config is not None:\n            self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.lora_config:\n            self.lora_config.verify_with_cache_config(self.cache_config)\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n        if self.quant_config is None and \\\n            self.model_config is not None and self.load_config is not None:\n            self.quant_config = VllmConfig._get_quantization_config(\n                self.model_config, self.load_config)\n\n        from vllm.platforms import current_platform\n        if self.scheduler_config is not None and \\\n            self.model_config is not None and \\\n            self.scheduler_config.chunked_prefill_enabled and \\\n            self.model_config.dtype == torch.float32 and \\\n            current_platform.get_device_capability() == (7, 5):\n            logger.warning_once(\n                \"Turing devices tensor cores do not support float32 matmul. \"\n                \"To workaround this limitation, vLLM will set 'ieee' input \"\n                \"precision for chunked prefill triton kernels.\")\n\n        if self.compilation_config is None:\n            self.compilation_config = CompilationConfig()\n        if envs.VLLM_USE_V1 and self.model_config is not None and \\\n            not self.model_config.enforce_eager:\n            # NOTE(woosuk): Currently, we use inductor because the piecewise\n            # CUDA graphs do not work properly with the custom CUDA kernels.\n            # FIXME(woosuk): Disable inductor to reduce the compilation time\n            # and avoid any potential issues with the inductor.\n            self.compilation_config.custom_ops = [\"none\"]\n            self.compilation_config.use_cudagraph = True\n            self.compilation_config.use_inductor = True\n            self.compilation_config.cudagraph_num_of_warmups = 1\n            self.compilation_config.pass_config.enable_fusion = False\n            self.compilation_config.pass_config.enable_reshape = False\n            self.compilation_config.level = CompilationLevel.PIECEWISE\n\n        self._set_cudagraph_sizes()\n\n        if self.cache_config is not None and \\\n            self.cache_config.cpu_offload_gb > 0 and \\\n            self.compilation_config.level != CompilationLevel.NO_COMPILATION:\n            logger.warning(\n                \"CPU offload is not supported with `torch.compile` yet.\"\n                \" Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if self.lora_config is not None and self.compilation_config.level !=\\\n             CompilationLevel.NO_COMPILATION:\n            logger.warning(\"LoRA is not supported with `torch.compile` yet. \"\n                           \"Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        current_platform.check_and_update_config(self)\n\n        if not self.instance_id:\n            self.instance_id = random_uuid()[:5]\n\n    def _set_cudagraph_sizes(self):\n        \"\"\"\n        cudagraph batchsize padding logic:\n\n        `[1, 2, 4] + [8 * i for i in range(1, 1025)]` is a list of all possible\n        batch sizes that cudagraph will capture.\n\n        Depending on the engine's configuration of `max_num_seqs`, the\n        candidate batch sizes to capture cudagraph will shrink to the subset\n        which just cover the range of `[1, max_num_seqs]`. In the common case,\n        `max_num_seqs` is 256, and the cudagraph batch sizes will be\n        `[1, 2, 4, 8, 16, 24, 32, 40, ..., 256]`.\n\n        However, if users specify the cudagraph capture sizes through\n        compilation config, we will use the specified sizes instead.\n\n        In the end, `vllm_config.compilation_config.cudagraph_capture_sizes`\n        will be the final sizes to capture cudagraph (in descending order).\n\n        During runtime, if batchsize is larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        no cudagraph will be used.\n        If the batch size is no larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        we can quickly find the padded graph size for a given batch size by\n        looking up `vllm_config.compilation_config.bs_to_padded_graph_size`.\n        \"\"\"\n\n        # calculate the default `batch_size_capture_list`\n        if not envs.VLLM_USE_V1:\n            batch_size_capture_list = []\n            max_batchsize_to_capture = 0\n            if self.scheduler_config is not None and \\\n                self.model_config is not None and \\\n                    not self.model_config.enforce_eager:\n\n                possible_sizes = [1, 2, 4] + [8 * i for i in range(1, 1025)]\n                # find the minimum size that is larger than max_num_seqs,\n                # which then becomes the max_batchsize_to_capture\n                larger_sizes = [\n                    x for x in possible_sizes\n                    if x >= self.scheduler_config.max_num_seqs\n                ]\n                if larger_sizes:\n                    max_batchsize_to_capture = larger_sizes[0]\n                else:\n                    max_batchsize_to_capture = possible_sizes[-1]\n\n                # filter out the sizes that are\n                # larger than max_batchsize_to_capture\n                batch_size_capture_list = [\n                    size for size in possible_sizes\n                    if size <= max_batchsize_to_capture\n                ]\n        else:\n            batch_size_capture_list = []\n            if self.model_config is not None and \\\n                not self.model_config.enforce_eager:\n                batch_size_capture_list = [1, 2, 4\n                                           ] + [i for i in range(8, 513, 8)]\n\n        self.compilation_config.init_with_cudagraph_sizes(\n            batch_size_capture_list)\n\n    def __str__(self):\n        return (\n            f\"model={self.model_config.model!r},\"\n            f\" speculative_config={self.speculative_config!r},\"\n            f\" tokenizer={self.model_config.tokenizer!r}, \"\n            f\"skip_tokenizer_init={self.model_config.skip_tokenizer_init},\"\n            f\" tokenizer_mode={self.model_config.tokenizer_mode}, \"\n            f\"revision={self.model_config.revision}, \"\n            f\"override_neuron_config={self.model_config.override_neuron_config},\"\n            f\" tokenizer_revision={self.model_config.tokenizer_revision}, \"\n            f\"trust_remote_code={self.model_config.trust_remote_code}, \"\n            f\"dtype={self.model_config.dtype}, \"\n            f\"max_seq_len={self.model_config.max_model_len},\"\n            f\" download_dir={self.load_config.download_dir!r}, \"\n            f\"load_format={self.load_config.load_format}, \"\n            f\"tensor_parallel_size={self.parallel_config.tensor_parallel_size},\"\n            f\" pipeline_parallel_size={self.parallel_config.pipeline_parallel_size}, \"  # noqa\n            f\"disable_custom_all_reduce={self.parallel_config.disable_custom_all_reduce}, \"  # noqa\n            f\"quantization={self.model_config.quantization}, \"\n            f\"enforce_eager={self.model_config.enforce_eager}, \"\n            f\"kv_cache_dtype={self.cache_config.cache_dtype}, \"\n            f\" device_config={self.device_config.device}, \"\n            f\"decoding_config={self.decoding_config!r}, \"\n            f\"observability_config={self.observability_config!r}, \"\n            f\"seed={self.model_config.seed}, \"\n            f\"served_model_name={self.model_config.served_model_name}, \"\n            f\"num_scheduler_steps={self.scheduler_config.num_scheduler_steps}, \"\n            f\"multi_step_stream_outputs={self.scheduler_config.multi_step_stream_outputs}, \"  # noqa\n            f\"enable_prefix_caching={self.cache_config.enable_prefix_caching}, \"\n            f\"chunked_prefill_enabled={self.scheduler_config.chunked_prefill_enabled}, \"  # noqa\n            f\"use_async_output_proc={self.model_config.use_async_output_proc}, \"\n            f\"disable_mm_preprocessor_cache={self.model_config.disable_mm_preprocessor_cache!r}, \"  # noqa\n            f\"mm_processor_kwargs={self.model_config.mm_processor_kwargs}, \"\n            f\"pooler_config={self.model_config.pooler_config!r}, \"\n            f\"compilation_config={self.compilation_config!r}\")\n\n\n_current_vllm_config: Optional[VllmConfig] = None\n\n\n@contextmanager\ndef set_current_vllm_config(vllm_config: VllmConfig, check_compile=False):\n    \"\"\"\n    Temporarily set the current VLLM config.\n    Used during model initialization.\n    We save the current VLLM config in a global variable,\n    so that all modules can access it, e.g. custom ops\n    can access the VLLM config to determine how to dispatch.\n    \"\"\"\n    global _current_vllm_config\n    old_vllm_config = _current_vllm_config\n    from vllm.compilation.counter import compilation_counter\n    num_models_seen = compilation_counter.num_models_seen\n    try:\n        _current_vllm_config = vllm_config\n        yield\n    finally:\n        logger.debug(\"enabled custom ops: %s\",\n                     vllm_config.compilation_config.enabled_custom_ops)\n        logger.debug(\"disabled custom ops: %s\",\n                     vllm_config.compilation_config.disabled_custom_ops)\n        if check_compile and \\\n            vllm_config.compilation_config.level == CompilationLevel.PIECEWISE \\\n            and compilation_counter.num_models_seen == num_models_seen:\n            # If the model supports compilation,\n            # compilation_counter.num_models_seen should be increased\n            # by at least 1.\n            # If it is not increased, it means the model does not support\n            # compilation (does not have @support_torch_compile decorator).\n            logger.warning(\n                \"`torch.compile` is turned on, but the model %s\"\n                \" does not support it. Please open an issue on GitHub\"\n                \"if you want it to be supported.\",\n                vllm_config.model_config.model)\n        _current_vllm_config = old_vllm_config\n\n\ndef get_current_vllm_config() -> VllmConfig:\n    if _current_vllm_config is None:\n        # in ci, usually when we test custom ops/modules directly,\n        # we don't set the vllm config. In that case, we set a default\n        # config.\n        logger.warning(\"Current VLLM config is not set.\")\n        from vllm.config import VllmConfig\n        return VllmConfig()\n    return _current_vllm_config\n",
      "diff": "diff --git a/vllm/config.py b/vllm/config.py\nindex f6bd8b1ad..f998502ee 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -739,18 +739,19 @@ class ModelConfig:\n     @property\n     def is_deepseek_mla(self) -> bool:\n         # TODO add deepseek_v3\n-        return hasattr(self.hf_text_config,\n-                       \"model_type\") and (self.hf_text_config.model_type\n-                                          in ('deepseek_v2'))\n+        return (hasattr(self.hf_text_config, \"model_type\")) \\\n+                and (self.hf_text_config.model_type in \\\n+                    ('deepseek_v2', 'deepseek_v3'))\\\n+                and (self.hf_text_config.kv_lora_rank is not None)\n \n     def get_head_size(self) -> int:\n         # TODO remove hard code\n         if self.is_deepseek_mla:\n+            qk_rope_head_dim = getattr(self.hf_text_config, \"qk_rope_head_dim\",\n+                                       0)\n             if self.use_mla:\n-                return self.hf_text_config.kv_lora_rank\n+                return self.hf_text_config.kv_lora_rank + qk_rope_head_dim\n             else:\n-                qk_rope_head_dim = getattr(self.hf_text_config,\n-                                           \"qk_rope_head_dim\", 0)\n                 qk_nope_head_dim = getattr(self.hf_text_config,\n                                            \"qk_nope_head_dim\", 0)\n                 if qk_rope_head_dim and qk_nope_head_dim:\n@@ -969,6 +970,32 @@ class ModelConfig:\n \n     @property\n     def use_mla(self) -> bool:\n+        if self.quantization is not None and self.quantization not in [\\\n+            \"fp8\", \"compressed-tensors\"]:\n+            logger.warning(\n+                \"MLA is not supported with %s quantization. \"\n+                \"Disabling MLA.\", self.quantization)\n+            return False\n+\n+        # If using a \"compressed-tensors\" checkpoint, check that all groups\n+        # have fp8 for both weights and activations.\n+        if self.quantization == \"compressed-tensors\":\n+            quant_config = self._parse_quant_hf_config()\n+            for group_name, cfg in quant_config.get(\"config_groups\",\n+                                                    (\"\", {})).items():\n+                act_cfg = cfg.get(\"input_activations\", {})\n+                act_type = None if act_cfg is None else act_cfg.get(\"type\", \"\")\n+                w_cfg = cfg.get(\"weights\", {})\n+                w_type = None if w_cfg is None else w_cfg.get(\"type\", \"\")\n+                if act_type != \"fp8\" or w_type != \"fp8\":\n+                    logger.warning(\n+                        \"compressed-tensors MLA support requires fp8 \"\n+                        \"activations and weights in group '%s', but got \"\n+                        \"activations type '%s' and weights type '%s'.\\n \"\n+                        \"Full config: %s\", group_name, act_type, w_type,\n+                        quant_config)\n+                    return False\n+\n         use_mla = (self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE)\n         return use_mla",
      "change_type": "modified",
      "lines_added": 34,
      "lines_removed": 7
    },
    {
      "file_path": "vllm/envs.py",
      "old_content": "import os\nimport tempfile\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n\nif TYPE_CHECKING:\n    VLLM_HOST_IP: str = \"\"\n    VLLM_PORT: Optional[int] = None\n    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()\n    VLLM_USE_MODELSCOPE: bool = False\n    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60\n    VLLM_NCCL_SO_PATH: Optional[str] = None\n    LD_LIBRARY_PATH: Optional[str] = None\n    VLLM_USE_TRITON_FLASH_ATTN: bool = False\n    VLLM_FLASH_ATTN_VERSION: Optional[int] = None\n    LOCAL_RANK: int = 0\n    CUDA_VISIBLE_DEVICES: Optional[str] = None\n    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60\n    VLLM_API_KEY: Optional[str] = None\n    S3_ACCESS_KEY_ID: Optional[str] = None\n    S3_SECRET_ACCESS_KEY: Optional[str] = None\n    S3_ENDPOINT_URL: Optional[str] = None\n    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")\n    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")\n    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"\n    VLLM_NO_USAGE_STATS: bool = False\n    VLLM_DO_NOT_TRACK: bool = False\n    VLLM_USAGE_SOURCE: str = \"\"\n    VLLM_CONFIGURE_LOGGING: int = 1\n    VLLM_LOGGING_LEVEL: str = \"INFO\"\n    VLLM_LOGGING_PREFIX: str = \"\"\n    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None\n    VLLM_TRACE_FUNCTION: int = 0\n    VLLM_ATTENTION_BACKEND: Optional[str] = None\n    VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None\n    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False\n    VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False\n    VLLM_PP_LAYER_PARTITION: Optional[str] = None\n    VLLM_CPU_KVCACHE_SPACE: int = 0\n    VLLM_CPU_OMP_THREADS_BIND: str = \"\"\n    VLLM_OPENVINO_DEVICE: str = \"CPU\"\n    VLLM_OPENVINO_KVCACHE_SPACE: int = 0\n    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None\n    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False\n    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")\n    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024\n    VLLM_USE_RAY_SPMD_WORKER: bool = False\n    VLLM_USE_RAY_COMPILED_DAG: bool = False\n    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True\n    VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False\n    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"\n    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")\n    VLLM_IMAGE_FETCH_TIMEOUT: int = 5\n    VLLM_VIDEO_FETCH_TIMEOUT: int = 30\n    VLLM_AUDIO_FETCH_TIMEOUT: int = 10\n    VLLM_TARGET_DEVICE: str = \"cuda\"\n    MAX_JOBS: Optional[str] = None\n    NVCC_THREADS: Optional[str] = None\n    VLLM_USE_PRECOMPILED: bool = False\n    VLLM_NO_DEPRECATION_WARNING: bool = False\n    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False\n    CMAKE_BUILD_TYPE: Optional[str] = None\n    VERBOSE: bool = False\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False\n    VLLM_TEST_FORCE_FP8_MARLIN: bool = False\n    VLLM_RPC_TIMEOUT: int = 10000  # ms\n    VLLM_PLUGINS: Optional[List[str]] = None\n    VLLM_TORCH_PROFILER_DIR: Optional[str] = None\n    VLLM_USE_TRITON_AWQ: bool = False\n    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n    VLLM_SKIP_P2P_CHECK: bool = False\n    VLLM_DISABLED_KERNELS: List[str] = []\n    VLLM_USE_V1: bool = False\n    VLLM_ENABLE_V1_MULTIPROCESSING: bool = True\n    VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n    VLLM_DISABLE_COMPILE_CACHE: bool = False\n    K_SCALE_CONSTANT: int = 200\n    V_SCALE_CONSTANT: int = 100\n    VLLM_SERVER_DEV_MODE: bool = False\n    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n    VLLM_MLA_DISABLE: bool = False\n    VLLM_MLA_PERFORM_MATRIX_ABSORPTION: bool = True\n\n\ndef get_default_cache_root():\n    return os.getenv(\n        \"XDG_CACHE_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n    )\n\n\ndef get_default_config_root():\n    return os.getenv(\n        \"XDG_CONFIG_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n    )\n\n\ndef maybe_convert_int(value: Optional[str]) -> Optional[int]:\n    if value is None:\n        return None\n    return int(value)\n\n\n# The begin-* and end* here are used by the documentation generator\n# to extract the used env vars.\n\n# begin-env-vars-definition\n\nenvironment_variables: Dict[str, Callable[[], Any]] = {\n\n    # ================== Installation Time Env Vars ==================\n\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, neuron, cpu, openvino]\n    \"VLLM_TARGET_DEVICE\":\n    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),\n\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\":\n    lambda: os.getenv(\"MAX_JOBS\", None),\n\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\":\n    lambda: os.getenv(\"NVCC_THREADS\", None),\n\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\":\n    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")) or bool(\n        os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),\n\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\":\n    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),\n\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\":\n    lambda: bool(int(os.getenv('VERBOSE', '0'))),\n\n    # Root directory for VLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )),\n\n    # ================== Runtime Env Vars ==================\n\n    # Root directory for VLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )),\n\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    'VLLM_HOST_IP':\n    lambda: os.getenv('VLLM_HOST_IP', \"\"),\n\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    # '0' is used to make mypy happy\n    'VLLM_PORT':\n    lambda: int(os.getenv('VLLM_PORT', '0'))\n    if 'VLLM_PORT' in os.environ else None,\n\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    'VLLM_RPC_BASE_PATH':\n    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),\n\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\":\n    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),\n\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\":\n    lambda: os.environ.get(\"CUDA_HOME\", None),\n\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\":\n    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\":\n    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n\n    # flag to control if vllm should use triton flash attention\n    \"VLLM_USE_TRITON_FLASH_ATTN\":\n    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Force vllm to use a specific flash-attention version (2 or 3), only valid\n    # when using the flash-attention backend.\n    \"VLLM_FLASH_ATTN_VERSION\":\n    lambda: maybe_convert_int(os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)),\n\n    # Internal flag to enable Dynamo fullgraph capture\n    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":\n    lambda: bool(\n        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),\n\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\":\n    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\":\n    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":\n    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),\n\n    # API key for VLLM API server\n    \"VLLM_API_KEY\":\n    lambda: os.environ.get(\"VLLM_API_KEY\", None),\n\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\":\n    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\":\n    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\":\n    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\":\n    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),\n    \"VLLM_NO_USAGE_STATS\":\n    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DO_NOT_TRACK\":\n    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(\n        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",\n    \"VLLM_USAGE_SOURCE\":\n    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\":\n    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),\n    \"VLLM_LOGGING_CONFIG_PATH\":\n    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\":\n    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),\n\n    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages\n    \"VLLM_LOGGING_PREFIX\":\n    lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\":\n    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n\n    # Backend for attention computation\n    # Available options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"XFORMERS\": use XFormers\n    # - \"ROCM_FLASH\": use ROCmFlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    \"VLLM_ATTENTION_BACKEND\":\n    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),\n\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\":\n    lambda: bool(int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"]))\n    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ else None,\n\n    # If set, vllm will force flashinfer to use tensor cores;\n    # otherwise will use heuristic based on model architecture.\n    \"VLLM_FLASHINFER_FORCE_TENSOR_CORES\":\n    lambda: bool(int(os.getenv(\"VLLM_FLASHINFER_FORCE_TENSOR_CORES\", \"0\"))),\n\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\":\n    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n\n    # (CPU backend only) CPU key-value cache space.\n    # default is 4GB\n    \"VLLM_CPU_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),\n\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\":\n    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),\n\n    # OpenVINO device selection\n    # default is CPU\n    \"VLLM_OPENVINO_DEVICE\":\n    lambda: os.getenv(\"VLLM_OPENVINO_DEVICE\", \"CPU\").upper(),\n\n    # OpenVINO key-value cache space\n    # default is 4GB\n    \"VLLM_OPENVINO_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),\n\n    # OpenVINO KV cache precision\n    # default is bf16 if natively supported by platform, otherwise f16\n    # To enable KV cache compression, please, explicitly specify u8\n    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":\n    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),\n\n    # Enables weights compression during model export via HF Optimum\n    # default is False\n    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":\n    lambda: bool(os.getenv(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", False)),\n\n    # If the env var is set, then all workers will execute as separate\n    # processes from the engine, and we use the same mechanism to trigger\n    # execution on all workers.\n    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.\n    \"VLLM_USE_RAY_SPMD_WORKER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),\n\n    # If the env var is set, it uses the Ray's compiled DAG API\n    # which optimizes the control plane overhead.\n    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n    \"VLLM_USE_RAY_COMPILED_DAG\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),\n\n    # If the env var is set, it uses NCCL for communication in\n    # Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))\n                 ),\n\n    # If the env var is set, it enables GPU communication overlap\n    # (experimental feature) in Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))\n                 ),\n\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\":\n    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),\n\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )),\n\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n\n    # Timeout for fetching videos when serving multimodal models\n    # Default is 15 seconds\n    \"VLLM_VIDEO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"15\")),\n\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 10 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")),\n\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),\n\n    # If set, vllm will skip the deprecation warnings.\n    \"VLLM_NO_DEPRECATION_WARNING\":\n    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),\n\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":\n    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),\n\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\":\n    lambda:\n    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\":\n    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),\n\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\":\n    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[\n        \"VLLM_PLUGINS\"].split(\",\"),\n\n    # Enables torch profiler if set. Path to the directory where torch profiler\n    # traces are saved. Note that it must be an absolute path.\n    \"VLLM_TORCH_PROFILER_DIR\":\n    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os\n             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),\n\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # By default, vLLM will check the peer-to-peer capability itself,\n    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    # If this env var is set to 1, vLLM will skip the peer-to-peer check,\n    # and trust the driver's peer-to-peer capability report.\n    \"VLLM_SKIP_P2P_CHECK\":\n    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\":\n    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[\n        \"VLLM_DISABLED_KERNELS\"].split(\",\"),\n\n    # If set, use the V1 code path.\n    \"VLLM_USE_V1\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"0\"))),\n\n    # Divisor for dynamic key scale factor calculation for FP8 KV Cache\n    \"K_SCALE_CONSTANT\":\n    lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),\n\n    # Divisor for dynamic value scale factor calculation for FP8 KV Cache\n    \"V_SCALE_CONSTANT\":\n    lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),\n    # If set, enable multiprocessing in LLM for the V1 code path.\n    \"VLLM_ENABLE_V1_MULTIPROCESSING\":\n    lambda: bool(int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))),\n    \"VLLM_LOG_BATCHSIZE_INTERVAL\":\n    lambda: float(os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")),\n    \"VLLM_DISABLE_COMPILE_CACHE\":\n    lambda: bool(int(os.getenv(\"VLLM_DISABLE_COMPILE_CACHE\", \"0\"))),\n\n    # If set, vllm will run in development mode, which will enable\n    # some additional endpoints for developing and debugging,\n    # e.g. `/reset_prefix_cache`\n    \"VLLM_SERVER_DEV_MODE\":\n    lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n\n    # Controls the maximum number of requests to handle in a\n    # single asyncio task when processing per-token outputs in the\n    # V1 AsyncLLM interface. It is applicable when handling a high\n    # concurrency of streaming requests.\n    # Setting this too high can result in a higher variance of\n    # inter-message latencies. Setting it too low can negatively impact\n    # TTFT and overall throughput.\n    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n\n    # If set, vLLM will disable the MLA attention optimizations.\n    \"VLLM_MLA_DISABLE\":\n    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\n\n    # Flag that can control whether or not we perform matrix-absorption for MLA\n    # decode, i.e. absorb W_UK into W_Q/W_UK and W_UV into W_O, absorbing the\n    # matrices reduces the runtime FLOPs needed to compute MLA but requires\n    # storing more weights, W_Q_UK and W_UV_O, so can increase memory usage,\n    # the is enabled by default\n    \"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\":\n    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\")))\n}\n\n# end-env-vars-definition\n\n\ndef __getattr__(name: str):\n    # lazy evaluation of environment variables\n    if name in environment_variables:\n        return environment_variables[name]()\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef __dir__():\n    return list(environment_variables.keys())\n",
      "diff": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 2a18e3b9b..25098070b 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -79,6 +79,7 @@ if TYPE_CHECKING:\n     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n     VLLM_MLA_DISABLE: bool = False\n     VLLM_MLA_PERFORM_MATRIX_ABSORPTION: bool = True\n+    VLLM_MLA_DISABLE_REQUANTIZATION: bool = False\n \n \n def get_default_cache_root():\n@@ -519,7 +520,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # storing more weights, W_Q_UK and W_UV_O, so can increase memory usage,\n     # the is enabled by default\n     \"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\":\n-    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\")))\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\"))),\n+\n+    # When running MLA with matrix-absorption enabled and fp8 quantized weights\n+    # we perform the matrix-absorption in float32 precision, after the matrices\n+    # are absorbed we requantize the weights back to fp8, this flag can be used\n+    # to disable the requantization step, and instead convert the absorbed\n+    # matrices to match the activation type. This can lead to higher memory and\n+    # compute usage but better preserves the accuracy of the original model.\n+    \"VLLM_MLA_DISABLE_REQUANTIZATION\":\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE_REQUANTIZATION\", \"0\")))\n }\n \n # end-env-vars-definition",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
      "old_content": "# Adapted from https://github.com/sgl-project/sglang/pull/2575\nimport functools\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nimport triton\nimport triton.language as tl\n\nfrom vllm import _custom_ops as ops\nfrom vllm.logger import init_logger\nfrom vllm.platforms import current_platform\n\nlogger = init_logger(__name__)\n\n\ndef apply_w8a8_block_fp8_linear(\n    input: torch.Tensor,\n    weight: torch.Tensor,\n    block_size: List[int],\n    weight_scale: torch.Tensor,\n    input_scale: Optional[torch.Tensor] = None,\n    bias: Optional[torch.Tensor] = None,\n    cutlass_block_fp8_supported: bool = True,\n) -> torch.Tensor:\n    assert input_scale is None\n    # View input as 2D matrix for fp8 methods\n    input_2d = input.view(-1, input.shape[-1])\n    output_shape = [*input.shape[:-1], weight.shape[0]]\n\n    shape_supported_by_cutlass = (weight.shape[0] % 128 == 0\n                                  and weight.shape[1] % 128 == 0)\n    if cutlass_block_fp8_supported and shape_supported_by_cutlass:\n        q_input, x_scale = per_token_group_quant_fp8(input_2d,\n                                                     block_size[1],\n                                                     column_major_scales=True)\n        output = ops.cutlass_scaled_mm(q_input,\n                                       weight.T,\n                                       out_dtype=input.dtype,\n                                       scale_a=x_scale,\n                                       scale_b=weight_scale.T)\n    else:\n        q_input, x_scale = per_token_group_quant_fp8(input_2d,\n                                                     block_size[1],\n                                                     column_major_scales=False)\n        output = w8a8_block_fp8_matmul(q_input,\n                                       weight,\n                                       x_scale,\n                                       weight_scale,\n                                       block_size,\n                                       output_dtype=input.dtype)\n    if bias is not None:\n        output = output + bias\n    return output.to(dtype=input.dtype).view(*output_shape)\n\n\ndef input_to_float8(\n        x: torch.Tensor,\n        dtype: Optional[torch.dtype] = None\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"This function quantizes input values to float8 values \"\n    \"with tensor-wise quantization.\"\"\"\n    if dtype is None:\n        dtype = (torch.float8_e4m3fnuz\n                 if current_platform.is_rocm() else torch.float8_e4m3fn)\n    finfo = torch.finfo(dtype)\n    min_val, max_val = x.aminmax()\n    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)\n    scale = finfo.max / amax\n    x_scl_sat = (x * scale).clamp(min=finfo.min, max=finfo.max)\n    return x_scl_sat.to(dtype).contiguous(), scale.float().reciprocal()\n\n\ndef block_quant_to_tensor_quant(\n    x_q_block: torch.Tensor,\n    x_s: torch.Tensor,\n    block_size: List[int],\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"This function converts block-wise quantization to tensor-wise\n    quantization. The inputs are block-wise quantization tensor `x_q_block`,\n    block-wise quantization scale and the block size.\n    The outputs are tensor-wise quantization tensor and tensor-wise\n    quantization scale. Note only float8 is supported for now.\n    \"\"\"\n    block_n, block_k = block_size[0], block_size[1]\n    n, k = x_q_block.shape\n    n_tiles = (n + block_n - 1) // block_n\n    k_tiles = (k + block_k - 1) // block_k\n    assert n_tiles == x_s.shape[0]\n    assert k_tiles == x_s.shape[1]\n\n    x_dq_block = x_q_block.to(torch.float32)\n\n    x_dq_block_tiles = [[\n        x_dq_block[\n            j * block_n:min((j + 1) * block_n, n),\n            i * block_k:min((i + 1) * block_k, k),\n        ] for i in range(k_tiles)\n    ] for j in range(n_tiles)]\n\n    for i in range(k_tiles):\n        for j in range(n_tiles):\n            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n\n    x_q_tensor, scale = input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n    return x_q_tensor, scale\n\n\n@triton.jit\ndef _per_token_group_quant_fp8(\n    # Pointers to inputs and output\n    y_ptr,\n    y_q_ptr,\n    y_s_ptr,\n    group_size,\n    # Avoid to divide zero\n    eps,\n    # Information for float8\n    fp8_min,\n    fp8_max,\n    # Meta-parameters\n    BLOCK: tl.constexpr,\n):\n    \"\"\"A Triton-accelerated function to perform per-token-group\n    quantization on a tensor.\n    This function converts the tensor values into float8 values.\n    \"\"\"\n    # Map the program id to the row of X and Y it should compute.\n    g_id = tl.program_id(0)\n    y_ptr += g_id * group_size\n    y_q_ptr += g_id * group_size\n    y_s_ptr += g_id\n\n    cols = tl.arange(0, BLOCK)  # N <= BLOCK\n    mask = cols < group_size\n\n    y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n    # Quant\n    _absmax = tl.maximum(tl.max(tl.abs(y)), eps)\n    y_s = _absmax / fp8_max\n    y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)\n\n    tl.store(y_q_ptr + cols, y_q, mask=mask)\n    tl.store(y_s_ptr, y_s)\n\n\n@triton.jit\ndef _per_token_group_quant_fp8_colmajor(\n    # Pointers to inputs and output\n    y_ptr,\n    y_q_ptr,\n    y_s_ptr,\n    group_size,\n    # Num columns of y\n    y_num_columns,\n    # Stride from one column to the next of y_s\n    y_s_col_stride,\n    # Avoid to divide zero\n    eps,\n    # Information for float8\n    fp8_min,\n    fp8_max,\n    # Meta-parameters\n    BLOCK: tl.constexpr,\n):\n    \"\"\"A Triton-accelerated function to perform per-token-group\n    quantization on a tensor.\n    This function converts the tensor values into float8 values.\n    \"\"\"\n    # Map the program id to the row of X and Y it should compute.\n    g_id = tl.program_id(0)\n    y_ptr += g_id * group_size\n    y_q_ptr += g_id * group_size\n\n    # Convert g_id the flattened block coordinate to 2D so we can index\n    # into the output y_scales matrix\n    blocks_per_row = y_num_columns // group_size\n    scale_col = g_id % blocks_per_row\n    scale_row = g_id // blocks_per_row\n    y_s_ptr += scale_col * y_s_col_stride + scale_row\n\n    cols = tl.arange(0, BLOCK)  # group_size <= BLOCK\n    mask = cols < group_size\n\n    y = tl.load(y_ptr + cols, mask=mask, other=0.0).to(tl.float32)\n    # Quant\n    _absmax = tl.maximum(tl.max(tl.abs(y)), eps)\n    y_s = _absmax / fp8_max\n    y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)\n\n    tl.store(y_q_ptr + cols, y_q, mask=mask)\n    tl.store(y_s_ptr, y_s)\n\n\ndef per_token_group_quant_fp8(\n    x: torch.Tensor,\n    group_size: int,\n    eps: float = 1e-10,\n    dtype: Optional[torch.dtype] = None,\n    column_major_scales: bool = False,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Function to perform per-token-group quantization on an input tensor `x`.\n    It converts the tensor values into signed float8 values and returns the\n    quantized tensor along with the scaling factor used for quantization.\n    Args:\n        x: The input tensor with ndim >= 2.\n        group_size: The group size used for quantization.\n        eps: The minimum to avoid dividing zero.\n        dtype: The dype of output tensor. Note that only `torch.float8_e4m3fn`\n        is supported for now.\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: The quantized tensor and the\n        scaling factor for quantization.\n    \"\"\"\n    if dtype is None:\n        dtype = (torch.float8_e4m3fnuz\n                 if current_platform.is_rocm() else torch.float8_e4m3fn)\n    assert (x.shape[-1] % group_size == 0), (\n        f\"the last dimension of `x` {x.shape[-1]} must be divisible \"\n        f\"by `group_size` {group_size}\")\n    assert x.is_contiguous(), \"`x` must be contiguous\"\n\n    finfo = torch.finfo(dtype)\n    fp8_min = finfo.min\n    fp8_max = finfo.max\n\n    x_q = torch.empty_like(x, device=x.device, dtype=dtype)\n    M = x.numel() // group_size\n    N = group_size\n    if column_major_scales:\n        shape = (x.shape[-1] // group_size, ) + x.shape[:-1]\n        x_s = torch.empty(shape, device=x.device,\n                          dtype=torch.float32).permute(-1, -2)\n    else:\n        shape = x.shape[:-1] + (x.shape[-1] // group_size, )\n        x_s = torch.empty(shape, device=x.device, dtype=torch.float32)\n\n    BLOCK = triton.next_power_of_2(N)\n    # heuristics for number of warps\n    num_warps = min(max(BLOCK // 256, 1), 8)\n    num_stages = 1\n    if column_major_scales:\n        _per_token_group_quant_fp8_colmajor[(M, )](\n            x,\n            x_q,\n            x_s,\n            group_size,\n            x.shape[1],\n            x_s.stride(1),\n            eps,\n            fp8_min=fp8_min,\n            fp8_max=fp8_max,\n            BLOCK=BLOCK,\n            num_warps=num_warps,\n            num_stages=num_stages,\n        )\n    else:\n        _per_token_group_quant_fp8[(M, )](\n            x,\n            x_q,\n            x_s,\n            group_size,\n            eps,\n            fp8_min=fp8_min,\n            fp8_max=fp8_max,\n            BLOCK=BLOCK,\n            num_warps=num_warps,\n            num_stages=num_stages,\n        )\n\n    return x_q, x_s\n\n\n@triton.jit\ndef _w8a8_block_fp8_matmul(\n    # Pointers to inputs and output\n    A,\n    B,\n    C,\n    As,\n    Bs,\n    # Shape for matmul\n    M,\n    N,\n    K,\n    # Block size for block-wise quantization\n    group_n,\n    group_k,\n    # Stride for inputs and output\n    stride_am,\n    stride_ak,\n    stride_bk,\n    stride_bn,\n    stride_cm,\n    stride_cn,\n    stride_As_m,\n    stride_As_k,\n    stride_Bs_k,\n    stride_Bs_n,\n    # Meta-parameters\n    BLOCK_SIZE_M: tl.constexpr,\n    BLOCK_SIZE_N: tl.constexpr,\n    BLOCK_SIZE_K: tl.constexpr,\n    GROUP_SIZE_M: tl.constexpr,\n):\n    \"\"\"Triton-accelerated function used to perform linear operations (dot\n    product) on input tensors `A` and `B` with block-wise quantization, and\n    store the result in output tensor `C`.\n    \"\"\"\n\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + (pid % group_size_m)\n    pid_n = (pid % num_pid_in_group) // group_size_m\n\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = B + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n\n    As_ptrs = As + offs_am * stride_As_m\n    offs_bsn = offs_bn // group_n\n    Bs_ptrs = Bs + offs_bsn * stride_Bs_n\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs,\n                    mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n                    other=0.0)\n        b = tl.load(b_ptrs,\n                    mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n                    other=0.0)\n\n        k_start = k * BLOCK_SIZE_K\n        offs_ks = k_start // group_k\n        a_s = tl.load(As_ptrs + offs_ks * stride_As_k)\n        b_s = tl.load(Bs_ptrs + offs_ks * stride_Bs_k)\n\n        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n\n    if C.dtype.element_ty == tl.bfloat16:\n        c = accumulator.to(tl.bfloat16)\n    elif C.dtype.element_ty == tl.float16:\n        c = accumulator.to(tl.float16)\n    else:\n        c = accumulator.to(tl.float32)\n\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n\n\n@functools.lru_cache\ndef get_w8a8_block_fp8_configs(N: int, K: int, block_n: int,\n                               block_k: int) -> Optional[Dict[int, Any]]:\n    \"\"\"\n    Return optimized configurations for the w8a8 block fp8 kernel.\n    The return value will be a dictionary that maps an irregular grid of\n    batch sizes to configurations of the w8a8 block fp8 kernel. To evaluate the\n    kernel on a given batch size bs, the closest batch size in the grid should\n    be picked and the associated configuration chosen to invoke the kernel.\n    \"\"\"\n\n    # First look up if an optimized configuration is available in the configs\n    # directory\n    device_name = current_platform.get_device_name().replace(\" \", \"_\")\n    json_file_name = f\"N={N},K={K},device_name={device_name},dtype=fp8_w8a8,block_shape=[{block_n}, {block_k}].json\"  # noqa: E501\n\n    config_file_path = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"configs\", json_file_name)\n    if os.path.exists(config_file_path):\n        with open(config_file_path) as f:\n            logger.info(\n                \"Using configuration from %s for W8A8 Block FP8 kernel.\",\n                config_file_path,\n            )\n            # If a configuration has been found, return it\n            return {int(key): val for key, val in json.load(f).items()}\n\n    # If no optimized configuration is available, we will use the default\n    # configuration\n    logger.warning(\n        \"Using default W8A8 Block FP8 kernel config. Performance might \"\n        \"be sub-optimal! Config file not found at %s\",\n        config_file_path,\n    )\n    return None\n\n\ndef w8a8_block_fp8_matmul(\n    A: torch.Tensor,\n    B: torch.Tensor,\n    As: torch.Tensor,\n    Bs: torch.Tensor,\n    block_size: List[int],\n    output_dtype: torch.dtype = torch.float16,\n) -> torch.Tensor:\n    \"\"\"This function performs matrix multiplication with block-wise\n    quantization.\n    It takes two input tensors `A` and `B` with scales `As` and `Bs`.\n    The output is returned in the specified `output_dtype`.\n    Args:\n        A: The input tensor, e.g., activation.\n        B: The input tensor, e.g., weight.\n        As: The per-token-group quantization scale for `A`.\n        Bs: The per-block quantization scale for `B`.\n        block_size: The block size for per-block quantization. It should\n        be 2-dim, e.g., [128, 128].\n        output_dytpe: The dtype of the returned tensor.\n    Returns:\n        torch.Tensor: The result of matmul.\n    \"\"\"\n    assert len(block_size) == 2\n    block_n, block_k = block_size[0], block_size[1]\n\n    assert A.shape[-1] == B.shape[-1]\n    assert A.shape[:-1] == As.shape[:-1] and A.is_contiguous()\n    assert triton.cdiv(A.shape[-1], block_k) == As.shape[-1]\n    M = A.numel() // A.shape[-1]\n\n    assert B.ndim == 2 and B.is_contiguous() and Bs.ndim == 2\n    N, K = B.shape\n    assert triton.cdiv(N, block_n) == Bs.shape[0]\n    assert triton.cdiv(K, block_k) == Bs.shape[1]\n\n    C_shape = A.shape[:-1] + (N, )\n    C = A.new_empty(C_shape, dtype=output_dtype)\n\n    configs = get_w8a8_block_fp8_configs(N, K, block_size[0], block_size[1])\n    if configs:\n        # Get the optimal config if there is one\n        config = configs[min(configs.keys(), key=lambda x: abs(x - M))]\n    else:\n        # Default config\n        # Block-wise quant: BLOCK_SIZE_N must be divisible by block_size[0]\n        # BLOCK_SIZE_K must be divisible by block_size[1]\n        config = {\n            \"BLOCK_SIZE_M\": 64,\n            \"BLOCK_SIZE_N\": block_size[0],\n            \"BLOCK_SIZE_K\": block_size[1],\n            \"GROUP_SIZE_M\": 32,\n            \"num_warps\": 4,\n            \"num_stages\": 2,\n        }\n\n    def grid(META):\n        return (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) *\n                triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), )\n\n    _w8a8_block_fp8_matmul[grid](\n        A,\n        B,\n        C,\n        As,\n        Bs,\n        M,\n        N,\n        K,\n        block_n,\n        block_k,\n        A.stride(-2),\n        A.stride(-1),\n        B.stride(1),\n        B.stride(0),\n        C.stride(-2),\n        C.stride(-1),\n        As.stride(-2),\n        As.stride(-1),\n        Bs.stride(1),\n        Bs.stride(0),\n        **config,\n    )\n\n    return C\n",
      "diff": "diff --git a/vllm/model_executor/layers/quantization/utils/fp8_utils.py b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\nindex ccebff341..850820f66 100644\n--- a/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n@@ -2,7 +2,7 @@\n import functools\n import json\n import os\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import triton\n@@ -10,10 +10,24 @@ import triton.language as tl\n \n from vllm import _custom_ops as ops\n from vllm.logger import init_logger\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    _normalize_quant_group_shape, scaled_dequantize)\n+from vllm.model_executor.layers.quantization.utils.w8a8_utils import (\n+    apply_fp8_linear)\n from vllm.platforms import current_platform\n \n logger = init_logger(__name__)\n \n+current_platform_fp8_dtype = (torch.float8_e4m3fnuz\n+                              if current_platform.is_rocm() else\n+                              torch.float8_e4m3fn)\n+\n+\n+def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:\n+    if isinstance(x, torch.Tensor):\n+        x = x.dtype\n+    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz\n+\n \n def apply_w8a8_block_fp8_linear(\n     input: torch.Tensor,\n@@ -55,6 +69,42 @@ def apply_w8a8_block_fp8_linear(\n     return output.to(dtype=input.dtype).view(*output_shape)\n \n \n+# Unify the interface between `apply_w8a8_block_fp8_linear` and\n+# `apply_fp8_linear`\n+# NOTE(lucas): this is quite messy, we should think through this more formally\n+def apply_fp8_linear_generic(\n+        input: torch.Tensor,\n+        weight: torch.Tensor,\n+        weight_scale: torch.Tensor,\n+        input_group_shape: Tuple[int, int],\n+        weight_group_shape: Tuple[int, int],\n+        input_scale: Optional[torch.Tensor] = None,  # static scale if one\n+) -> torch.Tensor:\n+    # View input as 2D matrix for fp8 methods\n+    input = input.view(-1, input.shape[-1])\n+\n+    weight_group_shape = _normalize_quant_group_shape(\\\n+        weight, weight_group_shape)\n+    input_group_shape = _normalize_quant_group_shape(input, input_group_shape)\n+\n+    def is_dim_blocked(dim, shape, group_shape):\n+        return group_shape < shape[dim] and group_shape > 1\n+\n+    if is_dim_blocked(0, weight.shape, weight_group_shape[0])\\\n+     and is_dim_blocked(1, weight.shape, weight_group_shape[1]) and\\\n+     input_group_shape == (1, weight_group_shape[1]):\n+        return apply_w8a8_block_fp8_linear(input, weight,\n+                                           list(weight_group_shape),\n+                                           weight_scale)\n+    else:\n+        # Despite having linear in the it doesn't conform to\n+        # `torch.nn.functional.linear` which is defined as `input @ weight.T`\n+        # so we explicitly transpose the weight matrix here\n+        return apply_fp8_linear(input, weight.T, weight_scale.T,\n+                         use_per_token_if_dynamic=\\\n+                             (input_group_shape == (1, input.shape[1])))\n+\n+\n def input_to_float8(\n         x: torch.Tensor,\n         dtype: Optional[torch.dtype] = None\n@@ -75,7 +125,6 @@ def input_to_float8(\n def block_quant_to_tensor_quant(\n     x_q_block: torch.Tensor,\n     x_s: torch.Tensor,\n-    block_size: List[int],\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"This function converts block-wise quantization to tensor-wise\n     quantization. The inputs are block-wise quantization tensor `x_q_block`,\n@@ -83,26 +132,7 @@ def block_quant_to_tensor_quant(\n     The outputs are tensor-wise quantization tensor and tensor-wise\n     quantization scale. Note only float8 is supported for now.\n     \"\"\"\n-    block_n, block_k = block_size[0], block_size[1]\n-    n, k = x_q_block.shape\n-    n_tiles = (n + block_n - 1) // block_n\n-    k_tiles = (k + block_k - 1) // block_k\n-    assert n_tiles == x_s.shape[0]\n-    assert k_tiles == x_s.shape[1]\n-\n-    x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [[\n-        x_dq_block[\n-            j * block_n:min((j + 1) * block_n, n),\n-            i * block_k:min((i + 1) * block_k, k),\n-        ] for i in range(k_tiles)\n-    ] for j in range(n_tiles)]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n-\n+    x_dq_block = scaled_dequantize(x_q_block, x_s)\n     x_q_tensor, scale = input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n     return x_q_tensor, scale",
      "change_type": "modified",
      "lines_added": 53,
      "lines_removed": 23
    },
    {
      "file_path": "vllm/model_executor/layers/quantization/utils/quant_utils.py",
      "old_content": "\"\"\"This file is used for /tests and /benchmarks\"\"\"\nfrom typing import List, Optional\n\nimport numpy\nimport torch\n\nfrom vllm.model_executor.layers.quantization.qqq import (\n    MARLIN_QQQ_SUPPORTED_NUM_BITS)\nfrom vllm.scalar_type import ScalarType, scalar_types\n\nSUPPORTED_GPTQ_QUANT_TYPES = [scalar_types.uint4b8, scalar_types.uint8b128]\nSUPPORTED_GROUP_SIZES = [-1, 32, 64, 128]\n\n# Note: this is a hack. We should update each model to register the\n# stacked params and get it from there instead in a future PR.\n# fused_name: List[shard_name]\nFUSED_LAYER_NAME_MAPPING = {\n    \"qkv_proj\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n    \"gate_up_proj\": [\"gate_proj\", \"up_proj\"]\n}\n\n\ndef pack_quantized_values_into_int32(w_q: torch.Tensor,\n                                     wtype: ScalarType,\n                                     packed_dim: int = 0):\n    # move dim to pack to the end\n    perm = (*[i for i in range(len(w_q.shape)) if i != packed_dim], packed_dim)\n    inv_perm = tuple(perm.index(i) for i in range(len(perm)))\n    w_q_perm = w_q.permute(perm)\n\n    pack_factor = 32 // wtype.size_bits\n    mask = (1 << wtype.size_bits) - 1\n\n    new_shape_perm = list(w_q_perm.shape)\n    assert w_q_perm.shape[-1] % pack_factor == 0\n    new_shape_perm[-1] //= pack_factor\n\n    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)\n    for i in range(pack_factor):\n        res |= (w_q_perm[..., i::pack_factor] & mask) << wtype.size_bits * i\n\n    return res.permute(inv_perm)\n\n\ndef unpack_quantized_values_into_int32(w_q: torch.Tensor,\n                                       wtype: ScalarType,\n                                       packed_dim: int = 0):\n    # move dim to pack to the end\n    perm = (*[i for i in range(len(w_q.shape)) if i != packed_dim], packed_dim)\n    inv_perm = tuple(perm.index(i) for i in range(len(perm)))\n    w_q_perm = w_q.permute(perm)\n\n    pack_factor = 32 // wtype.size_bits\n    mask = (1 << wtype.size_bits) - 1\n\n    new_shape_perm = list(w_q_perm.shape)\n    new_shape_perm[-1] *= pack_factor\n\n    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)\n    for i in range(pack_factor):\n        res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask\n\n    return res.permute(inv_perm)\n\n\ndef is_layer_skipped(prefix: str, ignored_layers: List[str]) -> bool:\n    # prefix: model.layers.0.self_attn.q_proj\n    # proj_name: q_proj\n    proj_name = prefix.split(\".\")[-1]\n    if proj_name in FUSED_LAYER_NAME_MAPPING:\n        shard_prefixes = [\n            prefix.replace(proj_name, shard_proj_name)\n            for shard_proj_name in FUSED_LAYER_NAME_MAPPING[proj_name]\n        ]\n\n        is_skipped = None\n        for shard_prefix in shard_prefixes:\n            is_shard_skipped = shard_prefix in ignored_layers\n\n            if is_skipped is None:\n                is_skipped = is_shard_skipped\n            elif is_shard_skipped != is_skipped:\n                raise ValueError(\n                    f\"Detected some but not all shards of {prefix} \"\n                    \"are quantized. All shards of fused layers \"\n                    \"to have the same precision.\")\n    else:\n        is_skipped = prefix in ignored_layers\n\n    assert is_skipped is not None\n    return is_skipped\n\n\ndef get_pack_factor(num_bits):\n    assert 32 % num_bits == 0, f\"Unsupported num_bits = {num_bits}\"\n    return 32 // num_bits\n\n\ndef permute_rows(q_w: torch.Tensor,\n                 w_ref: torch.Tensor,\n                 group_size: int,\n                 test_perm: Optional[torch.Tensor] = None):\n    assert q_w.shape == w_ref.shape\n\n    orig_device = q_w.device\n    k_size, _ = q_w.shape\n\n    g_idx = torch.zeros((k_size, ), dtype=torch.int32)\n    for i in range(k_size):\n        g_idx[i] = i // group_size\n\n    # Simulate act_order by doing a random permutation on K\n    rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)\n\n    g_idx = g_idx[rand_perm].contiguous()\n    q_w = q_w[rand_perm, :].contiguous()\n    w_ref = w_ref[rand_perm, :].contiguous()\n\n    return (\n        w_ref.to(device=orig_device),\n        q_w.to(device=orig_device),\n        g_idx.to(device=orig_device),\n        rand_perm.to(device=orig_device),\n    )\n\n\ndef quantize_weights(w: torch.Tensor,\n                     quant_type: ScalarType,\n                     group_size: Optional[int],\n                     zero_points: bool = False,\n                     ref_zero_points_after_scales: bool = False):\n    assert quant_type.is_integer(), \\\n        \"Floating point quantization may work but has not been tested\"\n    assert not zero_points or group_size is not None, \\\n        \"to have group zero points, group_size must be provided \"\\\n        \"(-1 group_size is channelwise)\"\n\n    orig_device = w.device\n    orig_type = w.dtype\n    size_k, size_n = w.shape\n\n    assert w.is_floating_point(), \"w must be float\"\n\n    if group_size == -1:\n        group_size = size_k\n\n    # Reshape to [groupsize, -1]\n    if group_size is not None and group_size < size_k:\n        w = w.reshape((-1, group_size, size_n))\n        w = w.permute(1, 0, 2)\n        w = w.reshape((group_size, -1))\n\n    # Compute scale for each group\n    max_val = torch.max(w, 0, keepdim=True).values\n    min_val = torch.min(w, 0, keepdim=True).values\n\n    max_q_val = quant_type.max()\n    min_q_val = quant_type.min()\n\n    w_s = torch.Tensor([1.0]).to(w.device)  # unscaled case\n    maybe_w_zp = None\n    if group_size is not None:\n        if zero_points:\n            assert not quant_type.is_signed() and quant_type.max() > 0\n            w_s = (max_val - min_val).clamp(min=1e-5) / quant_type.max()\n            maybe_w_zp = torch.round(torch.abs(min_val / w_s)) \\\n                .clamp(min_q_val, max_q_val).int()\n        else:\n            # If the bias is such that there are no possible negative/positive\n            #  values, set the max value to inf to avoid divide by 0\n            w_s = torch.max(\n                abs(max_val / (max_q_val if max_q_val != 0 else torch.inf)),\n                abs(min_val / (min_q_val if min_q_val != 0 else torch.inf)))\n\n    # Quantize\n    w_q = torch.round(w / w_s).int() + (maybe_w_zp if zero_points else 0)\n    w_q = torch.clamp(w_q, min_q_val, max_q_val)\n\n    # Compute ref (dequantized)\n    # For some kernels (namely Machete) the zero-points are applied after the\n    # scales are applied, for this case computing the reference in similar way\n    # allows us to use tighter error tolerances in our unit tests.\n    if ref_zero_points_after_scales and maybe_w_zp is not None:\n        w_ref = w_q.to(orig_type) * w_s - maybe_w_zp.to(orig_type) * w_s\n    else:\n        w_ref = (w_q - (maybe_w_zp if zero_points else 0)).to(orig_type) * w_s\n\n    if quant_type.has_bias():\n        w_q += quant_type.bias\n\n    # Restore original shapes\n    if group_size is not None and group_size < size_k:\n\n        def reshape_w(w):\n            w = w.reshape((group_size, -1, size_n))\n            w = w.permute(1, 0, 2)\n            w = w.reshape((size_k, size_n)).contiguous()\n            return w\n\n        w_q = reshape_w(w_q)\n        w_ref = reshape_w(w_ref)\n        w_s = w_s.reshape((-1, size_n)).contiguous()\n\n    if maybe_w_zp is not None:\n        maybe_w_zp = maybe_w_zp.reshape((-1, size_n)).contiguous()\n        maybe_w_zp = maybe_w_zp.to(device=orig_device)\n\n    return (\n        w_ref.to(device=orig_device),\n        w_q.to(device=orig_device),\n        w_s if group_size is not None else None,\n        maybe_w_zp,\n    )\n\n\ndef gptq_quantize_weights(w: torch.Tensor,\n                          quant_type: ScalarType,\n                          group_size: int,\n                          act_order: bool,\n                          test_perm: Optional[torch.Tensor] = None):\n    size_k, _ = w.shape\n\n    assert w.is_floating_point(), \"w must be float\"\n    assert quant_type in SUPPORTED_GPTQ_QUANT_TYPES, \\\n        f\"Unsupported gptq type = {quant_type}\"\n    assert group_size in SUPPORTED_GROUP_SIZES + [\n        size_k\n    ], f\"Unsupported groupsize = {group_size}\"\n\n    w_ref, w_q, w_s, _ = quantize_weights(w, quant_type, group_size)\n\n    # Apply act_order\n    g_idx = torch.empty(0, dtype=torch.int, device=w.device)\n    rand_perm = torch.empty(0, dtype=torch.int, device=w.device)\n    if act_order:\n        assert (\n            group_size < size_k\n        ), \"For act_order, groupsize = {} must be less than size_k = {}\".format(\n            group_size, size_k)\n\n        w_ref, w_q, g_idx, rand_perm = permute_rows(w_q, w_ref, group_size,\n                                                    test_perm)\n\n    return w_ref, w_q, w_s, g_idx, rand_perm\n\n\n# QQQ employs different quant schemes for per-group and\n# per-channel quantization.\ndef qqq_quantize_weights(w: torch.Tensor, num_bits: int, group_size: int):\n    orig_device = w.device\n    size_k, size_n = w.shape\n\n    assert w.is_floating_point(), \"w must be float\"\n    assert num_bits in MARLIN_QQQ_SUPPORTED_NUM_BITS, \\\n           f\"Unsupported num_bits = {num_bits}\"\n    assert group_size in SUPPORTED_GROUP_SIZES + [\n        size_k\n    ], f\"Unsupported groupsize = {group_size}\"\n\n    if group_size == -1:\n        group_size = size_k\n    assert group_size <= size_k\n\n    if group_size < size_k:\n        # Reshape to [groupsize, -1]\n        w = w.reshape((-1, group_size, size_n))\n        w = w.permute(1, 0, 2)\n        w = w.reshape((group_size, -1))\n\n        max_q_val = 2**num_bits - 1\n        half_q_val = (max_q_val + 1) // 2\n\n        # Compute scale for each group\n        s_group = torch.max(torch.abs(w), 0, keepdim=True)[0]\n        s_group *= 2 / max_q_val  # 2 => symmetric\n\n        # Quantize\n        q_w = torch.round(w / s_group).int()\n        q_w += half_q_val\n        q_w = torch.clamp(q_w, 0, max_q_val)\n        # Compute ref (dequantized)\n        w_ref = (q_w - half_q_val).half() * s_group\n\n        # Restore original shapes\n        def reshape_w(w):\n            w = w.reshape((group_size, -1, size_n))\n            w = w.permute(1, 0, 2)\n            w = w.reshape((size_k, size_n)).contiguous()\n            return w\n\n        q_w = reshape_w(q_w)\n        w_ref = reshape_w(w_ref)\n\n        # Compute int8 quantization scale for each channel\n        s_channel = torch.max(torch.abs(w_ref), 0, keepdim=True)[0]\n        s_channel /= 127.0\n        t_int8 = (w_ref / s_channel).round().clamp(-128, 127).to(torch.int8)\n        w_ref = t_int8.half() * s_channel\n        s_channel = s_channel.reshape(1, -1).to(dtype=torch.float)\n\n        # Fuse scales\n        s_group = (s_group.reshape(-1, size_n).contiguous() /\n                   s_channel).to(dtype=torch.half)\n    else:\n        max_q_val = 2**(num_bits - 1) - 1\n\n        # Compute scale for each channel\n        s_channel = torch.max(torch.abs(w), 0, keepdim=True)[0]\n        s_channel /= max_q_val\n\n        # Quantize\n        q_w = torch.round(w / s_channel).int()\n        q_w = torch.clamp(q_w, -max_q_val, max_q_val)\n        # Compute ref (dequantized)\n        w_ref = q_w.half() * s_channel\n\n        s_group = torch.tensor([], dtype=torch.half)\n        # div 2 ** (8 - self.bits)) to offset right shift in unpacking\n        s_channel /= (2**(8 - num_bits))\n        s_channel = s_channel.reshape(-1, size_n).contiguous().to(torch.float)\n\n    return (\n        w_ref.to(device=orig_device),\n        q_w.to(device=orig_device),\n        s_group.to(device=orig_device),\n        s_channel.to(device=orig_device),\n    )\n\n\ndef sort_weights(q_w: torch.Tensor, g_idx: torch.Tensor):\n    orig_device = q_w.device\n\n    sort_indices = torch.argsort(g_idx).to(\n        dtype=torch.int32)  # Sort based on g_idx\n\n    g_idx = g_idx[sort_indices].contiguous()\n    q_w = q_w[sort_indices, :].contiguous()\n\n    return (\n        q_w.to(device=orig_device),\n        g_idx.to(device=orig_device),\n        sort_indices.to(device=orig_device),\n    )\n\n\ndef pack_rows(\n    q_w: torch.Tensor,\n    num_bits: int,\n    size_k: int,\n    size_n: int,\n):\n    assert q_w.shape == (size_k, size_n)\n\n    pack_factor = get_pack_factor(num_bits)\n    assert size_k % pack_factor == 0\n\n    orig_device = q_w.device\n\n    q_w = q_w.cpu().numpy().astype(numpy.uint32)\n\n    q_res = numpy.zeros((size_k // pack_factor, size_n), dtype=numpy.uint32)\n\n    for i in range(pack_factor):\n        q_res |= q_w[i::pack_factor, :] << num_bits * i\n\n    q_res = torch.from_numpy(q_res.astype(numpy.int32)).to(orig_device)\n    return q_res\n\n\ndef pack_cols(\n    q_w: torch.Tensor,\n    num_bits: int,\n    size_k: int,\n    size_n: int,\n):\n    assert q_w.shape == (size_k, size_n)\n\n    pack_factor = get_pack_factor(num_bits)\n    assert size_n % pack_factor == 0\n\n    orig_device = q_w.device\n\n    q_w = q_w.cpu().numpy().astype(numpy.uint32)\n\n    q_res = numpy.zeros((size_k, size_n // pack_factor), dtype=numpy.uint32)\n\n    for i in range(pack_factor):\n        q_res |= q_w[:, i::pack_factor] << num_bits * i\n\n    q_res = torch.from_numpy(q_res.astype(numpy.int32)).to(orig_device)\n    q_res = q_res.contiguous()\n\n    return q_res\n\n\ndef unpack_cols(\n    packed_q_w: torch.Tensor,\n    num_bits: int,\n    size_k: int,\n    size_n: int,\n):\n    pack_factor = get_pack_factor(num_bits)\n    assert size_n % pack_factor == 0\n    assert packed_q_w.shape == (\n        size_k, size_n // pack_factor\n    ), \"packed_q_w.shape = {} size_k = {}, size_n = {} pack_Factor = {}\".format(\n        packed_q_w.shape, size_k, size_n, pack_factor)\n\n    orig_device = packed_q_w.device\n\n    packed_q_w_cpu = packed_q_w.cpu().numpy().astype(numpy.uint32)\n    q_res = numpy.zeros((size_k, size_n), dtype=numpy.uint32)\n\n    mask = (1 << num_bits) - 1\n    for i in range(pack_factor):\n        vals = packed_q_w_cpu & mask\n        packed_q_w_cpu >>= num_bits\n        q_res[:, i::pack_factor] = vals\n\n    q_res = torch.from_numpy(q_res.astype(numpy.int32)).to(orig_device)\n    q_res = q_res.contiguous()\n\n    return q_res\n\n\ndef gptq_pack(\n    q_w: torch.Tensor,\n    num_bits: int,\n    size_k: int,\n    size_n: int,\n):\n    return pack_rows(q_w, num_bits, size_k, size_n)\n\n\ndef awq_pack(\n    q_w: torch.Tensor,\n    num_bits: int,\n    size_k: int,\n    size_n: int,\n):\n    assert q_w.shape == (size_k, size_n)\n\n    # Interleave column dim (for the dequantize code) and pack it to int32\n    if num_bits == 4:\n        interleave = numpy.array([0, 2, 4, 6, 1, 3, 5, 7])\n    elif num_bits == 8:\n        interleave = numpy.array([0, 2, 1, 3])\n    else:\n        raise Exception(\"num_bits must be 4 or 8, got {}\".format(num_bits))\n\n    q_w = q_w.reshape((-1, len(interleave)))[:, interleave].ravel()\n    q_w = q_w.reshape((-1, size_n)).contiguous()\n\n    return pack_cols(q_w, num_bits, size_k, size_n)\n",
      "diff": "diff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 83055d600..95e785dcc 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -1,5 +1,5 @@\n \"\"\"This file is used for /tests and /benchmarks\"\"\"\n-from typing import List, Optional\n+from typing import List, Optional, Tuple\n \n import numpy\n import torch\n@@ -20,6 +20,120 @@ FUSED_LAYER_NAME_MAPPING = {\n }\n \n \n+# Normalize the group_shape to the full extent for any dims that are -1\n+def _normalize_quant_group_shape(x: torch.Tensor, group_shape: Tuple[int,\n+                                                                     int]):\n+    # -1 means full extent\n+    return (group_shape[0] if group_shape[0] > 0 else x.shape[-2],\n+            group_shape[1] if group_shape[1] > 0 else x.shape[-1])\n+\n+\n+# Useful when treating N-dimensional group scaling as extended numpy-style\n+# broadcasting in numpy simply stretches dimensions with an extent of 1 to match\n+# the target shape by repeating the data along that dimension (broadcasting)\n+# , we extend these semantics to say if the extent of a dimension in the\n+# source shape is not 1 and does not match the target shape we repeat each\n+# element along that dimension src_shape[dim] // target_shape[dim] times\n+# example if we have:\n+#       a = [[1, 2], and target_shape = (2, 4)\n+#            [3, 4]]\n+# then we would expand a to:\n+#       a = [[1, 1, 2, 2],\n+#            [3, 3, 4, 4]]\n+# NOTE this function this function does not explicitly broadcast dimensions\n+# with an extent of 1, since this can be done implicitly by pytorch\n+def group_broadcast(t, shape):\n+    for i, s in enumerate(shape):\n+        if t.shape[i] != s and t.shape[i] != 1:\n+            assert s % t.shape[i] == 0\n+            t = t.unsqueeze(i + 1)\\\n+                .expand(*t.shape[:i+1], s // t.shape[i], *t.shape[i+1:])\\\n+                .flatten(i, i + 1)\n+    return t\n+\n+\n+# Quantize assuming once scale per group of elements with shape group_shape,\n+# example group shapes:\n+#  * (-1, -1)   for per-tensor quantization\n+#  * (1, -1)    for per-row quantization\n+#  * (-1, 1)    for per-column quantization\n+#  * (128, 128) for 128x128 deepseek style block quantization\n+#  * (1, 128)   for deepseek style activation quantization\n+#               (i.e. per-token-per-group)\n+def scaled_quantize(\n+    x: torch.Tensor,\n+    group_shape: Tuple[int, int],\n+    quant_dtype: torch.dtype,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    group_shape = _normalize_quant_group_shape(x, group_shape)\n+    assert quant_dtype.is_floating_point, \\\n+        \"currently `scaled_quantize` only supports floating point dtypes \" \\\n+        \"but could be extended to support other dtypes\"\n+\n+    finfo = torch.finfo(quant_dtype)\n+\n+    # Reshape (M, N) into (BLK_M, BLOCK_SIZE_M, BLK_N, BLOCK_SIZE_N)\n+    assert x.ndim == 2\n+    assert x.shape[0] % group_shape[0] == 0 and x.shape[1] % group_shape[1] == 0\n+    blk_m, blk_n = x.shape[0] // group_shape[0], x.shape[1] // group_shape[1]\n+    x_blkd = x.reshape(blk_m, group_shape[0], blk_n, group_shape[1])\n+\n+    # Permute to (BLK_M, BLK_N, BLOCK_SIZE_M, BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd.permute(0, 2, 1, 3)\n+    # Flatten to (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd_permd.flatten(start_dim=2)\n+\n+    # Compute scales\n+    min_val, max_val = x_blkd_permd.aminmax(dim=-1)\n+    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)\n+    scale = finfo.max / amax\n+\n+    # Apply scale and convert form:\n+    # (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N) to (M, N)\n+    x_scl_sat = (x_blkd_permd * scale.unsqueeze(-1))\\\n+        .clamp(min=finfo.min, max=finfo.max)\\\n+        .reshape(blk_m, blk_n, group_shape[0], group_shape[1])\\\n+        .permute(0, 2, 1, 3)\\\n+        .reshape(x.shape)\n+\n+    return x_scl_sat.to(quant_dtype).contiguous(), scale.float().reciprocal()\n+\n+\n+# inverses `scaled_quantize`\n+def scaled_dequantize(\n+    x_q: torch.Tensor,\n+    x_s: torch.Tensor,\n+    group_shape: Optional[Tuple[int, int]] = None,\n+    out_dtype: torch.dtype = torch.float32,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if group_shape is not None:\n+        group_shape = _normalize_quant_group_shape(x_q, group_shape)\n+\n+    if x_s.ndim == 0:  # scalar\n+        x_s = x_s.unsqueeze(-1).unsqueeze(-1)  # convert to (1, 1) tensor\n+    if x_s.ndim == 1:\n+        if group_shape is None:\n+            raise AssertionError(\n+                \"if x_s is 1D tensor, group_shape must be provided otherwise \"\n+                \"its ambiguous which dimension to broadcast x_s to\")\n+        # unsqueeze the scales for the dimension where we want to broadcast\n+        # across the full extent\n+        if group_shape[0] == x_q.shape[-2]:\n+            x_s = x_s.unsqueeze(-2)\n+        elif group_shape[1] == x_q.shape[-1]:\n+            x_s = x_s.unsqueeze(-1)\n+        else:\n+            raise AssertionError(\n+                \"if x_s is a vector we should be broadcasting it to the full \"\n+                \"extent of one of the dimensions\")\n+\n+    if group_shape is not None:\n+        assert x_s.shape[-1] == x_q.shape[-1] // group_shape[1]\n+        assert x_s.shape[-2] == x_q.shape[-2] // group_shape[0]\n+    x_s = group_broadcast(x_s.to(torch.float32), x_q.shape)\n+    return (x_q.to(torch.float32) * x_s).to(out_dtype)\n+\n+\n def pack_quantized_values_into_int32(w_q: torch.Tensor,\n                                      wtype: ScalarType,\n                                      packed_dim: int = 0):",
      "change_type": "modified",
      "lines_added": 116,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/model_loader/loader.py",
      "old_content": "# ruff: noqa: SIM117\nimport collections\nimport copy\nimport dataclasses\nimport fnmatch\nimport glob\nimport inspect\nimport itertools\nimport math\nimport os\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom contextlib import contextmanager\nfrom typing import (Any, Callable, Dict, Generator, Iterable, List, Optional,\n                    Tuple, cast)\n\nimport gguf\nimport huggingface_hub\nimport numpy as np\nimport torch\nfrom huggingface_hub import HfApi\nfrom torch import nn\nfrom transformers import AutoModelForCausalLM\nfrom transformers.utils import SAFE_WEIGHTS_INDEX_NAME\n\nfrom vllm.attention import Attention\nfrom vllm.config import (LoadConfig, LoadFormat, ModelConfig, ParallelConfig,\n                         VllmConfig, set_current_vllm_config)\nfrom vllm.distributed import (get_tensor_model_parallel_rank,\n                              get_tensor_model_parallel_world_size)\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.linear import (LinearBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               ReplicatedLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.quantization.base_config import (\n    QuantizeMethodBase)\nfrom vllm.model_executor.model_loader.tensorizer import (\n    TensorizerConfig, is_vllm_tensorized, load_with_tensorizer,\n    serialize_vllm_model, tensorizer_weights_iterator)\nfrom vllm.model_executor.model_loader.utils import (ParamMapping,\n                                                    get_model_architecture,\n                                                    set_default_torch_dtype)\nfrom vllm.model_executor.model_loader.weight_utils import (\n    download_safetensors_index_file_from_hf, download_weights_from_hf,\n    filter_duplicate_safetensors_files, filter_files_not_needed_for_inference,\n    get_gguf_extra_tensor_names, gguf_quant_weights_iterator,\n    initialize_dummy_weights, np_cache_weights_iterator, pt_weights_iterator,\n    runai_safetensors_weights_iterator, safetensors_weights_iterator)\nfrom vllm.model_executor.utils import set_weight_attrs\nfrom vllm.platforms import current_platform\nfrom vllm.transformers_utils.s3_utils import glob as s3_glob\nfrom vllm.transformers_utils.utils import is_s3\nfrom vllm.utils import is_pin_memory_available\n\n\n@contextmanager\ndef device_loading_context(module: torch.nn.Module,\n                           target_device: torch.device):\n    if target_device.type == \"cpu\":\n        # If target is CPU, no need to move anything\n        yield module\n        return\n\n    original_device_states: Dict[str, torch.device] = {}\n\n    # Store original device states and move parameters to GPU if they're on CPU\n    for name, p in module.named_parameters():\n        if p.device.type == \"cpu\":\n            original_device_states[name] = p.device\n            p.data = p.data.to(target_device)\n        # Parameters already on target device are not touched\n\n    try:\n        yield module\n\n    finally:\n        # Restore parameters to their original devices, ignoring new parameters\n        pin_memory = is_pin_memory_available()\n        for name, p in module.named_parameters():\n            if name in original_device_states:\n                original_device: torch.device = original_device_states[name]\n                if original_device.type == \"cpu\":\n                    # `torch.empty_like` does not support `pin_memory` argument\n                    cpu_data = torch.empty_strided(\n                        size=p.data.size(),\n                        stride=p.data.stride(),\n                        dtype=p.data.dtype,\n                        layout=p.data.layout,\n                        device=\"cpu\",\n                        pin_memory=pin_memory,\n                    )\n                    cpu_data.copy_(p.data)\n                    p.data = cpu_data\n                else:\n                    p.data = p.data.to(original_device)\n        # New parameters or parameters already on target device are untouched\n\n\nlogger = init_logger(__name__)\n\n\ndef _initialize_model(\n    vllm_config: VllmConfig,\n    *,\n    prefix: str = \"\",\n) -> nn.Module:\n    \"\"\"Initialize a model with the given configurations.\"\"\"\n    model_config = vllm_config.model_config\n    model_class, _ = get_model_architecture(model_config)\n\n    signatures = inspect.signature(model_class.__init__)\n    all_params = [param.name for param in signatures.parameters.values()]\n    if \"vllm_config\" in all_params and \"prefix\" in all_params:\n        # new-style model class\n        with set_current_vllm_config(vllm_config, check_compile=True):\n            return model_class(vllm_config=vllm_config, prefix=prefix)\n\n    msg = (\"vLLM model class should accept `vllm_config` and `prefix` as \"\n           \"input arguments. Possibly you have an old-style model class\"\n           \" registered from out of tree and it is used for new vLLM version. \"\n           \"Check https://docs.vllm.ai/en/latest/design/arch_overview.html \"\n           \"for the design and update the model class accordingly.\")\n    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n    logger.warning(\n        \"Trying to guess the arguments for old-style model class %s\",\n        model_class,\n    )\n    # try to be compatible with old-style model class\n    kwargs = {}\n    if \"prefix\" in all_params:\n        kwargs[\"prefix\"] = prefix\n    if \"config\" in all_params:\n        kwargs[\"config\"] = model_config.hf_config\n    if \"cache_config\" in all_params:\n        kwargs[\"cache_config\"] = vllm_config.cache_config\n    if \"quant_config\" in all_params:\n        kwargs[\"quant_config\"] = vllm_config.quant_config\n    if \"lora_config\" in all_params:\n        kwargs[\"lora_config\"] = vllm_config.lora_config\n    if \"scheduler_config\" in all_params:\n        kwargs[\"scheduler_config\"] = vllm_config.scheduler_config\n    with set_current_vllm_config(vllm_config, check_compile=True):\n        return model_class(**kwargs)\n\n\nclass BaseModelLoader(ABC):\n    \"\"\"Base class for model loaders.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        self.load_config = load_config\n\n    @abstractmethod\n    def download_model(self, model_config: ModelConfig) -> None:\n        \"\"\"Download a model so that it can be immediately loaded.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def load_model(self, *, vllm_config: VllmConfig) -> nn.Module:\n        \"\"\"Load a model with the given configurations.\"\"\"\n        raise NotImplementedError\n\n\nclass DefaultModelLoader(BaseModelLoader):\n    \"\"\"Model loader that can load different file types from disk.\"\"\"\n\n    @dataclasses.dataclass\n    class Source:\n        \"\"\"A source for weights.\"\"\"\n\n        model_or_path: str\n        \"\"\"The model ID or path.\"\"\"\n\n        revision: Optional[str]\n        \"\"\"The optional model revision.\"\"\"\n\n        prefix: str = \"\"\n        \"\"\"A prefix to prepend to all weights.\"\"\"\n\n        fall_back_to_pt: bool = True\n        \"\"\"Whether .pt weights can be used.\"\"\"\n\n        allow_patterns_overrides: Optional[list[str]] = None\n        \"\"\"If defined, weights will load exclusively using these patterns.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            raise ValueError(f\"Model loader extra config is not supported for \"\n                             f\"load format {load_config.load_format}\")\n\n    def _maybe_download_from_modelscope(\n            self, model: str, revision: Optional[str]) -> Optional[str]:\n        \"\"\"Download model from ModelScope hub if VLLM_USE_MODELSCOPE is True.\n\n        Returns the path to the downloaded model, or None if the model is not\n        downloaded from ModelScope.\"\"\"\n        if VLLM_USE_MODELSCOPE:\n            # download model from ModelScope hub,\n            # lazy import so that modelscope is not required for normal use.\n            # pylint: disable=C.\n            from modelscope.hub.snapshot_download import snapshot_download\n\n            if not os.path.exists(model):\n                model_path = snapshot_download(\n                    model_id=model,\n                    cache_dir=self.load_config.download_dir,\n                    local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n                    revision=revision,\n                    ignore_file_pattern=self.load_config.ignore_patterns,\n                )\n            else:\n                model_path = model\n            return model_path\n        return None\n\n    def _prepare_weights(\n        self,\n        model_name_or_path: str,\n        revision: Optional[str],\n        fall_back_to_pt: bool,\n        allow_patterns_overrides: Optional[list[str]],\n    ) -> Tuple[str, List[str], bool]:\n        \"\"\"Prepare weights for the model.\n\n        If the model is not local, it will be downloaded.\"\"\"\n        model_name_or_path = (self._maybe_download_from_modelscope(\n            model_name_or_path, revision) or model_name_or_path)\n\n        is_local = os.path.isdir(model_name_or_path)\n        load_format = self.load_config.load_format\n        use_safetensors = False\n        index_file = SAFE_WEIGHTS_INDEX_NAME\n        # Some quantized models use .pt files for storing the weights.\n        if load_format == LoadFormat.AUTO:\n            allow_patterns = [\"*.safetensors\", \"*.bin\"]\n        elif load_format == LoadFormat.SAFETENSORS:\n            use_safetensors = True\n            allow_patterns = [\"*.safetensors\"]\n        elif load_format == LoadFormat.MISTRAL:\n            use_safetensors = True\n            allow_patterns = [\"consolidated*.safetensors\"]\n            index_file = \"consolidated.safetensors.index.json\"\n        elif load_format == LoadFormat.PT:\n            allow_patterns = [\"*.pt\"]\n        elif load_format == LoadFormat.NPCACHE:\n            allow_patterns = [\"*.bin\"]\n        else:\n            raise ValueError(f\"Unknown load_format: {load_format}\")\n\n        if fall_back_to_pt:\n            allow_patterns += [\"*.pt\"]\n\n        if allow_patterns_overrides is not None:\n            allow_patterns = allow_patterns_overrides\n\n        if not is_local:\n            hf_folder = download_weights_from_hf(\n                model_name_or_path,\n                self.load_config.download_dir,\n                allow_patterns,\n                revision,\n                ignore_patterns=self.load_config.ignore_patterns,\n            )\n        else:\n            hf_folder = model_name_or_path\n\n        hf_weights_files: List[str] = []\n        for pattern in allow_patterns:\n            hf_weights_files += glob.glob(os.path.join(hf_folder, pattern))\n            if len(hf_weights_files) > 0:\n                if pattern == \"*.safetensors\":\n                    use_safetensors = True\n                break\n\n        if use_safetensors:\n            # For models like Mistral-7B-Instruct-v0.3\n            # there are both sharded safetensors files and a consolidated\n            # safetensors file. Using both breaks.\n            # Here, we download the `model.safetensors.index.json` and filter\n            # any files not found in the index.\n            if not is_local:\n                download_safetensors_index_file_from_hf(\n                    model_name_or_path,\n                    index_file,\n                    self.load_config.download_dir,\n                    revision,\n                )\n            hf_weights_files = filter_duplicate_safetensors_files(\n                hf_weights_files, hf_folder, index_file)\n        else:\n            hf_weights_files = filter_files_not_needed_for_inference(\n                hf_weights_files)\n\n        if len(hf_weights_files) == 0:\n            raise RuntimeError(\n                f\"Cannot find any model weights with `{model_name_or_path}`\")\n\n        return hf_folder, hf_weights_files, use_safetensors\n\n    def _get_weights_iterator(\n            self, source: \"Source\"\n    ) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        \"\"\"Get an iterator for the model weights based on the load format.\"\"\"\n        hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(\n            source.model_or_path, source.revision, source.fall_back_to_pt,\n            source.allow_patterns_overrides)\n        if self.load_config.load_format == LoadFormat.NPCACHE:\n            # Currently np_cache only support *.bin checkpoints\n            assert use_safetensors is False\n            weights_iterator = np_cache_weights_iterator(\n                source.model_or_path,\n                self.load_config.download_dir,\n                hf_folder,\n                hf_weights_files,\n            )\n        elif use_safetensors:\n            weights_iterator = safetensors_weights_iterator(hf_weights_files)\n        else:\n            weights_iterator = pt_weights_iterator(hf_weights_files)\n\n        if current_platform.is_tpu():\n            # In PyTorch XLA, we should call `xm.mark_step` frequently so that\n            # not too many ops are accumulated in the XLA program.\n            import torch_xla.core.xla_model as xm\n\n            def _xla_weights_iterator(iterator: Generator):\n                for weights in iterator:\n                    yield weights\n                    xm.mark_step()\n\n            weights_iterator = _xla_weights_iterator(weights_iterator)\n\n        # Apply the prefix.\n        return ((source.prefix + name, tensor)\n                for (name, tensor) in weights_iterator)\n\n    def _get_all_weights(\n        self,\n        model_config: ModelConfig,\n        model: nn.Module,\n    ) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        primary_weights = DefaultModelLoader.Source(\n            model_config.model,\n            model_config.revision,\n            prefix=\"\",\n            fall_back_to_pt=getattr(model, \"fall_back_to_pt_during_load\",\n                                    True),\n            allow_patterns_overrides=getattr(model, \"allow_patterns_overrides\",\n                                             None),\n        )\n        yield from self._get_weights_iterator(primary_weights)\n\n        secondary_weights = cast(\n            Iterable[DefaultModelLoader.Source],\n            getattr(model, \"secondary_weights\", ()),\n        )\n        for source in secondary_weights:\n            yield from self._get_weights_iterator(source)\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        self._prepare_weights(model_config.model,\n                              model_config.revision,\n                              fall_back_to_pt=True,\n                              allow_patterns_overrides=None)\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n\n        target_device = torch.device(device_config.device)\n        with set_default_torch_dtype(model_config.dtype):\n            with target_device:\n                model = _initialize_model(vllm_config=vllm_config)\n\n            weights_to_load = {name for name, _ in model.named_parameters()}\n            loaded_weights = model.load_weights(\n                self._get_all_weights(model_config, model))\n            # We only enable strict check for non-quantized models\n            # that have loaded weights tracking currently.\n            if model_config.quantization is None and loaded_weights is not None:\n                weights_not_loaded = weights_to_load - loaded_weights\n                if weights_not_loaded:\n                    raise ValueError(\n                        \"Following weights were not initialized from \"\n                        f\"checkpoint: {weights_not_loaded}\")\n\n            for _, module in model.named_modules():\n                quant_method = getattr(module, \"quant_method\", None)\n                if isinstance(quant_method, QuantizeMethodBase):\n                    # When quant methods need to process weights after loading\n                    # (for repacking, quantizing, etc), they expect parameters\n                    # to be on the global target device. This scope is for the\n                    # case where cpu offloading is used, where we will move the\n                    # parameters onto device for processing and back off after.\n                    with device_loading_context(module, target_device):\n                        quant_method.process_weights_after_loading(module)\n                elif isinstance(module, Attention) and \\\n                    hasattr(module, \"process_weights_after_loading\"):\n                    # When attention modules need to process weights after\n                    # currently only used by MLA\n                    module.process_weights_after_loading()\n        return model.eval()\n\n\nclass DummyModelLoader(BaseModelLoader):\n    \"\"\"Model loader that will set model weights to random values.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            raise ValueError(f\"Model loader extra config is not supported for \"\n                             f\"load format {load_config.load_format}\")\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        pass  # Nothing to download\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(vllm_config=vllm_config)\n            # NOTE(woosuk): For accurate performance evaluation, we assign\n            # random values to the weights.\n            initialize_dummy_weights(model)\n\n            for _, module in model.named_modules():\n                quant_method = getattr(module, \"quant_method\", None)\n                if quant_method is not None:\n                    # When quant methods need to process weights after loading\n                    # (for repacking, quantizing, etc), they expect parameters\n                    # to be on the global target device. This scope is for the\n                    # case where cpu offloading is used, where we will move the\n                    # parameters onto device for processing and back off after.\n                    with device_loading_context(\n                            module, torch.device(device_config.device)):\n                        quant_method.process_weights_after_loading(module)\n        return model.eval()\n\n\nclass TensorizerLoader(BaseModelLoader):\n    \"\"\"Model loader using CoreWeave's tensorizer library.\"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if isinstance(load_config.model_loader_extra_config, TensorizerConfig):\n            self.tensorizer_config = load_config.model_loader_extra_config\n        else:\n            self.tensorizer_config = TensorizerConfig(\n                **load_config.model_loader_extra_config)\n\n    def _verify_config(self, model_config: ModelConfig,\n                       parallel_config: ParallelConfig):\n        self.tensorizer_config.verify_with_model_config(model_config)\n        self.tensorizer_config.verify_with_parallel_config(parallel_config)\n\n    def _get_weights_iterator(\n        self, ) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        tensorizer_args = self.tensorizer_config._construct_tensorizer_args()\n        return tensorizer_weights_iterator(tensorizer_args)\n\n    def _load_model_serialized_cpu(\n        self,\n        vllm_config: VllmConfig,\n    ) -> nn.Module:\n        \"\"\"Load a serialized model with tensorizer to the CPU.\n\n        This is only necessary when the model isn't vLLM-tensorized (see\n        examples/other/tensorize_vllm_model.py) This should still\n        be faster than default HuggingFace loading, but will be slower than\n        loading a vLLM-tensorized model.\n        \"\"\"\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(vllm_config=vllm_config)\n\n            model.load_weights(self._get_weights_iterator())\n        return model.eval()\n\n    def _load_model_serialized(\n        self,\n        vllm_config: VllmConfig,\n    ) -> nn.Module:\n        \"\"\"Load a serialized model with tensorizer.\n\n        Expects a vLLM-tensorized model. See the\n        examples/other/tensorize_vllm_model.py example script\n        for serializing vLLM models.\"\"\"\n\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model_class = get_model_architecture(model_config)[0]\n\n                tensorizer_config = copy.copy(self.tensorizer_config)\n                tensorizer_config.model_class = model_class\n                tensorizer_config.hf_config = model_config.hf_config\n                tensorizer_config.dtype = model_config.dtype\n\n                model = load_with_tensorizer(tensorizer_config,\n                                             vllm_config=vllm_config)\n        return model.eval()\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        self.tensorizer_config.verify_with_model_config(model_config)\n\n        with self.tensorizer_config.open_stream():\n            pass\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        model_config = vllm_config.model_config\n        parallel_config = vllm_config.parallel_config\n        self._verify_config(model_config, parallel_config)\n\n        if parallel_config.tensor_parallel_size > 1:\n            from vllm.distributed import get_tensor_model_parallel_rank\n\n            self.tensorizer_config.tensorizer_uri = (\n                self.tensorizer_config.tensorizer_uri %\n                get_tensor_model_parallel_rank())\n\n        if is_vllm_tensorized(self.tensorizer_config):\n            return self._load_model_serialized(vllm_config=vllm_config)\n        return self._load_model_serialized_cpu(vllm_config=vllm_config)\n\n    @staticmethod\n    def save_model(\n        model: torch.nn.Module,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        serialize_vllm_model(\n            model=model,\n            tensorizer_config=tensorizer_config,\n        )\n\n\nclass ShardedStateLoader(BaseModelLoader):\n    \"\"\"\n    Model loader that directly loads each worker's model state dict, which\n    enables a fast load path for large tensor-parallel models where each worker\n    only needs to read its own shard rather than the entire checkpoint. See\n    `examples/offline_inference/save_sharded_state.py` for creating a sharded\n    checkpoint.\n    \"\"\"\n\n    DEFAULT_PATTERN = \"model-rank-{rank}-part-{part}.safetensors\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        extra_config = ({} if load_config.model_loader_extra_config is None\n                        else load_config.model_loader_extra_config.copy())\n        self.pattern = extra_config.pop(\"pattern\", self.DEFAULT_PATTERN)\n        if extra_config:\n            raise ValueError(f\"Unexpected extra config keys for load format \"\n                             f\"{load_config.load_format}: \"\n                             f\"{load_config.model_loader_extra_config.keys()}\")\n\n    @staticmethod\n    def _filter_subtensors(\n        tensors: Dict[str, torch.Tensor], ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Filter out all tensors that share the same memory or a subset of the\n        memory of another tensor.\n        \"\"\"\n        same_storage_groups: Dict[Any, List[Tuple[str, torch.Tensor]]] = (\n            collections.defaultdict(list))\n        for key, tensor in tensors.items():\n            if tensor.numel():\n                ptr = tensor.untyped_storage().data_ptr()\n                same_storage_groups[tensor.device, ptr].append((key, tensor))\n\n        def get_end_ptr(tensor: torch.Tensor) -> int:\n            return tensor.view(-1)[-1].data_ptr() + tensor.element_size()\n\n        result: Dict[str, torch.Tensor] = {}\n        for group in same_storage_groups.values():\n            for k, t in group:\n                a, b = t.data_ptr(), get_end_ptr(t)\n                for k2, t2 in group:\n                    if not t2.is_contiguous():\n                        continue\n                    a2, b2 = t2.data_ptr(), get_end_ptr(t2)\n                    if a < a2 or b2 < b:\n                        continue\n                    if a2 < a or b < b2 or not t.is_contiguous():\n                        break  # t2 covers strictly more memory than t.\n                    if k2 < k:\n                        # Same tensors, keep the one with the smaller key.\n                        break\n                else:\n                    result[k] = t\n        return result\n\n    def _prepare_weights(self, model_name_or_path: str,\n                         revision: Optional[str]):\n        if os.path.isdir(model_name_or_path):\n            return model_name_or_path\n        else:\n            allow_patterns = [\"*.safetensors\"]\n            return download_weights_from_hf(\n                model_name_or_path,\n                self.load_config.download_dir,\n                allow_patterns,\n                revision,\n                ignore_patterns=self.load_config.ignore_patterns,\n            )\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        self._prepare_weights(model_config.model, model_config.revision)\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n        from safetensors.torch import safe_open\n\n        from vllm.distributed import get_tensor_model_parallel_rank\n\n        local_model_path = self._prepare_weights(model_config.model,\n                                                 model_config.revision)\n\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(vllm_config=vllm_config)\n                for _, module in model.named_modules():\n                    quant_method = getattr(module, \"quant_method\", None)\n                    if quant_method is not None:\n                        quant_method.process_weights_after_loading(module)\n            rank = get_tensor_model_parallel_rank()\n            pattern = os.path.join(\n                local_model_path,\n                self.pattern.format(rank=rank, part=\"*\"),\n            )\n            filepaths = glob.glob(pattern)\n            if not filepaths:\n                # TODO: support un-sharded checkpoints too\n                raise ValueError(\n                    f\"Could not find checkpoint files '{pattern}', only \"\n                    f\"pre-sharded checkpoints are currently supported!\")\n            state_dict = self._filter_subtensors(model.state_dict())\n            for path in filepaths:\n                with safe_open(path, framework=\"pt\") as f:\n                    for key in f.keys():  # noqa: SIM118\n                        tensor = f.get_tensor(key)\n                        # If loading with LoRA enabled, additional padding may\n                        # be added to certain parameters. We only load into a\n                        # narrowed view of the parameter data.\n                        param_data = state_dict[key].data\n                        param_shape = state_dict[key].shape\n                        for dim, size in enumerate(tensor.shape):\n                            if size < param_shape[dim]:\n                                param_data = param_data.narrow(dim, 0, size)\n                        if tensor.shape != param_shape:\n                            logger.warning(\n                                \"loading tensor of shape %s into \"\n                                \"parameter '%s' of shape %s\",\n                                tensor.shape,\n                                key,\n                                param_shape,\n                            )\n                        param_data.copy_(tensor)\n                        state_dict.pop(key)\n            if state_dict:\n                raise ValueError(\n                    f\"Missing keys {tuple(state_dict)} in loaded state!\")\n        return model.eval()\n\n    @staticmethod\n    def save_model(\n        model: torch.nn.Module,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from safetensors.torch import save_file\n\n        from vllm.distributed import get_tensor_model_parallel_rank\n\n        if pattern is None:\n            pattern = ShardedStateLoader.DEFAULT_PATTERN\n        rank = get_tensor_model_parallel_rank()\n        part_idx = 0\n        total_size = 0\n        state_dict = ShardedStateLoader._filter_subtensors(model.state_dict())\n        state_dict_part: Dict[str, torch.Tensor] = {}\n        for key, tensor in state_dict.items():\n            param_size = tensor.nelement() * tensor.element_size()\n            if max_size is not None and total_size + param_size > max_size:\n                filename = pattern.format(rank=rank, part=part_idx)\n                save_file(\n                    state_dict_part,\n                    os.path.join(path, filename),\n                )\n                part_idx += 1\n                total_size = 0\n                state_dict_part = {}\n            state_dict_part[key] = tensor\n            total_size += param_size\n        if len(state_dict_part) > 0:\n            filename = pattern.format(rank=rank, part=part_idx)\n            save_file(\n                state_dict_part,\n                os.path.join(path, filename),\n            )\n\n\nclass BitsAndBytesModelLoader(BaseModelLoader):\n    \"\"\"Model loader to load model weights with BitAndBytes quantization.\"\"\"\n\n    possible_config_file_names = [\"adapter_config.json\"]\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n\n        # Save the module names without sharding.\n        self.unsharded_weights_modules: List[str] = []\n        # Save the module names that are sharded by column.\n        self.column_sharded_weights_modules: List[str] = []\n        # Store all module names (from transformers) that support\n        # BNB quantization.\n        self.target_modules: List[str] = []\n        # mapping weight names from transformers to vllm.\n        self.weight_mapper: Callable = lambda name: name\n\n    def _get_weight_files(\n        self,\n        model_name_or_path: str,\n        allowed_patterns: List[str],\n        revision: Optional[str] = None,\n    ) -> Tuple[List[str], str]:\n        \"\"\"Retrieve weight files. Download the files if necessary.\n\n        Return the weight files and the file pattern.\"\"\"\n        is_local = os.path.isdir(model_name_or_path)\n\n        if is_local:\n            for pattern in allowed_patterns:\n                weight_files = glob.glob(\n                    os.path.join(model_name_or_path, pattern))\n                if weight_files:\n                    return weight_files, pattern\n        else:\n            hf_api = HfApi()\n            repo_files = hf_api.list_repo_files(repo_id=model_name_or_path)\n            for pattern in allowed_patterns:\n                matching_files = fnmatch.filter(repo_files, pattern)\n                if matching_files:\n                    hf_folder = download_weights_from_hf(\n                        model_name_or_path,\n                        self.load_config.download_dir,\n                        [pattern],\n                        revision,\n                        ignore_patterns=self.load_config.ignore_patterns,\n                    )\n                    return glob.glob(os.path.join(hf_folder, pattern)), pattern\n\n        raise RuntimeError(\n            f\"No model weights found in: `{model_name_or_path}`\")\n\n    def _prepare_weights(self, model_name_or_path: str,\n                         revision: Optional[str]) -> Tuple[List[str], bool]:\n        \"\"\"Prepare weight files for the model.\"\"\"\n\n        allowed_patterns = [\"*.safetensors\", \"*.bin\", \"*.pt\"]\n\n        hf_weights_files, matched_pattern = self._get_weight_files(\n            model_name_or_path, allowed_patterns, revision)\n\n        if matched_pattern != \"*.safetensors\":\n            hf_weights_files = filter_files_not_needed_for_inference(\n                hf_weights_files)\n\n        if len(hf_weights_files) == 0:\n            raise RuntimeError(\n                f\"Cannot find any model weights with `{model_name_or_path}`\")\n\n        return hf_weights_files, matched_pattern == \"*.safetensors\"\n\n    def _hf_weight_iter(self, hf_weights_files, use_safetensors: bool):\n        if use_safetensors:\n            iterator = safetensors_weights_iterator(hf_weights_files)\n        else:\n            iterator = pt_weights_iterator(hf_weights_files)\n        for name, param in iterator:\n            # mapping weight names from transformers to vllm.\n            yield self.weight_mapper(name), param\n\n    def _get_quantized_weights_iterator(\n        self,\n        model_name_or_path: str,\n        revision: Optional[str],\n        pre_quant: bool,\n        load_8bit: bool,\n    ) -> Tuple[Generator[Tuple[str, torch.Tensor], None, None], Dict[str,\n                                                                     Any]]:\n        \"\"\"Get an iterator to the model weights with bitsandbytes quantization,\n        as well as the quantization state dictionary.\"\"\"\n\n        # only load the bitsandbytes module when needed\n        try:\n            import bitsandbytes\n\n            if bitsandbytes.__version__ < \"0.45.0\":\n                raise ImportError(\"bitsandbytes version is wrong. Please \"\n                                  \"install bitsandbytes>=0.45.0.\")\n        except ImportError as err:\n            raise ImportError(\"Please install bitsandbytes>=0.45.0 via \"\n                              \"`pip install bitsandbytes>=0.45.0` to use \"\n                              \"bitsandbytes quantizer.\") from err\n\n        hf_weights_files, use_safetensors = self._prepare_weights(\n            model_name_or_path, revision)\n\n        quant_state_dict: Dict[str, Any] = {}\n\n        if pre_quant:\n            if load_8bit:\n                return self._quantized_8bit_generator(\n                    hf_weights_files, use_safetensors,\n                    quant_state_dict), quant_state_dict\n            else:\n                return self._quantized_4bit_generator(\n                    hf_weights_files, use_safetensors,\n                    quant_state_dict), quant_state_dict\n\n        return self._unquantized_generator(hf_weights_files, use_safetensors,\n                                           quant_state_dict), quant_state_dict\n\n    def _is_8bit_weight_name(self, weight_name: str):\n        quantized_suffix = {\".scb\", \".weight_format\"}\n        return any(weight_name.lower().endswith(suffix)\n                   for suffix in quantized_suffix)\n\n    def _is_4bit_weight_name(self, weight_name: str):\n        quantized_suffix = {\n            \"absmax\",\n            \"quant_map\",\n            \"nested_absmax\",\n            \"nested_quant_map\",\n            \"bitsandbytes\",\n        }\n        suffix = weight_name.split(\".\")[-1]\n        return any(q_suffix in suffix for q_suffix in quantized_suffix)\n\n    def _quantized_8bit_generator(self, hf_weights_files, use_safetensors,\n                                  quant_state_dict) -> Generator:\n        for weight_name, weight_tensor in self._hf_weight_iter(\n                hf_weights_files, use_safetensors):\n            if not weight_name.lower().endswith(\".scb\"):\n                continue\n\n            weight_key = weight_name.lower().replace(\".scb\", \".weight\")\n            quant_state_dict[weight_key] = weight_tensor\n\n        for weight_name, weight_tensor in self._hf_weight_iter(\n                hf_weights_files, use_safetensors):\n            if self._is_8bit_weight_name(weight_name):\n                continue\n\n            if weight_name in quant_state_dict:\n                set_weight_attrs(weight_tensor, {\"load_in_8bit\": True})\n                yield weight_name, weight_tensor\n            else:\n                yield weight_name, weight_tensor\n\n    def _quantized_4bit_generator(self, hf_weights_files, use_safetensors,\n                                  quant_state_dict) -> Generator:\n        from bitsandbytes.functional import QuantState\n\n        # First iterate over all quant state weights\n        weight_iterator = self._hf_weight_iter(hf_weights_files,\n                                               use_safetensors)\n        temp_state_dict = {}\n        for weight_name, weight_tensor in weight_iterator:\n            if not self._is_4bit_weight_name(weight_name):\n                continue\n            # bitsandbytes library requires\n            # weight.quant_state.bitsandbytes__* in CPU\n            if \"quant_state.bitsandbytes\" in weight_name:\n                temp_state_dict[weight_name] = weight_tensor.cpu().data\n            else:\n                temp_state_dict[weight_name] = weight_tensor\n\n        # Closure to parse quant_state for each prequant weight\n        def _parse_quant_state(param_name: str,\n                               temp_state_dict: Dict) -> QuantState:\n            quant_state = {}\n            for k in temp_state_dict:\n                if param_name + \".\" in k:\n                    quant_state[k] = temp_state_dict[k]\n\n            return QuantState.from_dict(quant_state, device=\"cuda\")\n\n        # Second iterate over all prequant and normal weights\n        # pre quantized weights would have a quant_state\n        for weight_name, weight_tensor in self._hf_weight_iter(\n                hf_weights_files, use_safetensors):\n            if self._is_4bit_weight_name(weight_name):\n                continue\n\n            if (f\"{weight_name}.quant_state.bitsandbytes__nf4\"\n                    in temp_state_dict) or (\n                        f\"{weight_name}.quant_state.bitsandbytes__fp4\"\n                        in temp_state_dict):\n                quant_state = _parse_quant_state(weight_name, temp_state_dict)\n                quant_state_dict[weight_name] = quant_state\n                yield weight_name, weight_tensor\n            else:\n                yield weight_name, weight_tensor\n\n    def _unquantized_generator(self, hf_weights_files, use_safetensors,\n                               quant_state_dict) -> Generator:\n        from bitsandbytes.functional import quantize_4bit\n\n        tp_size = get_tensor_model_parallel_world_size()\n        tp_rank = get_tensor_model_parallel_rank()\n\n        for weight_name, weight_tensor in self._hf_weight_iter(\n                hf_weights_files, use_safetensors):\n            if any(target_module in weight_name for target_module in\n                   self.target_modules) and weight_name.endswith(\".weight\"):\n                # Without sharding\n                if any(\n                        weight_name.startswith(module)\n                        for module in self.unsharded_weights_modules):\n                    weight_sub_tensor = weight_tensor\n                # Shard by column\n                elif any(\n                        weight_name.startswith(module)\n                        for module in self.column_sharded_weights_modules):\n                    total_size = weight_tensor.size(-1)\n                    start_index = total_size // tp_size * tp_rank\n                    end_index = total_size // tp_size * (tp_rank + 1)\n                    weight_sub_tensor = weight_tensor[...,\n                                                      start_index:end_index]\n                # Weights have fused on disk. In this case, we assume that the\n                # weight and module use same name.\n                elif any(\n                        weight_name.startswith(module)\n                        for module in self.maybe_fused_weights_modules):\n                    # special case for fused weights\n                    # get the size of each shard weight tensor\n                    total_shard_sizes = next(\n                        (sizes for module, sizes in\n                         self.maybe_fused_weights_modules.items()\n                         if weight_name.startswith(module)))\n                    total_size = weight_tensor.size(0)\n                    assert total_size == sum(total_shard_sizes)\n                    # get the start/end index of each shard weight tensor\n                    total_start_index = list(\n                        itertools.accumulate([0] + total_shard_sizes))[:-1]\n                    shard_weights_index = [(\n                        idx + size // tp_size * tp_rank,\n                        idx + size // tp_size * (tp_rank + 1),\n                    ) for idx, size in zip(total_start_index,\n                                           total_shard_sizes)]\n                    # slice and reorder the weight tensor\n                    weight_tensor = [\n                        weight_tensor[start_index:end_index, ...]\n                        for start_index, end_index in shard_weights_index\n                    ]\n                    weight_sub_tensor = torch.cat(weight_tensor, dim=0)\n                # Shard by row\n                else:\n                    total_size = weight_tensor.size(0)\n                    start_index = total_size // tp_size * tp_rank\n                    end_index = total_size // tp_size * (tp_rank + 1)\n                    weight_sub_tensor = weight_tensor[start_index:end_index,\n                                                      ...]\n\n                # bitsandbytes requires data in GPU\n                if weight_sub_tensor.is_cuda:\n                    loaded_weight = weight_sub_tensor\n                else:\n                    loaded_weight = weight_sub_tensor.cuda()\n\n                # remove the following after the issue is fixed:\n                # https://github.com/bitsandbytes-foundation/bitsandbytes/issues/1342\n                if loaded_weight.is_contiguous() is False:\n                    loaded_weight = loaded_weight.contiguous()\n\n                with set_default_torch_dtype(torch.float32):\n                    processed_weight, quant_state = quantize_4bit(\n                        loaded_weight,\n                        compress_statistics=True,\n                        quant_type=\"nf4\",\n                    )\n\n                quant_state_dict[weight_name] = quant_state\n            else:\n                processed_weight = weight_tensor\n\n            yield weight_name, processed_weight\n\n    def _get_bnb_target_modules(self, model: nn.Module) -> None:\n\n        for name, module in model.named_modules():\n            if isinstance(module, (LinearBase, )):\n                last_name = name.split(\".\")[-1]\n                if sub_modules := self.modules_mapping.packed_mapping.get(\n                        last_name, []):\n                    # Map vllm's names to transformers's names.\n                    for sub_name in sub_modules:\n                        self.target_modules.append(\n                            name.replace(last_name, sub_name))\n                # Add original module name even if the module has stacked map,\n                # in case model has a mixture of disk-merged and disk-splitted\n                # weights with same last name.\n                self.target_modules.append(name)\n\n        assert (self.target_modules\n                ), \"vllm currently does not support BNB quantization for\"\n        f\" {type(model).__name__}\"\n\n    def _load_weights(self, model_config: ModelConfig,\n                      model: nn.Module) -> None:\n        if not hasattr(model, \"load_weights\"):\n            raise AttributeError(\n                \"The required method 'load_weights' is not defined in class\"\n                f\" {type(model).__name__}.\")\n\n        if not hasattr(model, \"packed_modules_mapping\"):\n            raise AttributeError(\n                f\"Model {type(model).__name__} does not support BitsAndBytes \"\n                \"quantization yet. No 'packed_modules_mapping' found.\")\n\n        self.modules_mapping = ParamMapping(\n            copy.deepcopy(model.packed_modules_mapping))\n\n        # For some models like Molmo, we need to use hf_to_vllm_mapper\n        # to ensure correct loading of weights.\n        if hf_to_vllm_mapper := getattr(model, \"hf_to_vllm_mapper\", None):\n            self.weight_mapper = lambda name: hf_to_vllm_mapper._map_name(name)\n\n        # Modules whose weights might have fused on disk\n        # we need their output_sizes to make shard in flight correctly with TP\n        self.maybe_fused_weights_modules: Dict[str, List[int]] = {}\n        self._get_bnb_target_modules(model)\n        for name, module in model.named_modules():\n            # Some modules like `ReplicatedLinear` should not have their weights\n            # sharded. The reason for implementing it this way is to avoid new\n            # static variable in the model implementation.\n            if isinstance(module, (ReplicatedLinear, )):\n                self.unsharded_weights_modules.append(name)\n            # `QKVParallelLinear` and `MergedColumnParallelLinear` might have\n            # fused weights on disk. We need to use the output sizes of these\n            # modules to shard the weights correctly.\n            elif isinstance(module,\n                            (QKVParallelLinear, MergedColumnParallelLinear)):\n                self.maybe_fused_weights_modules[name] = module.output_sizes\n            # In TP, these weights are partitioned along the column\n            # dimension (dim=-1)\n            elif isinstance(module, (RowParallelLinear, )):\n                self.column_sharded_weights_modules.append(name)\n\n        self.model_type = type(model).__name__\n\n        logger.info(\"Loading weights with BitsAndBytes quantization. \"\n                    \" May take a while ...\")\n\n        quant_config = getattr(model_config.hf_config, \"quantization_config\",\n                               None)\n\n        pre_quant = False\n        if quant_config is not None:\n            quant_method = quant_config.get(\"quant_method\")\n            if quant_method == \"bitsandbytes\":\n                pre_quant = True\n            else:\n                raise ValueError(\n                    f\"BitsAndBytes loader does not support {quant_method} \"\n                    \"quantization\")\n\n        # The quant_states in pre_quantized models cannot work with a split\n        # weight tensor. So TP does not work with pre_quantized bnb models.\n        if pre_quant and get_tensor_model_parallel_world_size() > 1:\n            raise ValueError(\n                \"Prequant BitsAndBytes models with tensor parallelism is not \"\n                \"supported. Please try with pipeline parallelism.\")\n\n        load_8bit = False\n        if pre_quant:\n            load_8bit = quant_config.get(\"load_in_8bit\", False)\n\n        qweight_iterator, quant_state_dict = (\n            self._get_quantized_weights_iterator(model_config.model,\n                                                 model_config.revision,\n                                                 pre_quant, load_8bit))\n\n        weights_to_load = {name for name, _ in model.named_parameters()}\n        loaded_weights = model.load_weights(qweight_iterator)\n        # Some models may have weights loading tracker unimplemented.\n        if loaded_weights is not None:\n            weights_not_loaded = weights_to_load - loaded_weights\n            if weights_not_loaded:\n                raise ValueError(\"Following weights were not initialized from \"\n                                 f\"checkpoint: {weights_not_loaded}\")\n\n        torch.cuda.empty_cache()\n\n        param_dict = dict(model.named_parameters())\n        stacked_quant_state_dict: Dict[str, Dict[int, Any]] = {}\n        # TODO: Change this lazy import to normal import\n        # after the checks are updated to run on a new version\n        from vllm.model_executor.models.utils import is_pp_missing_parameter\n\n        for quant_param_name in quant_state_dict:\n            if is_pp_missing_parameter(quant_param_name, model):\n                continue\n\n            non_stacked_param_name = quant_param_name\n\n            shard_index = 0\n            for shard_name, (\n                    weight_name,\n                    index,\n            ) in self.modules_mapping.inverse_packed_mapping.items():\n                # Some models, such as MiniCPM V2.5/2.6, contain both\n                # module names 'kv_proj' and 'qkv_proj'. To prevent 'kv_proj'\n                # from being incorrectly identified as being present in\n                # 'vpm.encoder.layers.0.self_attn.qkv_proj.weight\n                shard_pos = quant_param_name.find(shard_name)\n                can_correct_rename = (shard_pos\n                                      > 0) and (quant_param_name[shard_pos - 1]\n                                                == \".\")\n                # If the quant_param_name is packed, it won't occur in the\n                # param_dict before renaming.\n                new_quant_param_name = quant_param_name.replace(\n                    shard_name, weight_name)\n                need_rename = (quant_param_name not in param_dict) \\\n                              and (new_quant_param_name in param_dict)\n                if can_correct_rename and need_rename:\n                    shard_index = index\n                    quant_param_name = new_quant_param_name\n                    break\n\n            # Models like Clip/Siglip may skip some layers in initialization,\n            # causing unused quant_param_name in state_dict.\n            if quant_param_name not in param_dict:\n                continue\n\n            if quant_param_name not in stacked_quant_state_dict:\n                stacked_quant_state_dict[quant_param_name] = {}\n\n            stacked_quant_state_dict[quant_param_name][shard_index] = (\n                quant_state_dict[non_stacked_param_name])\n\n        # save quant_states and offsets as the attributes of the parameters\n        for param_name, param in param_dict.items():\n            if param_name in stacked_quant_state_dict:\n                quant_states = stacked_quant_state_dict[param_name]\n                set_weight_attrs(param, {\"bnb_quant_state\": quant_states})\n\n                pack_ratio = getattr(param, \"pack_factor\", -1)\n                if pack_ratio == -1:\n                    raise ValueError(\n                        f\"pack_factor not set for parameter {param_name}.\")\n\n                num_elements = [0] * len(quant_states)\n                for seq, quant_state in quant_states.items():\n                    num_elements[seq] = (math.prod(quant_state.shape) //\n                                         pack_ratio)\n\n                offsets = np.concatenate(([0], np.cumsum(num_elements)))\n                set_weight_attrs(param, {\"bnb_shard_offsets\": offsets})\n\n                if load_8bit:\n                    set_weight_attrs(\n                        param, {\"matmul_state\": [None] * len(quant_states)})\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        self._prepare_weights(model_config.model, model_config.revision)\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(vllm_config=vllm_config)\n\n                self._load_weights(model_config, model)\n\n        return model.eval()\n\n\nclass GGUFModelLoader(BaseModelLoader):\n    \"\"\"\n    Model loader that can load GGUF files. This is useful for loading models\n    that are quantized with GGUF and saved in the GGUF format. This loader\n    supports loading both full models and sharded models.\n    \"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            raise ValueError(f\"Model loader extra config is not supported for \"\n                             f\"load format {load_config.load_format}\")\n\n    def _prepare_weights(self, model_name_or_path: str):\n        if os.path.isfile(model_name_or_path):\n            return model_name_or_path\n        else:\n            raise ValueError(f\"{model_name_or_path} is not a file.\")\n\n    def _get_gguf_weights_map(self, model_config: ModelConfig):\n        \"\"\"\n        GGUF uses this naming convention for their tensors from HF checkpoint:\n        `blk.N.BB.weight` and `blk.N.BB.bias`\n        where N signifies the block number of a layer, and BB signifies the\n        attention/mlp layer components.\n        See \"Standardized tensor names\" in\n        https://github.com/ggerganov/ggml/blob/master/docs/gguf.md for details.\n        \"\"\"\n        config = model_config.hf_config\n        model_type = config.model_type\n        # hack: ggufs have a different name than transformers\n        if model_type == \"cohere\":\n            model_type = \"command-r\"\n        arch = None\n        for key, value in gguf.MODEL_ARCH_NAMES.items():\n            if value == model_type:\n                arch = key\n                break\n        if arch is None:\n            raise RuntimeError(f\"Unknown gguf model_type: {model_type}\")\n        num_layers = config.num_hidden_layers\n        name_map = gguf.get_tensor_name_map(arch, num_layers)\n        with torch.device(\"meta\"):\n            dummy_model = AutoModelForCausalLM.from_config(config)\n        state_dict = dummy_model.state_dict()\n\n        gguf_to_hf_name_map = {}\n        for hf_name in state_dict:\n            name, suffix = hf_name.rsplit(\".\", 1)\n            gguf_name = name_map.get_name(name)\n            gguf_to_hf_name_map[f\"{gguf_name}.{suffix}\"] = hf_name\n        return gguf_to_hf_name_map\n\n    def _get_weights_iterator(\n        self, model_name_or_path: str, gguf_to_hf_name_map: Dict[str, str]\n    ) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        return gguf_quant_weights_iterator(model_name_or_path,\n                                           gguf_to_hf_name_map)\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        self._prepare_weights(model_config.model)\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n        local_model_path = self._prepare_weights(model_config.model)\n        gguf_weights_map = self._get_gguf_weights_map(model_config)\n        # we can only know if tie word embeddings after mapping weights\n        if \"lm_head.weight\" in get_gguf_extra_tensor_names(\n                local_model_path, gguf_weights_map):\n            model_config.hf_config.update({\"tie_word_embeddings\": True})\n\n        with set_default_torch_dtype(model_config.dtype):\n            with torch.device(device_config.device):\n                model = _initialize_model(vllm_config=vllm_config)\n            model.load_weights(\n                self._get_weights_iterator(local_model_path, gguf_weights_map))\n        return model\n\n\nclass RunaiModelStreamerLoader(BaseModelLoader):\n    \"\"\"\n        Model loader that can load safetensors \n        files from local FS or S3 bucket.\n    \"\"\"\n\n    def __init__(self, load_config: LoadConfig):\n        super().__init__(load_config)\n        if load_config.model_loader_extra_config:\n            extra_config = load_config.model_loader_extra_config\n\n            if (\"concurrency\" in extra_config\n                    and isinstance(extra_config.get(\"concurrency\"), int)):\n                os.environ[\"RUNAI_STREAMER_CONCURRENCY\"] = str(\n                    extra_config.get(\"concurrency\"))\n\n            if (\"memory_limit\" in extra_config\n                    and isinstance(extra_config.get(\"memory_limit\"), int)):\n                os.environ[\"RUNAI_STREAMER_MEMORY_LIMIT\"] = str(\n                    extra_config.get(\"memory_limit\"))\n\n            runai_streamer_s3_endpoint = os.getenv(\n                'RUNAI_STREAMER_S3_ENDPOINT')\n            aws_endpoint_url = os.getenv('AWS_ENDPOINT_URL')\n            if (runai_streamer_s3_endpoint is None\n                    and aws_endpoint_url is not None):\n                os.environ[\"RUNAI_STREAMER_S3_ENDPOINT\"] = aws_endpoint_url\n\n    def _prepare_weights(self, model_name_or_path: str,\n                         revision: Optional[str]) -> List[str]:\n        \"\"\"Prepare weights for the model.\n\n        If the model is not local, it will be downloaded.\"\"\"\n        is_s3_path = is_s3(model_name_or_path)\n        is_local = os.path.isdir(model_name_or_path)\n        safetensors_pattern = \"*.safetensors\"\n        index_file = SAFE_WEIGHTS_INDEX_NAME\n\n        hf_folder = (model_name_or_path if\n                     (is_local or is_s3_path) else download_weights_from_hf(\n                         model_name_or_path,\n                         self.load_config.download_dir,\n                         [safetensors_pattern],\n                         revision,\n                         ignore_patterns=self.load_config.ignore_patterns,\n                     ))\n\n        if is_s3_path:\n            hf_weights_files = s3_glob(path=hf_folder,\n                                       allow_pattern=[safetensors_pattern])\n        else:\n            hf_weights_files = glob.glob(\n                os.path.join(hf_folder, safetensors_pattern))\n\n        if not is_local and not is_s3_path:\n            download_safetensors_index_file_from_hf(\n                model_name_or_path, index_file, self.load_config.download_dir,\n                revision)\n\n        if not hf_weights_files:\n            raise RuntimeError(\n                f\"Cannot find any safetensors model weights with \"\n                f\"`{model_name_or_path}`\")\n\n        return hf_weights_files\n\n    def _get_weights_iterator(\n            self, model_or_path: str,\n            revision: str) -> Generator[Tuple[str, torch.Tensor], None, None]:\n        \"\"\"Get an iterator for the model weights based on the load format.\"\"\"\n        hf_weights_files = self._prepare_weights(model_or_path, revision)\n        return runai_safetensors_weights_iterator(hf_weights_files)\n\n    def download_model(self, model_config: ModelConfig) -> None:\n        \"\"\"Download model if necessary\"\"\"\n        self._prepare_weights(model_config.model, model_config.revision)\n\n    def load_model(self, vllm_config: VllmConfig) -> nn.Module:\n        \"\"\"Perform streaming of the model to destination\"\"\"\n        device_config = vllm_config.device_config\n        model_config = vllm_config.model_config\n\n        target_device = torch.device(device_config.device)\n        with set_default_torch_dtype(model_config.dtype):\n            with target_device:\n                model = _initialize_model(vllm_config=vllm_config)\n\n            model_weights = model_config.model\n            if hasattr(model_config, \"model_weights\"):\n                model_weights = model_config.model_weights\n            model.load_weights(\n                self._get_weights_iterator(model_weights,\n                                           model_config.revision))\n\n            for _, module in model.named_modules():\n                quant_method = getattr(module, \"quant_method\", None)\n                if quant_method is not None:\n                    with device_loading_context(module, target_device):\n                        quant_method.process_weights_after_loading(module)\n        return model.eval()\n\n\ndef get_model_loader(load_config: LoadConfig) -> BaseModelLoader:\n    \"\"\"Get a model loader based on the load format.\"\"\"\n\n    if isinstance(load_config.load_format, type):\n        return load_config.load_format(load_config)\n\n    if load_config.load_format == LoadFormat.DUMMY:\n        return DummyModelLoader(load_config)\n\n    if load_config.load_format == LoadFormat.TENSORIZER:\n        return TensorizerLoader(load_config)\n\n    if load_config.load_format == LoadFormat.SHARDED_STATE:\n        return ShardedStateLoader(load_config)\n\n    if load_config.load_format == LoadFormat.BITSANDBYTES:\n        return BitsAndBytesModelLoader(load_config)\n\n    if load_config.load_format == LoadFormat.GGUF:\n        return GGUFModelLoader(load_config)\n\n    if load_config.load_format == LoadFormat.RUNAI_STREAMER:\n        return RunaiModelStreamerLoader(load_config)\n\n    return DefaultModelLoader(load_config)\n",
      "diff": "diff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py\nindex 62babcddd..4be511d12 100644\n--- a/vllm/model_executor/model_loader/loader.py\n+++ b/vllm/model_executor/model_loader/loader.py\n@@ -398,11 +398,13 @@ class DefaultModelLoader(BaseModelLoader):\n                     # parameters onto device for processing and back off after.\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n-                elif isinstance(module, Attention) and \\\n+                if isinstance(module, Attention) and \\\n                     hasattr(module, \"process_weights_after_loading\"):\n                     # When attention modules need to process weights after\n                     # currently only used by MLA\n-                    module.process_weights_after_loading()\n+                    # TODO(lucas): see if there is a way to unify the signatures\n+                    # of process_weights_after_loading\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -439,6 +441,11 @@ class DummyModelLoader(BaseModelLoader):\n                     with device_loading_context(\n                             module, torch.device(device_config.device)):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -633,6 +640,12 @@ class ShardedStateLoader(BaseModelLoader):\n                     quant_method = getattr(module, \"quant_method\", None)\n                     if quant_method is not None:\n                         quant_method.process_weights_after_loading(module)\n+                    if isinstance(module, Attention) and \\\n+                        hasattr(module, \"process_weights_after_loading\"):\n+                        # When attention modules need to process weights after\n+                        # currently only used by MLA\n+                        module.process_weights_after_loading(\n+                            model_config.dtype)\n             rank = get_tensor_model_parallel_rank()\n             pattern = os.path.join(\n                 local_model_path,\n@@ -1272,7 +1285,7 @@ class GGUFModelLoader(BaseModelLoader):\n \n class RunaiModelStreamerLoader(BaseModelLoader):\n     \"\"\"\n-        Model loader that can load safetensors \n+        Model loader that can load safetensors\n         files from local FS or S3 bucket.\n     \"\"\"\n \n@@ -1369,6 +1382,11 @@ class RunaiModelStreamerLoader(BaseModelLoader):\n                 if quant_method is not None:\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()",
      "change_type": "modified",
      "lines_added": 22,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/model_executor/models/deepseek_v3.py",
      "old_content": "# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2023 DeepSeek-AI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only DeepseekV3 model.\"\"\"\nfrom typing import Any, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom transformers import PretrainedConfig\n\nfrom vllm.attention import Attention, AttentionMetadata\nfrom vllm.config import CacheConfig, VllmConfig\nfrom vllm.distributed import (get_pp_group,\n                              get_tensor_model_parallel_world_size,\n                              tensor_model_parallel_all_reduce)\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.fused_moe import FusedMoE\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               MergedColumnParallelLinear,\n                                               ReplicatedLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.logits_processor import LogitsProcessor\nfrom vllm.model_executor.layers.quantization import QuantizationConfig\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import SamplerOutput, get_sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    ParallelLMHead, VocabParallelEmbedding)\nfrom vllm.model_executor.model_loader.weight_utils import default_weight_loader\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.sequence import IntermediateTensors\n\nfrom .interfaces import SupportsPP\nfrom .utils import (PPMissingLayer, is_pp_missing_parameter,\n                    make_empty_intermediate_tensors_factory, make_layers,\n                    maybe_prefix)\n\n\nclass DeepseekV3MLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        quant_config: Optional[QuantizationConfig] = None,\n        reduce_results: bool = True,\n        prefix: str = \"\",\n    ) -> None:\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.gate_up_proj\")\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           quant_config=quant_config,\n                                           reduce_results=reduce_results,\n                                           prefix=f\"{prefix}.down_proj\")\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass DeepseekV3MoE(nn.Module):\n\n    def __init__(\n        self,\n        config: PretrainedConfig,\n        quant_config: Optional[QuantizationConfig] = None,\n        prefix: str = \"\",\n    ):\n        super().__init__()\n        self.tp_size = get_tensor_model_parallel_world_size()\n        self.routed_scaling_factor = config.routed_scaling_factor\n        self.n_shared_experts = config.n_shared_experts\n        self.routed_scaling_factor = config.routed_scaling_factor\n        if self.tp_size > config.n_routed_experts:\n            raise ValueError(\n                f\"Tensor parallel size {self.tp_size} is greater than \"\n                f\"the number of experts {config.n_routed_experts}.\")\n\n        if config.hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {config.hidden_act}. \"\n                             \"Only silu is supported for now.\")\n\n        self.gate = ReplicatedLinear(config.hidden_size,\n                                     config.n_routed_experts,\n                                     bias=False,\n                                     quant_config=None,\n                                     prefix=f\"{prefix}.gate\")\n        if config.topk_method == \"noaux_tc\":\n            self.gate.e_score_correction_bias = nn.Parameter(\n                torch.empty(config.n_routed_experts))\n        else:\n            self.gate.e_score_correction_bias = None\n\n        self.experts = FusedMoE(\n            num_experts=config.n_routed_experts,\n            top_k=config.num_experts_per_tok,\n            hidden_size=config.hidden_size,\n            intermediate_size=config.moe_intermediate_size,\n            reduce_results=False,\n            renormalize=config.norm_topk_prob,\n            quant_config=quant_config,\n            use_grouped_topk=True,\n            num_expert_group=config.n_group,\n            topk_group=config.topk_group,\n            prefix=f\"{prefix}.experts\",\n            scoring_func=config.scoring_func,\n            e_score_correction_bias=self.gate.e_score_correction_bias)\n\n        if config.n_shared_experts is not None:\n            intermediate_size = (config.moe_intermediate_size *\n                                 config.n_shared_experts)\n            self.shared_experts = DeepseekV3MLP(\n                hidden_size=config.hidden_size,\n                intermediate_size=intermediate_size,\n                hidden_act=config.hidden_act,\n                quant_config=quant_config,\n                reduce_results=False,\n            )\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        num_tokens, hidden_dim = hidden_states.shape\n        hidden_states = hidden_states.view(-1, hidden_dim)\n        if self.n_shared_experts is not None:\n            shared_output = self.shared_experts(hidden_states)\n        # router_logits: (num_tokens, n_experts)\n        router_logits, _ = self.gate(hidden_states)\n        final_hidden_states = self.experts(\n            hidden_states=hidden_states,\n            router_logits=router_logits) * self.routed_scaling_factor\n        if shared_output is not None:\n            final_hidden_states = final_hidden_states + shared_output\n        if self.tp_size > 1:\n            final_hidden_states = tensor_model_parallel_all_reduce(\n                final_hidden_states)\n\n        return final_hidden_states.view(num_tokens, hidden_dim)\n\n\ndef yarn_get_mscale(scale: float = 1, mscale: float = 1) -> float:\n    import math\n    if scale <= 1:\n        return 1.0\n    return 0.1 * mscale * math.log(scale) + 1.0\n\n\nclass DeepseekV3Attention(nn.Module):\n\n    def __init__(\n        self,\n        config: PretrainedConfig,\n        hidden_size: int,\n        num_heads: int,\n        qk_nope_head_dim: int,\n        qk_rope_head_dim: int,\n        v_head_dim: int,\n        q_lora_rank: int,\n        kv_lora_rank: int,\n        rope_theta: float = 10000,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        max_position_embeddings: int = 8192,\n        cache_config: Optional[CacheConfig] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n        prefix: str = \"\",\n    ) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.qk_nope_head_dim = qk_nope_head_dim\n        self.qk_rope_head_dim = qk_rope_head_dim\n        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n        self.v_head_dim = v_head_dim\n        self.q_lora_rank = q_lora_rank\n        self.kv_lora_rank = kv_lora_rank\n        self.num_heads = num_heads\n        tp_size = get_tensor_model_parallel_world_size()\n        assert num_heads % tp_size == 0\n        self.num_local_heads = num_heads // tp_size\n        self.scaling = self.qk_head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        if self.q_lora_rank is not None:\n            self.q_a_proj = ReplicatedLinear(self.hidden_size,\n                                             self.q_lora_rank,\n                                             bias=False,\n                                             quant_config=quant_config,\n                                             prefix=f\"{prefix}.q_a_proj\")\n            self.q_a_layernorm = RMSNorm(self.q_lora_rank,\n                                         eps=config.rms_norm_eps)\n            self.q_b_proj = ColumnParallelLinear(q_lora_rank,\n                                                 self.num_heads *\n                                                 self.qk_head_dim,\n                                                 bias=False,\n                                                 quant_config=quant_config,\n                                                 prefix=f\"{prefix}.q_b_proj\")\n        else:\n            self.q_proj = ColumnParallelLinear(self.hidden_size,\n                                               self.num_heads *\n                                               self.qk_head_dim,\n                                               bias=False,\n                                               quant_config=quant_config,\n                                               prefix=f\"{prefix}.q_proj\")\n\n        self.kv_a_proj_with_mqa = ReplicatedLinear(\n            self.hidden_size,\n            self.kv_lora_rank + self.qk_rope_head_dim,\n            bias=False,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,\n                                      eps=config.rms_norm_eps)\n        self.kv_b_proj = ColumnParallelLinear(\n            self.kv_lora_rank,\n            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n            bias=False,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.kv_b_proj\")\n        # O projection.\n        self.o_proj = RowParallelLinear(self.num_heads * self.v_head_dim,\n                                        self.hidden_size,\n                                        bias=False,\n                                        quant_config=quant_config,\n                                        prefix=f\"{prefix}.o_proj\")\n        if rope_scaling:\n            rope_scaling[\"rope_type\"] = 'deepseek_yarn'\n            self.use_normal_rope = False\n        else:\n            self.use_normal_rope = True\n        self.rotary_emb = get_rope(qk_rope_head_dim,\n                                   rotary_dim=qk_rope_head_dim,\n                                   max_position=max_position_embeddings,\n                                   base=rope_theta,\n                                   rope_scaling=rope_scaling,\n                                   is_neox_style=False)\n\n        if rope_scaling:\n            mscale_all_dim = rope_scaling.get(\"mscale_all_dim\", False)\n            scaling_factor = rope_scaling[\"factor\"]\n            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))\n            self.scaling = self.scaling * mscale * mscale\n\n        self.attn = Attention(self.num_local_heads,\n                              self.qk_head_dim,\n                              self.scaling,\n                              num_kv_heads=self.num_local_heads,\n                              cache_config=cache_config,\n                              quant_config=quant_config,\n                              prefix=f\"{prefix}.attn\")\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        if self.q_lora_rank is not None:\n            q = self.q_a_proj(hidden_states)[0]\n            q = self.q_a_layernorm(q)\n            q = self.q_b_proj(q)[0].view(-1, self.num_local_heads,\n                                         self.qk_head_dim)\n        else:\n            q = self.q_proj(hidden_states)[0].view(-1, self.num_local_heads,\n                                                   self.qk_head_dim)\n        q_nope, q_pe = q.split([self.qk_nope_head_dim, self.qk_rope_head_dim],\n                               dim=-1)\n        latent_cache = self.kv_a_proj_with_mqa(hidden_states)[0]\n        kv_a, _ = latent_cache.split(\n            [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n        latent_cache = latent_cache.unsqueeze(1)\n        kv_a = self.kv_a_layernorm(kv_a.contiguous())\n        kv = self.kv_b_proj(kv_a)[0]\n        kv = kv.view(-1, self.num_local_heads,\n                     self.qk_nope_head_dim + self.v_head_dim)\n        k_nope, v = kv.split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n        k_pe = latent_cache[:, :, self.kv_lora_rank:]\n\n        if self.use_normal_rope:\n            seq_len = positions.size(0)\n            ori_q_pe_shape, ori_k_pe_shape = q_pe.shape, k_pe.shape\n            q_pe = q_pe.reshape(seq_len, -1)\n            k_pe = k_pe.reshape(seq_len, -1)\n\n        q_pe, k_pe = self.rotary_emb(positions, q_pe, k_pe)\n\n        if self.use_normal_rope:\n            q_pe, k_pe = q_pe.view(ori_q_pe_shape), k_pe.view(ori_k_pe_shape)\n\n        q[..., self.qk_nope_head_dim:] = q_pe\n        k = torch.empty_like(q)\n        k[..., :self.qk_nope_head_dim] = k_nope\n        k[..., self.qk_nope_head_dim:] = k_pe\n        # padding value to qk_head_dim for alignment\n        v = torch.nn.functional.pad(\n            v, [0, self.qk_head_dim - self.v_head_dim],\n            value=0).view(-1, self.num_local_heads * self.qk_head_dim)\n        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n        attn_output = attn_output.view(\n            -1, self.num_local_heads,\n            self.qk_head_dim)[..., :self.v_head_dim].reshape(\n                -1, self.num_local_heads * self.v_head_dim)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass DeepseekV3DecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: PretrainedConfig,\n        prefix: str,\n        cache_config: Optional[CacheConfig] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        # DecoderLayers are created with `make_layers` which passes the prefix\n        # with the layer's index.\n        layer_idx = int(prefix.split(sep='.')[-1])\n        self.self_attn = DeepseekV3Attention(\n            config=config,\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            qk_nope_head_dim=config.qk_nope_head_dim,\n            qk_rope_head_dim=config.qk_rope_head_dim,\n            v_head_dim=config.v_head_dim,\n            q_lora_rank=config.q_lora_rank\n            if hasattr(config, \"q_lora_rank\") else None,\n            kv_lora_rank=config.kv_lora_rank,\n            rope_theta=rope_theta,\n            rope_scaling=rope_scaling,\n            max_position_embeddings=max_position_embeddings,\n            cache_config=cache_config,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.self_attn\",\n        )\n        if (config.n_routed_experts is not None\n                and layer_idx >= config.first_k_dense_replace\n                and layer_idx % config.moe_layer_freq == 0):\n            self.mlp = DeepseekV3MoE(\n                config=config,\n                quant_config=quant_config,\n                prefix=f\"{prefix}.mlp\",\n            )\n        else:\n            self.mlp = DeepseekV3MLP(\n                hidden_size=config.hidden_size,\n                intermediate_size=config.intermediate_size,\n                hidden_act=config.hidden_act,\n                quant_config=quant_config,\n                prefix=f\"{prefix}.mlp\",\n            )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            attn_metadata=attn_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\n# TODO(simon): check whether we support torch compile for Deepseek V3\n# @support_torch_compile\nclass DeepseekV3Model(nn.Module):\n\n    fall_back_to_pt_during_load = False\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__()\n\n        config = vllm_config.model_config.hf_config\n        cache_config = vllm_config.cache_config\n        quant_config = vllm_config.quant_config\n\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        if get_pp_group().is_first_rank:\n            self.embed_tokens = VocabParallelEmbedding(\n                config.vocab_size,\n                config.hidden_size,\n            )\n        else:\n            self.embed_tokens = PPMissingLayer()\n\n        self.start_layer, self.end_layer, self.layers = make_layers(\n            config.num_hidden_layers,\n            lambda prefix: DeepseekV3DecoderLayer(\n                config,\n                prefix,\n                cache_config=cache_config,\n                quant_config=quant_config,\n            ),\n            prefix=f\"{prefix}.layers\")\n\n        if get_pp_group().is_last_rank:\n            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        else:\n            self.norm = PPMissingLayer()\n        self.make_empty_intermediate_tensors = (\n            make_empty_intermediate_tensors_factory(\n                [\"hidden_states\", \"residual\"], config.hidden_size))\n\n    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n        return self.embed_tokens(input_ids)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        inputs_embeds: Optional[torch.Tensor] = None,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        if get_pp_group().is_first_rank:\n            if inputs_embeds is not None:\n                hidden_states = inputs_embeds\n            else:\n                hidden_states = self.get_input_embeddings(input_ids)\n            residual = None\n        else:\n            assert intermediate_tensors is not None\n            hidden_states = intermediate_tensors[\"hidden_states\"]\n            residual = intermediate_tensors[\"residual\"]\n\n        for i in range(self.start_layer, self.end_layer):\n            layer = self.layers[i]\n            hidden_states, residual = layer(positions, hidden_states,\n                                            kv_caches[i - self.start_layer],\n                                            attn_metadata, residual)\n\n        if not get_pp_group().is_last_rank:\n            return IntermediateTensors({\n                \"hidden_states\": hidden_states,\n                \"residual\": residual\n            })\n\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass DeepseekV3ForCausalLM(nn.Module, SupportsPP):\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__()\n        config = vllm_config.model_config.hf_config\n        quant_config = vllm_config.quant_config\n        self.config = config\n        self.quant_config = quant_config\n        self.model = DeepseekV3Model(vllm_config=vllm_config,\n                                     prefix=maybe_prefix(prefix, \"model\"))\n        self.lm_head = ParallelLMHead(config.vocab_size,\n                                      config.hidden_size,\n                                      quant_config=quant_config)\n        self.logits_processor = LogitsProcessor(config.vocab_size)\n        self.sampler = get_sampler()\n        self.make_empty_intermediate_tensors = (\n            self.model.make_empty_intermediate_tensors)\n\n    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n        return self.model.get_input_embeddings(input_ids)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   attn_metadata, intermediate_tensors,\n                                   inputs_embeds)\n        return hidden_states\n\n    def compute_logits(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> Optional[torch.Tensor]:\n        logits = self.logits_processor(self.lm_head, hidden_states,\n                                       sampling_metadata)\n        return logits\n\n    def sample(\n        self,\n        logits: Optional[torch.Tensor],\n        sampling_metadata: SamplingMetadata,\n    ) -> Optional[SamplerOutput]:\n        next_tokens = self.sampler(logits, sampling_metadata)\n        return next_tokens\n\n    def make_empty_intermediate_tensors(\n            self, batch_size: int, dtype: torch.dtype,\n            device: torch.device) -> IntermediateTensors:\n        return IntermediateTensors({\n            \"hidden_states\":\n            torch.zeros((batch_size, self.config.hidden_size),\n                        dtype=dtype,\n                        device=device),\n            \"residual\":\n            torch.zeros((batch_size, self.config.hidden_size),\n                        dtype=dtype,\n                        device=device),\n        })\n\n    def load_weights(self, weights: Iterable[Tuple[str,\n                                                   torch.Tensor]]) -> Set[str]:\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n\n        # Params for weights, fp8 weight scales, fp8 activation scales\n        # (param_name, weight_name, expert_id, shard_id)\n        expert_params_mapping = FusedMoE.make_expert_params_mapping(\n            ckpt_gate_proj_name=\"gate_proj\",\n            ckpt_down_proj_name=\"down_proj\",\n            ckpt_up_proj_name=\"up_proj\",\n            num_experts=self.config.n_routed_experts)\n\n        params_dict = dict(self.named_parameters())\n        loaded_params: Set[str] = set()\n        for name, loaded_weight in weights:\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n\n            # TODO(simon): support nextn predict layers\n            if hasattr(self.config, \"num_nextn_predict_layers\"\n                       ) and self.config.num_nextn_predict_layers > 0:\n                assert self.config.num_nextn_predict_layers == 1\n                layer_idx = self.config.num_hidden_layers\n                if name.startswith(f\"model.layers.{layer_idx}\"):\n                    continue\n\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                # Skip non-stacked layers and experts (experts handled below).\n                if weight_name not in name:\n                    continue\n                # We have mlp.experts[0].gate_proj in the checkpoint.\n                # Since we handle the experts below in expert_params_mapping,\n                # we need to skip here BEFORE we update the name, otherwise\n                # name will be updated to mlp.experts[0].gate_up_proj, which\n                # will then be updated below in expert_params_mapping\n                # for mlp.experts[0].gate_gate_up_proj, which breaks load.\n                if ((\"mlp.experts.\" in name) and name not in params_dict):\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n\n                if is_pp_missing_parameter(name, self):\n                    continue\n\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                for mapping in expert_params_mapping:\n                    param_name, weight_name, expert_id, shard_id = mapping\n                    if weight_name not in name:\n                        continue\n                    name = name.replace(weight_name, param_name)\n\n                    if is_pp_missing_parameter(name, self):\n                        continue\n\n                    param = params_dict[name]\n                    weight_loader = param.weight_loader\n                    weight_loader(param,\n                                  loaded_weight,\n                                  name,\n                                  shard_id=shard_id,\n                                  expert_id=expert_id)\n                    break\n                else:\n                    # Skip loading extra bias for GPTQ models.\n                    if name.endswith(\".bias\") and name not in params_dict:\n                        continue\n\n                    if is_pp_missing_parameter(name, self):\n                        continue\n\n                    param = params_dict[name]\n                    weight_loader = getattr(param, \"weight_loader\",\n                                            default_weight_loader)\n                    weight_loader(param, loaded_weight)\n            loaded_params.add(name)\n        return loaded_params\n",
      "diff": "diff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py\nindex 0b44f0d06..f6ab53c85 100644\n--- a/vllm/model_executor/models/deepseek_v3.py\n+++ b/vllm/model_executor/models/deepseek_v3.py\n@@ -27,7 +27,7 @@ from torch import nn\n from transformers import PretrainedConfig\n \n from vllm.attention import Attention, AttentionMetadata\n-from vllm.config import CacheConfig, VllmConfig\n+from vllm.config import CacheConfig, ModelConfig, VllmConfig\n from vllm.distributed import (get_pp_group,\n                               get_tensor_model_parallel_world_size,\n                               tensor_model_parallel_all_reduce)\n@@ -333,12 +333,156 @@ class DeepseekV3Attention(nn.Module):\n         return output\n \n \n+class DeepseekV3MLAAttention(nn.Module):\n+    \"\"\"\n+    Main reference: DeepseekV2 paper, and FlashInfer Implementation\n+    (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n+    \n+    For more info see MLACommonImpl in: vllm/attention/backends/mla/utils.py\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        hidden_size: int,\n+        num_heads: int,\n+        qk_nope_head_dim: int,\n+        qk_rope_head_dim: int,\n+        v_head_dim: int,\n+        q_lora_rank: Optional[int],\n+        kv_lora_rank: int,\n+        rope_theta: float = 10000,\n+        rope_scaling: Optional[Dict[str, Any]] = None,\n+        max_position_embeddings: int = 8192,\n+        cache_config: Optional[CacheConfig] = None,\n+        quant_config: Optional[QuantizationConfig] = None,\n+        prefix: str = \"\",\n+    ) -> None:\n+        super().__init__()\n+        self.hidden_size = hidden_size\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+\n+        self.q_lora_rank = q_lora_rank\n+        self.kv_lora_rank = kv_lora_rank\n+\n+        self.num_heads = num_heads\n+        tp_size = get_tensor_model_parallel_world_size()\n+        assert num_heads % tp_size == 0\n+        self.num_local_heads = num_heads // tp_size\n+\n+        self.scaling = self.qk_head_dim**-0.5\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+\n+        if self.q_lora_rank is not None:\n+            self.q_a_proj = ReplicatedLinear(self.hidden_size,\n+                                             self.q_lora_rank,\n+                                             bias=False,\n+                                             quant_config=quant_config,\n+                                             prefix=f\"{prefix}.q_a_proj\")\n+            self.q_a_layernorm = RMSNorm(self.q_lora_rank,\n+                                         eps=config.rms_norm_eps)\n+            self.q_b_proj = ColumnParallelLinear(q_lora_rank,\n+                                                 self.num_heads *\n+                                                 self.qk_head_dim,\n+                                                 bias=False,\n+                                                 quant_config=quant_config,\n+                                                 prefix=f\"{prefix}.q_b_proj\")\n+        else:\n+            self.q_proj = ColumnParallelLinear(self.hidden_size,\n+                                               self.num_heads *\n+                                               self.qk_head_dim,\n+                                               bias=False,\n+                                               quant_config=quant_config,\n+                                               prefix=f\"{prefix}.q_proj\")\n+\n+        self.kv_a_proj_with_mqa = ReplicatedLinear(\n+            self.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n+        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,\n+                                      eps=config.rms_norm_eps)\n+        self.kv_b_proj = ColumnParallelLinear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_b_proj\")\n+        self.o_proj = RowParallelLinear(self.num_heads * self.v_head_dim,\n+                                        self.hidden_size,\n+                                        bias=False,\n+                                        quant_config=quant_config,\n+                                        prefix=f\"{prefix}.o_proj\")\n+\n+        rope_scaling[\"rope_type\"] = 'deepseek_yarn'\n+        self.rotary_emb = get_rope(qk_rope_head_dim,\n+                                   rotary_dim=qk_rope_head_dim,\n+                                   max_position=max_position_embeddings,\n+                                   base=rope_theta,\n+                                   rope_scaling=rope_scaling,\n+                                   is_neox_style=False)\n+        if rope_scaling:\n+            mscale_all_dim = rope_scaling.get(\"mscale_all_dim\", False)\n+            scaling_factor = rope_scaling[\"factor\"]\n+            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))\n+            self.scaling = self.scaling * mscale * mscale\n+\n+        self.mla_attn = Attention(\n+            num_heads=self.num_local_heads,\n+            head_size=self.kv_lora_rank,\n+            scale=self.scaling,\n+            num_kv_heads=1,\n+            cache_config=cache_config,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.attn\",\n+            use_mla=True,\n+            # MLA Args\n+            q_lora_rank=self.q_lora_rank,\n+            kv_lora_rank=self.kv_lora_rank,\n+            qk_nope_head_dim=self.qk_nope_head_dim,\n+            qk_rope_head_dim=self.qk_rope_head_dim,\n+            qk_head_dim=self.qk_head_dim,\n+            v_head_dim=self.v_head_dim,\n+            rotary_emb=self.rotary_emb,\n+            q_proj=self.q_proj if self.q_lora_rank is None else self.q_b_proj,\n+            kv_b_proj=self.kv_b_proj,\n+            o_proj=self.o_proj,\n+        )\n+\n+        self.prefix = prefix\n+        self.debug_layer_idx = int(self.prefix.split(\".\")[-2])\n+\n+    def forward(\n+        self,\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        kv_cache: torch.Tensor,\n+        attn_metadata: AttentionMetadata,\n+    ) -> torch.Tensor:\n+        if self.q_lora_rank is not None:\n+            ckq = self.q_a_proj(hidden_states)[0]\n+            hidden_states_or_q_c = self.q_a_layernorm(ckq)\n+        else:\n+            hidden_states_or_q_c = hidden_states\n+        kv_c, k_pe = self.kv_a_proj_with_mqa(hidden_states)[0].split(\n+            [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+        kv_c_normed = self.kv_a_layernorm(kv_c.contiguous())\n+        return self.mla_attn(hidden_states_or_q_c, kv_c_normed, k_pe, kv_cache,\n+                             attn_metadata)\n+\n+\n class DeepseekV3DecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: PretrainedConfig,\n         prefix: str,\n+        model_config: ModelConfig,\n         cache_config: Optional[CacheConfig] = None,\n         quant_config: Optional[QuantizationConfig] = None,\n     ) -> None:\n@@ -351,7 +495,11 @@ class DeepseekV3DecoderLayer(nn.Module):\n         # DecoderLayers are created with `make_layers` which passes the prefix\n         # with the layer's index.\n         layer_idx = int(prefix.split(sep='.')[-1])\n-        self.self_attn = DeepseekV3Attention(\n+        if model_config.use_mla:\n+            attn_cls = DeepseekV3MLAAttention\n+        else:\n+            attn_cls = DeepseekV3Attention\n+        self.self_attn = attn_cls(\n             config=config,\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n@@ -428,6 +576,7 @@ class DeepseekV3Model(nn.Module):\n         super().__init__()\n \n         config = vllm_config.model_config.hf_config\n+        model_config = vllm_config.model_config\n         cache_config = vllm_config.cache_config\n         quant_config = vllm_config.quant_config\n \n@@ -447,6 +596,7 @@ class DeepseekV3Model(nn.Module):\n             lambda prefix: DeepseekV3DecoderLayer(\n                 config,\n                 prefix,\n+                model_config=model_config,\n                 cache_config=cache_config,\n                 quant_config=quant_config,\n             ),",
      "change_type": "modified",
      "lines_added": 153,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/worker/cache_engine.py",
      "old_content": "\"\"\"CacheEngine class for managing the KV cache.\"\"\"\nfrom typing import List\n\nimport torch\n\nfrom vllm.attention import get_attn_backend\nfrom vllm.config import CacheConfig, DeviceConfig, ModelConfig, ParallelConfig\nfrom vllm.logger import init_logger\nfrom vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, LayerBlockType,\n                        get_dtype_size, is_pin_memory_available)\n\nlogger = init_logger(__name__)\n\n\nclass CacheEngine:\n    \"\"\"Manages the KV cache.\n\n    This class is responsible for initializing and managing the GPU and CPU KV\n    caches. It also provides methods for performing KV cache operations, such\n    as swapping and copying.\n    \"\"\"\n\n    def __init__(\n        self,\n        cache_config: CacheConfig,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        device_config: DeviceConfig,\n    ) -> None:\n        self.cache_config = cache_config\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.device_config = device_config\n\n        self.head_size = model_config.get_head_size()\n        # Models like Jamba, have mixed typed layers, E.g Mamba\n        self.num_attention_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\n\n        self.block_size = cache_config.block_size\n        self.num_gpu_blocks = cache_config.num_gpu_blocks\n        if self.num_gpu_blocks:\n            self.num_gpu_blocks //= parallel_config.pipeline_parallel_size\n        self.num_cpu_blocks = cache_config.num_cpu_blocks\n        if self.num_cpu_blocks:\n            self.num_cpu_blocks //= parallel_config.pipeline_parallel_size\n\n        if cache_config.cache_dtype == \"auto\":\n            self.dtype = model_config.dtype\n        else:\n            self.dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]\n\n        # Get attention backend.\n        self.attn_backend = get_attn_backend(self.head_size,\n                                             model_config.dtype,\n                                             cache_config.cache_dtype,\n                                             self.block_size,\n                                             model_config.is_attention_free,\n                                             use_mla=model_config.use_mla)\n\n        # Initialize the cache.\n        self.gpu_cache = self._allocate_kv_cache(\n            self.num_gpu_blocks, self.device_config.device_type)\n        self.cpu_cache = self._allocate_kv_cache(self.num_cpu_blocks, \"cpu\")\n\n    def _allocate_kv_cache(\n        self,\n        num_blocks: int,\n        device: str,\n    ) -> List[torch.Tensor]:\n        \"\"\"Allocates KV cache on the specified device.\"\"\"\n        kv_cache_shape = self.attn_backend.get_kv_cache_shape(\n            num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n        pin_memory = is_pin_memory_available() if device == \"cpu\" else False\n        kv_cache: List[torch.Tensor] = []\n        for _ in range(self.num_attention_layers):\n            # null block in CpuGpuBlockAllocator requires at least that\n            # block to be zeroed-out.\n            # We zero-out everything for simplicity.\n            kv_cache.append(\n                torch.zeros(kv_cache_shape,\n                            dtype=self.dtype,\n                            pin_memory=pin_memory,\n                            device=device))\n        return kv_cache\n\n    def swap_in(self, src_to_dst: torch.Tensor) -> None:\n        for i in range(self.num_attention_layers):\n            self.attn_backend.swap_blocks(self.cpu_cache[i], self.gpu_cache[i],\n                                          src_to_dst)\n\n    def swap_out(self, src_to_dst: torch.Tensor) -> None:\n        for i in range(self.num_attention_layers):\n            self.attn_backend.swap_blocks(self.gpu_cache[i], self.cpu_cache[i],\n                                          src_to_dst)\n\n    def copy(self, src_to_dsts: torch.Tensor) -> None:\n        self.attn_backend.copy_blocks(self.gpu_cache, src_to_dsts)\n\n    @staticmethod\n    def get_cache_block_size(\n        cache_config: CacheConfig,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n    ) -> int:\n        head_size = model_config.get_head_size()\n        num_heads = model_config.get_num_kv_heads(parallel_config)\n        num_attention_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n\n        key_cache_block = cache_config.block_size * num_heads * head_size\n        value_cache_block = key_cache_block\n        total = num_attention_layers * (key_cache_block + value_cache_block)\n        if cache_config.cache_dtype == \"auto\":\n            dtype = model_config.dtype\n        else:\n            dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_config.cache_dtype]\n        dtype_size = get_dtype_size(dtype)\n        return dtype_size * total\n",
      "diff": "diff --git a/vllm/worker/cache_engine.py b/vllm/worker/cache_engine.py\nindex 08316ba74..c427b759b 100644\n--- a/vllm/worker/cache_engine.py\n+++ b/vllm/worker/cache_engine.py\n@@ -110,7 +110,9 @@ class CacheEngine:\n             parallel_config, LayerBlockType.attention)\n \n         key_cache_block = cache_config.block_size * num_heads * head_size\n-        value_cache_block = key_cache_block\n+        # For MLA there is no value cache, since the latent vector\n+        # is joint keys and values.\n+        value_cache_block = key_cache_block if not model_config.use_mla else 0\n         total = num_attention_layers * (key_cache_block + value_cache_block)\n         if cache_config.cache_dtype == \"auto\":\n             dtype = model_config.dtype",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 2
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 10,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 10
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_flashmla, test_model_runner, test_block_fp8, test_fp8_quant, test_fp8)",
    "is_benchmark_actually_there": "",
    "sample_clues": "after, attention, backends"
  }
}