{
  "commit_hash": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
  "parent_hash": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03",
  "message": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2024-12-11 23:14:20 -0800",
  "files_changed": [
    {
      "file_path": "vllm/v1/worker/gpu_input_batch.py",
      "old_content": "# Datastructures defining an input batch\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Set\n\nimport numpy as np\nimport torch\n\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.v1.sample.metadata import SamplingMetadata\n\nif TYPE_CHECKING:\n    from vllm.multimodal.inputs import PlaceholderRange\n\n\n@dataclass\nclass CachedRequestState:\n\n    req_id: str\n    prompt_token_ids: List[int]\n    prompt: Optional[str]\n    mm_inputs: List[MultiModalKwargs]\n    mm_positions: List[\"PlaceholderRange\"]\n    sampling_params: SamplingParams\n    generator: Optional[torch.Generator]\n\n    block_ids: List[int]\n    num_computed_tokens: int\n    output_token_ids: List[int]\n\n    @property\n    def num_tokens(self) -> int:\n        return len(self.prompt_token_ids) + len(self.output_token_ids)\n\n\nclass InputBatch:\n\n    def __init__(\n        self,\n        max_num_reqs: int,\n        max_model_len: int,\n        max_num_blocks_per_req: int,\n        device: torch.device,\n        pin_memory: bool,\n    ):\n        self.max_num_reqs = max_num_reqs\n        self.max_model_len = max_model_len\n        self.max_num_blocks_per_req = max_num_blocks_per_req\n        self.device = device\n        self.pin_memory = pin_memory\n\n        self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n        self.req_id_to_index: Dict[str, int] = {}\n\n        self.token_ids_cpu = np.empty((max_num_reqs, max_model_len),\n                                      dtype=np.int32)\n        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n\n        # Attention-related.\n        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),\n                                       device=self.device,\n                                       dtype=torch.int32)\n        self.block_table_cpu_tensor = torch.zeros(\n            (max_num_reqs, max_num_blocks_per_req),\n            device=\"cpu\",\n            dtype=torch.int32,\n            pin_memory=pin_memory,\n        )\n        self.block_table_cpu = self.block_table_cpu_tensor.numpy()\n\n        # Sampling-related.\n        self.temperature = torch.empty((max_num_reqs, ),\n                                       dtype=torch.float32,\n                                       device=device)\n        self.temperature_cpu_tensor = torch.empty((max_num_reqs, ),\n                                                  dtype=torch.float32,\n                                                  device=\"cpu\",\n                                                  pin_memory=pin_memory)\n        self.temperature_cpu = self.temperature_cpu_tensor.numpy()\n        self.greedy_reqs: Set[str] = set()\n        self.random_reqs: Set[str] = set()\n\n        self.top_p = torch.empty((max_num_reqs, ),\n                                 dtype=torch.float32,\n                                 device=device)\n        self.top_p_cpu_tensor = torch.empty((max_num_reqs, ),\n                                            dtype=torch.float32,\n                                            device=\"cpu\",\n                                            pin_memory=pin_memory)\n        self.top_p_cpu = self.top_p_cpu_tensor.numpy()\n        self.top_p_reqs: Set[str] = set()\n\n        self.top_k = torch.empty((max_num_reqs, ),\n                                 dtype=torch.int32,\n                                 device=device)\n        self.top_k_cpu_tensor = torch.empty((max_num_reqs, ),\n                                            dtype=torch.int32,\n                                            device=\"cpu\",\n                                            pin_memory=pin_memory)\n        self.top_k_cpu = self.top_k_cpu_tensor.numpy()\n        self.top_k_reqs: Set[str] = set()\n\n        # req_index -> generator\n        # NOTE(woosuk): The indices of the requests that do not have their own\n        # generator should not be included in the dictionary.\n        self.generators: Dict[int, torch.Generator] = {}\n\n        self.num_logprobs: Dict[str, int] = {}\n        self.prompt_logprob_reqs: Set[str] = set()\n\n    def add_request(\n        self,\n        request: \"CachedRequestState\",\n        req_index: Optional[int] = None,\n    ) -> None:\n        if req_index is None:\n            req_index = self.num_reqs\n        assert req_index < self.max_num_reqs\n\n        req_id = request.req_id\n        self.req_ids[req_index] = req_id\n        self.req_id_to_index[req_id] = req_index\n\n        # Copy the prompt token ids and output token ids.\n        num_prompt_tokens = len(request.prompt_token_ids)\n        self.token_ids_cpu[\n            req_index, :num_prompt_tokens] = request.prompt_token_ids\n        start_idx = num_prompt_tokens\n        end_idx = start_idx + len(request.output_token_ids)\n        self.token_ids_cpu[req_index,\n                           start_idx:end_idx] = request.output_token_ids\n\n        self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens\n        num_blocks = len(request.block_ids)\n        self.block_table_cpu[req_index, :num_blocks] = request.block_ids\n\n        sampling_params = request.sampling_params\n        self.temperature_cpu[req_index] = sampling_params.temperature\n        if sampling_params.sampling_type == SamplingType.GREEDY:\n            self.greedy_reqs.add(req_id)\n        else:\n            self.random_reqs.add(req_id)\n\n        self.top_p_cpu[req_index] = sampling_params.top_p\n        if sampling_params.top_p < 1:\n            self.top_p_reqs.add(req_id)\n        self.top_k_cpu[req_index] = sampling_params.top_k\n        if sampling_params.top_k > 0:\n            self.top_k_reqs.add(req_id)\n\n        # NOTE(woosuk): self.generators should not include the requests that\n        # do not have their own generator.\n        if request.generator is not None:\n            self.generators[req_index] = request.generator\n\n        num_logprobs = sampling_params.logprobs\n        if num_logprobs is not None and num_logprobs > 0:\n            self.num_logprobs[req_id] = num_logprobs\n        if sampling_params.prompt_logprobs:\n            self.prompt_logprob_reqs.add(req_id)\n\n    def remove_request(self, req_id: str) -> Optional[int]:\n        req_index = self.req_id_to_index.pop(req_id, None)\n        if req_index is None:\n            return None\n        self.req_ids[req_index] = None\n\n        self.greedy_reqs.discard(req_id)\n        self.random_reqs.discard(req_id)\n        self.top_p_reqs.discard(req_id)\n        self.top_k_reqs.discard(req_id)\n        self.generators.pop(req_index, None)\n        self.num_logprobs.pop(req_id, None)\n        self.prompt_logprob_reqs.discard(req_id)\n        return req_index\n\n    def clear(self) -> None:\n        self.req_ids = [None] * self.max_num_reqs\n        self.req_id_to_index.clear()\n        self.greedy_reqs.clear()\n        self.random_reqs.clear()\n        self.top_p_reqs.clear()\n        self.top_k_reqs.clear()\n        self.generators.clear()\n        self.num_logprobs.clear()\n        self.prompt_logprob_reqs.clear()\n\n    def condense(self, empty_req_indices: List[int]) -> None:\n        if self.num_reqs == 0:\n            # The batched states are empty.\n            return\n\n        # NOTE(woosuk): This function assumes that the empty_req_indices\n        # is sorted in descending order.\n        last_req_index = self.num_reqs + len(empty_req_indices) - 1\n        while empty_req_indices:\n            # Find the largest non-empty index.\n            while last_req_index in empty_req_indices:\n                last_req_index -= 1\n\n            # Find the smallest empty index.\n            empty_index = empty_req_indices.pop()\n            if empty_index >= last_req_index:\n                break\n\n            # Swap the states.\n            req_id = self.req_ids[last_req_index]\n            self.req_ids[empty_index] = req_id\n            self.req_ids[last_req_index] = None\n            self.req_id_to_index[req_id] = empty_index\n\n            # TODO(woosuk): Optimize the copy of token_ids_cpu and\n            # block_table_cpu.\n            self.token_ids_cpu[empty_index] = self.token_ids_cpu[\n                last_req_index]\n            self.num_computed_tokens_cpu[\n                empty_index] = self.num_computed_tokens_cpu[last_req_index]\n            self.block_table_cpu[empty_index] = self.block_table_cpu[\n                last_req_index]\n            self.temperature_cpu[empty_index] = self.temperature_cpu[\n                last_req_index]\n            self.top_p_cpu[empty_index] = self.top_p_cpu[last_req_index]\n            self.top_k_cpu[empty_index] = self.top_k_cpu[last_req_index]\n            generator = self.generators.pop(last_req_index, None)\n            if generator is not None:\n                self.generators[empty_index] = generator\n\n            # Decrement last_req_index since it is now empty.\n            last_req_index -= 1\n\n    def make_sampling_metadata(\n        self,\n        skip_copy: bool = False,\n    ) -> SamplingMetadata:\n        if not skip_copy:\n            self.temperature[:self.num_reqs].copy_(\n                self.temperature_cpu_tensor[:self.num_reqs], non_blocking=True)\n            self.top_p[:self.num_reqs].copy_(\n                self.top_p_cpu_tensor[:self.num_reqs], non_blocking=True)\n            self.top_k[:self.num_reqs].copy_(\n                self.top_k_cpu_tensor[:self.num_reqs], non_blocking=True)\n        return SamplingMetadata(\n            temperature=self.temperature[:self.num_reqs],\n            all_greedy=self.all_greedy,\n            all_random=self.all_random,\n            top_p=self.top_p[:self.num_reqs],\n            top_k=self.top_k[:self.num_reqs],\n            no_top_p=self.no_top_p,\n            no_top_k=self.no_top_k,\n            generators=self.generators,\n            max_num_logprobs=self.max_num_logprobs,\n        )\n\n    @property\n    def num_reqs(self) -> int:\n        return len(self.req_id_to_index)\n\n    @property\n    def all_greedy(self) -> bool:\n        return len(self.random_reqs) == 0\n\n    @property\n    def all_random(self) -> bool:\n        return len(self.greedy_reqs) == 0\n\n    @property\n    def no_top_p(self) -> bool:\n        return len(self.top_p_reqs) == 0\n\n    @property\n    def no_top_k(self) -> bool:\n        return len(self.top_k_reqs) == 0\n\n    @property\n    def max_num_logprobs(self) -> int:\n        return max(self.num_logprobs.values()) if self.num_logprobs else 0\n\n    @property\n    def no_logprob(self) -> bool:\n        return len(self.num_logprobs) == 0\n\n    @property\n    def no_prompt_logprob(self) -> bool:\n        return len(self.prompt_logprob_reqs) == 0\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex 25d95ac6e..9046b37f6 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -53,14 +53,23 @@ class InputBatch:\n         self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n         self.req_id_to_index: Dict[str, int] = {}\n \n-        self.token_ids_cpu = np.empty((max_num_reqs, max_model_len),\n-                                      dtype=np.int32)\n+        # TODO(woosuk): This buffer could be too large if max_model_len is big.\n+        # Find a way to reduce the CPU memory usage.\n+        self.token_ids_cpu_tensor = torch.zeros(\n+            (max_num_reqs, max_model_len),\n+            device=\"cpu\",\n+            dtype=torch.int32,\n+            pin_memory=pin_memory,\n+        )\n+        self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),\n-                                       device=self.device,\n-                                       dtype=torch.int32)\n+        self.block_table = torch.zeros(\n+            (max_num_reqs, max_num_blocks_per_req),\n+            device=self.device,\n+            dtype=torch.int32,\n+        )\n         self.block_table_cpu_tensor = torch.zeros(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",",
      "change_type": "modified",
      "lines_added": 15,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/v1/worker/gpu_model_runner.py",
      "old_content": "import gc\nimport time\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nfrom vllm.config import CompilationLevel, VllmConfig\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.forward_context import set_forward_context\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.sampling_params import SamplingType\nfrom vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,\n                        LayerBlockType, cdiv, is_pin_memory_available)\nfrom vllm.v1.attention.backends.flash_attn import (FlashAttentionBackend,\n                                                   FlashAttentionMetadata)\nfrom vllm.v1.outputs import ModelRunnerOutput\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch\n\nif TYPE_CHECKING:\n    from vllm.v1.core.scheduler import SchedulerOutput\n\nlogger = init_logger(__name__)\n\n\nclass GPUModelRunner:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        device: torch.device,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n    ):\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.load_config = vllm_config.load_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.speculative_config = vllm_config.speculative_config\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config\n        self.observability_config = vllm_config.observability_config\n\n        model_config = self.model_config\n        cache_config = self.cache_config\n        scheduler_config = self.scheduler_config\n        parallel_config = self.parallel_config\n        self.device = device\n        self.pin_memory = is_pin_memory_available()\n        self.dtype = self.model_config.dtype\n        if cache_config.cache_dtype == \"auto\":\n            self.kv_cache_dtype = self.dtype\n        else:\n            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[\n                cache_config.cache_dtype]\n\n        self.is_multimodal_model = model_config.is_multimodal_model\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_model_len = model_config.max_model_len\n        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n        self.max_num_tokens = scheduler_config.max_num_batched_tokens\n\n        # Model-related.\n        self.num_attn_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\n        self.head_size = model_config.get_head_size()\n        self.hidden_size = model_config.get_hidden_size()\n\n        # Multi-modal data support\n        self.input_registry = input_registry\n\n        # Lazy initialization\n        # self.model: nn.Module  # Set after load_model\n        self.kv_caches: List[torch.Tensor] = []\n        # req_id -> (input_id -> encoder_output)\n        self.encoder_cache: Dict[str, Dict[int, torch.Tensor]] = {}\n\n        # Request states.\n        self.requests: Dict[str, CachedRequestState] = {}\n        # Persistent batch.\n        self.input_batch = InputBatch(\n            max_num_reqs=self.scheduler_config.max_num_seqs,\n            max_model_len=self.max_model_len,\n            max_num_blocks_per_req=self.max_num_blocks_per_req,\n            device=self.device,\n            pin_memory=self.pin_memory,\n        )\n\n        self.use_cuda_graph = (self.vllm_config.compilation_config.level\n                               == CompilationLevel.PIECEWISE\n                               and not self.model_config.enforce_eager)\n        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.\n        # The convention is different.\n        # self.cudagraph_batch_sizes sorts in ascending order.\n        # The batch sizes in the config are in descending order.\n        self.cudagraph_batch_sizes = list(\n            reversed(self.vllm_config.compilation_config.capture_sizes))\n\n        # Persistent buffers for CUDA graphs.\n        self.input_ids = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int32,\n                                     device=self.device)\n        self.positions = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int64,\n                                     device=self.device)\n        self.inputs_embeds = torch.zeros(\n            (self.max_num_tokens, self.hidden_size),\n            dtype=self.dtype,\n            device=self.device)\n\n    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n        # Remove stopped requests from the cached states.\n        # Keep the states of the pre-empted requests.\n        for req_id in scheduler_output.finished_req_ids:\n            self.requests.pop(req_id, None)\n            self.encoder_cache.pop(req_id, None)\n\n        # Free the cached encoder outputs.\n        for req_id, input_id in scheduler_output.free_encoder_input_ids:\n            encoder_outputs = self.encoder_cache.get(req_id)\n            if encoder_outputs is not None:\n                encoder_outputs.pop(input_id, None)\n                if not encoder_outputs:\n                    self.encoder_cache.pop(req_id, None)\n\n        # Remove the requests from the persistent batch.\n        stopped_req_ids = set().union(\n            scheduler_output.preempted_req_ids,\n            scheduler_output.finished_req_ids,\n        )\n        removed_req_indices: List[int] = []\n        for req_id in stopped_req_ids:\n            req_index = self.input_batch.remove_request(req_id)\n            if req_index is not None:\n                removed_req_indices.append(req_index)\n\n        # Update the states of the running requests.\n        for req_data in scheduler_output.scheduled_running_reqs:\n            req_id = req_data.req_id\n            req_state = self.requests[req_id]\n            req_index = self.input_batch.req_id_to_index[req_id]\n\n            # Update the num_computed_tokens.\n            req_state.num_computed_tokens = req_data.num_computed_tokens\n            self.input_batch.num_computed_tokens_cpu[req_index] = (\n                req_data.num_computed_tokens)\n\n            # Update the block table.\n            num_new_blocks = len(req_data.new_block_ids)\n            if num_new_blocks == 0:\n                continue\n            start_index = len(req_state.block_ids)\n            end_index = start_index + num_new_blocks\n            req_state.block_ids.extend(req_data.new_block_ids)\n            self.input_batch.block_table_cpu[\n                req_index, start_index:end_index] = req_data.new_block_ids\n\n        req_ids_to_add: List[str] = []\n        # Add new requests to the cached states.\n        for req_data in scheduler_output.scheduled_new_reqs:\n            req_id = req_data.req_id\n            sampling_params = req_data.sampling_params\n            if sampling_params.sampling_type == SamplingType.RANDOM_SEED:\n                generator = torch.Generator(device=self.device)\n                generator.manual_seed(sampling_params.seed)\n            else:\n                generator = None\n\n            self.requests[req_id] = CachedRequestState(\n                req_id=req_id,\n                prompt_token_ids=req_data.prompt_token_ids,\n                prompt=req_data.prompt,\n                mm_inputs=req_data.mm_inputs,\n                mm_positions=req_data.mm_positions,\n                sampling_params=sampling_params,\n                generator=generator,\n                block_ids=req_data.block_ids,\n                num_computed_tokens=req_data.num_computed_tokens,\n                output_token_ids=[],\n            )\n            req_ids_to_add.append(req_id)\n\n        # Update the cached states of the resumed requests.\n        for req_data in scheduler_output.scheduled_resumed_reqs:\n            req_id = req_data.req_id\n            req_state = self.requests[req_id]\n\n            req_state.block_ids = req_data.block_ids\n            req_state.num_computed_tokens = req_data.num_computed_tokens\n            req_ids_to_add.append(req_id)\n\n        # Add the new or resumed requests to the persistent batch.\n        # The smaller empty indices are filled first.\n        removed_req_indices = sorted(removed_req_indices, reverse=True)\n        for req_id in req_ids_to_add:\n            req_state = self.requests[req_id]\n            if removed_req_indices:\n                # Fill the empty index.\n                req_index = removed_req_indices.pop()\n            else:\n                # Append to the end.\n                req_index = None\n            self.input_batch.add_request(req_state, req_index)\n\n        # Condense the batched states if there are empty indices.\n        if removed_req_indices:\n            self.input_batch.condense(removed_req_indices)\n\n    def _prepare_inputs(self, scheduler_output: \"SchedulerOutput\"):\n        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        assert total_num_scheduled_tokens > 0\n        num_reqs = self.input_batch.num_reqs\n        assert num_reqs > 0\n\n        # OPTIMIZATION: Start copying the block table first.\n        # This way, we can overlap the copy with the following CPU operations.\n        self.input_batch.block_table[:num_reqs].copy_(\n            self.input_batch.block_table_cpu_tensor[:num_reqs],\n            non_blocking=True)\n\n        # Get the number of scheduled tokens for each request.\n        # TODO: The Python loop can be slow. Optimize.\n        num_scheduled_tokens = []\n        max_num_scheduled_tokens = 0\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_scheduled_tokens.append(num_tokens)\n            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n                                           num_tokens)\n        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n        assert max_num_scheduled_tokens > 0\n\n        # Get request indices.\n        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n        indices = np.arange(num_reqs)\n        req_indices = np.repeat(indices, num_scheduled_tokens)\n\n        # Get batched arange.\n        # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        arange_matrix = np.tile(np.arange(max_num_scheduled_tokens),\n                                (num_reqs, 1))\n        mask = arange_matrix < num_scheduled_tokens[:, np.newaxis]\n        arange = arange_matrix[mask]\n\n        # Get positions.\n        positions = torch.empty((total_num_scheduled_tokens, ),\n                                dtype=torch.int32,\n                                device=\"cpu\",\n                                pin_memory=self.pin_memory)\n        positions_np = positions.numpy()\n        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n               arange,\n               out=positions_np)\n\n        # Get token indices.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]\n        # where M is the max_model_len.\n        token_indices = (positions_np +\n                         req_indices * self.input_batch.token_ids_cpu.shape[1])\n        token_indices = torch.from_numpy(token_indices)\n        input_ids = torch.empty((total_num_scheduled_tokens, ),\n                                dtype=torch.int32,\n                                device=\"cpu\",\n                                pin_memory=self.pin_memory)\n        torch.index_select(torch.from_numpy(\n            self.input_batch.token_ids_cpu).flatten(),\n                           0,\n                           token_indices,\n                           out=input_ids)\n\n        # Calculate the slot mapping.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]\n        # where K is the max_num_blocks_per_req and the block size is 2.\n        # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n        # because M (max_model_len) is not necessarily divisible by block_size.\n        block_numbers = self.input_batch.block_table_cpu_tensor.flatten()[\n            req_indices * self.max_num_blocks_per_req +\n            positions_np // self.block_size]\n        block_offsets = torch.from_numpy(positions_np % self.block_size)\n        slot_mapping = torch.empty((total_num_scheduled_tokens, ),\n                                   dtype=torch.int32,\n                                   device=\"cpu\",\n                                   pin_memory=self.pin_memory)\n        torch.add(block_numbers * self.block_size,\n                  block_offsets,\n                  out=slot_mapping)\n\n        # Prepare the attention metadata.\n        query_start_loc = torch.empty((num_reqs + 1, ),\n                                      dtype=torch.int32,\n                                      device=\"cpu\",\n                                      pin_memory=self.pin_memory)\n        query_start_loc_np = query_start_loc.numpy()\n        query_start_loc_np[0] = 0\n        np.cumsum(num_scheduled_tokens, out=query_start_loc_np[1:])\n\n        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                    num_scheduled_tokens)\n        max_seq_len = seq_lens.max()\n        seq_start_loc = torch.empty((num_reqs + 1, ),\n                                    dtype=torch.int32,\n                                    device=\"cpu\",\n                                    pin_memory=self.pin_memory)\n        seq_start_loc_np = seq_start_loc.numpy()\n        seq_start_loc_np[0] = 0\n        np.cumsum(seq_lens, out=seq_start_loc_np[1:])\n\n        self.input_ids[:total_num_scheduled_tokens].copy_(input_ids,\n                                                          non_blocking=True)\n        self.positions[:total_num_scheduled_tokens].copy_(positions,\n                                                          non_blocking=True)\n        query_start_loc = query_start_loc.to(self.device, non_blocking=True)\n        seq_start_loc = seq_start_loc.to(self.device, non_blocking=True)\n        slot_mapping = slot_mapping.to(self.device, non_blocking=True).long()\n        attn_metadata = FlashAttentionMetadata(\n            num_actual_tokens=total_num_scheduled_tokens,\n            max_query_len=max_num_scheduled_tokens,\n            query_start_loc=query_start_loc,\n            max_seq_len=max_seq_len,\n            seq_start_loc=seq_start_loc,\n            block_table=self.input_batch.block_table[:num_reqs],\n            slot_mapping=slot_mapping,\n        )\n        # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial\n        # request in the batch. While we should not sample any token from this\n        # partial request, we do so for simplicity. We will ignore the sampled\n        # token from the partial request.\n        # TODO: Support prompt logprobs.\n        logits_indices = query_start_loc[1:] - 1\n        return attn_metadata, logits_indices\n\n    def _prepare_sampling(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> SamplingMetadata:\n        skip_copy = True\n        if (scheduler_output.finished_req_ids\n                or scheduler_output.preempted_req_ids):\n            skip_copy = False\n        if (scheduler_output.scheduled_new_reqs\n                or scheduler_output.scheduled_resumed_reqs):\n            skip_copy = False\n        # Create the sampling metadata.\n        sampling_metadata = self.input_batch.make_sampling_metadata(skip_copy)\n        return sampling_metadata\n\n    def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n        scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n        if not scheduled_encoder_inputs:\n            return\n\n        # Batch the multi-modal inputs.\n        mm_inputs: List[MultiModalKwargs] = []\n        req_input_ids: List[Tuple[int, int]] = []\n        for req_id, encoder_input_ids in scheduled_encoder_inputs.items():\n            req_state = self.requests[req_id]\n            for input_id in encoder_input_ids:\n                mm_inputs.append(req_state.mm_inputs[input_id])\n                req_input_ids.append((req_id, input_id))\n        batched_mm_inputs = MultiModalKwargs.batch(mm_inputs)\n        batched_mm_inputs = MultiModalKwargs.as_kwargs(batched_mm_inputs,\n                                                       device=self.device)\n\n        # Run the encoder.\n        # `encoder_outputs` is either of the following:\n        # 1. A tensor of shape [num_images, feature_size, hidden_size]\n        # in case when feature_size is fixed across all images.\n        # 2. A list (length: num_images) of tensors, each of shape\n        # [feature_size, hidden_size] in case when the feature size is\n        # dynamic depending on input images.\n        encoder_outputs = self.model.get_multimodal_embeddings(\n            **batched_mm_inputs)\n\n        # Cache the encoder outputs.\n        for (req_id, input_id), output in zip(req_input_ids, encoder_outputs):\n            if req_id not in self.encoder_cache:\n                self.encoder_cache[req_id] = {}\n            self.encoder_cache[req_id][input_id] = output\n\n    def _gather_encoder_outputs(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> List[torch.Tensor]:\n        encoder_outputs: List[torch.Tensor] = []\n        num_reqs = self.input_batch.num_reqs\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[\n                req_id]\n            req_state = self.requests[req_id]\n            num_computed_tokens = req_state.num_computed_tokens\n            mm_positions = req_state.mm_positions\n            for i, pos_info in enumerate(mm_positions):\n                start_pos = pos_info[\"offset\"]\n                num_encoder_tokens = pos_info[\"length\"]\n\n                # The encoder output is needed if the two ranges overlap:\n                # [num_computed_tokens,\n                #  num_computed_tokens + num_scheduled_tokens) and\n                # [start_pos, start_pos + num_encoder_tokens)\n                if start_pos >= num_computed_tokens + num_scheduled_tokens:\n                    # The encoder output is not needed in this step.\n                    break\n                if start_pos + num_encoder_tokens <= num_computed_tokens:\n                    # The encoder output is already processed and stored\n                    # in the decoder's KV cache.\n                    continue\n\n                start_idx = max(num_computed_tokens - start_pos, 0)\n                end_idx = min(\n                    num_computed_tokens - start_pos + num_scheduled_tokens,\n                    num_encoder_tokens)\n                assert start_idx < end_idx\n                assert req_id in self.encoder_cache\n                assert i in self.encoder_cache[req_id]\n                encoder_output = self.encoder_cache[req_id][i]\n                encoder_outputs.append(encoder_output[start_idx:end_idx])\n        return encoder_outputs\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> ModelRunnerOutput:\n        self._update_states(scheduler_output)\n\n        if self.is_multimodal_model:\n            # Run the multimodal encoder if any.\n            self._execute_encoder(scheduler_output)\n            encoder_outputs = self._gather_encoder_outputs(scheduler_output)\n        else:\n            encoder_outputs = []\n\n        # Prepare the decoder inputs.\n        attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\n        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        if (self.use_cuda_graph\n                and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n            # Use piecewise CUDA graphs.\n            # Add padding to the batch size.\n            num_input_tokens = self._get_padded_batch_size(\n                num_scheduled_tokens)\n        else:\n            # Eager mode.\n            num_input_tokens = num_scheduled_tokens\n        attn_metadata.num_input_tokens = num_input_tokens\n\n        if self.is_multimodal_model:\n            # NOTE(woosuk): To unify token ids and soft tokens (vision\n            # embeddings), we always use embeddings (rather than token ids)\n            # as input to the multimodal model, even when the input is text.\n            input_ids = self.input_ids[:num_scheduled_tokens]\n            if encoder_outputs:\n                inputs_embeds = self.model.get_input_embeddings(\n                    input_ids, encoder_outputs)\n            else:\n                inputs_embeds = self.model.get_input_embeddings(input_ids)\n            # TODO(woosuk): Avoid the copy. Optimize.\n            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)\n            inputs_embeds = self.inputs_embeds[:num_input_tokens]\n            input_ids = None\n        else:\n            # For text-only models, we use token ids as input.\n            # While it is possible to use embeddings as input just like the\n            # multimodal models, it is not desirable for performance since\n            # then the embedding layer is not included in the CUDA graph.\n            input_ids = self.input_ids[:num_input_tokens]\n            inputs_embeds = None\n\n        # Run the decoder.\n        # Use persistent buffers for CUDA graphs.\n        with set_forward_context(attn_metadata, self.vllm_config):\n            hidden_states = self.model(\n                input_ids=input_ids,\n                positions=self.positions[:num_input_tokens],\n                kv_caches=self.kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        hidden_states = hidden_states[:num_scheduled_tokens]\n        hidden_states = hidden_states[logits_indices]\n        logits = self.model.compute_logits(hidden_states, None)\n\n        # Sample the next token and get logprobs if needed.\n        sampling_metadata = self._prepare_sampling(scheduler_output)\n        sampler_output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        sampled_token_ids = sampler_output.sampled_token_ids\n        # TODO(woosuk): The following loop can be slow since it iterates over\n        # the requests one by one. Optimize.\n        num_reqs = self.input_batch.num_reqs\n        for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n            req_state = self.requests[req_id]\n            seq_len = (req_state.num_computed_tokens +\n                       scheduler_output.num_scheduled_tokens[req_id])\n            assert seq_len <= req_state.num_tokens\n            if seq_len == req_state.num_tokens:\n                # Append the sampled token to the output token ids.\n                token_id = sampled_token_ids[i]\n                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                req_state.output_token_ids.append(token_id)\n            else:\n                # Ignore the sampled token from the partial request.\n                # Rewind the generator state as if the token was not sampled.\n                generator = self.input_batch.generators.get(i)\n                if generator is not None:\n                    # This relies on cuda-specific torch-internal impl details\n                    generator.set_offset(generator.get_offset() - 4)\n\n        if sampler_output.logprob_token_ids is None:\n            logprob_token_ids = None\n        else:\n            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n        if sampler_output.logprobs is None:\n            logprobs = None\n        else:\n            logprobs = sampler_output.logprobs.cpu()\n        model_runner_output = ModelRunnerOutput(\n            req_ids=self.input_batch.req_ids[:num_reqs],\n            req_id_to_index=self.input_batch.req_id_to_index,\n            sampled_token_ids=sampled_token_ids,\n            logprob_token_ids_cpu=logprob_token_ids,\n            logprobs_cpu=logprobs,\n        )\n        return model_runner_output\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with DeviceMemoryProfiler() as m:  # noqa: SIM117\n            self.model = get_model(vllm_config=self.vllm_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n    @torch.inference_mode()\n    def _dummy_run(\n        self,\n        model: nn.Module,\n        num_tokens: int,\n        kv_caches: List[torch.Tensor],\n    ) -> torch.Tensor:\n        if self.is_multimodal_model:\n            input_ids = None\n            inputs_embeds = self.inputs_embeds[:num_tokens]\n        else:\n            input_ids = self.input_ids[:num_tokens]\n            inputs_embeds = None\n        with set_forward_context(None, self.vllm_config):\n            hidden_states = model(\n                input_ids=input_ids,\n                positions=self.positions[:num_tokens],\n                kv_caches=kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        return hidden_states\n\n    def profile_run(self) -> None:\n        # TODO(woosuk): Profile the max memory usage of the encoder and\n        # the encoder cache.\n        # use an empty tensor instead of `None`` to force Dynamo to pass\n        # it by reference, rather by specializing on the value `None`.\n        # the `dtype` argument does not matter, and we use `float32` as\n        # a placeholder (it has wide hardware support).\n        # it is important to create tensors inside the loop, rather than\n        # multiplying the list, to avoid Dynamo from treating them as\n        # tensor aliasing.\n        dummy_kv_caches = [\n            torch.tensor([], dtype=torch.float32, device=self.device)\n            for _ in range(self.num_attn_layers)\n        ]\n        # Trigger compilation for general shape.\n        hidden_states = self._dummy_run(self.model, self.max_num_tokens,\n                                        dummy_kv_caches)\n        logits = self.model.compute_logits(hidden_states, None)\n        logits = logits[:self.max_num_tokens]\n        # TODO(woosuk): Consider the memory usage of the sampler.\n        torch.cuda.synchronize()\n        del hidden_states, logits\n        gc.collect()\n\n    def capture_model(self) -> None:\n        if not self.use_cuda_graph:\n            logger.warning(\n                \"Skipping CUDA graph capture. Please add \"\n                \"-O %s to use CUDA graphs.\", CompilationLevel.PIECEWISE)\n            return\n\n        start_time = time.perf_counter()\n        start_free_gpu_memory = torch.cuda.mem_get_info()[0]\n\n        # Trigger CUDA graph capture for specific shapes.\n        # Capture the large shapes first so that the smaller shapes\n        # can reuse the memory pool allocated for the large shapes.\n        with graph_capture():\n            for num_tokens in reversed(self.cudagraph_batch_sizes):\n                for _ in range(self.vllm_config.compilation_config.\n                               cudagraph_num_of_warmups):\n                    self._dummy_run(self.model, num_tokens, self.kv_caches)\n                self._dummy_run(self.model, num_tokens, self.kv_caches)\n\n        end_time = time.perf_counter()\n        end_free_gpu_memory = torch.cuda.mem_get_info()[0]\n        elapsed_time = end_time - start_time\n        cuda_graph_size = start_free_gpu_memory - end_free_gpu_memory\n        # This usually takes 5~20 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs, took %.2f GiB\",\n                    elapsed_time, cuda_graph_size / (1 << 30))\n\n    def initialize_kv_cache(self, num_blocks: int) -> None:\n        assert len(self.kv_caches) == 0\n        kv_cache_shape = FlashAttentionBackend.get_kv_cache_shape(\n            num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n        for _ in range(self.num_attn_layers):\n            self.kv_caches.append(\n                torch.zeros(kv_cache_shape,\n                            dtype=self.kv_cache_dtype,\n                            device=self.device))\n\n    def _get_padded_batch_size(self, batch_size: int) -> Optional[int]:\n        # TODO: Optimize this?\n        for size in self.cudagraph_batch_sizes:\n            if batch_size <= size:\n                return size\n        return None\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex e75be21ef..aa91255e6 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -67,6 +67,7 @@ class GPUModelRunner:\n         self.max_model_len = model_config.max_model_len\n         self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n         self.max_num_tokens = scheduler_config.max_num_batched_tokens\n+        self.max_num_reqs = scheduler_config.max_num_seqs\n \n         # Model-related.\n         self.num_attn_layers = model_config.get_num_layers_by_block_type(\n@@ -88,7 +89,7 @@ class GPUModelRunner:\n         self.requests: Dict[str, CachedRequestState] = {}\n         # Persistent batch.\n         self.input_batch = InputBatch(\n-            max_num_reqs=self.scheduler_config.max_num_seqs,\n+            max_num_reqs=self.max_num_reqs,\n             max_model_len=self.max_model_len,\n             max_num_blocks_per_req=self.max_num_blocks_per_req,\n             device=self.device,\n@@ -117,6 +118,32 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int32,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.input_ids_np = self.input_ids_cpu.numpy()\n+        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int64,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.positions_np = self.positions_cpu.numpy()\n+        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+                                            dtype=torch.int32,\n+                                            device=\"cpu\",\n+                                            pin_memory=self.pin_memory)\n+        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                               dtype=torch.int32,\n+                                               device=\"cpu\",\n+                                               pin_memory=self.pin_memory)\n+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                             dtype=torch.int32,\n+                                             device=\"cpu\",\n+                                             pin_memory=self.pin_memory)\n+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n         # Keep the states of the pre-empted requests.\n@@ -241,22 +268,14 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        indices = np.arange(num_reqs)\n-        req_indices = np.repeat(indices, num_scheduled_tokens)\n+        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange_matrix = np.tile(np.arange(max_num_scheduled_tokens),\n-                                (num_reqs, 1))\n-        mask = arange_matrix < num_scheduled_tokens[:, np.newaxis]\n-        arange = arange_matrix[mask]\n+        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n \n         # Get positions.\n-        positions = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        positions_np = positions.numpy()\n+        positions_np = self.positions_np[:total_num_scheduled_tokens]\n         np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n                arange,\n                out=positions_np)\n@@ -267,16 +286,13 @@ class GPUModelRunner:\n         # where M is the max_model_len.\n         token_indices = (positions_np +\n                          req_indices * self.input_batch.token_ids_cpu.shape[1])\n-        token_indices = torch.from_numpy(token_indices)\n-        input_ids = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        torch.index_select(torch.from_numpy(\n-            self.input_batch.token_ids_cpu).flatten(),\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                            0,\n-                           token_indices,\n-                           out=input_ids)\n+                           torch.from_numpy(token_indices),\n+                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n \n         # Calculate the slot mapping.\n         # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n@@ -284,45 +300,40 @@ class GPUModelRunner:\n         # where K is the max_num_blocks_per_req and the block size is 2.\n         # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n         # because M (max_model_len) is not necessarily divisible by block_size.\n-        block_numbers = self.input_batch.block_table_cpu_tensor.flatten()[\n-            req_indices * self.max_num_blocks_per_req +\n-            positions_np // self.block_size]\n-        block_offsets = torch.from_numpy(positions_np % self.block_size)\n-        slot_mapping = torch.empty((total_num_scheduled_tokens, ),\n-                                   dtype=torch.int32,\n-                                   device=\"cpu\",\n-                                   pin_memory=self.pin_memory)\n-        torch.add(block_numbers * self.block_size,\n-                  block_offsets,\n-                  out=slot_mapping)\n+        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n+                               positions_np // self.block_size)\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n+                         [block_table_indices].numpy())\n+        block_offsets = positions_np % self.block_size\n+        np.add(block_numbers * self.block_size,\n+               block_offsets,\n+               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n \n         # Prepare the attention metadata.\n-        query_start_loc = torch.empty((num_reqs + 1, ),\n-                                      dtype=torch.int32,\n-                                      device=\"cpu\",\n-                                      pin_memory=self.pin_memory)\n-        query_start_loc_np = query_start_loc.numpy()\n-        query_start_loc_np[0] = 0\n-        np.cumsum(num_scheduled_tokens, out=query_start_loc_np[1:])\n+        self.query_start_loc_np[0] = 0\n+        np.cumsum(num_scheduled_tokens,\n+                  out=self.query_start_loc_np[1:num_reqs + 1])\n \n         seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                     num_scheduled_tokens)\n         max_seq_len = seq_lens.max()\n-        seq_start_loc = torch.empty((num_reqs + 1, ),\n-                                    dtype=torch.int32,\n-                                    device=\"cpu\",\n-                                    pin_memory=self.pin_memory)\n-        seq_start_loc_np = seq_start_loc.numpy()\n-        seq_start_loc_np[0] = 0\n-        np.cumsum(seq_lens, out=seq_start_loc_np[1:])\n-\n-        self.input_ids[:total_num_scheduled_tokens].copy_(input_ids,\n-                                                          non_blocking=True)\n-        self.positions[:total_num_scheduled_tokens].copy_(positions,\n-                                                          non_blocking=True)\n-        query_start_loc = query_start_loc.to(self.device, non_blocking=True)\n-        seq_start_loc = seq_start_loc.to(self.device, non_blocking=True)\n-        slot_mapping = slot_mapping.to(self.device, non_blocking=True).long()\n+        self.seq_start_loc_np[0] = 0\n+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])\n+\n+        # Copy the tensors to the GPU.\n+        self.input_ids[:total_num_scheduled_tokens].copy_(\n+            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        self.positions[:total_num_scheduled_tokens].copy_(\n+            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n+            self.device, non_blocking=True).long()\n         attn_metadata = FlashAttentionMetadata(\n             num_actual_tokens=total_num_scheduled_tokens,\n             max_query_len=max_num_scheduled_tokens,",
      "change_type": "modified",
      "lines_added": 66,
      "lines_removed": 55
    }
  ],
  "affected_apis": [
    "InputBatch.__init__",
    "GPUModelRunner.__init__",
    "GPUModelRunner._prepare_inputs"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 2
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_gpu_input_batch, test_gpu_model_runner)",
    "is_benchmark_actually_there": "",
    "sample_clues": "gpu_input_batch, gpu_model_runner, gpumodelrunner"
  }
}