{
  "commit_hash": "eb6d3c264d0cd8e44dec16bca7947fbe96415ce9",
  "parent_hash": "97b030005c7f5cde7c1b97c718a8841db7d6220b",
  "message": "[Core] Eliminate parallel worker per-step task scheduling overhead (#4894)",
  "author": "Nick Hill <nickhill@us.ibm.com>",
  "date": "2024-05-23 06:17:27 +0900",
  "files_changed": [
    {
      "file_path": "vllm/engine/async_llm_engine.py",
      "old_content": "import asyncio\nimport time\nfrom functools import partial\nfrom typing import (AsyncIterator, Callable, Dict, Iterable, List, Optional,\n                    Set, Tuple, Type, Union)\n\nfrom transformers import PreTrainedTokenizer\n\nimport vllm.envs as envs\nfrom vllm.config import DecodingConfig, ModelConfig\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest, MultiModalData, SamplerOutput\nfrom vllm.usage.usage_lib import UsageContext\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _raise_exception_on_finish(\n        task: asyncio.Task, error_callback: Callable[[Exception],\n                                                     None]) -> None:\n    msg = (\"Task finished unexpectedly. This should never happen! \"\n           \"Please open an issue on Github.\")\n\n    exception = None\n    try:\n        task.result()\n        # NOTE: This will be thrown if task exits normally (which it should not)\n        raise AsyncEngineDeadError(msg)\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            msg + \" See stack trace above for the actual cause.\") from e\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously.\"\"\"\n\n    def __init__(self, request_id: str) -> None:\n        self.request_id = request_id\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(self) -> None:\n        self._queue.put_nowait(StopAsyncIteration())\n        self._finished = True\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self) -> Union[RequestOutput, EmbeddingRequestOutput]:\n        result = await self._queue.get()\n        if isinstance(result, Exception):\n            raise result\n        return result\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._finished_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self._request_streams[request_id].put(exc)\n            self.abort_request(request_id)\n        else:\n            for rid, stream in self._request_streams.items():\n                stream.put(exc)\n                self.abort_request(rid)\n\n    def process_request_output(self,\n                               request_output: Union[RequestOutput,\n                                                     EmbeddingRequestOutput],\n                               *,\n                               verbose: bool = False) -> None:\n        \"\"\"Process a request output from the engine.\"\"\"\n        request_id = request_output.request_id\n\n        self._request_streams[request_id].put(request_output)\n        if request_output.finished:\n            if verbose:\n                logger.info(\"Finished request %s.\", request_id)\n            self.abort_request(request_id)\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: Exception,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        self._request_streams[request_id].put(exception)\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id)\n\n    def add_request(self, request_id: str,\n                    **engine_add_request_kwargs) -> AsyncStream:\n        \"\"\"Add a request to be sent to the engine on the next background\n        loop iteration.\"\"\"\n        if request_id in self._request_streams:\n            raise KeyError(f\"Request {request_id} already exists.\")\n\n        stream = AsyncStream(request_id)\n        self._new_requests.put_nowait((stream, {\n            \"request_id\": request_id,\n            **engine_add_request_kwargs\n        }))\n\n        self.new_requests_event.set()\n\n        return stream\n\n    def abort_request(self, request_id: str, *, verbose: bool = False) -> None:\n        \"\"\"Abort a request during next background loop iteration.\"\"\"\n        if verbose:\n            logger.info(\"Aborted request %s.\", request_id)\n\n        self._finished_requests.put_nowait(request_id)\n\n        if request_id not in self._request_streams or self._request_streams[\n                request_id].finished:\n            # The request has already finished or been aborted.\n            return\n\n        self._request_streams[request_id].finish()\n\n    def get_new_and_finished_requests(self) -> Tuple[List[Dict], Set[str]]:\n        \"\"\"Get the new requests and finished requests to be\n        sent to the engine.\"\"\"\n        new_requests: List[Dict] = []\n        finished_requests: Set[str] = set()\n\n        while not self._finished_requests.empty():\n            request_id = self._finished_requests.get_nowait()\n            finished_requests.add(request_id)\n            self._request_streams.pop(request_id, None)\n\n        while not self._new_requests.empty():\n            stream, new_request = self._new_requests.get_nowait()\n            if stream.request_id in finished_requests:\n                # The request has already been aborted.\n                stream.finish()\n                continue\n            self._request_streams[stream.request_id] = stream\n            new_requests.append(new_request)\n\n        return new_requests, finished_requests\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(\n            self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n\n        if not scheduler_outputs.is_empty():\n            # Execute the model.\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n            )\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        return request_outputs\n\n    async def encode_request_async(\n        self,\n        request_id: str,  # pylint: disable=unused-argument\n        prompt: Optional[str],\n        prompt_token_ids: Optional[List[int]] = None,\n        lora_request: Optional[LoRARequest] = None,\n    ):\n        if prompt_token_ids is None:\n            assert prompt is not None\n            prompt_token_ids = await self.tokenizer.encode_async(\n                request_id=request_id,\n                prompt=prompt,\n                lora_request=lora_request)\n        return prompt_token_ids\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        params: Union[SamplingParams, PoolingParams],\n        prompt_token_ids: Optional[List[int]] = None,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> None:\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n        prompt_token_ids = await self.encode_request_async(\n            request_id=request_id,\n            prompt=prompt,\n            prompt_token_ids=prompt_token_ids,\n            lora_request=lora_request)\n\n        return self.add_request(request_id,\n                                prompt=prompt,\n                                params=params,\n                                prompt_token_ids=prompt_token_ids,\n                                arrival_time=arrival_time,\n                                lora_request=lora_request,\n                                multi_modal_data=multi_modal_data)\n\n    async def check_health_async(self) -> None:\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for LLMEngine.\n\n    This class is used to wrap the LLMEngine class to make it asynchronous. It\n    uses asyncio to create a background loop that keeps processing incoming\n    requests. The LLMEngine is kicked by the generate method when there\n    are requests in the waiting queue. The generate method yields the outputs\n    from the LLMEngine to the caller.\n\n    NOTE: For the comprehensive list of arguments, see `LLMEngine`.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        max_log_len: Maximum number of prompt characters or prompt ID numbers\n            being printed in log.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for LLMEngine.\n        *kwargs: Arguments for LLMEngine.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 max_log_len: Optional[int] = None,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.max_log_len = max_log_len\n        self.engine = self._init_engine(*args, **kwargs)\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: AsyncEngineArgs,\n        start_engine_loop: bool = True,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n    ) -> \"AsyncLLMEngine\":\n        \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config()\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n\n        if engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with the CPU backend.\")\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        # Create the async LLM engine.\n        engine = cls(\n            distributed_executor_backend == \"ray\",\n            engine_args.engine_use_ray,\n            **engine_config.to_dict(),\n            executor_class=executor_class,\n            log_requests=not engine_args.disable_log_requests,\n            log_stats=not engine_args.disable_log_stats,\n            max_log_len=engine_args.max_log_len,\n            start_engine_loop=start_engine_loop,\n            usage_context=usage_context,\n        )\n        return engine\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(self) -> \"PreTrainedTokenizer\":\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote()  # type: ignore\n        else:\n            return self.engine.get_tokenizer()\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_raise_exception_on_finish,\n                    error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if parallel_config.tensor_parallel_size == 1:\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, finished_requests = (\n            self._request_tracker.get_new_and_finished_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if finished_requests:\n            await self._engine_abort(finished_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await self.engine.step.remote()  # type: ignore\n        else:\n            request_outputs = await self.engine.step_async()\n\n        # Put the outputs into the corresponding streams.\n        for request_output in request_outputs:\n            self._request_tracker.process_request_output(\n                request_output, verbose=self.log_requests)\n\n        return len(request_outputs) > 0\n\n    async def _engine_abort(self, request_ids: Iterable[str]):\n        if self.engine_use_ray:\n            await self.engine.abort_request.remote(request_ids)  # type: ignore\n        else:\n            self.engine.abort_request(request_ids)\n\n    async def run_engine_loop(self):\n        has_requests_in_progress = False\n        while True:\n            if not has_requests_in_progress:\n                logger.debug(\"Waiting for new requests...\")\n                await self._request_tracker.wait_for_new_requests()\n                logger.debug(\"Got new requests!\")\n\n            # Abort if iteration takes too long due to unrecoverable errors\n            # (eg. NCCL timeouts).\n            try:\n                has_requests_in_progress = await asyncio.wait_for(\n                    self.engine_step(), ENGINE_ITERATION_TIMEOUT_S)\n            except asyncio.TimeoutError as exc:\n                logger.error(\n                    \"Engine iteration timed out. This should never happen!\")\n                self.set_errored(exc)\n                raise\n            await asyncio.sleep(0)\n\n    async def add_request(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        params: Union[SamplingParams, PoolingParams],\n        prompt_token_ids: Optional[List[int]] = None,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> AsyncStream:\n        if self.log_requests:\n            shortened_prompt = prompt\n            shortened_token_ids = prompt_token_ids\n            if self.max_log_len is not None:\n                if shortened_prompt is not None:\n                    shortened_prompt = shortened_prompt[:self.max_log_len]\n                if shortened_token_ids is not None:\n                    shortened_token_ids = shortened_token_ids[:self.\n                                                              max_log_len]\n            logger.info(\n                \"Received request %s: prompt: %r, \"\n                \"params: %s, prompt_token_ids: %s, \"\n                \"lora_request: %s.\", request_id, shortened_prompt, params,\n                shortened_token_ids, lora_request)\n\n        if not self.is_running:\n            if self.start_engine_loop:\n                self.start_background_loop()\n            else:\n                raise AsyncEngineDeadError(\n                    \"Background loop is not running. If it was running, \"\n                    \"inspect the output to find the stacktrace of the \"\n                    \"error that caused the background loop to stop \"\n                    \"(AsyncEngineDeadError).\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        if self.engine_use_ray:\n            prompt_token_ids = await (\n                self.engine.encode_request_async.remote(  # type: ignore\n                    request_id=request_id,\n                    prompt=prompt,\n                    prompt_token_ids=prompt_token_ids,\n                    lora_request=lora_request))\n        else:\n            prompt_token_ids = await self.engine.encode_request_async(\n                request_id=request_id,\n                prompt=prompt,\n                prompt_token_ids=prompt_token_ids,\n                lora_request=lora_request)\n\n        stream = self._request_tracker.add_request(\n            request_id,\n            prompt=prompt,\n            params=params,\n            prompt_token_ids=prompt_token_ids,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            multi_modal_data=multi_modal_data,\n        )\n\n        return stream\n\n    async def generate(\n        self,\n        prompt: Optional[str],\n        sampling_params: SamplingParams,\n        request_id: str,\n        prompt_token_ids: Optional[List[int]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None\n    ) -> AsyncIterator[RequestOutput]:\n        \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt string. Can be None if prompt_token_ids is\n                provided.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            prompt_token_ids: The token IDs of the prompt. If None, we\n                use the tokenizer to convert the prompts to token IDs.\n            lora_request: LoRA request to use for generation, if any.\n            multi_modal_data: Multi modal data per request.\n\n        Yields:\n            The output `RequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"prompt\": \"What is LLM?\",\n            >>>     \"stream\": False, # assume the non-streaming case\n            >>>     \"temperature\": 0.0,\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.generate(\n            >>>    example_input[\"prompt\"],\n            >>>    SamplingParams(temperature=example_input[\"temperature\"]),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in self.process_request(\n                request_id,\n                prompt,\n                sampling_params,\n                prompt_token_ids,\n                lora_request,\n                multi_modal_data,\n        ):\n            yield output\n\n    async def encode(\n        self,\n        prompt: Optional[str],\n        pooling_params: PoolingParams,\n        request_id: str,\n        prompt_token_ids: Optional[List[int]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None\n    ) -> AsyncIterator[EmbeddingRequestOutput]:\n        \"\"\"Generate outputs for a request from an embedding model.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt string. Can be None if prompt_token_ids is\n                provided.\n            pooling_params: The pooling parameters of the request.\n            request_id: The unique id of the request.\n            prompt_token_ids: The token IDs of the prompt. If None, we\n                use the tokenizer to convert the prompts to token IDs.\n            lora_request: LoRA request to use for generation, if any.\n            multi_modal_data: Multi modal data per request.\n\n        Yields:\n            The output `EmbeddingRequestOutput` objects from the LLMEngine \n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"input\": \"What is LLM?\",\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.encode(\n            >>>    example_input[\"input\"],\n            >>>    PoolingParams(),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in self.process_request(\n                request_id,\n                prompt,\n                pooling_params,\n                prompt_token_ids,\n                lora_request,\n                multi_modal_data,\n        ):\n            yield output\n\n    async def process_request(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        params: Union[SamplingParams, PoolingParams],\n        prompt_token_ids: Optional[List[int]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> AsyncIterator[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Common logic to process requests with SamplingParams or\n        PoolingParams.\"\"\"\n        arrival_time = time.time()\n\n        stream = await self.add_request(\n            request_id,\n            prompt,\n            params,\n            prompt_token_ids=prompt_token_ids,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            multi_modal_data=multi_modal_data,\n        )\n\n        try:\n            async for request_output in stream:\n                yield request_output\n        except (Exception, asyncio.CancelledError) as e:\n            self._abort(request_id)\n            raise e\n\n    async def abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        if not self.is_running:\n            raise AsyncEngineDeadError(\n                \"Background loop is not running. If it was running, \"\n                \"inspect the output to find the stacktrace of the \"\n                \"error that caused the background loop to stop \"\n                \"(AsyncEngineDeadError).\")\n\n        return self._abort(request_id)\n\n    def _abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        self._request_tracker.abort_request(request_id,\n                                            verbose=self.log_requests)\n\n    async def get_model_config(self) -> ModelConfig:\n        \"\"\"Get the model configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_model_config.remote()  # type: ignore\n        else:\n            return self.engine.get_model_config()\n\n    async def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Get the decoding configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_decoding_config.remote(  # type: ignore\n            )\n        else:\n            return self.engine.get_decoding_config()\n\n    async def do_log_stats(\n            self,\n            scheduler_outputs: Optional[SchedulerOutputs] = None,\n            model_output: Optional[List[SamplerOutput]] = None) -> None:\n        if self.engine_use_ray:\n            await self.engine.do_log_stats.remote(  # type: ignore\n                scheduler_outputs, model_output)\n        else:\n            self.engine.do_log_stats()\n\n    async def check_health(self) -> None:\n        \"\"\"Raises an error if engine is unhealthy.\"\"\"\n        t = time.perf_counter()\n        logger.debug(\"Starting health check...\")\n        if self.is_stopped:\n            raise AsyncEngineDeadError(\"Background loop is stopped.\")\n\n        if self.engine_use_ray:\n            try:\n                await self.engine.check_health.remote()  # type: ignore\n            except ray.exceptions.RayActorError as e:\n                raise RuntimeError(\"Engine is dead.\") from e\n        else:\n            await self.engine.check_health_async()\n        logger.debug(\"Health check took %fs\", time.perf_counter() - t)\n",
      "diff": "diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 8a37bac02..5a15ed67e 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -234,6 +234,14 @@ class _AsyncLLMEngine(LLMEngine):\n         # Log stats.\n         self.do_log_stats(scheduler_outputs, output)\n \n+        if not request_outputs:\n+            # Stop the execute model loop in parallel workers until there are\n+            # more requests to process. This avoids waiting indefinitely in\n+            # torch.distributed ops which may otherwise timeout, and unblocks\n+            # the RPC thread in the workers so that they can process any other\n+            # queued control plane messages, such as add/remove lora adapters.\n+            await self.model_executor.stop_remote_worker_execution_loop_async()\n+\n         return request_outputs\n \n     async def encode_request_async(\n@@ -687,7 +695,7 @@ class AsyncLLMEngine:\n             multi_modal_data: Multi modal data per request.\n \n         Yields:\n-            The output `EmbeddingRequestOutput` objects from the LLMEngine \n+            The output `EmbeddingRequestOutput` objects from the LLMEngine\n             for the request.\n \n         Details:",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/engine/llm_engine.py",
      "old_content": "import time\nfrom typing import Iterable, List, Optional, Type, Union\n\nfrom transformers import GenerationConfig, PreTrainedTokenizer\n\nimport vllm\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig, LoadConfig,\n                         LoRAConfig, ModelConfig, ParallelConfig,\n                         SchedulerConfig, SpeculativeConfig,\n                         VisionLanguageConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics import StatLogger, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           MultiModalData, PoolerOutput, SamplerOutput,\n                           Sequence, SequenceGroup, SequenceGroupMetadata,\n                           SequenceStatus)\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer_group import (BaseTokenizerGroup,\n                                                     get_tokenizer_group)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig):\n    try:\n        return GenerationConfig.from_pretrained(\n            model_config.model,\n            revision=model_config.revision,\n        ).to_diff_dict()\n    except OSError:\n        # Not found.\n        return {}\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The `LLM` class wraps this class for offline batched inference and the\n    `AsyncLLMEngine` class wraps this class for online serving.\n\n    NOTE: The config arguments are derived from the `EngineArgs` class. For the\n    comprehensive list of arguments, see `EngineArgs`.\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        vision_language_config (Optional): The configuration related to vision\n            language models.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        vision_language_config: Optional[VisionLanguageConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"rope_scaling=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, seed=%d, served_model_name=%s)\",\n            vllm.__version__,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.rope_scaling,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            model_config.seed,\n            model_config.served_model_name,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.vision_language_config = vision_language_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.log_stats = log_stats\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer: BaseTokenizerGroup\n            self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n        else:\n            self.detokenizer = None\n            self.tokenizer = None\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            vision_language_config=vision_language_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    cache_config.cache_dtype,\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = Scheduler(scheduler_config, cache_config, lora_config)\n\n        # Metric Logging.\n        if self.log_stats:\n            self.stat_logger = StatLogger(\n                local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                labels=dict(model_name=model_config.served_model_name),\n                max_model_len=self.model_config.max_model_len)\n            self.stat_logger.info(\"cache_config\", self.cache_config)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                self.get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    self.get_tokenizer_for_seq,\n                ),\n            ))\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config()\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n\n        # Initialize the cluster and specify the executor class.\n        if engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n\n        # Create the LLM engine.\n        engine = cls(\n            **engine_config.to_dict(),\n            executor_class=executor_class,\n            log_stats=not engine_args.disable_log_stats,\n            usage_context=usage_context,\n        )\n        return engine\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    def get_tokenizer(self) -> \"PreTrainedTokenizer\":\n        return self.tokenizer.get_lora_tokenizer(None)\n\n    def get_tokenizer_for_seq(self,\n                              sequence: Sequence) -> \"PreTrainedTokenizer\":\n        return self.tokenizer.get_lora_tokenizer(sequence.lora_request)\n\n    def _init_tokenizer(self, **tokenizer_init_kwargs):\n        init_kwargs = dict(\n            tokenizer_id=self.model_config.tokenizer,\n            enable_lora=bool(self.lora_config),\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n            max_input_length=None,\n            tokenizer_mode=self.model_config.tokenizer_mode,\n            trust_remote_code=self.model_config.trust_remote_code,\n            revision=self.model_config.tokenizer_revision)\n        init_kwargs.update(tokenizer_init_kwargs)\n        self.tokenizer = get_tokenizer_group(\n            self.parallel_config.tokenizer_pool_config, **init_kwargs)\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n\n    def encode_request(\n        self,\n        request_id: str,  # pylint: disable=unused-argument\n        prompt: Optional[str],\n        prompt_token_ids: Optional[List[int]] = None,\n        lora_request: Optional[LoRARequest] = None,\n    ):\n        if prompt_token_ids is None:\n            assert prompt is not None\n            prompt_token_ids = self.tokenizer.encode(request_id=request_id,\n                                                     prompt=prompt,\n                                                     lora_request=lora_request)\n        return prompt_token_ids\n\n    def add_request(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        params: Union[SamplingParams, PoolingParams],\n        prompt_token_ids: Optional[List[int]] = None,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            prompt: The prompt string. Can be None if prompt_token_ids is\n                provided.\n            params: Parameters for sampling or pooling. SamplingParams\n                for text generation. PoolingParams for pooling.\n            prompt_token_ids: The token IDs of the prompt. If None, we\n                use the tokenizer to convert the prompts to token IDs.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            multi_modal_data: Multi modal data per request.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `best_of` number of :class:`~vllm.Sequence` objects.\n            - Create a :class:`~vllm.SequenceGroup` object\n              from the list of :class:`~vllm.Sequence`.\n            - Add the :class:`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n        prompt_token_ids = self.encode_request(\n            request_id=request_id,\n            prompt=prompt,\n            prompt_token_ids=prompt_token_ids,\n            lora_request=lora_request)\n\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = None\n        if self.tokenizer:\n            eos_token_id = self.tokenizer.get_lora_tokenizer(\n                lora_request).eos_token_id\n        else:\n            logger.warning(\"Use None for EOS token id because tokenizer is \"\n                           \"not initialized\")\n        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size,\n                       eos_token_id, lora_request)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time,\n                lora_request,\n                multi_modal_data,\n            )\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time,\n                lora_request,\n                multi_modal_data,\n            )\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler.\n        self.scheduler.add_seq_group(seq_group)\n\n    def _create_sequence_group_with_sampling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        sampling_params: SamplingParams,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with SamplingParams.\"\"\"\n        max_logprobs = self.get_model_config().max_logprobs\n        if (sampling_params.logprobs\n                and sampling_params.logprobs > max_logprobs) or (\n                    sampling_params.prompt_logprobs\n                    and sampling_params.prompt_logprobs > max_logprobs):\n            raise ValueError(f\"Cannot request more than \"\n                             f\"{max_logprobs} logprobs.\")\n\n        # Defensive copy of SamplingParams, which are used by the sampler,\n        # this doesn't deep-copy LogitsProcessor objects\n        sampling_params = sampling_params.clone()\n        # Add the eos token id into the sampling_params to support min_tokens\n        # processing\n        if seq.eos_token_id is not None:\n            sampling_params.all_stop_token_ids.add(seq.eos_token_id)\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields)\n\n        # Create the sequence group.\n        seq_group = SequenceGroup(request_id=request_id,\n                                  seqs=[seq],\n                                  arrival_time=arrival_time,\n                                  sampling_params=sampling_params,\n                                  lora_request=lora_request,\n                                  multi_modal_data=multi_modal_data)\n\n        return seq_group\n\n    def _create_sequence_group_with_pooling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        pooling_params: PoolingParams,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        multi_modal_data: Optional[MultiModalData] = None,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with PoolingParams.\"\"\"\n        # Defensive copy of PoolingParams, which are used by the pooler\n        pooling_params = pooling_params.clone()\n        # Create the sequence group.\n        seq_group = SequenceGroup(request_id=request_id,\n                                  seqs=[seq],\n                                  arrival_time=arrival_time,\n                                  lora_request=lora_request,\n                                  multi_modal_data=multi_modal_data,\n                                  pooling_params=pooling_params)\n        return seq_group\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n\n        Details:\n            - Refer to the\n              :meth:`~vllm.core.scheduler.Scheduler.abort_seq_group`\n              from class :class:`~vllm.core.scheduler.Scheduler`.\n\n        Example:\n            >>> # initialize engine and add a request with request_id\n            >>> request_id = str(0)\n            >>> # abort the request\n            >>> engine.abort_request(request_id)\n        \"\"\"\n        self.scheduler.abort_seq_group(request_id)\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Gets the decoding configuration.\"\"\"\n        return self.decoding_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return self.scheduler.get_num_unfinished_seq_groups()\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return self.scheduler.has_unfinished_seqs()\n\n    def _process_sequence_group_outputs(\n        self,\n        seq_group: SequenceGroup,\n        outputs: List[EmbeddingSequenceGroupOutput],\n    ) -> None:\n        seq_group.embeddings = outputs[0].embeddings\n\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.FINISHED_STOPPED\n\n        return\n\n    def _process_model_outputs(\n        self,\n        output: List[Union[SamplerOutput, PoolerOutput]],\n        scheduled_seq_groups: List[ScheduledSequenceGroup],\n        ignored_seq_groups: List[SequenceGroup],\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Apply the model output to the sequences in the scheduled seq groups.\n\n        Returns RequestOutputs that can be returned to the client.\n        \"\"\"\n\n        now = time.time()\n\n        # Organize outputs by [sequence group][step] instead of\n        # [step][sequence group].\n        output_by_sequence_group = create_output_by_sequence_group(\n            sampler_outputs=output, num_seq_groups=len(scheduled_seq_groups))\n\n        # Update the scheduled sequence groups with the model outputs.\n        for scheduled_seq_group, outputs, seq_group_meta in zip(\n                scheduled_seq_groups, output_by_sequence_group,\n                seq_group_metadata_list):\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.update_num_computed_tokens(\n                scheduled_seq_group.token_chunk_size)\n            if self.model_config.embedding_mode:\n                self._process_sequence_group_outputs(seq_group, outputs)\n                continue\n\n            self.output_processor.process_prompt_logprob(seq_group, outputs)\n            if seq_group_meta.do_sample:\n                self.output_processor.process_outputs(seq_group, outputs)\n\n        # Free the finished sequence groups.\n        self.scheduler.free_finished_seq_groups()\n\n        # Create the outputs.\n        request_outputs: List[Union[RequestOutput,\n                                    EmbeddingRequestOutput]] = []\n        for scheduled_seq_group in scheduled_seq_groups:\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            request_output = RequestOutputFactory.create(seq_group)\n            request_outputs.append(request_output)\n        for seq_group in ignored_seq_groups:\n            request_output = RequestOutputFactory.create(seq_group)\n            request_outputs.append(request_output)\n        return request_outputs\n\n    def step(self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        .. figure:: https://i.imgur.com/sv2HssD.png\n            :alt: Overview of the step function\n            :align: center\n\n            Overview of the step function.\n\n        Details:\n            - Step 1: Schedules the sequences to be executed in the next\n              iteration and the token blocks to be swapped in/out/copy.\n\n                - Depending on the scheduling policy,\n                  sequences may be `preempted/reordered`.\n                - A Sequence Group (SG) refer to a group of sequences\n                  that are generated from the same prompt.\n\n            - Step 2: Calls the distributed executor to execute the model.\n            - Step 3: Processes the model output. This mainly includes:\n\n                - Decodes the relevant outputs.\n                - Updates the scheduled sequence groups with model outputs\n                  based on its `sampling parameters` (`use_beam_search` or not).\n                - Frees the finished sequence groups.\n\n            - Finally, it creates and returns the newly generated results.\n\n        Example:\n            >>> # Please see the example/ folder for more detailed examples.\n            >>>\n            >>> # initialize engine and request arguments\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> example_inputs = [(0, \"What is LLM?\",\n            >>>    SamplingParams(temperature=0.0))]\n            >>>\n            >>> # Start the engine with an event loop\n            >>> while True:\n            >>>     if example_inputs:\n            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)\n            >>>         engine.add_request(str(req_id),prompt,sampling_params)\n            >>>\n            >>>     # continue the request processing\n            >>>     request_outputs = engine.step()\n            >>>     for request_output in request_outputs:\n            >>>         if request_output.finished:\n            >>>             # return or show the request output\n            >>>\n            >>>     if not (engine.has_unfinished_requests() or example_inputs):\n            >>>         break\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n\n        if not scheduler_outputs.is_empty():\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n            )\n            output = self.model_executor.execute_model(\n                execute_model_req=execute_model_req)\n        else:\n            output = []\n\n        request_outputs = self._process_model_outputs(\n            output, scheduler_outputs.scheduled_seq_groups,\n            scheduler_outputs.ignored_seq_groups, seq_group_metadata_list)\n\n        # Log stats.\n        self.do_log_stats(scheduler_outputs, output)\n\n        return request_outputs\n\n    def do_log_stats(\n            self,\n            scheduler_outputs: Optional[SchedulerOutputs] = None,\n            model_output: Optional[List[SamplerOutput]] = None) -> None:\n        \"\"\"Forced log when no requests active.\"\"\"\n        if self.log_stats:\n            self.stat_logger.log(\n                self._get_stats(scheduler_outputs, model_output))\n\n    def _get_stats(\n            self,\n            scheduler_outputs: Optional[SchedulerOutputs],\n            model_output: Optional[List[SamplerOutput]] = None) -> Stats:\n        \"\"\"Get Stats to be Logged to Prometheus.\n\n        Args:\n            scheduler_outputs: Optional, used to populate metrics related to\n                the scheduled batch,\n            model_output: Optional, used to emit speculative decoding metrics\n                which are created by the workers.\n        \"\"\"\n        now = time.time()\n\n        # System State\n        #   Scheduler State\n        num_running_sys = len(self.scheduler.running)\n        num_swapped_sys = len(self.scheduler.swapped)\n        num_waiting_sys = len(self.scheduler.waiting)\n\n        # KV Cache Usage in %\n        num_total_gpu = self.cache_config.num_gpu_blocks\n        gpu_cache_usage_sys = 0.\n        if num_total_gpu is not None:\n            num_free_gpu = self.scheduler.block_manager.get_num_free_gpu_blocks(\n            )\n            gpu_cache_usage_sys = 1.0 - (num_free_gpu / num_total_gpu)\n\n        num_total_cpu = self.cache_config.num_cpu_blocks\n        cpu_cache_usage_sys = 0.\n        if num_total_cpu is not None and num_total_cpu > 0:\n            num_free_cpu = self.scheduler.block_manager.get_num_free_cpu_blocks(\n            )\n            cpu_cache_usage_sys = 1.0 - (num_free_cpu / num_total_cpu)\n\n        # Iteration stats\n        num_prompt_tokens_iter = 0\n        num_generation_tokens_iter = 0\n        time_to_first_tokens_iter: List[float] = []\n        time_per_output_tokens_iter: List[float] = []\n        num_preemption_iter = (0 if scheduler_outputs is None else\n                               scheduler_outputs.preempted)\n\n        # Request stats\n        #   Latency\n        time_e2e_requests: List[float] = []\n        #   Metadata\n        num_prompt_tokens_requests: List[int] = []\n        num_generation_tokens_requests: List[int] = []\n        best_of_requests: List[int] = []\n        n_requests: List[int] = []\n        finished_reason_requests: List[str] = []\n\n        # NOTE: This loop assumes prefill seq_groups are before\n        # decode seq_groups in scheduled_seq_groups.\n        if scheduler_outputs is not None:\n            num_generation_tokens_from_prefill_groups = 0.\n            # NOTE: if scheduler_outputs.num_prefill_groups > 0 and\n            # the len of scheduler_outputs.scheduled_seq_groups is !=\n            # scheduler_outputs.num_prefill_groups, this means that\n            # chunked prefills have been detected.\n\n            for idx, scheduled_seq_group in enumerate(\n                    scheduler_outputs.scheduled_seq_groups):\n                group_was_prefill = idx < scheduler_outputs.num_prefill_groups\n                seq_group = scheduled_seq_group.seq_group\n\n                # NOTE: a seq_group that completed all of its prefill tokens\n                # in the last iteration will have seq_group.is_prefill() = False\n                # with group_was_prefill = True\n                if group_was_prefill:\n                    # Number of prompt tokens.\n                    num_prompt_tokens_iter += (\n                        scheduled_seq_group.token_chunk_size)\n\n                    # If the seq_group just finished the prefill state\n                    # get TTFT.\n                    if not seq_group.is_prefill():\n                        latency = seq_group.get_last_latency(now)\n                        time_to_first_tokens_iter.append(latency)\n\n                        # One generation token per finished prefill.\n                        num_generation_tokens_from_prefill_groups += (\n                            seq_group.num_seqs())\n                else:\n                    # TPOTs.\n                    latency = seq_group.get_last_latency(now)\n                    time_per_output_tokens_iter.append(latency)\n\n                # Because of chunked prefill, we can have a single sequence\n                # group that does multiple prompt_runs. To prevent logging\n                # the same metadata more than once per request, we standardize\n                # on logging request level information for finished requests,\n                # which can only happen once.\n                if seq_group.is_finished():\n                    # Latency timings\n                    time_e2e_requests.append(now -\n                                             seq_group.metrics.arrival_time)\n\n                    # Metadata\n                    num_prompt_tokens_requests.append(\n                        len(seq_group.prompt_token_ids))\n                    num_generation_tokens_requests.extend([\n                        seq.get_output_len()\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n                    if seq_group.sampling_params is not None:\n                        best_of_requests.append(\n                            seq_group.sampling_params.best_of)\n                        n_requests.append(seq_group.sampling_params.n)\n                    finished_reason_requests.extend([\n                        SequenceStatus.get_finished_reason(seq.status)\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n\n            # Number of generation tokens.\n            #   num_batched_tokens equals the number of prompt_tokens plus the\n            #   number of decode_tokens in a single iteration. So,\n            #   num_generation_tokens = num_batched_tokens - num_prompt_tokens\n            #   + num_generation_tokens_from_prefill_groups (since we generate\n            #   one token on prefills on iters where the prefill finishes).\n            num_generation_tokens_iter = (\n                scheduler_outputs.num_batched_tokens - num_prompt_tokens_iter +\n                num_generation_tokens_from_prefill_groups)\n\n        # Spec decode, if enabled, emits specialized metrics from the worker in\n        # sampler output.\n        if model_output and (model_output[0].spec_decode_worker_metrics\n                             is not None):\n            spec_decode_metrics = model_output[0].spec_decode_worker_metrics\n        else:\n            spec_decode_metrics = None\n\n        return Stats(\n            now=now,\n            # System stats\n            #   Scheduler State\n            num_running_sys=num_running_sys,\n            num_swapped_sys=num_swapped_sys,\n            num_waiting_sys=num_waiting_sys,\n            #   KV Cache Usage in %\n            gpu_cache_usage_sys=gpu_cache_usage_sys,\n            cpu_cache_usage_sys=cpu_cache_usage_sys,\n\n            # Iteration stats\n            num_prompt_tokens_iter=num_prompt_tokens_iter,\n            num_generation_tokens_iter=num_generation_tokens_iter,\n            time_to_first_tokens_iter=time_to_first_tokens_iter,\n            time_per_output_tokens_iter=time_per_output_tokens_iter,\n            spec_decode_metrics=spec_decode_metrics,\n            num_preemption_iter=num_preemption_iter,\n\n            # Request stats\n            #   Latency\n            time_e2e_requests=time_e2e_requests,\n            #   Metadata\n            num_prompt_tokens_requests=num_prompt_tokens_requests,\n            num_generation_tokens_requests=num_generation_tokens_requests,\n            best_of_requests=best_of_requests,\n            n_requests=n_requests,\n            finished_reason_requests=finished_reason_requests,\n        )\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_executor.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_executor.remove_lora(lora_id)\n\n    def list_loras(self) -> List[int]:\n        return self.model_executor.list_loras()\n\n    def check_health(self) -> None:\n        self.model_executor.check_health()\n",
      "diff": "diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 60e23d4df..0631c0de7 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -692,6 +692,14 @@ class LLMEngine:\n         # Log stats.\n         self.do_log_stats(scheduler_outputs, output)\n \n+        if not request_outputs:\n+            # Stop the execute model loop in parallel workers until there are\n+            # more requests to process. This avoids waiting indefinitely in\n+            # torch.distributed ops which may otherwise timeout, and unblocks\n+            # the RPC thread in the workers so that they can process any other\n+            # queued control plane messages, such as add/remove lora adapters.\n+            self.model_executor.stop_remote_worker_execution_loop()\n+\n         return request_outputs\n \n     def do_log_stats(",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/executor/distributed_gpu_executor.py",
      "old_content": "from abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, Set, Tuple\n\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.gpu_executor import GPUExecutor\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import SamplerOutput\n\nlogger = init_logger(__name__)\n\n\nclass DistributedGPUExecutor(GPUExecutor):\n    \"\"\"Abstract superclass of multi-GPU executor implementations.\"\"\"\n\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Determine the number of available KV blocks.\n\n        This invokes `determine_num_available_blocks` on each worker and takes\n        the min of the results, guaranteeing that the selected cache sizes are\n        compatible with all workers.\n\n        Returns:\n            - tuple[num_gpu_blocks, num_cpu_blocks]\n        \"\"\"\n        # Get the maximum number of blocks that can be allocated on GPU and CPU.\n        num_blocks = self._run_workers(\"determine_num_available_blocks\", )\n\n        # Since we use a shared centralized controller, we take the minimum\n        # number of blocks across all workers to make sure all the memory\n        # operators can be applied to all workers.\n        num_gpu_blocks = min(b[0] for b in num_blocks)\n        num_cpu_blocks = min(b[1] for b in num_blocks)\n\n        return num_gpu_blocks, num_cpu_blocks\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the KV cache in all workers.\n        \"\"\"\n\n        # NOTE: We log here to avoid multiple logs when number of workers is\n        # greater than one. We could log in the engine, but not all executors\n        # have GPUs.\n        logger.info(\"# GPU blocks: %d, # CPU blocks: %d\", num_gpu_blocks,\n                    num_cpu_blocks)\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self._run_workers(\"initialize_cache\",\n                          num_gpu_blocks=num_gpu_blocks,\n                          num_cpu_blocks=num_cpu_blocks)\n\n    def execute_model(self, *args, **kwargs) -> List[SamplerOutput]:\n        all_outputs = self._run_workers(\"execute_model\",\n                                        driver_args=args,\n                                        driver_kwargs=kwargs)\n\n        # Only the driver worker returns the sampling results.\n        return all_outputs[0]\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        assert lora_request.lora_int_id > 0, \"lora_id must be greater than 0.\"\n        return self._run_workers(\n            \"add_lora\",\n            lora_request=lora_request,\n        )\n\n    def remove_lora(self, lora_id: int) -> bool:\n        assert lora_id > 0, \"lora_id must be greater than 0.\"\n        return self._run_workers(\n            \"remove_lora\",\n            lora_id=lora_id,\n        )\n\n    def list_loras(self) -> Set[int]:\n        return self._run_workers(\"list_loras\")\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        self._run_workers(\"save_sharded_state\",\n                          path=path,\n                          pattern=pattern,\n                          max_size=max_size)\n\n    @abstractmethod\n    def _run_workers(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        max_concurrent_workers: Optional[int] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        raise NotImplementedError\n\n\nclass DistributedGPUExecutorAsync(DistributedGPUExecutor, ExecutorAsyncBase):\n\n    @abstractmethod\n    async def _run_workers_async(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        raise NotImplementedError\n\n    async def execute_model_async(self, *args,\n                                  **kwargs) -> List[SamplerOutput]:\n        all_outputs = await self._run_workers_async(\"execute_model\",\n                                                    driver_args=args,\n                                                    driver_kwargs=kwargs)\n\n        # Only the driver worker returns the sampling results.\n        return all_outputs[0]\n",
      "diff": "diff --git a/vllm/executor/distributed_gpu_executor.py b/vllm/executor/distributed_gpu_executor.py\nindex c5b1e6111..f7c608af1 100644\n--- a/vllm/executor/distributed_gpu_executor.py\n+++ b/vllm/executor/distributed_gpu_executor.py\n@@ -1,11 +1,12 @@\n+import asyncio\n from abc import abstractmethod\n-from typing import Any, Dict, List, Optional, Set, Tuple\n+from typing import Any, Awaitable, Dict, List, Optional, Set, Tuple, Union\n \n from vllm.executor.executor_base import ExecutorAsyncBase\n from vllm.executor.gpu_executor import GPUExecutor\n from vllm.logger import init_logger\n from vllm.lora.request import LoRARequest\n-from vllm.sequence import SamplerOutput\n+from vllm.sequence import ExecuteModelRequest, SamplerOutput\n \n logger = init_logger(__name__)\n \n@@ -13,6 +14,16 @@ logger = init_logger(__name__)\n class DistributedGPUExecutor(GPUExecutor):\n     \"\"\"Abstract superclass of multi-GPU executor implementations.\"\"\"\n \n+    def __init__(self, *args, **kwargs):\n+        # This is non-None when the execute model loop is running\n+        # in the parallel workers. It's a coroutine in the AsyncLLMEngine case.\n+        self.parallel_worker_tasks: Optional[Union[Any, Awaitable[Any]]] = None\n+        # Updated by implementations that require additional args to be passed\n+        # to the _run_workers execute_model call\n+        self.extra_execute_model_run_workers_kwargs: Dict[str, Any] = {}\n+\n+        super().__init__(*args, **kwargs)\n+\n     def determine_num_available_blocks(self) -> Tuple[int, int]:\n         \"\"\"Determine the number of available KV blocks.\n \n@@ -52,13 +63,28 @@ class DistributedGPUExecutor(GPUExecutor):\n                           num_gpu_blocks=num_gpu_blocks,\n                           num_cpu_blocks=num_cpu_blocks)\n \n-    def execute_model(self, *args, **kwargs) -> List[SamplerOutput]:\n-        all_outputs = self._run_workers(\"execute_model\",\n-                                        driver_args=args,\n-                                        driver_kwargs=kwargs)\n+    def execute_model(\n+            self,\n+            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n+        if self.parallel_worker_tasks is None:\n+            self.parallel_worker_tasks = self._run_workers(\n+                \"start_worker_execution_loop\",\n+                async_run_remote_workers_only=True,\n+                **self.extra_execute_model_run_workers_kwargs)\n \n         # Only the driver worker returns the sampling results.\n-        return all_outputs[0]\n+        return self._driver_execute_model(execute_model_req)\n+\n+    def stop_remote_worker_execution_loop(self) -> None:\n+        if self.parallel_worker_tasks is None:\n+            return\n+\n+        self._driver_execute_model()\n+        parallel_worker_tasks = self.parallel_worker_tasks\n+        self.parallel_worker_tasks = None\n+        # Ensure that workers exit model loop cleanly\n+        # (this will raise otherwise)\n+        self._wait_for_tasks_completion(parallel_worker_tasks)\n \n     def add_lora(self, lora_request: LoRARequest) -> bool:\n         assert lora_request.lora_int_id > 0, \"lora_id must be greater than 0.\"\n@@ -88,39 +114,84 @@ class DistributedGPUExecutor(GPUExecutor):\n                           pattern=pattern,\n                           max_size=max_size)\n \n+    @abstractmethod\n+    def _driver_execute_model(\n+        self,\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        \"\"\"Run execute_model in the driver worker.\n+\n+        Passing None will cause the driver to stop the model execution\n+        loop running in each of the remote workers.\n+        \"\"\"\n+        raise NotImplementedError\n+\n     @abstractmethod\n     def _run_workers(\n         self,\n         method: str,\n         *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n+        async_run_remote_workers_only: bool = False,\n         max_concurrent_workers: Optional[int] = None,\n         **kwargs,\n     ) -> Any:\n-        \"\"\"Runs the given method on all workers.\"\"\"\n+        \"\"\"Runs the given method on all workers.\n+\n+        Args:\n+            async_run_remote_workers_only: If True the method will be run only\n+                in the remote workers, not the driver worker. It will also be\n+                run asynchronously and return a list of futures rather than\n+                blocking on the results.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    @abstractmethod\n+    def _wait_for_tasks_completion(self, parallel_worker_tasks: Any) -> None:\n+        \"\"\"Wait for futures returned from _run_workers() with\n+        async_run_remote_workers_only to complete.\"\"\"\n         raise NotImplementedError\n \n \n class DistributedGPUExecutorAsync(DistributedGPUExecutor, ExecutorAsyncBase):\n \n+    async def execute_model_async(\n+            self,\n+            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n+        if self.parallel_worker_tasks is None:\n+            # Start model execution loop running in the parallel workers\n+            self.parallel_worker_tasks = asyncio.create_task(\n+                self._start_worker_execution_loop())\n+\n+        # Only the driver worker returns the sampling results.\n+        return await self._driver_execute_model_async(execute_model_req)\n+\n+    async def stop_remote_worker_execution_loop_async(self) -> None:\n+        if self.parallel_worker_tasks is None:\n+            return\n+\n+        await self._driver_execute_model_async()\n+        parallel_worker_tasks = self.parallel_worker_tasks\n+        self.parallel_worker_tasks = None\n+        # Ensure that workers exit model loop cleanly\n+        # (this will raise otherwise)\n+        await parallel_worker_tasks\n+\n     @abstractmethod\n-    async def _run_workers_async(\n+    async def _driver_execute_model_async(\n         self,\n-        method: str,\n-        *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n-        **kwargs,\n-    ) -> Any:\n-        \"\"\"Runs the given method on all workers.\"\"\"\n-        raise NotImplementedError\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        \"\"\"Execute the model asynchronously in the driver worker.\n \n-    async def execute_model_async(self, *args,\n-                                  **kwargs) -> List[SamplerOutput]:\n-        all_outputs = await self._run_workers_async(\"execute_model\",\n-                                                    driver_args=args,\n-                                                    driver_kwargs=kwargs)\n+        Passing None will cause the driver to stop the model execution\n+        loop running in each of the remote workers.\n+        \"\"\"\n+        raise NotImplementedError\n \n-        # Only the driver worker returns the sampling results.\n-        return all_outputs[0]\n+    @abstractmethod\n+    async def _start_worker_execution_loop(self):\n+        \"\"\"Run execution loop on all workers. It guarantees all workers run\n+        the loop or None of them is running the loop. Loop can be stopped by\n+        `stop_remote_worker_execution_loop`.\n+        The API is idempotent (guarantee only 1 loop run at any moment).\"\"\"\n+        raise NotImplementedError",
      "change_type": "modified",
      "lines_added": 98,
      "lines_removed": 27
    },
    {
      "file_path": "vllm/executor/executor_base.py",
      "old_content": "from abc import ABC, abstractmethod\nfrom typing import List, Optional, Set, Tuple\n\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ParallelConfig, SchedulerConfig,\n                         SpeculativeConfig, VisionLanguageConfig)\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\n\n\nclass ExecutorBase(ABC):\n    \"\"\"Base class for all executors.\n\n    An executor is responsible for executing the model on a specific device\n    type (e.g., CPU, GPU, Neuron, etc.). Or it can be a distributed executor\n    that can execute the model on multiple devices.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        vision_language_config: Optional[VisionLanguageConfig],\n        speculative_config: Optional[SpeculativeConfig],\n    ) -> None:\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.vision_language_config = vision_language_config\n        self.speculative_config = speculative_config\n\n        self._init_executor()\n\n    @abstractmethod\n    def _init_executor(self) -> None:\n        pass\n\n    @abstractmethod\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Determine the number of available blocks for the GPU KV cache and\n        swappable CPU KV cache.\n\n        Normally, this should simply delegate to the underlying Worker. Some\n        ExecutorBase may require modification of the result, e.g. to ensure the\n        selected cache sizes are compatible with all workers.\n\n        Returns a Tuple[num_gpu_blocks, num_cpu_blocks], where num_gpu_blocks\n        are blocks that are \"active\" on the device and can be appended to.\n        num_cpu_blocks refers to \"swapped\" blocks in CPU memory and cannot be\n        appended to.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the KV cache with the given size in blocks.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def execute_model(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        \"\"\"Executes at least one model step on the given sequences.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    @abstractmethod\n    def remove_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    @abstractmethod\n    def list_loras(self) -> Set[int]:\n        raise NotImplementedError\n\n    @abstractmethod\n    def check_health(self) -> None:\n        \"\"\"Checks if the executor is healthy. If not, it should raise an\n        exception.\"\"\"\n        raise NotImplementedError\n\n    def shutdown(self) -> None:\n        \"\"\"Shutdown the executor.\"\"\"\n        return\n\n    def __del__(self):\n        self.shutdown()\n\n\nclass ExecutorAsyncBase(ExecutorBase):\n\n    @abstractmethod\n    async def execute_model_async(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        \"\"\"Executes one model step on the given sequences.\"\"\"\n        raise NotImplementedError\n\n    async def check_health_async(self) -> None:\n        \"\"\"Checks if the executor is healthy. If not, it should raise an\n        exception.\"\"\"\n        self.check_health()\n",
      "diff": "diff --git a/vllm/executor/executor_base.py b/vllm/executor/executor_base.py\nindex 08aa58999..4d01939c2 100644\n--- a/vllm/executor/executor_base.py\n+++ b/vllm/executor/executor_base.py\n@@ -74,6 +74,10 @@ class ExecutorBase(ABC):\n         \"\"\"Executes at least one model step on the given sequences.\"\"\"\n         raise NotImplementedError\n \n+    def stop_remote_worker_execution_loop(self) -> None:\n+        \"\"\"Releases parallel workers from model loop.\"\"\"\n+        return\n+\n     @abstractmethod\n     def add_lora(self, lora_request: LoRARequest) -> bool:\n         raise NotImplementedError\n@@ -109,6 +113,10 @@ class ExecutorAsyncBase(ExecutorBase):\n         \"\"\"Executes one model step on the given sequences.\"\"\"\n         raise NotImplementedError\n \n+    async def stop_remote_worker_execution_loop_async(self) -> None:\n+        \"\"\"Releases parallel workers from model loop.\"\"\"\n+        return\n+\n     async def check_health_async(self) -> None:\n         \"\"\"Checks if the executor is healthy. If not, it should raise an\n         exception.\"\"\"",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/executor/multiproc_gpu_executor.py",
      "old_content": "import asyncio\nimport os\nfrom functools import partial\nfrom typing import Any, Dict, Optional, Tuple\n\nfrom vllm.executor.distributed_gpu_executor import (  # yapf: disable\n    DistributedGPUExecutor, DistributedGPUExecutorAsync)\nfrom vllm.executor.multiproc_worker_utils import (ProcessWorkerWrapper,\n                                                  ResultHandler, WorkerMonitor)\nfrom vllm.logger import init_logger\nfrom vllm.utils import (get_distributed_init_method, get_ip, get_open_port,\n                        get_vllm_instance_id, make_async)\n\nlogger = init_logger(__name__)\n\n\nclass MultiprocessingGPUExecutor(DistributedGPUExecutor):\n    \"\"\"Python multiprocessing-based multi-GPU executor\"\"\"\n\n    def _init_executor(self) -> None:\n        assert (\n            not self.speculative_config\n        ), \"Speculative decoding not yet supported for MultiProcGPU backend.\"\n\n        # Create the parallel GPU workers.\n        world_size = self.parallel_config.tensor_parallel_size\n\n        # Set CUDA_VISIBLE_DEVICES for the driver, inherited by workers\n        if \"CUDA_VISIBLE_DEVICES\" not in os.environ:\n            os.environ[\"CUDA_VISIBLE_DEVICES\"] = (\",\".join(\n                map(str, range(world_size))))\n\n        # Ensure that VLLM_INSTANCE_ID is set, to be inherited by workers\n        os.environ[\"VLLM_INSTANCE_ID\"] = get_vllm_instance_id()\n\n        from torch.cuda import device_count\n        assert world_size <= device_count(), (\n            \"please set tensor_parallel_size to less than max local gpu count\")\n\n        distributed_init_method = get_distributed_init_method(\n            get_ip(), get_open_port())\n\n        if world_size == 1:\n            self.workers = []\n        else:\n            result_handler = ResultHandler()\n            self.workers = [\n                ProcessWorkerWrapper(\n                    result_handler,\n                    partial(\n                        self._create_worker,\n                        rank=rank,\n                        local_rank=rank,\n                        distributed_init_method=distributed_init_method,\n                    )) for rank in range(1, world_size)\n            ]\n\n            self.worker_monitor = WorkerMonitor(self.workers, result_handler)\n            result_handler.start()\n            self.worker_monitor.start()\n\n        self.driver_worker = self._create_worker(\n            distributed_init_method=distributed_init_method)\n        self._run_workers(\"init_device\")\n        self._run_workers(\"load_model\",\n                          max_concurrent_workers=self.parallel_config.\n                          max_parallel_loading_workers)\n\n    def shutdown(self):\n        if (worker_monitor := getattr(self, \"worker_monitor\",\n                                      None)) is not None:\n            worker_monitor.close()\n\n    def _run_workers(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        max_concurrent_workers: Optional[int] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n\n        if max_concurrent_workers:\n            raise NotImplementedError(\n                \"max_concurrent_workers is not supported yet.\")\n\n        # Start the workers first.\n        worker_outputs = [\n            worker.execute_method(method, *args, **kwargs)\n            for worker in self.workers\n        ]\n\n        if driver_args is None:\n            driver_args = args\n        if driver_kwargs is None:\n            driver_kwargs = kwargs\n\n        # Start the driver worker after all the ray workers.\n        driver_worker_method = getattr(self.driver_worker, method)\n        driver_worker_output = driver_worker_method(*driver_args,\n                                                    **driver_kwargs)\n\n        # Get the results of the workers.\n        return [driver_worker_output\n                ] + [output.get() for output in worker_outputs]\n\n    def check_health(self) -> None:\n        \"\"\"Raises an error if engine is unhealthy.\"\"\"\n        if not self.worker_monitor.is_alive():\n            raise RuntimeError(\"Worker processes are not running\")\n\n\nclass MultiprocessingGPUExecutorAsync(MultiprocessingGPUExecutor,\n                                      DistributedGPUExecutorAsync):\n\n    async def _run_workers_async(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        if driver_args is None:\n            driver_args = args\n        if driver_kwargs is None:\n            driver_kwargs = kwargs\n\n        driver_executor = make_async(getattr(self.driver_worker, method))\n\n        # Run all the workers asynchronously.\n        coros = [driver_executor(*driver_args, **driver_kwargs)] + [\n            worker.execute_method_async(method, *args, **kwargs)\n            for worker in self.workers\n        ]\n\n        return await asyncio.gather(*coros)\n",
      "diff": "diff --git a/vllm/executor/multiproc_gpu_executor.py b/vllm/executor/multiproc_gpu_executor.py\nindex 2a7b99c9d..8fa544549 100644\n--- a/vllm/executor/multiproc_gpu_executor.py\n+++ b/vllm/executor/multiproc_gpu_executor.py\n@@ -1,13 +1,14 @@\n import asyncio\n import os\n from functools import partial\n-from typing import Any, Dict, Optional, Tuple\n+from typing import Any, List, Optional\n \n from vllm.executor.distributed_gpu_executor import (  # yapf: disable\n     DistributedGPUExecutor, DistributedGPUExecutorAsync)\n from vllm.executor.multiproc_worker_utils import (ProcessWorkerWrapper,\n                                                   ResultHandler, WorkerMonitor)\n from vllm.logger import init_logger\n+from vllm.sequence import ExecuteModelRequest, SamplerOutput\n from vllm.utils import (get_distributed_init_method, get_ip, get_open_port,\n                         get_vllm_instance_id, make_async)\n \n@@ -71,16 +72,34 @@ class MultiprocessingGPUExecutor(DistributedGPUExecutor):\n                                       None)) is not None:\n             worker_monitor.close()\n \n+    def _driver_execute_model(\n+        self,\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        \"\"\"Run execute_model in the driver worker.\n+\n+        Passing None will cause the driver to stop the model execution\n+        loop running in each of the remote workers.\n+        \"\"\"\n+        return self.driver_worker.execute_model(\n+            execute_model_req=execute_model_req)\n+\n     def _run_workers(\n         self,\n         method: str,\n         *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n+        async_run_remote_workers_only: bool = False,\n         max_concurrent_workers: Optional[int] = None,\n         **kwargs,\n     ) -> Any:\n-        \"\"\"Runs the given method on all workers.\"\"\"\n+        \"\"\"Runs the given method on all workers.\n+\n+        Args:\n+            async_run_remote_workers_only: If True the method will be run only\n+                in the remote workers, not the driver worker. It will also be\n+                run asynchronously and return a list of futures rather than\n+                blocking on the results.\n+        \"\"\"\n \n         if max_concurrent_workers:\n             raise NotImplementedError(\n@@ -92,15 +111,12 @@ class MultiprocessingGPUExecutor(DistributedGPUExecutor):\n             for worker in self.workers\n         ]\n \n-        if driver_args is None:\n-            driver_args = args\n-        if driver_kwargs is None:\n-            driver_kwargs = kwargs\n+        if async_run_remote_workers_only:\n+            # Just return futures\n+            return worker_outputs\n \n-        # Start the driver worker after all the ray workers.\n         driver_worker_method = getattr(self.driver_worker, method)\n-        driver_worker_output = driver_worker_method(*driver_args,\n-                                                    **driver_kwargs)\n+        driver_worker_output = driver_worker_method(*args, **kwargs)\n \n         # Get the results of the workers.\n         return [driver_worker_output\n@@ -111,30 +127,29 @@ class MultiprocessingGPUExecutor(DistributedGPUExecutor):\n         if not self.worker_monitor.is_alive():\n             raise RuntimeError(\"Worker processes are not running\")\n \n+    def _wait_for_tasks_completion(self, parallel_worker_tasks: Any) -> None:\n+        \"\"\"Wait for futures returned from _run_workers() with\n+        async_run_remote_workers_only to complete.\"\"\"\n+        for result in parallel_worker_tasks:\n+            result.get()\n+\n \n class MultiprocessingGPUExecutorAsync(MultiprocessingGPUExecutor,\n                                       DistributedGPUExecutorAsync):\n \n-    async def _run_workers_async(\n-        self,\n-        method: str,\n-        *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n-        **kwargs,\n-    ) -> Any:\n-        \"\"\"Runs the given method on all workers.\"\"\"\n-        if driver_args is None:\n-            driver_args = args\n-        if driver_kwargs is None:\n-            driver_kwargs = kwargs\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.driver_exec_model = make_async(self.driver_worker.execute_model)\n \n-        driver_executor = make_async(getattr(self.driver_worker, method))\n+    async def _driver_execute_model_async(\n+        self,\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        return await self.driver_exec_model(execute_model_req)\n \n-        # Run all the workers asynchronously.\n-        coros = [driver_executor(*driver_args, **driver_kwargs)] + [\n-            worker.execute_method_async(method, *args, **kwargs)\n+    async def _start_worker_execution_loop(self):\n+        coros = [\n+            worker.execute_method_async(\"start_worker_execution_loop\")\n             for worker in self.workers\n         ]\n-\n         return await asyncio.gather(*coros)",
      "change_type": "modified",
      "lines_added": 45,
      "lines_removed": 30
    },
    {
      "file_path": "vllm/executor/ray_gpu_executor.py",
      "old_content": "import asyncio\nimport os\nimport pickle\nfrom collections import defaultdict\nfrom itertools import islice, repeat\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\nimport vllm.envs as envs\nfrom vllm.executor.distributed_gpu_executor import (  # yapf: disable\n    DistributedGPUExecutor, DistributedGPUExecutorAsync)\nfrom vllm.executor.ray_utils import RayWorkerWrapper, ray\nfrom vllm.logger import init_logger\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.utils import (get_distributed_init_method, get_ip, get_open_port,\n                        get_vllm_instance_id, make_async)\n\nif ray is not None:\n    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\nlogger = init_logger(__name__)\n\nUSE_RAY_COMPILED_DAG = envs.VLLM_USE_RAY_COMPILED_DAG\n\n\nclass RayGPUExecutor(DistributedGPUExecutor):\n\n    def _init_executor(self) -> None:\n        assert self.parallel_config.distributed_executor_backend == \"ray\"\n        placement_group = self.parallel_config.placement_group\n\n        # Disable Ray usage stats collection.\n        ray_usage = os.environ.get(\"RAY_USAGE_STATS_ENABLED\", \"0\")\n        if ray_usage != \"1\":\n            os.environ[\"RAY_USAGE_STATS_ENABLED\"] = \"0\"\n\n        # Create the parallel GPU workers.\n        self._init_workers_ray(placement_group)\n\n        self.forward_dag = None\n        if USE_RAY_COMPILED_DAG:\n            self.forward_dag = self._compiled_ray_dag()\n\n    def _configure_ray_workers_use_nsight(self,\n                                          ray_remote_kwargs) -> Dict[str, Any]:\n        # If nsight profiling is enabled, we need to set the profiling\n        # configuration for the ray workers as runtime env.\n        runtime_env = ray_remote_kwargs.setdefault(\"runtime_env\", {})\n        runtime_env.update({\n            \"nsight\": {\n                \"t\": \"cuda,cudnn,cublas\",\n                \"o\": \"'worker_process_%p'\",\n                \"cuda-graph-trace\": \"node\",\n            }\n        })\n\n        return ray_remote_kwargs\n\n    def _init_workers_ray(self, placement_group: \"PlacementGroup\",\n                          **ray_remote_kwargs):\n        if self.parallel_config.tensor_parallel_size == 1:\n            # For single GPU case, we use a ray worker with constrained memory.\n            num_gpus = self.cache_config.gpu_memory_utilization\n        else:\n            # Otherwise, the ray workers are allocated with a full GPU.\n            num_gpus = 1\n\n        # The driver dummy worker does not actually use any resources.\n        # It holds the resource for the driver worker.\n        self.driver_dummy_worker: Optional[RayWorkerWrapper] = None\n        # The remaining workers are the actual ray actors.\n        self.workers: List[RayWorkerWrapper] = []\n\n        if self.parallel_config.ray_workers_use_nsight:\n            ray_remote_kwargs = self._configure_ray_workers_use_nsight(\n                ray_remote_kwargs)\n\n        # Create the workers.\n        driver_ip = get_ip()\n        for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n            if not bundle.get(\"GPU\", 0):\n                continue\n            scheduling_strategy = PlacementGroupSchedulingStrategy(\n                placement_group=placement_group,\n                placement_group_capture_child_tasks=True,\n                placement_group_bundle_index=bundle_id,\n            )\n\n            if self.speculative_config is not None:\n                worker_module_name = \"vllm.spec_decode.spec_decode_worker\"\n                worker_class_name = \"create_spec_worker\"\n            else:\n                worker_module_name = \"vllm.worker.worker\"\n                worker_class_name = \"Worker\"\n\n            worker = ray.remote(\n                num_cpus=0,\n                num_gpus=num_gpus,\n                scheduling_strategy=scheduling_strategy,\n                **ray_remote_kwargs,\n            )(RayWorkerWrapper).remote(\n                worker_module_name=worker_module_name,\n                worker_class_name=worker_class_name,\n                trust_remote_code=self.model_config.trust_remote_code,\n            )\n\n            worker_ip = ray.get(worker.get_node_ip.remote())\n            if worker_ip == driver_ip and self.driver_dummy_worker is None:\n                # If the worker is on the same node as the driver, we use it\n                # as the resource holder for the driver process.\n                self.driver_dummy_worker = worker\n                self.driver_worker = RayWorkerWrapper(\n                    worker_module_name=worker_module_name,\n                    worker_class_name=worker_class_name,\n                    trust_remote_code=self.model_config.trust_remote_code,\n                )\n            else:\n                # Else, added to the list of workers.\n                self.workers.append(worker)\n\n        if self.driver_dummy_worker is None:\n            raise ValueError(\n                \"Ray does not allocate any GPUs on the driver node. Consider \"\n                \"adjusting the Ray placement group or running the driver on a \"\n                \"GPU node.\")\n\n        # Get the set of GPU IDs used on each node.\n        worker_node_and_gpu_ids = self._run_workers(\"get_node_and_gpu_ids\",\n                                                    use_dummy_driver=True)\n\n        node_workers = defaultdict(list)\n        node_gpus = defaultdict(list)\n\n        for i, (node_id, gpu_ids) in enumerate(worker_node_and_gpu_ids):\n            node_workers[node_id].append(i)\n            node_gpus[node_id].extend(gpu_ids)\n        for node_id, gpu_ids in node_gpus.items():\n            node_gpus[node_id] = sorted(gpu_ids)\n\n        VLLM_INSTANCE_ID = get_vllm_instance_id()\n\n        # Set environment variables for the driver and workers.\n        all_args_to_update_environment_variables = [({\n            \"CUDA_VISIBLE_DEVICES\":\n            \",\".join(map(str, node_gpus[node_id])),\n            \"VLLM_INSTANCE_ID\":\n            VLLM_INSTANCE_ID,\n            \"VLLM_TRACE_FUNCTION\":\n            str(envs.VLLM_TRACE_FUNCTION),\n        }, ) for (node_id, _) in worker_node_and_gpu_ids]\n        self._run_workers(\"update_environment_variables\",\n                          all_args=all_args_to_update_environment_variables)\n\n        distributed_init_method = get_distributed_init_method(\n            driver_ip, get_open_port())\n\n        # Initialize the actual workers inside worker wrapper.\n        init_worker_all_kwargs = [\n            self._get_worker_kwargs(\n                local_rank=node_workers[node_id].index(rank),\n                rank=rank,\n                distributed_init_method=distributed_init_method,\n            ) for rank, (node_id, _) in enumerate(worker_node_and_gpu_ids)\n        ]\n        self._run_workers(\"init_worker\", all_kwargs=init_worker_all_kwargs)\n\n        self._run_workers(\"init_device\")\n        self._run_workers(\"load_model\",\n                          max_concurrent_workers=self.parallel_config.\n                          max_parallel_loading_workers)\n\n    def execute_model(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        all_outputs = self._run_workers(\n            \"execute_model\",\n            driver_kwargs={\"execute_model_req\": execute_model_req},\n            use_ray_compiled_dag=USE_RAY_COMPILED_DAG)\n\n        # Only the driver worker returns the sampling results.\n        return all_outputs[0]\n\n    def _run_workers(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        all_args: Optional[List[Tuple[Any, ...]]] = None,\n        all_kwargs: Optional[List[Dict[str, Any]]] = None,\n        use_dummy_driver: bool = False,\n        max_concurrent_workers: Optional[int] = None,\n        use_ray_compiled_dag: bool = False,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers. Can be used in the following\n        ways:\n\n        - args/kwargs: All workers share the same args/kwargs\n        - args/kwargs and driver_args/driver_kwargs: Driver worker has\n          different args\n        - all_args/all_kwargs: args/kwargs for each worker are specified\n          individually\n        \"\"\"\n\n        if max_concurrent_workers:\n            raise NotImplementedError(\n                \"max_concurrent_workers is not supported yet.\")\n\n        if driver_args is None:\n            driver_args = args if all_args is None else all_args[0]\n        if driver_kwargs is None:\n            driver_kwargs = kwargs if all_kwargs is None else all_kwargs[0]\n\n        count = len(self.workers)\n        all_worker_args = repeat(args, count) if all_args is None \\\n            else islice(all_args, 1, None)\n        all_worker_kwargs = repeat(kwargs, count) if all_kwargs is None \\\n            else islice(all_kwargs, 1, None)\n\n        if use_ray_compiled_dag:\n            # Right now, compiled DAG can only accept a single\n            # input. TODO(sang): Fix it.\n            assert self.forward_dag is not None\n            output_channels = self.forward_dag.execute(1)\n        else:\n            # Start the ray workers first.\n            ray_worker_outputs = [\n                worker.execute_method.remote(method, *worker_args,\n                                             **worker_kwargs)\n                for (worker, worker_args, worker_kwargs\n                     ) in zip(self.workers, all_worker_args, all_worker_kwargs)\n            ]\n\n        # Start the driver worker after all the ray workers.\n        if not use_dummy_driver:\n            driver_worker_output = self.driver_worker.execute_method(\n                method, *driver_args, **driver_kwargs)\n        else:\n            assert self.driver_dummy_worker is not None\n            driver_worker_output = ray.get(\n                self.driver_dummy_worker.execute_method.remote(\n                    method, *driver_args, **driver_kwargs))\n        # Get the results of the ray workers.\n        if self.workers:\n            if use_ray_compiled_dag:\n                try:\n                    ray_worker_outputs = [\n                        pickle.loads(chan.begin_read())\n                        for chan in output_channels\n                    ]\n                finally:\n                    # Has to call end_read in order to reuse the DAG.\n                    for chan in output_channels:\n                        chan.end_read()\n            else:\n                ray_worker_outputs = ray.get(ray_worker_outputs)\n\n        return [driver_worker_output] + ray_worker_outputs\n\n    def _compiled_ray_dag(self):\n        import pkg_resources\n        required_version = \"2.9\"\n        current_version = pkg_resources.get_distribution(\"ray\").version\n        if current_version < required_version:\n            raise ValueError(f\"Ray version {required_version} or greater is \"\n                             f\"required, but found {current_version}\")\n\n        from ray.dag import InputNode, MultiOutputNode\n        assert self.parallel_config.distributed_executor_backend == \"ray\"\n\n        # Right now, compiled DAG requires at least 1 arg. We send\n        # a dummy value for now. It will be fixed soon.\n        with InputNode() as input_data:\n            forward_dag = MultiOutputNode([\n                worker.execute_model_compiled_dag_remote.\n                bind(  # type: ignore[attr-defined]\n                    input_data) for worker in self.workers\n            ])\n        return forward_dag.experimental_compile()\n\n    def check_health(self) -> None:\n        \"\"\"Raises an error if engine is unhealthy.\"\"\"\n        self._check_if_any_actor_is_dead()\n\n    def _check_if_any_actor_is_dead(self):\n        if not self.workers:\n            return\n\n        dead_actors = []\n        for actor in self.workers:\n            actor_state = ray.state.actors(actor._ray_actor_id.hex())  # pylint: disable=protected-access\n            if actor_state[\"State\"] == \"DEAD\":\n                dead_actors.append(actor)\n        if dead_actors:\n            raise RuntimeError(\"At least one Worker is dead. \"\n                               f\"Dead Workers: {dead_actors}. \")\n\n\nclass RayGPUExecutorAsync(RayGPUExecutor, DistributedGPUExecutorAsync):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.driver_executor = make_async(self.driver_worker.execute_method)\n\n    async def _run_workers_async(\n        self,\n        method: str,\n        *args,\n        driver_args: Optional[Tuple[Any, ...]] = None,\n        driver_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        coros = []\n\n        if driver_args is None:\n            driver_args = args\n        if driver_kwargs is None:\n            driver_kwargs = kwargs\n\n        coros.append(\n            self.driver_executor(method, *driver_args, **driver_kwargs))\n\n        # Run the ray workers asynchronously.\n        for worker in self.workers:\n            coros.append(worker.execute_method.remote(method, *args, **kwargs))\n\n        all_outputs = await asyncio.gather(*coros)\n        return all_outputs\n",
      "diff": "diff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex dd3ee6068..bed356d1b 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -42,6 +42,8 @@ class RayGPUExecutor(DistributedGPUExecutor):\n         self.forward_dag = None\n         if USE_RAY_COMPILED_DAG:\n             self.forward_dag = self._compiled_ray_dag()\n+            self.extra_execute_model_run_workers_kwargs[\n+                \"use_ray_compiled_dag\"] = True\n \n     def _configure_ray_workers_use_nsight(self,\n                                           ray_remote_kwargs) -> Dict[str, Any]:\n@@ -171,23 +173,23 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                           max_concurrent_workers=self.parallel_config.\n                           max_parallel_loading_workers)\n \n-    def execute_model(\n-            self,\n-            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n-        all_outputs = self._run_workers(\n-            \"execute_model\",\n-            driver_kwargs={\"execute_model_req\": execute_model_req},\n-            use_ray_compiled_dag=USE_RAY_COMPILED_DAG)\n+    def _driver_execute_model(\n+        self,\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        \"\"\"Run execute_model in the driver worker.\n \n-        # Only the driver worker returns the sampling results.\n-        return all_outputs[0]\n+        Passing None will cause the driver to stop the model execution\n+        loop running in each of the remote workers.\n+        \"\"\"\n+        return self.driver_worker.execute_method(\"execute_model\",\n+                                                 execute_model_req)\n \n     def _run_workers(\n         self,\n         method: str,\n         *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n+        async_run_remote_workers_only: bool = False,\n         all_args: Optional[List[Tuple[Any, ...]]] = None,\n         all_kwargs: Optional[List[Dict[str, Any]]] = None,\n         use_dummy_driver: bool = False,\n@@ -198,9 +200,11 @@ class RayGPUExecutor(DistributedGPUExecutor):\n         \"\"\"Runs the given method on all workers. Can be used in the following\n         ways:\n \n+        - async_run_remote_workers_only: If True the method will be run only\n+          in the remote workers, not the driver worker. It will also be\n+          run asynchronously and return a list of futures rather than blocking\n+          on the results.\n         - args/kwargs: All workers share the same args/kwargs\n-        - args/kwargs and driver_args/driver_kwargs: Driver worker has\n-          different args\n         - all_args/all_kwargs: args/kwargs for each worker are specified\n           individually\n         \"\"\"\n@@ -209,11 +213,6 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             raise NotImplementedError(\n                 \"max_concurrent_workers is not supported yet.\")\n \n-        if driver_args is None:\n-            driver_args = args if all_args is None else all_args[0]\n-        if driver_kwargs is None:\n-            driver_kwargs = kwargs if all_kwargs is None else all_kwargs[0]\n-\n         count = len(self.workers)\n         all_worker_args = repeat(args, count) if all_args is None \\\n             else islice(all_args, 1, None)\n@@ -225,6 +224,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             # input. TODO(sang): Fix it.\n             assert self.forward_dag is not None\n             output_channels = self.forward_dag.execute(1)\n+            ray_worker_outputs = []\n         else:\n             # Start the ray workers first.\n             ray_worker_outputs = [\n@@ -234,6 +234,13 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                      ) in zip(self.workers, all_worker_args, all_worker_kwargs)\n             ]\n \n+        if async_run_remote_workers_only:\n+            # Just return futures\n+            return ray_worker_outputs\n+\n+        driver_args = args if all_args is None else all_args[0]\n+        driver_kwargs = kwargs if all_kwargs is None else all_kwargs[0]\n+\n         # Start the driver worker after all the ray workers.\n         if not use_dummy_driver:\n             driver_worker_output = self.driver_worker.execute_method(\n@@ -260,6 +267,11 @@ class RayGPUExecutor(DistributedGPUExecutor):\n \n         return [driver_worker_output] + ray_worker_outputs\n \n+    def _wait_for_tasks_completion(self, parallel_worker_tasks: Any) -> None:\n+        \"\"\"Wait for futures returned from _run_workers() with\n+        async_run_remote_workers_only to complete.\"\"\"\n+        ray.get(parallel_worker_tasks)\n+\n     def _compiled_ray_dag(self):\n         import pkg_resources\n         required_version = \"2.9\"\n@@ -303,30 +315,18 @@ class RayGPUExecutorAsync(RayGPUExecutor, DistributedGPUExecutorAsync):\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n-        self.driver_executor = make_async(self.driver_worker.execute_method)\n+        self.driver_exec_method = make_async(self.driver_worker.execute_method)\n \n-    async def _run_workers_async(\n+    async def _driver_execute_model_async(\n         self,\n-        method: str,\n-        *args,\n-        driver_args: Optional[Tuple[Any, ...]] = None,\n-        driver_kwargs: Optional[Dict[str, Any]] = None,\n-        **kwargs,\n-    ) -> Any:\n-        \"\"\"Runs the given method on all workers.\"\"\"\n-        coros = []\n-\n-        if driver_args is None:\n-            driver_args = args\n-        if driver_kwargs is None:\n-            driver_kwargs = kwargs\n-\n-        coros.append(\n-            self.driver_executor(method, *driver_args, **driver_kwargs))\n-\n-        # Run the ray workers asynchronously.\n-        for worker in self.workers:\n-            coros.append(worker.execute_method.remote(method, *args, **kwargs))\n-\n-        all_outputs = await asyncio.gather(*coros)\n-        return all_outputs\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n+        return await self.driver_exec_method(\"execute_model\",\n+                                             execute_model_req)\n+\n+    async def _start_worker_execution_loop(self):\n+        coros = [\n+            worker.execute_method.remote(\"start_worker_execution_loop\")\n+            for worker in self.workers\n+        ]\n+        return await asyncio.gather(*coros)",
      "change_type": "modified",
      "lines_added": 44,
      "lines_removed": 44
    },
    {
      "file_path": "vllm/spec_decode/ngram_worker.py",
      "old_content": "import weakref\nfrom typing import List, Optional, Tuple\n\nimport torch\n\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.spec_decode.interfaces import SpeculativeProposals\nfrom vllm.spec_decode.top1_proposer import Top1Proposer\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase\n\n\nclass NGramWorker(LoraNotSupportedWorkerBase):\n    \"\"\"NGramWorker provides a light drafter without need for model.\n\n    Current NGramWorker only implement prompt lookup decoding,\n    and in future we may also do RAG type drafter and other scenerios\n    which don't rely on LLM model to give proposals.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Get local_rank/vocab_size from kwargs attribute\n        self.local_rank = kwargs[\"local_rank\"]\n        self.vocab_size = kwargs[\"model_config\"].get_vocab_size()\n\n        # Lazy initialization list.\n        self._proposer: Top1Proposer\n\n    def set_ngram_window_size(self, ngram_prompt_lookup_min: int,\n                              ngram_prompt_lookup_max: int):\n        # Search valid candidate window between\n        # ngram_prompt_lookup_min/ngram_prompt_lookup_max\n        self.ngram_prompt_lookup_max = ngram_prompt_lookup_max\n        self.ngram_prompt_lookup_min = ngram_prompt_lookup_min\n\n    def init_device(self):\n        self.device = torch.device(f\"cuda:{self.local_rank}\")\n        self.load_model = lambda *args, **kwargs: None\n\n        # Current only support Top1Proposer\n        self._proposer = Top1Proposer(\n            weakref.proxy(self),\n            device=self.device,\n            vocab_size=self.vocab_size,\n        )\n\n    def set_include_gpu_probs_tensor(self):\n        # NGram don't need gpu sampler\n        pass\n\n    def execute_model(self, execute_model_req: ExecuteModelRequest) -> None:\n        \"\"\"NGram doesn't depend on model execution, just pass this function\"\"\"\n        pass\n\n    def determine_num_available_blocks(self) -> None:\n        \"\"\"NGram doesn't depend on model execution, no need to check blocks\"\"\"\n        pass\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"As there is no cache need to handle, just pass this function\"\"\"\n        pass\n\n    def get_cache_block_size_bytes(self):\n        \"\"\"Return the size of a cache block in bytes.\"\"\"\n        return 0\n\n    def sampler_output(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        sample_len: int,\n    ) -> Tuple[Optional[List[SamplerOutput]], bool]:\n        \"\"\"NGram match algo to pick proposal candidate. Returns the list of\n        sampler output, one per SequenceGroupMetadata.\n\n        For ngram worker, we already done needed transposed internal, so the\n        indicator pass to sampler_output_to_torch shall be False.\n        \"\"\"\n        self._raise_if_unsupported(execute_model_req)\n\n        has_spec_out = False\n        token_id_list = []\n        token_prob_list = []\n        for idx, seq_group_metadata in enumerate(\n                execute_model_req.seq_group_metadata_list):\n            seq_data = next(iter(seq_group_metadata.seq_data.values()))\n\n            input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                        dtype=torch.long,\n                                        device=self.device)\n            input_length = seq_data.get_len()\n\n            for ngram_size in range(\n                    min(self.ngram_prompt_lookup_max, input_length - 1),\n                    self.ngram_prompt_lookup_min - 1,\n                    -1,\n            ):\n                ngram_tensor = input_ids[-ngram_size:]\n                proposal_start_idx = None\n                if ngram_size == 1:\n                    # Do not match itself and do not use unfold and all\n                    matches = (input_ids[:-1] == ngram_tensor)\n                else:\n                    windows = input_ids.unfold(dimension=0,\n                                               size=ngram_size,\n                                               step=1)\n                    # Do not match itself\n                    matches = (windows[:-1] == ngram_tensor).all(dim=-1)\n\n                # first_match includes \"values\" (bool), indicating whether\n                # the match is found, and \"indices\", indicating the index\n                # of the first match.\n                # Note that \"first_match.values.item()\" triggers GPU-CPU\n                # sync so it is a bit inefficient, but we have not found\n                # a better way to do this.\n                first_match = matches.max(dim=-1)\n                if first_match.values.item():\n                    proposal_start_idx = first_match.indices.add_(ngram_size)\n                    spec_indices = (\n                        proposal_start_idx).repeat(sample_len) + torch.arange(\n                            sample_len, device=self.device)\n                    spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n                    res = input_ids.gather(dim=-1, index=spec_indices)\n                    token_id_list.append(res)\n                    token_prob_list.append(\n                        torch.nn.functional.one_hot(\n                            res,\n                            num_classes=self.vocab_size).to(torch.float32))\n                    has_spec_out = True\n                    break\n            else:\n                token_id_list.append(None)\n                token_prob_list.append(None)\n\n        if not has_spec_out:\n            return None, False\n\n        outputs: List[Optional[SamplerOutput]] = []\n        for idx in range(len(execute_model_req.seq_group_metadata_list)):\n            if token_id_list[idx] is None:\n                outputs.append(None)\n            else:\n                outputs.append(\n                    SamplerOutput(\n                        outputs=None,\n                        sampled_token_probs=token_prob_list[idx],\n                        logprobs=torch.zeros((sample_len, self.vocab_size),\n                                             dtype=torch.float32,\n                                             device=self.device),\n                        sampled_token_ids=token_id_list[idx],\n                    ))\n\n        return outputs, False\n\n    def get_spec_proposals(\n        self,\n        execute_model_req: ExecuteModelRequest,\n    ) -> SpeculativeProposals:\n        \"\"\"Produce speculations given an input batch of sequences. The number of\n        speculative tokens per sequence is determined by max_proposal_len.\n        \"\"\"\n\n        return self._proposer.get_proposals(execute_model_req)\n\n    def _raise_if_unsupported(\n        self,\n        execute_model_req: ExecuteModelRequest,\n    ) -> None:\n        \"\"\"NGramWorker does not yet implement support for cache swap\n        operations or beam search.\n        \"\"\"\n        if any([\n                execute_model_req.blocks_to_swap_in,\n                execute_model_req.blocks_to_swap_out,\n                execute_model_req.blocks_to_copy\n        ]):\n            raise NotImplementedError(\n                \"NGramWorker does not support cache operations\")\n\n        if any(\n                len(seq_group_metadata.seq_data.keys()) != 1\n                for seq_group_metadata in\n                execute_model_req.seq_group_metadata_list):\n            raise NotImplementedError(\n                \"NGramWorker does not support beam search.\")\n",
      "diff": "diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 9628f7af5..c2b22f2ac 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -47,7 +47,9 @@ class NGramWorker(LoraNotSupportedWorkerBase):\n         # NGram don't need gpu sampler\n         pass\n \n-    def execute_model(self, execute_model_req: ExecuteModelRequest) -> None:\n+    def execute_model(\n+            self,\n+            execute_model_req: Optional[ExecuteModelRequest] = None) -> None:\n         \"\"\"NGram doesn't depend on model execution, just pass this function\"\"\"\n         pass",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/spec_decode/spec_decode_worker.py",
      "old_content": "from functools import cached_property\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\n\nfrom vllm.distributed.communication_op import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rejection_sampler import RejectionSampler\nfrom vllm.sequence import (ExecuteModelRequest, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.spec_decode.batch_expansion import BatchExpansionTop1Scorer\nfrom vllm.spec_decode.interfaces import (SpeculativeProposals,\n                                         SpeculativeScorer, SpeculativeScores)\nfrom vllm.spec_decode.metrics import AsyncMetricsCollector\nfrom vllm.spec_decode.multi_step_worker import MultiStepWorker\nfrom vllm.spec_decode.ngram_worker import NGramWorker\nfrom vllm.spec_decode.util import (create_sequence_group_output,\n                                   get_all_num_logprobs, get_all_seq_ids,\n                                   get_sampled_token_logprobs, nvtx_range,\n                                   split_batch_by_proposal_len)\nfrom vllm.worker.worker import Worker\nfrom vllm.worker.worker_base import LoraNotSupportedWorkerBase, WorkerBase\n\nlogger = init_logger(__name__)\n\n\ndef create_spec_worker(*args, **kwargs) -> \"SpecDecodeWorker\":\n    \"\"\"Helper method that is the entrypoint for Executors which use\n    WorkerWrapper. It constructs a SpecDecodeWorker from the speculative config.\n    \"\"\"\n    assert \"speculative_config\" in kwargs\n    speculative_config = kwargs.get(\"speculative_config\")\n    assert speculative_config is not None\n\n    target_worker = Worker(*args, **kwargs)\n\n    draft_worker_kwargs = kwargs.copy()\n    # Override draft-model specific worker args.\n    draft_worker_kwargs.update(\n        model_config=speculative_config.draft_model_config,\n        parallel_config=speculative_config.draft_parallel_config,\n        ngram_prompt_lookup_max=speculative_config.ngram_prompt_lookup_max,\n        ngram_prompt_lookup_min=speculative_config.ngram_prompt_lookup_min,\n        # TODO allow draft-model specific load config.\n        #load_config=load_config,\n    )\n\n    spec_decode_worker = SpecDecodeWorker.create_worker(\n        scorer_worker=target_worker,\n        draft_worker_kwargs=draft_worker_kwargs,\n        disable_by_batch_size=speculative_config.\n        speculative_disable_by_batch_size,\n    )\n\n    return spec_decode_worker\n\n\nclass SpecDecodeWorker(LoraNotSupportedWorkerBase):\n    \"\"\"Worker which implements speculative decoding.\n\n    Speculative decoding reduces decoding per-token latency by using a proposal\n    method, such as a small draft model, to speculate ahead of a larger LLM. The\n    probabilities of the speculative tokens are then determined by the larger\n    LLM, after which some verification routine determines which (if any) of the\n    speculative tokens are accepted by the larger LLM.\n\n    See https://github.com/vllm-project/vllm/pull/2188 and\n    https://github.com/vllm-project/vllm/pull/3103 for more info.\n\n    The current implementation has the following limitations:\n    * Only draft-model proposal is implemented (contributions for more forms are\n        welcome!).\n    * Only top-1 proposal and scoring are implemented. Tree-attention is left as\n        future work.\n    * Only lossless rejection sampling is supported. Contributions adding lossy\n        verification routines are welcome (e.g. Medusa's typical acceptance).\n    * All sequences in a batch must have the same proposal length, or zero. This\n        can be improved by having per-sequence speculation in the future.\n    * The scoring forward pass is done without an MQA kernel, which is\n        suboptimal especially as the batch size, proposal length, and sequence\n        lengths grow. Contributions to add a MQA scoring are welcome once\n        correctness tests pass.\n        More info here https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit.\n    \"\"\"\n\n    @classmethod\n    def create_worker(\n        cls,\n        scorer_worker: WorkerBase,\n        draft_worker_kwargs: Dict[str, Any],\n        disable_by_batch_size: Optional[int],\n    ) -> \"SpecDecodeWorker\":\n\n        ngram_prompt_lookup_max = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_max\"))\n        ngram_prompt_lookup_min = (\n            draft_worker_kwargs.pop(\"ngram_prompt_lookup_min\"))\n\n        disable_bonus_tokens = True\n        if ngram_prompt_lookup_max > 0:\n            disable_bonus_tokens = False\n            proposer_worker = NGramWorker(**draft_worker_kwargs)\n            proposer_worker.set_ngram_window_size(ngram_prompt_lookup_min,\n                                                  ngram_prompt_lookup_max)\n        else:\n            proposer_worker = MultiStepWorker(**draft_worker_kwargs)\n\n        logger.info(\"Configuring SpecDecodeWorker with proposer=%s\",\n                    type(proposer_worker))\n\n        return SpecDecodeWorker(\n            proposer_worker,\n            scorer_worker,\n            disable_by_batch_size=disable_by_batch_size,\n            rejection_sampler=RejectionSampler(\n                disable_bonus_tokens=disable_bonus_tokens, ))\n\n    def __init__(\n        self,\n        proposer_worker: WorkerBase,\n        scorer_worker: WorkerBase,\n        rejection_sampler: RejectionSampler,\n        metrics_collector: Optional[AsyncMetricsCollector] = None,\n        disable_by_batch_size: Optional[int] = None,\n    ):\n        \"\"\"\n        Create a SpecDecodeWorker.\n\n        Args:\n            proposer_worker: A worker that can produce speculative tokens for\n                sequences.\n            scorer_worker: A worker that produces probabilities of speculative\n                tokens according to some base model. Typically a vanilla vLLM\n                Worker.\n            rejection_sampler: A Torch module used to perform modified rejection\n                sampling for speculative decoding.\n            disable_by_batch_size: If the batch size is larger than this,\n                disable speculative decoding for new incoming requests.\n            metrics_collector: Helper class for collecting metrics; can be set\n                for testing purposes.\n        \"\"\"\n        self.proposer_worker = proposer_worker\n        self.scorer_worker = scorer_worker\n        self.disable_by_batch_size = disable_by_batch_size or float(\"inf\")\n        self.rejection_sampler = rejection_sampler\n\n        self._metrics = AsyncMetricsCollector(\n            rejection_sampler\n        ) if metrics_collector is None else metrics_collector\n\n        self.probs_dtype = self.rejection_sampler.probs_dtype\n        self.token_id_dtype = self.rejection_sampler.token_id_dtype\n\n        # Lazy initiazliation.\n        self.scorer: SpeculativeScorer\n\n    def init_device(self) -> None:\n        \"\"\"Initialize both scorer and proposer models.\n        \"\"\"\n        # The scorer worker model is initialized first in case the proposer\n        # model has a smaller TP degree than the target worker.\n        self.scorer_worker.init_device()\n        self.proposer_worker.init_device()\n\n        # NOTE(cade): load_model is not part of the WorkerBase interface.\n        self.scorer_worker.load_model()\n        self.proposer_worker.load_model()\n\n        self._metrics.init_gpu_tensors(self.rank)\n        self.rejection_sampler.init_gpu_tensors(self.rank)\n        self.scorer = BatchExpansionTop1Scorer(\n            scorer_worker=self.scorer_worker,\n            device=self.device,\n            vocab_size=self._vocab_size)\n\n        self._configure_model_sampler_for_spec_decode()\n\n    def load_model(self, *args, **kwargs):\n        pass\n\n    def _configure_model_sampler_for_spec_decode(self):\n        \"\"\"Configure model sampler to emit GPU tensors. This allows spec decode\n        to keep data on device without transferring to CPU and serializing,\n        which significantly reduces overhead of rejection sampling.\n\n        NOTE(cade): This breaks abstraction boundaries pretty badly. The better\n        design is to have the \"move to CPU and serialize\" sampling decision be\n        done outside of the model/sampler; this way the \"last-mile\" worker\n        object which interfaces with the scheduler can serialize and incur the\n        performance hit as necessary. This allows us to run the worker several\n        iterations in a row without incurring the \"move to CPU and serialize\"\n        performance penalty.\n\n        Since this requires a large change to vLLM, we defer it to later and\n        temporarily accept this broken abstraction boundary.\n\n        NOTE(cade): This will require a special check if the proposer worker\n        does not have a sampler (e.g. ngram speculation).\n        \"\"\"\n        (self.scorer_worker.model_runner.model.sampler.include_gpu_probs_tensor\n         ) = True\n        self.proposer_worker.set_include_gpu_probs_tensor()\n\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Determine the number of cache blocks to use.\n\n        This is done by profiling the scorer model (which is typically the\n        larger of the two). Then the total memory which would be used by the\n        scorer cache is divided evenly between the proposer and scorer model KV,\n        such that the number of blocks is equal in both KV caches.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.scorer_worker.determine_num_available_blocks())\n\n        scorer_cache_block_size_bytes = (\n            self.scorer_worker.get_cache_block_size_bytes())\n        proposer_cache_block_size_bytes = (\n            self.proposer_worker.get_cache_block_size_bytes())\n\n        new_num_gpu_blocks = split_num_cache_blocks_evenly(\n            scorer_cache_block_size_bytes, proposer_cache_block_size_bytes,\n            num_gpu_blocks)\n        return new_num_gpu_blocks, num_cpu_blocks\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the cache engine of the scorer and proposer workers.\n        \"\"\"\n        self.scorer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                            num_cpu_blocks=num_cpu_blocks)\n        self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                              num_cpu_blocks=num_cpu_blocks)\n\n    def _broadcast_control_flow_decision(\n            self,\n            execute_model_req: Optional[ExecuteModelRequest] = None,\n            disable_all_speculation: bool = False) -> Tuple[int, bool]:\n        \"\"\"Broadcast how many lookahead slots are scheduled for this step, and\n        whether all speculation is disabled, to all non-driver workers.\n\n        This is required as if the number of draft model runs changes\n        dynamically, the non-driver workers won't know unless we perform a\n        communication to inform then.\n\n        Returns the broadcasted num_lookahead_slots and disable_all_speculation.\n        \"\"\"\n\n        if self.rank == self._driver_rank:\n            assert execute_model_req is not None\n\n            broadcast_dict = dict(\n                num_lookahead_slots=execute_model_req.num_lookahead_slots,\n                disable_all_speculation=disable_all_speculation,\n            )\n            broadcast_tensor_dict(broadcast_dict, src=self._driver_rank)\n        else:\n            assert execute_model_req is None\n            broadcast_dict = broadcast_tensor_dict(src=self._driver_rank)\n\n        return (broadcast_dict[\"num_lookahead_slots\"],\n                broadcast_dict[\"disable_all_speculation\"])\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        execute_model_req: Optional[ExecuteModelRequest] = None\n    ) -> List[SamplerOutput]:\n        \"\"\"Perform speculative decoding on the input batch.\n        \"\"\"\n\n        disable_all_speculation = False\n        if self.rank == self._driver_rank:\n            disable_all_speculation = self._should_disable_all_speculation(\n                execute_model_req)\n\n        (num_lookahead_slots,\n         disable_all_speculation) = self._broadcast_control_flow_decision(\n             execute_model_req, disable_all_speculation)\n\n        if self.rank == self._driver_rank:\n            assert execute_model_req is not None\n            assert execute_model_req.seq_group_metadata_list is not None, (\n                \"speculative decoding requires non-None seq_group_metadata_list\"\n            )\n\n            self._maybe_disable_speculative_tokens(\n                disable_all_speculation,\n                execute_model_req.seq_group_metadata_list)\n\n            # If no spec tokens, call the proposer and scorer workers normally.\n            # Used for prefill.\n            if num_lookahead_slots == 0 or len(\n                    execute_model_req.seq_group_metadata_list) == 0:\n                return self._run_no_spec(execute_model_req,\n                                         skip_proposer=disable_all_speculation)\n\n            return self._run_speculative_decoding_step(execute_model_req,\n                                                       num_lookahead_slots)\n        else:\n            self._run_non_driver_rank(num_lookahead_slots)\n            return []\n\n    def _should_disable_all_speculation(\n            self, execute_model_req: ExecuteModelRequest) -> bool:\n        # When the batch size is too large, disable speculative decoding\n        # to stop trading off throughput for latency.\n        disable_all_speculation = (execute_model_req.running_queue_size >=\n                                   self.disable_by_batch_size)\n\n        return disable_all_speculation\n\n    def _maybe_disable_speculative_tokens(\n            self, disable_all_speculation: bool,\n            seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        if not disable_all_speculation:\n            return\n\n        for seq_group_metadata in seq_group_metadata_list:\n            # Once num_speculative_tokens is set to 0, the spec decode\n            # of this request will be disabled forever.\n            # TODO(comaniac): We currently store spec decoding specific\n            # state in the global data structure, but we should maintain\n            # this state within spec decode worker.\n            seq_group_metadata.num_speculative_tokens = 0\n\n    @nvtx_range(\"spec_decode_worker._run_no_spec\")\n    def _run_no_spec(self, execute_model_req: ExecuteModelRequest,\n                     skip_proposer: bool) -> List[SamplerOutput]:\n        \"\"\"Run a prefill step, without any speculation. The input is sent to\n        the proposer and scorer model so that the KV cache is consistent\n        between the two. When skip_proposer is True, the proposer model is\n        not called, meaning that the kv-cache in proposer for requests is not\n        updated, so they cannot enable spec decode in the rest decoding.\n        \"\"\"\n        if not skip_proposer:\n            self.proposer_worker.execute_model(execute_model_req)\n\n        sampler_output = self.scorer_worker.execute_model(execute_model_req)\n        assert len(sampler_output) == 1\n        sampler_output = sampler_output[0]\n\n        # Clear device tensors from sampler output. This reduces communication\n        # overhead when the engine runs in a different process than the workers.\n        sampler_output.probs = None\n        sampler_output.sampled_tokens = None\n        sampler_output.logprobs = None\n        return [sampler_output]\n\n    def _run_non_driver_rank(self, num_lookahead_slots: int) -> None:\n        \"\"\"Run proposer and verifier model in non-driver workers. This is used\n        for both speculation cases (num_lookahead_slots>0) and non-speculation\n        cases (e.g. prefill).\n        \"\"\"\n        # In non-driver workers the input is None\n        execute_model_req = None\n\n        # Even if num_lookahead_slots is zero, we want to run the proposer model\n        # as it may have KV.\n        #\n        # We run the proposer once per lookahead slot. In the future we should\n        # delegate how many times it runs to the proposer.\n        for _ in range(max(num_lookahead_slots, 1)):\n            self.proposer_worker.execute_model(execute_model_req)\n\n        self.scorer_worker.execute_model(execute_model_req)\n\n    @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n    def _run_speculative_decoding_step(\n            self, execute_model_req: ExecuteModelRequest,\n            num_lookahead_slots: int) -> List[SamplerOutput]:\n        \"\"\"Execute a single step of speculative decoding.\n\n        This invokes the proposer worker to get k speculative tokens for each\n        sequence, then scores each speculative token using the scoring worker.\n\n        Returns a list of SamplerOutput, each containing a single token per\n        sequence.\n        \"\"\"\n        assert num_lookahead_slots == execute_model_req.num_lookahead_slots\n\n        # Generate proposals using draft worker.\n        proposals = self.proposer_worker.get_spec_proposals(execute_model_req)\n\n        proposal_scores = self.scorer.score_proposals(\n            execute_model_req,\n            proposals,\n        )\n\n        accepted_token_ids, target_logprobs = self._verify_tokens(\n            execute_model_req.seq_group_metadata_list, proposal_scores,\n            proposals, execute_model_req.num_lookahead_slots)\n\n        return self._create_output_sampler_list(\n            execute_model_req.seq_group_metadata_list,\n            accepted_token_ids,\n            target_logprobs=target_logprobs,\n            k=execute_model_req.num_lookahead_slots)\n\n    @nvtx_range(\"spec_decode_worker._verify_tokens\")\n    def _verify_tokens(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        proposal_scores: SpeculativeScores,\n        proposals: SpeculativeProposals,\n        max_proposal_len: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Determine which speculative tokens are accepted using the\n        probabilities of each token according to the proposer and scorer models.\n\n        Returns a tuple of Tensors, one for the accepted token ids and one for\n        the logprobs according to the scoring model.\n        \"\"\"\n        proposal_lens_list = proposals.proposal_lens.tolist()\n\n        # vLLM currently only supports proposal lens equal to zero or the batch\n        # proposal len. This adds some complexity (splitting the batch into spec\n        # and non spec sequences) and should be removed in the future. It can be\n        # done by supporting per-sequence proposal lens.\n        _, spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=False)\n        _, non_spec_indices = split_batch_by_proposal_len(\n            seq_group_metadata_list,\n            proposal_lens_list,\n            select_proposal_len_zero=True)\n        original_indices = spec_indices + non_spec_indices\n\n        # Get probabilities of target model, excluding bonus token.\n        proposal_verifier_probs = proposal_scores.probs[spec_indices, :-1]\n\n        # Get non-speculative sampled tokens from target model.\n        non_spec_token_ids = proposal_scores.token_ids[non_spec_indices]\n\n        # Get bonus tokens from target model.\n        bonus_token_ids = proposal_scores.token_ids[spec_indices, -1:]\n\n        # Get probabilities according to proposal method.\n        proposal_probs = proposals.proposal_probs[spec_indices]\n\n        # Get proposed tokens.\n        proposal_token_ids = proposals.proposal_token_ids[spec_indices]\n\n        accepted_token_ids = self.rejection_sampler(\n            target_probs=proposal_verifier_probs,\n            bonus_token_ids=bonus_token_ids,\n            draft_probs=proposal_probs,\n            draft_token_ids=proposal_token_ids,\n        )\n\n        # Append output tokens from non-speculative sequences to\n        # the accepted token ids tensor.\n        non_spec_token_ids = non_spec_token_ids.expand(-1, max_proposal_len +\n                                                       1).clone()\n        non_spec_token_ids[:, 1:] = -1\n        accepted_token_ids = torch.cat(\n            [accepted_token_ids, non_spec_token_ids])\n        logprobs = proposal_scores.logprobs\n\n        # Rearrange so that results are in the order of the original seq group\n        # metadata.\n        accepted_token_ids[original_indices] = accepted_token_ids.clone()\n\n        return accepted_token_ids, logprobs\n\n    def _create_output_sampler_list(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        accepted_token_ids: torch.Tensor,  # shape: [batch_size, k+1]\n        target_logprobs: torch.Tensor,  # shape: [batch_size, k+1, vocab_size]\n        k: int,\n    ) -> List[SamplerOutput]:\n        \"\"\"Given the accepted token ids, create a list of SamplerOutput.\n\n        The output is padded with -1 tokens such that each sequence has\n        the same number of outputs.\n        \"\"\"\n        batch_size, num_steps = accepted_token_ids.shape\n\n        # Organize input tensors by step instead of by sequence.\n        target_logprobs_by_step = target_logprobs.transpose(0, 1)\n        accepted_token_ids_by_step = accepted_token_ids.transpose(0, 1)\n\n        # Get the logprobs/rank of the accepted tokens.\n        (accepted_token_id_ranks_by_step,\n         accepted_token_id_logprobs_by_step) = get_sampled_token_logprobs(\n             logprob_tensor=target_logprobs_by_step,\n             sampled_token_ids=accepted_token_ids_by_step,\n         )\n\n        # Get the top-k logprobs (which may or may not include the logprob of\n        # the accepted token).\n        (topk_logprobs_by_step,\n         topk_indices_by_step) = target_logprobs_by_step.topk(\n             k=self.scorer_worker.model_config.max_logprobs,\n             dim=-1,\n         )\n\n        # Get the sequence ids and num_logprobs (sampling parameter) in the\n        # batch.\n        seq_ids = get_all_seq_ids(seq_group_metadata_list)\n        num_logprobs_per_seq = get_all_num_logprobs(seq_group_metadata_list)\n\n        # Serialize all tensors to CPU Python lists.\n        accepted_token_ids_by_step = accepted_token_ids_by_step.tolist()\n        accepted_token_id_ranks_by_step = (\n            accepted_token_id_ranks_by_step.tolist())\n        accepted_token_id_logprobs_by_step = (\n            accepted_token_id_logprobs_by_step.tolist())\n        topk_logprobs_by_step = topk_logprobs_by_step.tolist()\n        topk_indices_by_step = topk_indices_by_step.tolist()\n\n        # Construct the output on a per-step, per-sequence basis.\n        sampler_output_list = []\n        for step_index in range(num_steps):\n            if all(token_id == -1\n                   for token_id in accepted_token_ids_by_step[step_index]):\n                break\n\n            step_output_token_ids = []\n            for sequence_index in range(batch_size):\n                # Each sequence may have a different num_logprobs; retrieve it.\n                num_logprobs = num_logprobs_per_seq[sequence_index]\n\n                step_output_token_ids.append(\n                    create_sequence_group_output(\n                        token_id=accepted_token_ids_by_step[step_index]\n                        [sequence_index],\n                        token_id_logprob_rank=accepted_token_id_ranks_by_step[\n                            step_index][sequence_index],\n                        token_id_logprob=accepted_token_id_logprobs_by_step[\n                            step_index][sequence_index],\n                        seq_id=seq_ids[sequence_index],\n                        topk_token_ids=topk_indices_by_step[step_index]\n                        [sequence_index][:num_logprobs],\n                        topk_logprobs=topk_logprobs_by_step[step_index]\n                        [sequence_index][:num_logprobs],\n                    ))\n\n            sampler_output_list.append(\n                SamplerOutput(outputs=step_output_token_ids))\n\n        maybe_rejsample_metrics = (\n            self._metrics.maybe_collect_rejsample_metrics(k))\n        if maybe_rejsample_metrics is not None:\n            sampler_output_list[\n                0].spec_decode_worker_metrics = maybe_rejsample_metrics\n\n        return sampler_output_list\n\n    @cached_property\n    def _vocab_size(self) -> int:\n        \"\"\"Get the vocab size of the model and make sure it's consistent between\n        draft and target workers.\n        \"\"\"\n        vocab_sizes = [\n            worker.vocab_size\n            for worker in [self.proposer_worker, self.scorer_worker]\n        ]\n        assert all(vocab_sizes[0] == vocab_size for vocab_size in vocab_sizes)\n        return vocab_sizes[0]\n\n    @property\n    def rank(self):\n        return self.scorer_worker.rank\n\n    @property\n    def device(self):\n        return self.scorer_worker.device\n\n    @property\n    def _driver_rank(self) -> int:\n        return 0\n\n    def get_cache_block_size_bytes(self):\n        \"\"\"Return the size of a cache block in bytes.\n        \n        This function is only used to compose workers within a SpecDecodeWorker.\n        We leave composing a SpecDecodeWorker within a SpecDecodeWorker\n        undefined for now, although it could be implemented in the future.\n        See https://arxiv.org/abs/2308.04623.\n        \"\"\"\n        raise NotImplementedError\n\n\ndef split_num_cache_blocks_evenly(scorer_cache_block_size_bytes: int,\n                                  proposer_cache_block_size_bytes: int,\n                                  total_num_gpu_blocks: int) -> int:\n    \"\"\"Given total_num_gpu_blocks, the number of GPU blocks that could be\n    allocate to the target model, this function calculates how many blocks\n    should be given to the draft and target model.\n\n    Note that usually the block size, in bytes, of each model is different,\n    as it's a function of number of KV/layer, number of heads, and hidden\n    dimension size.\n\n    Since the target and draft models allocate the same number of blocks, we\n    simply calculate the number of blocks where if allocated by both models,\n    the total memory usage from KV cache is no larger than the number of\n    blocks allocatable by the target model alone.\n    \"\"\"\n    new_num_gpu_blocks = int(\n        total_num_gpu_blocks * scorer_cache_block_size_bytes /\n        (proposer_cache_block_size_bytes + scorer_cache_block_size_bytes))\n\n    return new_num_gpu_blocks\n",
      "diff": "diff --git a/vllm/spec_decode/spec_decode_worker.py b/vllm/spec_decode/spec_decode_worker.py\nindex ef17b8c1e..3462a876c 100644\n--- a/vllm/spec_decode/spec_decode_worker.py\n+++ b/vllm/spec_decode/spec_decode_worker.py\n@@ -231,35 +231,6 @@ class SpecDecodeWorker(LoraNotSupportedWorkerBase):\n         self.proposer_worker.initialize_cache(num_gpu_blocks=num_gpu_blocks,\n                                               num_cpu_blocks=num_cpu_blocks)\n \n-    def _broadcast_control_flow_decision(\n-            self,\n-            execute_model_req: Optional[ExecuteModelRequest] = None,\n-            disable_all_speculation: bool = False) -> Tuple[int, bool]:\n-        \"\"\"Broadcast how many lookahead slots are scheduled for this step, and\n-        whether all speculation is disabled, to all non-driver workers.\n-\n-        This is required as if the number of draft model runs changes\n-        dynamically, the non-driver workers won't know unless we perform a\n-        communication to inform then.\n-\n-        Returns the broadcasted num_lookahead_slots and disable_all_speculation.\n-        \"\"\"\n-\n-        if self.rank == self._driver_rank:\n-            assert execute_model_req is not None\n-\n-            broadcast_dict = dict(\n-                num_lookahead_slots=execute_model_req.num_lookahead_slots,\n-                disable_all_speculation=disable_all_speculation,\n-            )\n-            broadcast_tensor_dict(broadcast_dict, src=self._driver_rank)\n-        else:\n-            assert execute_model_req is None\n-            broadcast_dict = broadcast_tensor_dict(src=self._driver_rank)\n-\n-        return (broadcast_dict[\"num_lookahead_slots\"],\n-                broadcast_dict[\"disable_all_speculation\"])\n-\n     @torch.inference_mode()\n     def execute_model(\n         self,\n@@ -267,39 +238,58 @@ class SpecDecodeWorker(LoraNotSupportedWorkerBase):\n     ) -> List[SamplerOutput]:\n         \"\"\"Perform speculative decoding on the input batch.\n         \"\"\"\n+        if self.rank != self._driver_rank:\n+            self._run_non_driver_rank()\n+            return []\n \n-        disable_all_speculation = False\n-        if self.rank == self._driver_rank:\n-            disable_all_speculation = self._should_disable_all_speculation(\n-                execute_model_req)\n-\n-        (num_lookahead_slots,\n-         disable_all_speculation) = self._broadcast_control_flow_decision(\n-             execute_model_req, disable_all_speculation)\n-\n-        if self.rank == self._driver_rank:\n-            assert execute_model_req is not None\n-            assert execute_model_req.seq_group_metadata_list is not None, (\n-                \"speculative decoding requires non-None seq_group_metadata_list\"\n-            )\n-\n-            self._maybe_disable_speculative_tokens(\n-                disable_all_speculation,\n-                execute_model_req.seq_group_metadata_list)\n-\n-            # If no spec tokens, call the proposer and scorer workers normally.\n-            # Used for prefill.\n-            if num_lookahead_slots == 0 or len(\n-                    execute_model_req.seq_group_metadata_list) == 0:\n-                return self._run_no_spec(execute_model_req,\n-                                         skip_proposer=disable_all_speculation)\n-\n-            return self._run_speculative_decoding_step(execute_model_req,\n-                                                       num_lookahead_slots)\n-        else:\n-            self._run_non_driver_rank(num_lookahead_slots)\n+        if execute_model_req is None:\n+            # This signals that there's no more requests to process for now.\n+            # All workers are running infinite loop with broadcast_tensor_dict,\n+            # and it stops the loop when the driver broadcasts an empty input.\n+            # Send an empty input to notify all other workers to stop their\n+            # execution loop.\n+            broadcast_tensor_dict({}, src=0)\n             return []\n \n+        disable_all_speculation = self._should_disable_all_speculation(\n+            execute_model_req)\n+        num_lookahead_slots = execute_model_req.num_lookahead_slots\n+\n+        # Broadcast how many lookahead slots are scheduled for this step, and\n+        # whether all speculation is disabled, to all non-driver workers.\n+\n+        # This is required as if the number of draft model runs changes\n+        # dynamically, the non-driver workers won't know unless we perform a\n+        # communication to inform then.\n+        broadcast_dict = dict(\n+            num_lookahead_slots=num_lookahead_slots,\n+            disable_all_speculation=disable_all_speculation,\n+        )\n+        broadcast_tensor_dict(broadcast_dict, src=self._driver_rank)\n+\n+        assert execute_model_req.seq_group_metadata_list is not None, (\n+            \"speculative decoding requires non-None seq_group_metadata_list\")\n+\n+        self._maybe_disable_speculative_tokens(\n+            disable_all_speculation, execute_model_req.seq_group_metadata_list)\n+\n+        # If no spec tokens, call the proposer and scorer workers normally.\n+        # Used for prefill.\n+        if num_lookahead_slots == 0 or len(\n+                execute_model_req.seq_group_metadata_list) == 0:\n+            return self._run_no_spec(execute_model_req,\n+                                     skip_proposer=disable_all_speculation)\n+\n+        return self._run_speculative_decoding_step(execute_model_req,\n+                                                   num_lookahead_slots)\n+\n+    @torch.inference_mode()\n+    def start_worker_execution_loop(self) -> None:\n+        \"\"\"Execute model loop to perform speculative decoding\n+        in parallel worker.\"\"\"\n+        while self._run_non_driver_rank():\n+            pass\n+\n     def _should_disable_all_speculation(\n             self, execute_model_req: ExecuteModelRequest) -> bool:\n         # When the batch size is too large, disable speculative decoding\n@@ -346,13 +336,19 @@ class SpecDecodeWorker(LoraNotSupportedWorkerBase):\n         sampler_output.logprobs = None\n         return [sampler_output]\n \n-    def _run_non_driver_rank(self, num_lookahead_slots: int) -> None:\n+    def _run_non_driver_rank(self) -> bool:\n         \"\"\"Run proposer and verifier model in non-driver workers. This is used\n         for both speculation cases (num_lookahead_slots>0) and non-speculation\n         cases (e.g. prefill).\n+\n+        Returns True iff there are remaining sequences to process.\n         \"\"\"\n-        # In non-driver workers the input is None\n-        execute_model_req = None\n+        assert self.rank != self._driver_rank\n+\n+        data = broadcast_tensor_dict(src=self._driver_rank)\n+        if not data:\n+            return False\n+        num_lookahead_slots = data[\"num_lookahead_slots\"]\n \n         # Even if num_lookahead_slots is zero, we want to run the proposer model\n         # as it may have KV.\n@@ -360,9 +356,10 @@ class SpecDecodeWorker(LoraNotSupportedWorkerBase):\n         # We run the proposer once per lookahead slot. In the future we should\n         # delegate how many times it runs to the proposer.\n         for _ in range(max(num_lookahead_slots, 1)):\n-            self.proposer_worker.execute_model(execute_model_req)\n+            self.proposer_worker.execute_model()\n \n-        self.scorer_worker.execute_model(execute_model_req)\n+        self.scorer_worker.execute_model()\n+        return True\n \n     @nvtx_range(\"spec_decode_worker._run_speculative_decoding_step\")\n     def _run_speculative_decoding_step(",
      "change_type": "modified",
      "lines_added": 62,
      "lines_removed": 65
    },
    {
      "file_path": "vllm/worker/embedding_model_runner.py",
      "old_content": "from typing import Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.attention import AttentionMetadata\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ParallelConfig, SchedulerConfig,\n                         VisionLanguageConfig)\nfrom vllm.distributed import broadcast_tensor_dict\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.pooling_metadata import PoolingMetadata\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.sequence import PoolerOutput, SequenceData, SequenceGroupMetadata\nfrom vllm.worker.model_runner import ModelRunner\n\nlogger = init_logger(__name__)\n\n\nclass EmbeddingModelRunner(ModelRunner):\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        vision_language_config: Optional[VisionLanguageConfig] = None,\n    ):\n        super().__init__(model_config,\n                         parallel_config,\n                         scheduler_config,\n                         device_config,\n                         cache_config,\n                         load_config,\n                         lora_config=lora_config,\n                         kv_cache_dtype=kv_cache_dtype,\n                         is_driver_worker=is_driver_worker,\n                         vision_language_config=vision_language_config)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        kv_caches: List[torch.Tensor],\n    ) -> Optional[PoolerOutput]:\n        (input_tokens, input_positions, attn_metadata, pooling_metadata,\n         lora_requests, lora_mapping, multi_modal_input\n         ) = self.prepare_input_tensors(seq_group_metadata_list)\n\n        if self.lora_config:\n            self.set_active_loras(lora_requests, lora_mapping)\n\n        # Currently cuda graph is only supported by the decode phase.\n        prefill_meta = attn_metadata.prefill_metadata\n        decode_meta = attn_metadata.decode_metadata\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            graph_batch_size = input_tokens.shape[0]\n            model_executable = self.graph_runners[graph_batch_size]\n        else:\n            model_executable = self.model\n\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n\n        execute_model_kwargs = {\n            \"input_ids\": input_tokens,\n            \"positions\": input_positions,\n            \"kv_caches\": kv_caches,\n            \"attn_metadata\": attn_metadata,\n        }\n        if self.vision_language_config:\n            execute_model_kwargs.update({\"image_input\": multi_modal_input})\n        hidden_states = model_executable(**execute_model_kwargs)\n\n        return self.model.pooler(hidden_states=hidden_states,\n                                 pooling_metadata=pooling_metadata)\n\n    def prepare_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, PoolingMetadata,\n               Set[LoRARequest], LoRAMapping, torch.Tensor]:\n        if self.is_driver_worker:\n            # Prepare input tensors.\n            (\n                input_tokens,\n                input_positions,\n                attn_metadata,\n                seq_lens,\n                _,\n                lora_mapping,\n                lora_requests,\n                multi_modal_input,\n                slot_mapping,\n                num_prefill_tokens,\n                num_decode_tokens,\n                num_prefills,\n            ) = self._prepare_model_input(seq_group_metadata_list)\n            # Prepare PoolingMetadata\n            pooling_metadata = self._prepare_pooling(seq_group_metadata_list,\n                                                     seq_lens)\n\n            metadata_dict = {\n                \"input_tokens\": input_tokens,\n                \"input_positions\": input_positions,\n                \"lora_requests\": lora_requests,\n                \"lora_mapping\": lora_mapping,\n                \"multi_modal_input\": multi_modal_input,\n                \"num_prefill_tokens\": num_prefill_tokens,\n                \"num_decode_tokens\": num_decode_tokens,\n                \"slot_mapping\": slot_mapping,\n                \"num_prefills\": num_prefills,\n            }\n            if attn_metadata:\n                metadata_dict.update(attn_metadata.asdict_zerocopy())\n            broadcast_tensor_dict(metadata_dict, src=0)\n        else:\n            metadata_dict = broadcast_tensor_dict(src=0)\n            input_tokens = metadata_dict.pop(\"input_tokens\")\n            input_positions = metadata_dict.pop(\"input_positions\")\n            lora_mapping = metadata_dict.pop(\"lora_mapping\")\n            lora_requests = metadata_dict.pop(\"lora_requests\")\n            multi_modal_input = metadata_dict.pop(\"multi_modal_input\")\n            if metadata_dict:\n                attn_metadata = self.attn_backend.make_metadata(\n                    **metadata_dict)\n            else:\n                attn_metadata = None\n            pooling_metadata = PoolingMetadata(seq_groups=None,\n                                               seq_data=None,\n                                               prompt_lens=None)\n\n        return (input_tokens, input_positions, attn_metadata, pooling_metadata,\n                lora_requests, lora_mapping, multi_modal_input)\n\n    def _prepare_pooling(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        prompt_lens: List[int],\n    ) -> PoolingMetadata:\n        \"\"\"Prepare PoolingMetadata for the sequence group metadata list.\"\"\"\n        seq_groups: List[Tuple[List[int], PoolingParams]] = []\n        for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            pooling_params = seq_group_metadata.pooling_params\n            seq_groups.append((seq_ids, pooling_params))\n\n        seq_data: Dict[int, SequenceData] = {}\n        for seq_group_metadata in seq_group_metadata_list:\n            seq_data.update(seq_group_metadata.seq_data)\n\n        pooling_metadata = PoolingMetadata(\n            seq_groups=seq_groups,\n            seq_data=seq_data,\n            prompt_lens=prompt_lens,\n        )\n\n        return pooling_metadata\n",
      "diff": "diff --git a/vllm/worker/embedding_model_runner.py b/vllm/worker/embedding_model_runner.py\nindex 91f30978e..ef02de95f 100644\n--- a/vllm/worker/embedding_model_runner.py\n+++ b/vllm/worker/embedding_model_runner.py\n@@ -47,7 +47,7 @@ class EmbeddingModelRunner(ModelRunner):\n     @torch.inference_mode()\n     def execute_model(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n         kv_caches: List[torch.Tensor],\n     ) -> Optional[PoolerOutput]:\n         (input_tokens, input_positions, attn_metadata, pooling_metadata,\n@@ -84,10 +84,11 @@ class EmbeddingModelRunner(ModelRunner):\n \n     def prepare_input_tensors(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n     ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, PoolingMetadata,\n                Set[LoRARequest], LoRAMapping, torch.Tensor]:\n         if self.is_driver_worker:\n+            assert seq_group_metadata_list is not None\n             # Prepare input tensors.\n             (\n                 input_tokens,",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import time\nimport warnings\nfrom typing import Dict, List, NamedTuple, Optional, Set, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ParallelConfig, SchedulerConfig,\n                         VisionLanguageConfig)\nfrom vllm.distributed import broadcast_tensor_dict\nfrom vllm.distributed.communication_op import graph_capture\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (MultiModalData, SamplerOutput, SequenceData,\n                           SequenceGroupMetadata)\nfrom vllm.utils import (CudaMemoryProfiler, get_kv_cache_torch_dtype, is_hip,\n                        is_pin_memory_available, make_tensor_with_pad)\n\nlogger = init_logger(__name__)\n\n_PAD_SLOT_ID = -1\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# Capture graphs for token size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)\n]\n\n\nclass ModelInput(NamedTuple):\n    input_tokens: torch.Tensor\n    input_positions: torch.Tensor\n    attn_metadata: Optional[AttentionMetadata]\n    seq_lens: List[int]\n    query_lens: List[int]\n    lora_mapping: Optional[LoRAMapping]\n    lora_requests: Set[LoRARequest]\n    multi_modal_input: Optional[torch.Tensor]\n    slot_mapping: torch.Tensor\n    num_prefill_tokens: int\n    num_decode_tokens: int\n    num_prefills: int\n\n    @classmethod\n    def empty(cls, device):\n        return ModelInput(\n            input_tokens=torch.empty(0, device=device),\n            input_positions=torch.empty(0, device=device),\n            attn_metadata=None,\n            seq_lens=[],\n            query_lens=[],\n            lora_mapping=None,\n            lora_requests=set(),\n            multi_modal_input=None,\n            slot_mapping=torch.empty(0, device=device),\n            num_prefill_tokens=0,\n            num_decode_tokens=0,\n            num_prefills=0,\n        )\n\n\nclass ModelRunner:\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        vision_language_config: Optional[VisionLanguageConfig] = None,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        self.vision_language_config = vision_language_config\n\n        self.device = self.device_config.device\n        self.pin_memory = is_pin_memory_available()\n\n        self.kv_cache_dtype = kv_cache_dtype\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_seq_len_to_capture = self.model_config.max_seq_len_to_capture\n        self.graph_runners: Dict[int, CUDAGraphRunner] = {}\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = np.zeros(\n            (max(_BATCH_SIZES_TO_CAPTURE), self.get_max_block_per_batch()),\n            dtype=np.int32)\n        self.attn_backend = get_attn_backend(\n            self.model_config.get_num_attention_heads(self.parallel_config),\n            self.model_config.get_head_size(),\n            self.model_config.get_num_kv_heads(self.parallel_config),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.kv_cache_dtype,\n            self.block_size,\n        )\n\n        # Lazy initialization\n        self.model: nn.Module  # Set after load_model\n        # Set if the backend is flashinfer.\n        self.flashinfer_workspace_buffer: torch.Tensor\n        # Set after load_model.\n        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None\n\n    def load_model(self) -> None:\n        with CudaMemoryProfiler() as m:\n            self.model = get_model(\n                model_config=self.model_config,\n                device_config=self.device_config,\n                load_config=self.load_config,\n                lora_config=self.lora_config,\n                vision_language_config=self.vision_language_config,\n                parallel_config=self.parallel_config,\n                scheduler_config=self.scheduler_config,\n                cache_config=self.cache_config,\n            )\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert hasattr(self.model, \"supported_lora_modules\"\n                           ) and self.model.supported_lora_modules, (\n                               \"Model does not support LoRA\")\n            assert hasattr(\n                self.model,\n                \"embedding_modules\"), \"Model does not have embedding_modules\"\n            assert hasattr(self.model, \"embedding_padding_modules\"\n                           ), \"Model does not have embedding_padding_modules\"\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens,\n                self.vocab_size,\n                self.lora_config,\n                self.device,\n                self.model.embedding_modules,\n                self.model.embedding_padding_modules,\n                max_position_embeddings=self.model.config.\n                max_position_embeddings,\n            )\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently only ROCm accepts kv-cache scaling factors\n            # via quantization_param_path and this will be deprecated\n            # in the future.\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    warnings.warn(\n                        \"Loading kv cache scaling factor from JSON is \"\n                        \"deprecated and will be removed. Please include \"\n                        \"kv cache scaling factors in the model checkpoint.\",\n                        FutureWarning,\n                        stacklevel=2)\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                    logger.info(\"Loaded KV cache scaling factors from %s\",\n                                self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import ShardedStateLoader\n        ShardedStateLoader.save_model(\n            self.model,\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> ModelInput:\n        \"\"\"Prepare the model input based on a given sequence group.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        input_tokens: List[int] = []\n        input_positions: List[int] = []\n        slot_mapping: List[int] = []\n        lora_index_mapping: List[int] = []\n        lora_prompt_mapping: List[int] = []\n        lora_requests: Set[LoRARequest] = set()\n\n        seq_lens: List[int] = []\n        prefill_seq_lens: List[int] = []\n        decode_seq_lens: List[int] = []\n        context_lens: List[int] = []\n        query_lens: List[int] = []\n        block_tables: List[List[int]] = []\n        multi_modal_input_list: List[torch.Tensor] = []\n        decode_only = True\n        num_prefills = 0\n        num_prefill_tokens = 0\n        num_decode_tokens = 0\n\n        # The following fields are only for flashinfer\n        # Please follow https://docs.flashinfer.ai/tutorials/kv_layout.html#page-layout\n        # for the precise definition of the following fields.\n        # An example:\n        # request 1, page indices [0, 5, 8]\n        # request 2, page indices [1, 6, 7]\n        # request 3, page indices [3, 4]\n        # paged_kv_indices is a concatenation of page indices of all requests:\n        # [0, 5, 8, 1, 6, 7, 3, 4]\n        # paged_kv_indptr is used to index into paged_kv_indices:\n        # [0, 3, 6, 8]\n        paged_kv_indices: List[int] = []\n        # 0 at the beginning of paged_kv_indptr indicates the start of the\n        # first requests page indices in the paged_kv_indices list.\n        paged_kv_indptr: List[int] = [0]\n        # paged_kv_last_page_len is the length of the last page of each request\n        paged_kv_last_page_len: List[int] = []\n\n        if len(seq_group_metadata_list) == 0:\n            return ModelInput.empty(self.device)\n\n        for seq_group_metadata in seq_group_metadata_list:\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            is_prompt = seq_group_metadata.is_prompt\n\n            for seq_id in seq_ids:\n                computed_block_nums = seq_group_metadata.computed_block_nums\n                if (self.scheduler_config is not None\n                        and self.scheduler_config.chunked_prefill_enabled\n                        and not (computed_block_nums is None\n                                 or computed_block_nums == [])):\n                    raise RuntimeError(\n                        \"chunked prefill cannot be used with prefix caching \"\n                        \"now.\")\n\n                seq_data = seq_group_metadata.seq_data[seq_id]\n                if is_prompt:\n                    context_len = seq_data.get_num_computed_tokens()\n                else:\n                    # get_num_computed_tokens is incorrect for spec decoding.\n                    # So, we should have a special logic here.\n                    # TODO(sang): Fix it.\n                    context_len = seq_data.get_len() - 1\n\n                seq_len = min(\n                    seq_data.get_len(),\n                    context_len + seq_group_metadata.token_chunk_size)\n                if is_prompt:\n                    tokens = seq_data.get_token_ids()[context_len:seq_len]\n                else:\n                    # Optimization. get_token_ids requires the entire copy of\n                    # tokens.\n                    tokens = [seq_data.get_last_token_id()]\n\n                # Prefix cache was hit.\n                # Prefix is not supported with sliding_window\n                prefix_cache_hit = (computed_block_nums is not None\n                                    and len(computed_block_nums) > 0\n                                    and self.sliding_window is None\n                                    and is_prompt)\n\n                # TODO(sang): Combine chunked prefill and prefix caching by\n                # only allowing multiple of block_size chunk size.\n                # NOTE: This only works for oooooooxxx style attention.\n                if prefix_cache_hit:\n                    assert computed_block_nums is not None\n                    context_len = len(computed_block_nums) * self.block_size\n                    tokens = tokens[context_len:]\n                    if self.attn_backend.get_name() == \"flash-attn\":\n                        # NOTE(woosuk): For flash-attn, the block table should\n                        # include the entries for the incoming prefill tokens.\n                        # TODO(woosuk): This is a temporary fix. We should\n                        # provide a unified interface for different backends.\n                        block_table = seq_group_metadata.block_tables[seq_id]\n                    else:\n                        block_table = computed_block_nums\n                elif (self.scheduler_config.chunked_prefill_enabled\n                      or not is_prompt):\n                    if seq_group_metadata.block_tables is not None:\n                        # chunked prefill or decode\n                        block_table = seq_group_metadata.block_tables[seq_id]\n                        if self.sliding_window is not None:\n                            # chunked prefill doesn't support sliding window.\n                            assert (not self.scheduler_config.\n                                    chunked_prefill_enabled)\n                            sliding_window_blocks = (self.sliding_window //\n                                                     self.block_size)\n                            block_table = block_table[-sliding_window_blocks:]\n\n                        if self.attn_backend.get_name() == \"flashinfer\":\n                            paged_kv_indices.extend(block_table)\n                            paged_kv_indptr.append(paged_kv_indptr[-1] +\n                                                   len(block_table))\n                            last_page_len = seq_data.get_len(\n                            ) % self.block_size\n                            if last_page_len == 0:\n                                last_page_len = self.block_size\n                            paged_kv_last_page_len.append(last_page_len)\n                    else:\n                        # Only happens when memory profiling runs.\n                        block_table = []\n                else:\n                    # Prefill without chunked prefill or memory profiling.\n                    block_table = []\n                block_tables.append(block_table)\n\n                # TODO(sang): This is a hack to make sliding window work with\n                # paged attn. We can remove it if we make paged attn kernel\n                # to properly handle slinding window attn.\n                if (self.sliding_window is not None and not is_prompt):\n                    seq_len = min(seq_len, self.sliding_window)\n                    context_len = seq_len - 1\n\n                seq_lens.append(seq_len)\n                context_lens.append(context_len)\n                query_len = seq_len - context_len\n                query_lens.append(query_len)\n                input_tokens.extend(tokens)\n                input_positions.extend(list(range(context_len, seq_len)))\n                lora_id = seq_group_metadata.lora_int_id\n\n                if is_prompt:\n                    assert len(seq_ids) == 1\n                    num_prefills += 1\n                    num_prefill_tokens += len(tokens)\n                    decode_only = False\n                    prefill_seq_lens.append(seq_len)\n                else:\n                    assert query_len == 1, (\n                        \"seq_len: {}, context_len: {}, query_len: {}\".format(\n                            seq_len, context_len, query_len))\n                    num_decode_tokens += query_len\n                    decode_seq_lens.append(seq_len)\n\n                if lora_id > 0:\n                    lora_requests.add(seq_group_metadata.lora_request)\n\n                lora_index_mapping += [lora_id] * (seq_len - context_len)\n                lora_prompt_mapping.extend(\n                    [lora_id] *\n                    (seq_len -\n                     context_len if seq_group_metadata.sampling_params\n                     and seq_group_metadata.sampling_params.prompt_logprobs\n                     else 1))\n\n                if seq_group_metadata.multi_modal_data:\n                    multi_modal_input_list.append(\n                        seq_group_metadata.multi_modal_data.data)\n\n                if _is_block_tables_empty(seq_group_metadata.block_tables):\n                    # During memory profiling, the block tables are not\n                    # initialized yet. In this case, we just use a dummy\n                    # slot mapping.\n                    # In embeddings, the block tables are {seq_id: None}.\n                    slot_mapping.extend([_PAD_SLOT_ID] * seq_len)\n                    continue\n\n                # Compute the slot mapping.\n                block_table = seq_group_metadata.block_tables[seq_id]\n\n                # Mask the [0, start_idx) tokens of the prompt with\n                # _PAD_SLOT_ID, where start_idx is max(0, seq_len -\n                # sliding_window). For example, if the prompt len is 10,\n                # sliding window is 8, and block size is 4, the first two\n                # tokens are masked and the slot mapping will be\n                # [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n                start_idx = 0\n                if self.sliding_window is not None:\n                    if is_prompt:\n                        assert context_len == 0, (\n                            \"Prefix caching is currently not supported with \"\n                            \"sliding window attention\")\n                    # It is an optimization. When it is decoding, it is always\n                    # 0. When prefill, we use it to not write slots to kv cache\n                    # to save memory.\n                    start_idx = max(0, query_len - self.sliding_window)\n\n                for i in range(context_len, seq_len):\n                    if i < start_idx:\n                        slot_mapping.append(_PAD_SLOT_ID)\n                        continue\n\n                    block_number = block_table[i // self.block_size]\n                    block_offset = i % self.block_size\n                    slot = block_number * self.block_size + block_offset\n                    slot_mapping.append(slot)\n\n        batch_size = len(input_tokens)\n        max_query_len = max(query_lens)\n        max_prefill_seq_len = max(prefill_seq_lens, default=0)\n        max_decode_seq_len = max(decode_seq_lens, default=0)\n\n        # If cuda graph can be used, pad tensors accordingly.\n        # See `capture_model` API for more details.\n        # vLLM uses cuda graph only for decoding requests.\n        use_captured_graph = (\n            decode_only and not self.model_config.enforce_eager\n            and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n            and max_decode_seq_len <= self.max_seq_len_to_capture)\n        if use_captured_graph:\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            for _ in range(graph_batch_size - batch_size):\n                input_tokens.append(0)\n                input_positions.append(0)\n                slot_mapping.append(_PAD_SLOT_ID)\n                seq_lens.append(1)\n                block_tables.append([])\n                lora_index_mapping.append(0)\n            batch_size = graph_batch_size\n            num_decode_tokens = batch_size\n\n        if use_captured_graph:\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.tensor(input_block_tables, device=self.device)\n        else:\n            max_block_table_len = max(\n                len(block_table) for block_table in block_tables)\n            block_tables = make_tensor_with_pad(\n                block_tables,\n                max_len=max_block_table_len,\n                pad=0,\n                dtype=torch.int,\n                device=self.device,\n            )\n        assert max_query_len > 0, (\"query_lens: {}\".format(query_lens))\n\n        context_lens_tensor = torch.tensor(context_lens,\n                                           dtype=torch.int,\n                                           device=self.device)\n\n        if multi_modal_input_list:\n            assert self.vision_language_config, (\n                \"Multi-modal inputs are only supported by \"\n                \"vision language models.\")\n            multi_modal_input = torch.cat(multi_modal_input_list,\n                                          dim=0).to(self.device)\n        else:\n            multi_modal_input = None\n\n        seq_lens_tensor = torch.tensor(seq_lens,\n                                       dtype=torch.int,\n                                       device=self.device)\n        query_lens_tensor = torch.tensor(query_lens,\n                                         dtype=torch.long,\n                                         device=self.device)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=self.device)\n\n        seq_lens_tensor = torch.tensor(seq_lens,\n                                       dtype=torch.int,\n                                       device=self.device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=self.device)\n\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n\n        input_tokens_tensor = torch.tensor(input_tokens,\n                                           dtype=torch.long,\n                                           device=self.device)\n        input_positions_tensor = torch.tensor(input_positions,\n                                              dtype=torch.long,\n                                              device=self.device)\n        slot_mapping_tensor = torch.tensor(slot_mapping,\n                                           dtype=torch.long,\n                                           device=self.device)\n\n        if self.attn_backend.get_name() == \"flashinfer\":\n            if not hasattr(self, \"flashinfer_workspace_buffer\"):\n                # Allocate 16MB workspace buffer\n                # Follow the example of flashinfer: https://docs.flashinfer.ai/api/python/decode.html\n                self.flashinfer_workspace_buffer = torch.empty(\n                    16 * 1024 * 1024, dtype=torch.uint8, device=self.device)\n            paged_kv_indptr_tensor = torch.tensor(paged_kv_indptr,\n                                                  dtype=torch.int,\n                                                  device=self.device)\n            paged_kv_indices_tensor = torch.tensor(paged_kv_indices,\n                                                   dtype=torch.int,\n                                                   device=self.device)\n            paged_kv_last_page_len_tensor = torch.tensor(\n                paged_kv_last_page_len, dtype=torch.int, device=self.device)\n            kv_cache_dtype = get_kv_cache_torch_dtype(self.kv_cache_dtype,\n                                                      self.model_config.dtype)\n            attn_metadata = self.attn_backend.make_metadata(\n                num_prefills=num_prefills,\n                slot_mapping=slot_mapping_tensor,\n                num_prefill_tokens=num_prefill_tokens,\n                num_decode_tokens=num_decode_tokens,\n                use_cuda_graph=False,\n                max_prefill_seq_len=max_prefill_seq_len,\n                block_tables=block_tables,\n                workspace_buffer=self.flashinfer_workspace_buffer,\n                paged_kv_indptr=paged_kv_indptr_tensor,\n                paged_kv_indices=paged_kv_indices_tensor,\n                paged_kv_last_page_len=paged_kv_last_page_len_tensor,\n                num_qo_heads=self.model_config.get_num_attention_heads(\n                    self.parallel_config),\n                num_kv_heads=self.model_config.get_num_kv_heads(\n                    self.parallel_config),\n                head_dim=self.model_config.get_head_size(),\n                page_size=16,\n                seq_start_loc=seq_start_loc,\n                data_type=kv_cache_dtype)\n        else:\n            attn_metadata = self.attn_backend.make_metadata(\n                num_prefills=num_prefills,\n                slot_mapping=slot_mapping_tensor,\n                num_prefill_tokens=num_prefill_tokens,\n                num_decode_tokens=num_decode_tokens,\n                seq_lens=seq_lens,\n                seq_lens_tensor=seq_lens_tensor,\n                max_query_len=max_query_len,\n                max_prefill_seq_len=max_prefill_seq_len,\n                max_decode_seq_len=max_decode_seq_len,\n                query_start_loc=query_start_loc,\n                seq_start_loc=seq_start_loc,\n                context_lens_tensor=context_lens_tensor,\n                block_tables=block_tables,\n                use_cuda_graph=use_captured_graph,\n            )\n\n        if self.lora_config:\n            lora_mapping = LoRAMapping(\n                lora_index_mapping,\n                lora_prompt_mapping,\n            )\n        else:\n            lora_mapping = None\n\n        return ModelInput(\n            input_tokens=input_tokens_tensor,\n            input_positions=input_positions_tensor,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_mapping=lora_mapping,\n            lora_requests=lora_requests,\n            multi_modal_input=multi_modal_input,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            num_prefills=num_prefills,\n        )\n\n    def prepare_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, SamplingMetadata,\n               Set[LoRARequest], LoRAMapping, torch.Tensor]:\n        if self.is_driver_worker:\n            # Prepare input tensors.\n            (\n                input_tokens,\n                input_positions,\n                attn_metadata,\n                seq_lens,\n                query_lens,\n                lora_mapping,\n                lora_requests,\n                multi_modal_input,\n                slot_mapping,\n                num_prefill_tokens,\n                num_decode_tokens,\n                num_prefills,\n            ) = self._prepare_model_input(seq_group_metadata_list)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, seq_lens, query_lens, self.device,\n                self.pin_memory)\n\n            metadata_dict = {\n                \"input_tokens\": input_tokens,\n                \"input_positions\": input_positions,\n                \"selected_token_indices\":\n                sampling_metadata.selected_token_indices,\n                \"lora_requests\": lora_requests,\n                \"lora_mapping\": lora_mapping,\n                \"multi_modal_input\": multi_modal_input,\n                \"num_prefill_tokens\": num_prefill_tokens,\n                \"num_decode_tokens\": num_decode_tokens,\n                \"slot_mapping\": slot_mapping,\n                \"num_prefills\": num_prefills,\n            }\n            if attn_metadata:\n                metadata_dict.update(attn_metadata.asdict_zerocopy())\n            broadcast_tensor_dict(metadata_dict, src=0)\n        else:\n            metadata_dict = broadcast_tensor_dict(src=0)\n            input_tokens = metadata_dict.pop(\"input_tokens\")\n            input_positions = metadata_dict.pop(\"input_positions\")\n            selected_token_indices = metadata_dict.pop(\n                \"selected_token_indices\")\n            lora_mapping = metadata_dict.pop(\"lora_mapping\")\n            lora_requests = metadata_dict.pop(\"lora_requests\")\n            multi_modal_input = metadata_dict.pop(\"multi_modal_input\")\n            if metadata_dict:\n                attn_metadata = self.attn_backend.make_metadata(\n                    **metadata_dict)\n            else:\n                attn_metadata = None\n            sampling_metadata = SamplingMetadata(\n                seq_groups=None,\n                selected_token_indices=selected_token_indices,\n                categorized_sample_indices=None,\n                num_prompts=0,\n            )\n\n        return (input_tokens, input_positions, attn_metadata,\n                sampling_metadata, lora_requests, lora_mapping,\n                multi_modal_input)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        kv_caches: List[torch.Tensor],\n    ) -> Optional[SamplerOutput]:\n        (input_tokens, input_positions, attn_metadata, sampling_metadata,\n         lora_requests, lora_mapping, multi_modal_input\n         ) = self.prepare_input_tensors(seq_group_metadata_list)\n\n        if self.lora_config:\n            self.set_active_loras(lora_requests, lora_mapping)\n\n        # Currently cuda graph is only supported by the decode phase.\n        prefill_meta = attn_metadata.prefill_metadata\n        decode_meta = attn_metadata.decode_metadata\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            graph_batch_size = input_tokens.shape[0]\n            model_executable = self.graph_runners[graph_batch_size]\n        else:\n            model_executable = self.model\n        execute_model_kwargs = {\n            \"input_ids\": input_tokens,\n            \"positions\": input_positions,\n            \"kv_caches\": kv_caches,\n            \"attn_metadata\": attn_metadata,\n        }\n        if self.vision_language_config:\n            execute_model_kwargs.update({\"image_input\": multi_modal_input})\n        hidden_states = model_executable(**execute_model_kwargs)\n\n        # Compute the logits.\n        logits = self.model.compute_logits(hidden_states, sampling_metadata)\n\n        # Only perform sampling in the driver worker.\n        if not self.is_driver_worker:\n            return None\n\n        # Sample the next token.\n        output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        return output\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests = []\n        dummy_lora_requests_per_seq = []\n        if self.lora_config:\n            assert self.lora_manager is not None\n            with self.lora_manager.dummy_lora_cache():\n                for idx in range(self.lora_config.max_loras):\n                    lora_id = idx + 1\n                    dummy_lora_request = LoRARequest(\n                        lora_name=f\"warmup_{lora_id}\",\n                        lora_int_id=lora_id,\n                        lora_local_path=\"/not/a/real/path\",\n                    )\n                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                     rank=LORA_WARMUP_RANK)\n                    dummy_lora_requests.append(dummy_lora_request)\n                dummy_lora_requests_per_seq = [\n                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n                    for idx in range(max_num_seqs)\n                ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for vision encoding, which needs\n        # to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n        if self.vision_language_config:\n            max_num_seqs = min(\n                max_num_seqs,\n                int(max_num_batched_tokens /\n                    self.vision_language_config.image_feature_size))\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            seq_data, fake_multi_modal_input = _prepare_fake_inputs(\n                seq_len, self.vision_language_config)\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=fake_multi_modal_input,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        self.execute_model(seqs, kv_caches)\n        torch.cuda.synchronize()\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_loras()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_loras(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_loras()\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[torch.Tensor]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping = torch.empty(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping.fill_(_PAD_SLOT_ID)\n        seq_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()\n        block_tables = torch.from_numpy(self.graph_block_tables).cuda()\n\n        graph_batch_size = _get_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        with graph_capture() as graph_capture_context:\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for batch_size in reversed(batch_size_capture_list):\n                # Create dummy attn_metadata.\n                attn_metadata = self.attn_backend.make_metadata(\n                    num_prefills=0,\n                    num_prefill_tokens=0,\n                    num_decode_tokens=batch_size,\n                    slot_mapping=slot_mapping[:batch_size],\n                    seq_lens=None,\n                    seq_lens_tensor=seq_lens[:batch_size],\n                    max_query_len=None,\n                    max_prefill_seq_len=0,\n                    max_decode_seq_len=self.max_seq_len_to_capture,\n                    query_start_loc=None,\n                    seq_start_loc=None,\n                    context_lens_tensor=None,\n                    block_tables=block_tables[:batch_size],\n                    use_cuda_graph=True,\n                )\n\n                if self.lora_config:\n                    lora_mapping = LoRAMapping(\n                        [0] * batch_size,\n                        [0] * batch_size,\n                    )\n                    self.set_active_loras(set(), lora_mapping)\n\n                graph_runner = CUDAGraphRunner(self.model)\n                graph_runner.capture(\n                    input_tokens[:batch_size],\n                    input_positions[:batch_size],\n                    kv_caches,\n                    attn_metadata,\n                    memory_pool=self.graph_memory_pool,\n                    stream=graph_capture_context.stream,\n                )\n                self.graph_memory_pool = graph_runner.graph.pool()\n                self.graph_runners[batch_size] = graph_runner\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module):\n        self.model = model\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool: Optional[Tuple[int, int]],\n        stream: torch.cuda.Stream,\n        **kwargs,\n    ) -> None:\n        assert self._graph is None\n        # Run the model once without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        self.model(\n            input_ids,\n            positions,\n            kv_caches,\n            attn_metadata,\n            **kwargs,\n        )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            hidden_states = self.model(\n                input_ids,\n                positions,\n                kv_caches,\n                attn_metadata,\n                **kwargs,\n            )\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\": input_ids,\n            \"positions\": positions,\n            \"kv_caches\": kv_caches,\n            \"slot_mapping\": attn_metadata.slot_mapping,\n            \"seq_lens_tensor\": attn_metadata.decode_metadata.seq_lens_tensor,\n            \"block_tables\": attn_metadata.decode_metadata.block_tables,\n        }\n        self.output_buffers = {\"hidden_states\": hidden_states}\n        return\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(attn_metadata.slot_mapping,\n                                                 non_blocking=True)\n        self.input_buffers[\"seq_lens_tensor\"].copy_(\n            attn_metadata.decode_metadata.seq_lens_tensor, non_blocking=True)\n        self.input_buffers[\"block_tables\"].copy_(\n            attn_metadata.decode_metadata.block_tables, non_blocking=True)\n        # Run the graph.\n        self.graph.replay()\n\n        # Return the output tensor.\n        return self.output_buffers[\"hidden_states\"]\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n\n\ndef _prepare_fake_inputs(\n        seq_len: int, vision_language_config: Optional[VisionLanguageConfig]):\n    \"\"\"Prepare fake inputs for profile run.\"\"\"\n    if vision_language_config:\n        prompt_tokens = [\n            vision_language_config.image_token_id\n        ] * vision_language_config.image_feature_size + [0] * (\n            seq_len - vision_language_config.image_feature_size)\n        fake_image_input = MultiModalData(\n            type=MultiModalData.Type.IMAGE,\n            data=torch.zeros(vision_language_config.image_input_shape,\n                             dtype=torch.float16))\n    else:\n        prompt_tokens = [0] * seq_len\n        fake_image_input = None\n    return SequenceData(prompt_tokens), fake_image_input\n\n\ndef _is_block_tables_empty(block_tables: Union[None, Dict]):\n    \"\"\"\n    Check if block_tables is None or a dictionary with all None values.\n    \"\"\"\n    if block_tables is None:\n        return True\n    if isinstance(block_tables, dict) and all(\n            value is None for value in block_tables.values()):\n        return True\n    return False\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 9720363ac..87d5f5c1b 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -609,10 +609,11 @@ class ModelRunner:\n \n     def prepare_input_tensors(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n     ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, SamplingMetadata,\n                Set[LoRARequest], LoRAMapping, torch.Tensor]:\n         if self.is_driver_worker:\n+            assert seq_group_metadata_list is not None\n             # Prepare input tensors.\n             (\n                 input_tokens,\n@@ -676,7 +677,7 @@ class ModelRunner:\n     @torch.inference_mode()\n     def execute_model(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n         kv_caches: List[torch.Tensor],\n     ) -> Optional[SamplerOutput]:\n         (input_tokens, input_positions, attn_metadata, sampling_metadata,",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/worker/worker.py",
      "old_content": "\"\"\"A GPU worker class.\"\"\"\nimport gc\nimport os\nfrom typing import Any, Dict, List, Optional, Set, Tuple, Union\n\nimport torch\nimport torch.distributed\n\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ParallelConfig, SchedulerConfig,\n                         SpeculativeConfig, VisionLanguageConfig)\nfrom vllm.distributed import (broadcast_tensor_dict,\n                              ensure_model_parallel_initialized,\n                              init_distributed_environment,\n                              set_custom_all_reduce)\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor import set_random_seed\nfrom vllm.sequence import ExecuteModelRequest, PoolerOutput, SamplerOutput\nfrom vllm.worker.cache_engine import CacheEngine\nfrom vllm.worker.embedding_model_runner import EmbeddingModelRunner\nfrom vllm.worker.model_runner import ModelRunner\nfrom vllm.worker.worker_base import WorkerBase\n\n\nclass Worker(WorkerBase):\n    \"\"\"A worker class that executes (a partition of) the model on a GPU.\n\n    Each worker is associated with a single GPU. The worker is responsible for\n    maintaining the KV cache and executing the model on the GPU. In case of\n    distributed inference, each worker is assigned a partition of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        local_rank: int,\n        rank: int,\n        distributed_init_method: str,\n        lora_config: Optional[LoRAConfig] = None,\n        vision_language_config: Optional[VisionLanguageConfig] = None,\n        speculative_config: Optional[SpeculativeConfig] = None,\n        is_driver_worker: bool = False,\n    ) -> None:\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.local_rank = local_rank\n        self.rank = rank\n        self.distributed_init_method = distributed_init_method\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        if self.is_driver_worker:\n            assert self.rank == 0, \"The driver worker must have rank 0.\"\n\n        if self.model_config.trust_remote_code:\n            # note: lazy import to avoid importing torch before initializing\n            from vllm.utils import init_cached_hf_modules\n            init_cached_hf_modules()\n        self.vision_language_config = vision_language_config\n        if self.vision_language_config:\n            assert not self.lora_config, (\n                \"To be tested: vision language model with LoRA settings.\")\n\n        ModelRunnerClass = (EmbeddingModelRunner if\n                            self.model_config.embedding_mode else ModelRunner)\n        self.model_runner = ModelRunnerClass(\n            model_config,\n            parallel_config,\n            scheduler_config,\n            device_config,\n            cache_config,\n            load_config=load_config,\n            lora_config=self.lora_config,\n            kv_cache_dtype=self.cache_config.cache_dtype,\n            is_driver_worker=is_driver_worker,\n            vision_language_config=vision_language_config,\n        )\n        # Uninitialized cache engine. Will be initialized by\n        # initialize_cache.\n        self.cache_engine: CacheEngine\n        # Initialize gpu_cache as embedding models don't initialize kv_caches\n        self.gpu_cache: Optional[List[torch.tensor]] = None\n\n    def init_device(self) -> None:\n        if self.device_config.device.type == \"cuda\":\n            # torch.distributed.all_reduce does not free the input tensor until\n            # the synchronization point. This causes the memory usage to grow\n            # as the number of all_reduce calls increases. This env var disables\n            # this behavior.\n            # Related issue:\n            # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n            os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n\n            # This env var set by Ray causes exceptions with graph building.\n            os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n            self.device = torch.device(f\"cuda:{self.local_rank}\")\n            torch.cuda.set_device(self.device)\n\n            _check_if_gpu_supports_dtype(self.model_config.dtype)\n            torch.cuda.empty_cache()\n            self.init_gpu_memory = torch.cuda.mem_get_info()[0]\n        else:\n            raise RuntimeError(\n                f\"Not support device type: {self.device_config.device}\")\n        # Initialize the distributed environment.\n        init_worker_distributed_environment(self.parallel_config, self.rank,\n                                            self.distributed_init_method,\n                                            self.local_rank)\n        # Set random seed.\n        set_random_seed(self.model_config.seed)\n\n    def load_model(self):\n        self.model_runner.load_model()\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        self.model_runner.save_sharded_state(\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    @torch.inference_mode()\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Profiles the peak memory usage of the model to determine how many\n        KV blocks may be allocated without OOMs.\n\n        The engine will first conduct a profiling of the existing memory usage.\n        Then, it calculate the maximum possible number of GPU and CPU blocks\n        that can be allocated with the remaining free memory.\n\n        .. tip::\n            You may limit the usage of GPU memory\n            by adjusting the `gpu_memory_utilization` parameter.\n        \"\"\"\n        # Profile the memory usage of the model and get the maximum number of\n        # cache blocks that can be allocated with the remaining free memory.\n        torch.cuda.empty_cache()\n\n        # Execute a forward pass with dummy inputs to profile the memory usage\n        # of the model.\n        self.model_runner.profile_run()\n\n        # Calculate the number of blocks that can be allocated with the\n        # profiled peak memory.\n        torch.cuda.synchronize()\n        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n        # NOTE(woosuk): Here we assume that the other processes using the same\n        # GPU did not change their memory usage during the profiling.\n        peak_memory = self.init_gpu_memory - free_gpu_memory\n        assert peak_memory > 0, (\n            \"Error in memory profiling. This happens when the GPU memory was \"\n            \"not properly cleaned up before initializing the vLLM instance.\")\n\n        cache_block_size = self.get_cache_block_size_bytes()\n        num_gpu_blocks = int(\n            (total_gpu_memory * self.cache_config.gpu_memory_utilization -\n             peak_memory) // cache_block_size)\n        num_cpu_blocks = int(self.cache_config.swap_space_bytes //\n                             cache_block_size)\n        num_gpu_blocks = max(num_gpu_blocks, 0)\n        num_cpu_blocks = max(num_cpu_blocks, 0)\n        if self.model_runner.lora_manager:\n            self.model_runner.remove_all_loras()\n        gc.collect()\n        torch.cuda.empty_cache()\n        return num_gpu_blocks, num_cpu_blocks\n\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Allocate GPU and CPU KV cache with the specified number of blocks.\n\n        This also warms up the model, which may record CUDA graphs.\n        \"\"\"\n        raise_if_cache_size_invalid(num_gpu_blocks,\n                                    self.cache_config.block_size,\n                                    self.model_config.max_model_len)\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self._init_cache_engine()\n        self._warm_up_model()\n\n    def _init_cache_engine(self):\n        assert self.cache_config.num_gpu_blocks is not None\n        self.cache_engine = CacheEngine(self.cache_config, self.model_config,\n                                        self.parallel_config)\n        self.gpu_cache = self.cache_engine.gpu_cache\n\n    def _warm_up_model(self) -> None:\n        if not self.model_config.enforce_eager:\n            self.model_runner.capture_model(self.gpu_cache)\n        # Reset the seed to ensure that the random state is not affected by\n        # the model initialization and profiling.\n        set_random_seed(self.model_config.seed)\n\n    def cache_swap(\n        self,\n        blocks_to_swap_in: torch.Tensor,\n        blocks_to_swap_out: torch.Tensor,\n        blocks_to_copy: torch.Tensor,\n    ) -> None:\n        # Issue cache operations.\n        if blocks_to_swap_in.numel() > 0:\n            self.cache_engine.swap_in(blocks_to_swap_in)\n        if blocks_to_swap_out.numel() > 0:\n            self.cache_engine.swap_out(blocks_to_swap_out)\n        if blocks_to_copy.numel() > 0:\n            self.cache_engine.copy(blocks_to_copy)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        execute_model_req: Optional[ExecuteModelRequest] = None\n    ) -> List[Union[SamplerOutput, PoolerOutput]]:\n\n        if execute_model_req is None:\n            seq_group_metadata_list = None\n        else:\n            seq_group_metadata_list = execute_model_req.seq_group_metadata_list\n\n        blocks_to_swap_in: torch.Tensor\n        blocks_to_swap_out: torch.Tensor\n        blocks_to_copy: torch.Tensor\n        if self.is_driver_worker:\n            assert seq_group_metadata_list is not None\n            assert execute_model_req is not None\n            num_seq_groups = len(seq_group_metadata_list)\n            # `blocks_to_swap_in` and `blocks_to_swap_out` are cpu tensors.\n            # they contain parameters to launch cudamemcpyasync.\n            blocks_to_swap_in = torch.tensor(\n                execute_model_req.blocks_to_swap_in,\n                device=\"cpu\",\n                dtype=torch.int64).view(-1, 2)\n            blocks_to_swap_out = torch.tensor(\n                execute_model_req.blocks_to_swap_out,\n                device=\"cpu\",\n                dtype=torch.int64).view(-1, 2)\n            # `blocks_to_copy` is a gpu tensor. The src and tgt of\n            # blocks to copy are in the same device, and `blocks_to_copy`\n            # can be used directly within cuda kernels.\n            blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,\n                                          device=self.device,\n                                          dtype=torch.int64).view(-1, 2)\n            data: Dict[str, Any] = {\n                \"num_seq_groups\": num_seq_groups,\n                \"blocks_to_swap_in\": blocks_to_swap_in,\n                \"blocks_to_swap_out\": blocks_to_swap_out,\n                \"blocks_to_copy\": blocks_to_copy,\n            }\n            broadcast_tensor_dict(data, src=0)\n        else:\n            data = broadcast_tensor_dict(src=0)\n            num_seq_groups = data[\"num_seq_groups\"]\n            blocks_to_swap_in = data[\"blocks_to_swap_in\"]\n            blocks_to_swap_out = data[\"blocks_to_swap_out\"]\n            blocks_to_copy = data[\"blocks_to_copy\"]\n\n        self.cache_swap(blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\n\n        # If there is no input, we don't need to execute the model.\n        if num_seq_groups == 0:\n            return []\n\n        output = self.model_runner.execute_model(seq_group_metadata_list,\n                                                 self.gpu_cache)\n\n        # Worker only supports single-step execution. Wrap the output in a list\n        # to conform to interface.\n        return [output]\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_runner.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_runner.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_runner.list_loras()\n\n    @property\n    def max_model_len(self) -> int:\n        return self.model_config.max_model_len\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_runner.vocab_size\n\n    def get_cache_block_size_bytes(self) -> int:\n        \"\"\"Get the size of the KV cache block size in bytes.\n        \"\"\"\n        return CacheEngine.get_cache_block_size(self.cache_config,\n                                                self.model_config,\n                                                self.parallel_config)\n\n\ndef init_worker_distributed_environment(\n    parallel_config: ParallelConfig,\n    rank: int,\n    distributed_init_method: Optional[str] = None,\n    local_rank: int = -1,\n) -> None:\n    \"\"\"Initialize the distributed environment.\"\"\"\n    set_custom_all_reduce(not parallel_config.disable_custom_all_reduce)\n\n    init_distributed_environment(parallel_config.world_size, rank,\n                                 distributed_init_method, local_rank)\n\n    ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,\n                                      parallel_config.pipeline_parallel_size)\n\n\ndef _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):\n    # Check if the GPU supports the dtype.\n    if torch_dtype == torch.bfloat16:\n        compute_capability = torch.cuda.get_device_capability()\n        if compute_capability[0] < 8:\n            gpu_name = torch.cuda.get_device_name()\n            raise ValueError(\n                \"Bfloat16 is only supported on GPUs with compute capability \"\n                f\"of at least 8.0. Your {gpu_name} GPU has compute capability \"\n                f\"{compute_capability[0]}.{compute_capability[1]}. \"\n                \"You can use float16 instead by explicitly setting the\"\n                \"`dtype` flag in CLI, for example: --dtype=half.\")\n\n\ndef raise_if_cache_size_invalid(num_gpu_blocks, block_size,\n                                max_model_len) -> None:\n    if num_gpu_blocks <= 0:\n        raise ValueError(\"No available memory for the cache blocks. \"\n                         \"Try increasing `gpu_memory_utilization` when \"\n                         \"initializing the engine.\")\n    max_seq_len = block_size * num_gpu_blocks\n    if max_model_len > max_seq_len:\n        raise ValueError(\n            f\"The model's max seq len ({max_model_len}) \"\n            \"is larger than the maximum number of tokens that can be \"\n            f\"stored in KV cache ({max_seq_len}). Try increasing \"\n            \"`gpu_memory_utilization` or decreasing `max_model_len` when \"\n            \"initializing the engine.\")\n",
      "diff": "diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py\nindex 97b3873b2..10411a2bf 100644\n--- a/vllm/worker/worker.py\n+++ b/vllm/worker/worker.py\n@@ -226,48 +226,42 @@ class Worker(WorkerBase):\n         self,\n         execute_model_req: Optional[ExecuteModelRequest] = None\n     ) -> List[Union[SamplerOutput, PoolerOutput]]:\n+        if not self.is_driver_worker:\n+            self._execute_model_non_driver()\n+            return []\n \n         if execute_model_req is None:\n-            seq_group_metadata_list = None\n-        else:\n-            seq_group_metadata_list = execute_model_req.seq_group_metadata_list\n+            # This signals that there's no more requests to process for now.\n+            # All workers are running infinite loop with broadcast_tensor_dict,\n+            # and it stops the loop when the driver broadcasts an empty input.\n+            # Send an empty input to notify all other workers to stop their\n+            # execution loop.\n+            broadcast_tensor_dict({}, src=0)\n+            return []\n \n-        blocks_to_swap_in: torch.Tensor\n-        blocks_to_swap_out: torch.Tensor\n-        blocks_to_copy: torch.Tensor\n-        if self.is_driver_worker:\n-            assert seq_group_metadata_list is not None\n-            assert execute_model_req is not None\n-            num_seq_groups = len(seq_group_metadata_list)\n-            # `blocks_to_swap_in` and `blocks_to_swap_out` are cpu tensors.\n-            # they contain parameters to launch cudamemcpyasync.\n-            blocks_to_swap_in = torch.tensor(\n-                execute_model_req.blocks_to_swap_in,\n-                device=\"cpu\",\n-                dtype=torch.int64).view(-1, 2)\n-            blocks_to_swap_out = torch.tensor(\n-                execute_model_req.blocks_to_swap_out,\n-                device=\"cpu\",\n-                dtype=torch.int64).view(-1, 2)\n-            # `blocks_to_copy` is a gpu tensor. The src and tgt of\n-            # blocks to copy are in the same device, and `blocks_to_copy`\n-            # can be used directly within cuda kernels.\n-            blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,\n-                                          device=self.device,\n+        seq_group_metadata_list = execute_model_req.seq_group_metadata_list\n+        num_seq_groups = len(seq_group_metadata_list)\n+        # `blocks_to_swap_in` and `blocks_to_swap_out` are cpu tensors.\n+        # they contain parameters to launch cudamemcpyasync.\n+        blocks_to_swap_in = torch.tensor(execute_model_req.blocks_to_swap_in,\n+                                         device=\"cpu\",\n+                                         dtype=torch.int64).view(-1, 2)\n+        blocks_to_swap_out = torch.tensor(execute_model_req.blocks_to_swap_out,\n+                                          device=\"cpu\",\n                                           dtype=torch.int64).view(-1, 2)\n-            data: Dict[str, Any] = {\n-                \"num_seq_groups\": num_seq_groups,\n-                \"blocks_to_swap_in\": blocks_to_swap_in,\n-                \"blocks_to_swap_out\": blocks_to_swap_out,\n-                \"blocks_to_copy\": blocks_to_copy,\n-            }\n-            broadcast_tensor_dict(data, src=0)\n-        else:\n-            data = broadcast_tensor_dict(src=0)\n-            num_seq_groups = data[\"num_seq_groups\"]\n-            blocks_to_swap_in = data[\"blocks_to_swap_in\"]\n-            blocks_to_swap_out = data[\"blocks_to_swap_out\"]\n-            blocks_to_copy = data[\"blocks_to_copy\"]\n+        # `blocks_to_copy` is a gpu tensor. The src and tgt of\n+        # blocks to copy are in the same device, and `blocks_to_copy`\n+        # can be used directly within cuda kernels.\n+        blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,\n+                                      device=self.device,\n+                                      dtype=torch.int64).view(-1, 2)\n+        data: Dict[str, Any] = {\n+            \"num_seq_groups\": num_seq_groups,\n+            \"blocks_to_swap_in\": blocks_to_swap_in,\n+            \"blocks_to_swap_out\": blocks_to_swap_out,\n+            \"blocks_to_copy\": blocks_to_copy,\n+        }\n+        broadcast_tensor_dict(data, src=0)\n \n         self.cache_swap(blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\n \n@@ -282,6 +276,39 @@ class Worker(WorkerBase):\n         # to conform to interface.\n         return [output]\n \n+    @torch.inference_mode()\n+    def start_worker_execution_loop(self) -> None:\n+        \"\"\"Execute model loop in parallel worker.\n+\n+        You can stop the loop by executing a driver worker with an empty output.\n+        See `stop_remote_worker_execution_loop` for more details.\n+        \"\"\"\n+        while self._execute_model_non_driver():\n+            pass\n+\n+    def _execute_model_non_driver(self) -> bool:\n+        \"\"\"Execute model in parallel worker.\n+\n+        Returns True iff there are remaining sequences to process.\n+        \"\"\"\n+        assert not self.is_driver_worker\n+        data = broadcast_tensor_dict(src=0)\n+        if not data:\n+            return False\n+\n+        num_seq_groups = data.get(\"num_seq_groups\", 0)\n+        blocks_to_swap_in = data.get(\"blocks_to_swap_in\")\n+        blocks_to_swap_out = data.get(\"blocks_to_swap_out\")\n+        blocks_to_copy = data.get(\"blocks_to_copy\")\n+        self.cache_swap(blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy)\n+\n+        # If there is no input, we don't need to execute the model.\n+        if num_seq_groups == 0:\n+            return False\n+\n+        self.model_runner.execute_model(None, self.gpu_cache)\n+        return True\n+\n     def add_lora(self, lora_request: LoRARequest) -> bool:\n         return self.model_runner.add_lora(lora_request)",
      "change_type": "modified",
      "lines_added": 66,
      "lines_removed": 39
    },
    {
      "file_path": "vllm/worker/worker_base.py",
      "old_content": "import importlib\nimport os\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, List, Set, Tuple\n\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import ExecuteModelRequest, SamplerOutput\nfrom vllm.utils import (enable_trace_function_call_for_thread,\n                        update_environment_variables)\n\nlogger = init_logger(__name__)\n\n\nclass WorkerBase(ABC):\n    \"\"\"Worker interface that allows vLLM to cleanly separate implementations for\n    different hardware.\n    \"\"\"\n\n    @abstractmethod\n    def init_device(self) -> None:\n        \"\"\"Initialize device state, such as loading the model or other on-device\n        memory allocations.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def determine_num_available_blocks(self) -> Tuple[int, int]:\n        \"\"\"Determine the number of available blocks for the GPU KV cache and\n        swappable CPU KV cache.\n\n        The implementation may run profiling or other heuristics to determine\n        the size of caches.\n\n        Returns a Tuple[num_gpu_blocks, num_cpu_blocks], where num_gpu_blocks\n        are blocks that are \"active\" on the device and can be appended to.\n        num_cpu_blocks refers to \"swapped\" blocks in CPU memory and cannot be\n        appended to.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def initialize_cache(self, num_gpu_blocks: int,\n                         num_cpu_blocks: int) -> None:\n        \"\"\"Initialize the KV cache with the given size in blocks.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def execute_model(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        \"\"\"Executes at least one model step on the given sequences, unless no\n        sequences are provided.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_cache_block_size_bytes(self) -> int:\n        \"\"\"Return the size of a single cache block, in bytes. Used in\n        speculative decoding.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    @abstractmethod\n    def remove_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    @abstractmethod\n    def list_loras(self) -> Set[int]:\n        raise NotImplementedError\n\n\nclass LoraNotSupportedWorkerBase(WorkerBase):\n    \"\"\"Partial implementation of WorkerBase that raises exceptions when LoRA\n    methods are invoked.\n    \"\"\"\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        raise ValueError(f\"{type(self)} does not support LoRA\")\n\n    def remove_lora(self, lora_id: int) -> bool:\n        raise ValueError(f\"{type(self)} does not support LoRA\")\n\n    def list_loras(self) -> Set[int]:\n        raise ValueError(f\"{type(self)} does not support LoRA\")\n\n\nclass WorkerWrapperBase:\n    \"\"\"\n    The whole point of this class is to lazily initialize the worker.\n    We first instantiate the WorkerWrapper, which remembers the worker module\n    and class name. Then, when we call `update_environment_variables`, and the\n    real initialization happens in `init_worker`.\n    \"\"\"\n\n    def __init__(self,\n                 worker_module_name=None,\n                 worker_class_name=None,\n                 trust_remote_code: bool = False) -> None:\n        self.worker_module_name = worker_module_name\n        self.worker_class_name = worker_class_name\n        self.worker = None\n        if trust_remote_code:\n            # note: lazy import to avoid importing torch before initializing\n            from vllm.utils import init_cached_hf_modules\n            init_cached_hf_modules()\n\n    @staticmethod\n    def update_environment_variables(envs: Dict[str, str]) -> None:\n        key = 'CUDA_VISIBLE_DEVICES'\n        if key in envs and key in os.environ:\n            # overwriting CUDA_VISIBLE_DEVICES is desired behavior\n            # suppress the warning in `update_environment_variables`\n            del os.environ[key]\n        update_environment_variables(envs)\n\n    def init_worker(self, *args, **kwargs):\n        \"\"\"\n        Actual initialization of the worker class, and set up\n        function tracing if required.\n        Arguments are passed to the worker class constructor.\n        \"\"\"\n        enable_trace_function_call_for_thread()\n\n        mod = importlib.import_module(self.worker_module_name)\n        worker_class = getattr(mod, self.worker_class_name)\n        self.worker = worker_class(*args, **kwargs)\n\n    def execute_method(self, method, *args, **kwargs):\n        try:\n            target = self if self.worker is None else self.worker\n            executor = getattr(target, method)\n            return executor(*args, **kwargs)\n        except Exception as e:\n            # if the driver worker also execute methods,\n            # exceptions in the rest worker may cause deadlock in rpc like ray\n            # see https://github.com/vllm-project/vllm/issues/3455\n            # print the error and inform the user to solve the error\n            msg = (f\"Error executing method {method}. \"\n                   \"This might cause deadlock in distributed execution.\")\n            logger.exception(msg)\n            raise e\n",
      "diff": "diff --git a/vllm/worker/worker_base.py b/vllm/worker/worker_base.py\nindex 1f04f821e..dbac1b5ba 100644\n--- a/vllm/worker/worker_base.py\n+++ b/vllm/worker/worker_base.py\n@@ -1,7 +1,7 @@\n import importlib\n import os\n from abc import ABC, abstractmethod\n-from typing import Dict, List, Set, Tuple\n+from typing import Dict, List, Optional, Set, Tuple\n \n from vllm.logger import init_logger\n from vllm.lora.request import LoRARequest\n@@ -48,8 +48,9 @@ class WorkerBase(ABC):\n \n     @abstractmethod\n     def execute_model(\n-            self,\n-            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n+        self,\n+        execute_model_req: Optional[ExecuteModelRequest] = None\n+    ) -> List[SamplerOutput]:\n         \"\"\"Executes at least one model step on the given sequences, unless no\n         sequences are provided.\"\"\"\n         raise NotImplementedError",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 4
    }
  ],
  "affected_apis": [
    "DistributedGPUExecutor.execute_model",
    "DistributedGPUExecutor.stop_remote_worker_execution_loop",
    "DistributedGPUExecutorAsync.execute_model_async",
    "ExecutorBase.stop_remote_worker_execution_loop",
    "ExecutorAsyncBase.stop_remote_worker_execution_loop_async"
  ],
  "summary": {
    "total_files": 12,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 12
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (too much BS)",
    "is_benchmark_actually_there": "",
    "sample_clues": "async, async_llm_engine, asyncllmengine"
  }
}