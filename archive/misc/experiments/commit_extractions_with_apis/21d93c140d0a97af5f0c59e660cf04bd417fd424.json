{
  "commit_hash": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
  "parent_hash": "f1c8520146031a650404a6ab120ee11e91c10bed",
  "message": "Optimize Mixtral with expert parallelism (#2090)",
  "author": "Antoni Baum <antoni.baum@protonmail.com>",
  "date": "2023-12-13 23:55:07 -0800",
  "files_changed": [
    {
      "file_path": "Dockerfile",
      "old_content": "FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev\n\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\n# install development dependencies\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-dev.txt\n\n# image to build pytorch extensions\nFROM dev AS build\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements-build.txt\n\n# copy input files\nCOPY csrc csrc\nCOPY setup.py setup.py\nCOPY requirements.txt requirements.txt\nCOPY pyproject.toml pyproject.toml\nCOPY vllm/__init__.py vllm/__init__.py\n\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nRUN python3 setup.py build_ext --inplace\n\n# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\nRUN apt-get install -y git && \\\n    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n    cd megablocks && \\\n    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n\n# image to run unit testing suite\nFROM dev AS test\n\n# copy pytorch extensions separately to avoid having to rebuild\n# when python code changes\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY tests tests\nCOPY vllm vllm\n\nENTRYPOINT [\"python3\", \"-m\", \"pytest\", \"tests\"]\n\n# use CUDA base as CUDA runtime dependencies are already installed via pip\nFROM nvidia/cuda:12.1.0-base-ubuntu22.04 AS vllm-base\n\n# libnccl required for ray\nRUN apt-get update -y \\\n    && apt-get install -y python3-pip\n\nWORKDIR /workspace\nCOPY requirements.txt requirements.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -r requirements.txt\n\nFROM vllm-base AS vllm\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY vllm vllm\n\nEXPOSE 8000\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.api_server\"]\n\n# openai api server alternative\nFROM vllm-base AS vllm-openai\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate\n\nCOPY vllm vllm\nCOPY --from=build /workspace/vllm/*.so /workspace/vllm/\nCOPY --from=build /workspace/megablocks/dist/*.whl /tmp/\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n\n",
      "diff": "diff --git a/Dockerfile b/Dockerfile\nindex f41753aeb..6ef03b843 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -41,14 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads\n \n RUN python3 setup.py build_ext --inplace\n \n-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\n-RUN apt-get install -y git && \\\n-    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n-    cd megablocks && \\\n-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n-\n # image to run unit testing suite\n FROM dev AS test\n \n@@ -85,12 +77,8 @@ FROM vllm-base AS vllm-openai\n RUN --mount=type=cache,target=/root/.cache/pip \\\n     pip install accelerate\n \n-COPY vllm vllm\n COPY --from=build /workspace/vllm/*.so /workspace/vllm/\n-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/\n-RUN --mount=type=cache,target=/root/.cache/pip \\\n-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n+COPY vllm vllm\n \n ENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 14
    },
    {
      "file_path": "README.md",
      "old_content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://vllm.ai\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://discord.gg/jz7wjKhh6g\"><b>Discord</b></a> |\n\n</p>\n\n---\n\n*Latest News* ðŸ”¥\n- [2023/12] Added ROCm support to vLLM.\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) in SF! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/09] We created our [Discord server](https://discord.gg/jz7wjKhh6g)! Join us to discuss vLLM and LLM serving! We will also post the latest announcements and updates there.\n- [2023/09] We released our [PagedAttention paper](https://arxiv.org/abs/2309.06180) on arXiv!\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/07] Added support for LLaMA-2! You can run and serve 7B/13B/70B LLaMA-2s on vLLM with a single command!\n- [2023/06] Serving vLLM On any Cloud with SkyPilot. Check out a 1-click [example](https://github.com/skypilot-org/skypilot/blob/master/llm/vllm) to start the vLLM demo, and the [blog post](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/) for the story behind vLLM development on the clouds.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n---\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with **PagedAttention**\n- Continuous batching of incoming requests\n- Optimized CUDA kernels\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA CUDA and AMD ROCm.\n\nvLLM seamlessly supports many Hugging Face models, including the following architectures:\n\n- Aquila & Aquila2 (`BAAI/AquilaChat2-7B`, `BAAI/AquilaChat2-34B`, `BAAI/Aquila-7B`, `BAAI/AquilaChat-7B`, etc.)\n- Baichuan & Baichuan2 (`baichuan-inc/Baichuan2-13B-Chat`, `baichuan-inc/Baichuan-7B`, etc.)\n- BLOOM (`bigscience/bloom`, `bigscience/bloomz`, etc.)\n- ChatGLM (`THUDM/chatglm2-6b`, `THUDM/chatglm3-6b`, etc.)\n- Falcon (`tiiuae/falcon-7b`, `tiiuae/falcon-40b`, `tiiuae/falcon-rw-7b`, etc.)\n- GPT-2 (`gpt2`, `gpt2-xl`, etc.)\n- GPT BigCode (`bigcode/starcoder`, `bigcode/gpt_bigcode-santacoder`, etc.)\n- GPT-J (`EleutherAI/gpt-j-6b`, `nomic-ai/gpt4all-j`, etc.)\n- GPT-NeoX (`EleutherAI/gpt-neox-20b`, `databricks/dolly-v2-12b`, `stabilityai/stablelm-tuned-alpha-7b`, etc.)\n- InternLM (`internlm/internlm-7b`, `internlm/internlm-chat-7b`, etc.)\n- LLaMA & LLaMA-2 (`meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`, `openlm-research/open_llama_13b`, etc.)\n- Mistral (`mistralai/Mistral-7B-v0.1`, `mistralai/Mistral-7B-Instruct-v0.1`, etc.)\n- Mixtral (`mistralai/Mixtral-8x7B-v0.1`, `mistralai/Mixtral-8x7B-Instruct-v0.1`, etc.)\n- MPT (`mosaicml/mpt-7b`, `mosaicml/mpt-30b`, etc.)\n- OPT (`facebook/opt-66b`, `facebook/opt-iml-max-30b`, etc.)\n- Phi-1.5 (`microsoft/phi-1_5`, etc.)\n- Qwen (`Qwen/Qwen-7B`, `Qwen/Qwen-7B-Chat`, etc.)\n- Yi (`01-ai/Yi-6B`, `01-ai/Yi-34B`, etc.)\n\nInstall vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):\n\n```bash\npip install vllm\n```\n**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n```bash\npip install megablocks\n```\n\n## Getting Started\n\nVisit our [documentation](https://vllm.readthedocs.io/en/latest/) to get started.\n- [Installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)\n- [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n- [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n",
      "diff": "diff --git a/README.md b/README.md\nindex 84cadee48..e4b3b5026 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get\n ```bash\n pip install vllm\n ```\n-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n-```bash\n-pip install megablocks\n-```\n \n ## Getting Started",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 5
    },
    {
      "file_path": "docs/source/models/supported_models.rst",
      "old_content": ".. _supported_models:\n\nSupported Models\n================\n\nvLLM supports a variety of generative Transformer models in `HuggingFace Transformers <https://huggingface.co/models>`_.\nThe following is the list of model architectures that are currently supported by vLLM.\nAlongside each architecture, we include some popular models that use it.\n\n.. list-table::\n  :widths: 25 25 50\n  :header-rows: 1\n\n  * - Architecture\n    - Models\n    - Example HuggingFace Models\n  * - :code:`AquilaForCausalLM`\n    - Aquila\n    - :code:`BAAI/Aquila-7B`, :code:`BAAI/AquilaChat-7B`, etc.\n  * - :code:`BaiChuanForCausalLM`\n    - Baichuan\n    - :code:`baichuan-inc/Baichuan2-13B-Chat`, :code:`baichuan-inc/Baichuan-7B`, etc.\n  * - :code:`ChatGLMModel`\n    - ChatGLM\n    - :code:`THUDM/chatglm2-6b`, :code:`THUDM/chatglm3-6b`, etc.\n  * - :code:`BloomForCausalLM`\n    - BLOOM, BLOOMZ, BLOOMChat\n    - :code:`bigscience/bloom`, :code:`bigscience/bloomz`, etc.\n  * - :code:`FalconForCausalLM`\n    - Falcon\n    - :code:`tiiuae/falcon-7b`, :code:`tiiuae/falcon-40b`, :code:`tiiuae/falcon-rw-7b`, etc.\n  * - :code:`GPT2LMHeadModel`\n    - GPT-2\n    - :code:`gpt2`, :code:`gpt2-xl`, etc.\n  * - :code:`GPTBigCodeForCausalLM`\n    - StarCoder, SantaCoder, WizardCoder\n    - :code:`bigcode/starcoder`, :code:`bigcode/gpt_bigcode-santacoder`, :code:`WizardLM/WizardCoder-15B-V1.0`, etc.\n  * - :code:`GPTJForCausalLM`\n    - GPT-J\n    - :code:`EleutherAI/gpt-j-6b`, :code:`nomic-ai/gpt4all-j`, etc.\n  * - :code:`GPTNeoXForCausalLM`\n    - GPT-NeoX, Pythia, OpenAssistant, Dolly V2, StableLM\n    - :code:`EleutherAI/gpt-neox-20b`, :code:`EleutherAI/pythia-12b`, :code:`OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5`, :code:`databricks/dolly-v2-12b`, :code:`stabilityai/stablelm-tuned-alpha-7b`, etc.\n  * - :code:`InternLMForCausalLM`\n    - InternLM\n    - :code:`internlm/internlm-7b`, :code:`internlm/internlm-chat-7b`, etc.\n  * - :code:`LlamaForCausalLM`\n    - LLaMA, LLaMA-2, Vicuna, Alpaca, Koala, Guanaco\n    - :code:`meta-llama/Llama-2-13b-hf`, :code:`meta-llama/Llama-2-70b-hf`, :code:`openlm-research/open_llama_13b`, :code:`lmsys/vicuna-13b-v1.3`, :code:`young-geng/koala`, etc.\n  * - :code:`MistralForCausalLM`\n    - Mistral, Mistral-Instruct\n    - :code:`mistralai/Mistral-7B-v0.1`, :code:`mistralai/Mistral-7B-Instruct-v0.1`, etc.\n  * - :code:`MixtralForCausalLM`\n    - Mixtral-8x7B, Mixtral-8x7B-Instruct\n    - :code:`mistralai/Mixtral-8x7B-v0.1`, :code:`mistralai/Mixtral-8x7B-Instruct-v0.1`, etc.\n  * - :code:`MPTForCausalLM`\n    - MPT, MPT-Instruct, MPT-Chat, MPT-StoryWriter\n    - :code:`mosaicml/mpt-7b`, :code:`mosaicml/mpt-7b-storywriter`, :code:`mosaicml/mpt-30b`, etc.\n  * - :code:`OPTForCausalLM`\n    - OPT, OPT-IML\n    - :code:`facebook/opt-66b`, :code:`facebook/opt-iml-max-30b`, etc.\n  * - :code:`PhiForCausalLM`\n    - Phi-1.5\n    - :code:`microsoft/phi-1_5`, etc.\n  * - :code:`QWenLMHeadModel`\n    - Qwen\n    - :code:`Qwen/Qwen-7B`, :code:`Qwen/Qwen-7B-Chat`, etc.\n  * - :code:`YiForCausalLM`\n    - Yi\n    - :code:`01-ai/Yi-6B`, :code:`01-ai/Yi-34B`, etc.\n\nIf your model uses one of the above model architectures, you can seamlessly run your model with vLLM.\nOtherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for instructions on how to implement support for your model.\nAlternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.\n\n.. note::\n    Currently, the ROCm version of vLLM does not support Mixtral.\n    Additionally, it only supports Mistral for context lengths up to 4096.\n\n.. tip::\n    The easiest way to check if your model is supported is to run the program below:\n\n    .. code-block:: python\n\n        from vllm import LLM\n\n        llm = LLM(model=...)  # Name or path of your model\n        output = llm.generate(\"Hello, my name is\")\n        print(output)\n\n    If vLLM successfully generates text, it indicates that your model is supported.\n\n.. tip::\n    To use models from `ModelScope <www.modelscope.cn>`_ instead of HuggingFace Hub, set an environment variable:\n\n    .. code-block:: shell\n\n       $ export VLLM_USE_MODELSCOPE=True\n\n    And use with :code:`trust_remote_code=True`.\n\n    .. code-block:: python\n\n        from vllm import LLM\n\n        llm = LLM(model=..., revision=..., trust_remote_code=True)  # Name or path of your model\n        output = llm.generate(\"Hello, my name is\")\n        print(output)\n",
      "diff": "diff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst\nindex e21cdd65d..44e4fe5ea 100644\n--- a/docs/source/models/supported_models.rst\n+++ b/docs/source/models/supported_models.rst\n@@ -74,8 +74,7 @@ Otherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for in\n Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.\n \n .. note::\n-    Currently, the ROCm version of vLLM does not support Mixtral.\n-    Additionally, it only supports Mistral for context lengths up to 4096.\n+    Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.\n \n .. tip::\n     The easiest way to check if your model is supported is to run the program below:",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/config.py",
      "old_content": "from typing import Optional, Union\nimport os\n\nimport torch\nfrom transformers import PretrainedConfig\n\nfrom vllm.logger import init_logger\nfrom vllm.transformers_utils.config import get_config\nfrom vllm.utils import get_cpu_memory, is_hip\n\nlogger = init_logger(__name__)\n\n_GB = 1 << 30\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, and \"slow\" will always use the slow tokenizer.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        download_dir: Directory to download and load the weights, default to the\n            default cache directory of huggingface.\n        load_format: The format of the model weights to load:\n            \"auto\" will try to load the weights in the safetensors format and\n                fall back to the pytorch bin format if safetensors format is\n                not available.\n            \"pt\" will load the weights in the pytorch bin format.\n            \"safetensors\" will load the weights in the safetensors format.\n            \"npcache\" will load the weights in pytorch format and store\n                a numpy cache to speed up the loading.\n            \"dummy\" will initialize the weights with random values, which is\n                mainly for profiling.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        tokenizer: str,\n        tokenizer_mode: str,\n        trust_remote_code: bool,\n        download_dir: Optional[str],\n        load_format: str,\n        dtype: Union[str, torch.dtype],\n        seed: int,\n        revision: Optional[str] = None,\n        tokenizer_revision: Optional[str] = None,\n        max_model_len: Optional[int] = None,\n        quantization: Optional[str] = None,\n    ) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.download_dir = download_dir\n        self.load_format = load_format\n        self.seed = seed\n        self.revision = revision\n        self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n\n        if os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\":\n            # download model from ModelScope hub,\n            # lazy import so that modelscope is not required for normal use.\n            from modelscope.hub.snapshot_download import snapshot_download  # pylint: disable=C\n            model_path = snapshot_download(model_id=model,\n                                           cache_dir=download_dir,\n                                           revision=revision)\n            self.model = model_path\n            self.download_dir = model_path\n            self.tokenizer = model_path\n\n        self.hf_config = get_config(self.model, trust_remote_code, revision)\n        self.dtype = _get_and_verify_dtype(self.hf_config, dtype)\n        self.max_model_len = _get_and_verify_max_len(self.hf_config,\n                                                     max_model_len)\n        self._verify_load_format()\n        self._verify_tokenizer_mode()\n        self._verify_quantization()\n\n    def _verify_load_format(self) -> None:\n        load_format = self.load_format.lower()\n        supported_load_format = [\n            \"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"\n        ]\n        rocm_not_supported_load_format = [\"safetensors\"]\n        if load_format not in supported_load_format:\n            raise ValueError(\n                f\"Unknown load format: {self.load_format}. Must be one of \"\n                \"'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.\")\n        if is_hip():\n            if load_format in [\"safetensors\"]:\n                rocm_supported_load_format = [\n                    f for f in supported_load_format\n                    if (f not in rocm_not_supported_load_format)\n                ]\n                raise ValueError(\n                    f\"load format \\'{load_format}\\' is not supported in ROCm. \"\n                    f\"Supported load format are \"\n                    f\"{rocm_supported_load_format}\")\n            # Force ROCm to load from pt weights if nothing specific is set\n            if load_format == \"auto\":\n                load_format = \"pt\"\n\n        # FIXME(woosuk): This is a temporary hack. Support safetensor weights.\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if \"MixtralForCausalLM\" in architectures and load_format != \"pt\":\n            logger.info(\n                \"Currently, only 'pt' format is supported for Mixtral. \"\n                \"Changing the format to 'pt'. This may re-download the \"\n                \"weights if you have downloaded the safetensor weights.\")\n            load_format = \"pt\"\n\n        self.load_format = load_format\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto' or 'slow'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = [\"awq\", \"squeezellm\"]\n        rocm_not_supported_quantization = [\"awq\"]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        hf_quant_config = getattr(self.hf_config, \"quantization_config\", None)\n        if hf_quant_config is not None:\n            hf_quant_method = str(hf_quant_config[\"quant_method\"]).lower()\n            if self.quantization is None:\n                self.quantization = hf_quant_method\n            elif self.quantization != hf_quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({hf_quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            if is_hip(\n            ) and self.quantization in rocm_not_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not supported \"\n                    f\"in ROCm.\")\n            logger.warning(f\"{self.quantization} quantization is not fully \"\n                           \"optimized yet. The speed can be slower than \"\n                           \"non-quantized models.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = self.hf_config.num_attention_heads\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if total_num_hidden_layers % pipeline_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of hidden layers ({total_num_hidden_layers}) \"\n                \"must be divisible by pipeline parallel size \"\n                f\"({pipeline_parallel_size}).\")\n\n    def get_sliding_window(self) -> Optional[int]:\n        return getattr(self.hf_config, \"sliding_window\", None)\n\n    def get_vocab_size(self) -> int:\n        return self.hf_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_config.hidden_size\n\n    def get_head_size(self) -> int:\n        # FIXME(woosuk): This may not be true for all models.\n        return self.hf_config.hidden_size // self.hf_config.num_attention_heads\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\n        return total_num_hidden_layers // parallel_config.pipeline_parallel_size\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: int,\n        sliding_window: Optional[int] = None,\n    ) -> None:\n        self.block_size = block_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.swap_space_bytes = swap_space * _GB\n        self.sliding_window = sliding_window\n        self._verify_args()\n\n        # Will be set after profiling.\n        self.num_gpu_blocks = None\n        self.num_cpu_blocks = None\n\n    def _verify_args(self) -> None:\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / _GB:.2f} GiB out of \"\n               f\"the {total_cpu_memory / _GB:.2f} GiB total CPU memory is \"\n               \"allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. \" + msg)\n\n\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\n\n    Args:\n        pipeline_parallel_size: Number of pipeline parallel groups.\n        tensor_parallel_size: Number of tensor parallel groups.\n        worker_use_ray: Whether to use Ray for model workers. Will be set to\n            True if either pipeline_parallel_size or tensor_parallel_size is\n            greater than 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_parallel_size: int,\n        tensor_parallel_size: int,\n        worker_use_ray: bool,\n        max_parallel_loading_workers: Optional[int] = None,\n    ) -> None:\n        self.pipeline_parallel_size = pipeline_parallel_size\n        self.tensor_parallel_size = tensor_parallel_size\n        self.worker_use_ray = worker_use_ray\n        self.max_parallel_loading_workers = max_parallel_loading_workers\n\n        self.world_size = pipeline_parallel_size * tensor_parallel_size\n        if self.world_size > 1:\n            self.worker_use_ray = True\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is not supported yet.\")\n\n\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\n\n    Args:\n        max_num_batched_tokens: Maximum number of tokens to be processed in\n            a single iteration.\n        max_num_seqs: Maximum number of sequences to be processed in a single\n            iteration.\n        max_model_len: Maximum length of a sequence (including prompt\n            and generated text).\n        max_paddings: Maximum number of paddings to be added to a batch.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_num_batched_tokens: Optional[int],\n        max_num_seqs: int,\n        max_model_len: int,\n        max_paddings: int,\n    ) -> None:\n        if max_num_batched_tokens is not None:\n            self.max_num_batched_tokens = max_num_batched_tokens\n        else:\n            # If max_model_len is too short, use 2048 as the default value for\n            # higher throughput.\n            self.max_num_batched_tokens = max(max_model_len, 2048)\n        self.max_num_seqs = max_num_seqs\n        self.max_model_len = max_model_len\n        self.max_paddings = max_paddings\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.max_num_batched_tokens < self.max_model_len:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE = [\"float\", \"float32\"]\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            if config_dtype == torch.float32:\n                # Following the common practice, we use float16 for float32\n                # models.\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    if is_hip() and torch_dtype == torch.float32:\n        rocm_supported_dtypes = [\n            k for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()\n            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)\n        ]\n        raise ValueError(f\"dtype \\'{dtype}\\' is not supported in ROCm. \"\n                         f\"Supported dtypes are {rocm_supported_dtypes}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(f\"Casting {config_dtype} to {torch_dtype}.\")\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    for key in possible_keys:\n        max_len_key = getattr(hf_config, key, None)\n        if max_len_key is not None:\n            derived_max_model_len = min(derived_max_model_len, max_len_key)\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            f\"{possible_keys}. Assuming the model's maximum length is \"\n            f\"{default_max_len}.\")\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        assert \"factor\" in rope_scaling\n        scaling_factor = rope_scaling[\"factor\"]\n        if rope_scaling[\"type\"] == \"yarn\":\n            derived_max_model_len = rope_scaling[\n                \"original_max_position_embeddings\"]\n        derived_max_model_len *= scaling_factor\n\n    if max_model_len is None:\n        max_model_len = derived_max_model_len\n    elif max_model_len > derived_max_model_len:\n        raise ValueError(\n            f\"User-specified max_model_len ({max_model_len}) is greater than \"\n            f\"the derived max_model_len ({max_len_key}={derived_max_model_len}\"\n            \" in model's config.json). This may lead to incorrect model \"\n            \"outputs or CUDA errors. Make sure the value is correct and \"\n            \"within the model context size.\")\n    return int(max_model_len)\n",
      "diff": "diff --git a/vllm/config.py b/vllm/config.py\nindex 6bafa73c7..eb1fee0f2 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -120,14 +120,16 @@ class ModelConfig:\n             if load_format == \"auto\":\n                 load_format = \"pt\"\n \n-        # FIXME(woosuk): This is a temporary hack. Support safetensor weights.\n+        # TODO: Remove this check once HF updates the pt weights of Mixtral.\n         architectures = getattr(self.hf_config, \"architectures\", [])\n-        if \"MixtralForCausalLM\" in architectures and load_format != \"pt\":\n-            logger.info(\n-                \"Currently, only 'pt' format is supported for Mixtral. \"\n-                \"Changing the format to 'pt'. This may re-download the \"\n-                \"weights if you have downloaded the safetensor weights.\")\n-            load_format = \"pt\"\n+        if \"MixtralForCausalLM\" in architectures:\n+            if load_format == \"pt\":\n+                raise ValueError(\n+                    \"Currently, the 'pt' format is not supported for Mixtral. \"\n+                    \"Please use the 'safetensors' format instead. \")\n+            elif load_format == \"auto\":\n+                # Do not fall back to pt weights.\n+                load_format = \"safetensors\"\n \n         self.load_format = load_format",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 8
    },
    {
      "file_path": "vllm/model_executor/models/__init__.py",
      "old_content": "import importlib\nfrom typing import List, Optional, Type\n\nimport torch.nn as nn\n\nfrom vllm.logger import init_logger\nfrom vllm.utils import is_hip\n\nlogger = init_logger(__name__)\n\n# Architecture -> (module, class).\n_MODELS = {\n    \"AquilaModel\": (\"aquila\", \"AquilaForCausalLM\"),\n    \"AquilaForCausalLM\": (\"aquila\", \"AquilaForCausalLM\"),  # AquilaChat2\n    \"BaiChuanForCausalLM\": (\"baichuan\", \"BaiChuanForCausalLM\"),  # baichuan-7b\n    \"BaichuanForCausalLM\": (\"baichuan\", \"BaichuanForCausalLM\"),  # baichuan-13b\n    \"BloomForCausalLM\": (\"bloom\", \"BloomForCausalLM\"),\n    \"ChatGLMModel\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"ChatGLMForConditionalGeneration\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"FalconForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"GPT2LMHeadModel\": (\"gpt2\", \"GPT2LMHeadModel\"),\n    \"GPTBigCodeForCausalLM\": (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n    \"GPTJForCausalLM\": (\"gpt_j\", \"GPTJForCausalLM\"),\n    \"GPTNeoXForCausalLM\": (\"gpt_neox\", \"GPTNeoXForCausalLM\"),\n    \"InternLMForCausalLM\": (\"internlm\", \"InternLMForCausalLM\"),\n    \"LlamaForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    # For decapoda-research/llama-*\n    \"LLaMAForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"MistralForCausalLM\": (\"mistral\", \"MistralForCausalLM\"),\n    \"MixtralForCausalLM\": (\"mixtral\", \"MixtralForCausalLM\"),\n    # transformers's mpt class has lower case\n    \"MptForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"MPTForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"OPTForCausalLM\": (\"opt\", \"OPTForCausalLM\"),\n    \"PhiForCausalLM\": (\"phi_1_5\", \"PhiForCausalLM\"),\n    \"QWenLMHeadModel\": (\"qwen\", \"QWenLMHeadModel\"),\n    \"RWForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"YiForCausalLM\": (\"yi\", \"YiForCausalLM\"),\n}\n\n# Models not supported by ROCm.\n_ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n\n# Models partially supported by ROCm.\n# Architecture -> Reason.\n_ROCM_PARTIALLY_SUPPORTED_MODELS = {\n    \"MistralForCausalLM\":\n    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n}\n\n\nclass ModelRegistry:\n\n    @staticmethod\n    def load_model_cls(model_arch: str) -> Optional[Type[nn.Module]]:\n        if model_arch not in _MODELS:\n            return None\n        if is_hip():\n            if model_arch in _ROCM_UNSUPPORTED_MODELS:\n                raise ValueError(\n                    f\"Model architecture {model_arch} is not supported by \"\n                    \"ROCm for now.\")\n            if model_arch in _ROCM_PARTIALLY_SUPPORTED_MODELS:\n                logger.warning(\n                    f\"Model architecture {model_arch} is partially supported \"\n                    \"by ROCm: \" + _ROCM_PARTIALLY_SUPPORTED_MODELS[model_arch])\n\n        module_name, model_cls_name = _MODELS[model_arch]\n        module = importlib.import_module(\n            f\"vllm.model_executor.models.{module_name}\")\n        return getattr(module, model_cls_name, None)\n\n    @staticmethod\n    def get_supported_archs() -> List[str]:\n        return list(_MODELS.keys())\n\n\n__all__ = [\n    \"ModelRegistry\",\n]\n",
      "diff": "diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 5596884f3..ab9a1636a 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -39,13 +39,15 @@ _MODELS = {\n }\n \n # Models not supported by ROCm.\n-_ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n+_ROCM_UNSUPPORTED_MODELS = []\n \n # Models partially supported by ROCm.\n # Architecture -> Reason.\n _ROCM_PARTIALLY_SUPPORTED_MODELS = {\n     \"MistralForCausalLM\":\n     \"Sliding window attention is not yet supported in ROCm's flash attention\",\n+    \"MixtralForCausalLM\":\n+    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n }",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/mixtral.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Mixtral model.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\nfrom transformers import MixtralConfig\n\ntry:\n    import megablocks.ops as ops\nexcept ImportError as e:\n    raise ImportError(\"MegaBlocks not found. \"\n                      \"Please install it by `pip install megablocks`.\") from e\ntry:\n    import stk\nexcept ImportError as e:\n    raise ImportError(\n        \"STK not found. \"\n        \"Please install it by `pip install stanford-stk`.\") from e\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.communication_op import (\n    tensor_model_parallel_all_reduce)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.model_executor.utils import set_weight_attrs\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\ndef promote_scalar(x: torch.Tensor) -> torch.Tensor:\n    return x.view(1) if len(x.size()) == 0 else x\n\n\nclass MixtralAttention(nn.Module):\n\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 num_kv_heads: int,\n                 max_position: int = 4096 * 32,\n                 rope_theta: float = 10000,\n                 sliding_window: Optional[int] = None) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.sliding_window = sliding_window\n\n        self.wqkv = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n        )\n        self.wo = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position,\n            base=int(self.rope_theta),\n            is_neox_style=False,  # weights not in HF format\n        )\n        self.attn = PagedAttention(\n            self.num_heads,\n            self.head_dim,\n            self.scaling,\n            num_kv_heads=self.num_kv_heads,\n            sliding_window=self.sliding_window,\n        )\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        qkv, _ = self.wqkv(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\n                                cache_event)\n        output, _ = self.wo(attn_output)\n        return output\n\n\nclass BlockSparseMoE(nn.Module):\n    \"\"\"\n    Built on the paper and library Megablocks as described in\n    https://arxiv.org/abs/2211.15841. This implementation is\n    strictly equivalent to standard MoE with full capacity (no\n    dropped tokens). It's faster since it formulates MoE operations\n    in terms of block-sparse operations to accomodate imbalanced\n    assignments of tokens to experts, whereas standard MoE either\n    (1) drop tokens at the cost of reduced performance or (2) set\n    capacity factor to number of experts and thus waste computation\n    and memory on padding.\n    \"\"\"\n\n    def __init__(self, hidden_dim: int, ffn_dim: int, num_experts: int,\n                 top_k: int):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.ffn_dim = ffn_dim\n        self.num_experts = num_experts\n        self.top_k = top_k\n\n        # gating\n        self.gate = nn.Linear(self.hidden_dim,\n                              self.num_experts,\n                              bias=False,\n                              device=torch.cuda.current_device())\n\n        tp_size = get_tensor_model_parallel_world_size()\n        assert self.ffn_dim % tp_size == 0\n        self.ffn_dim_per_partition = self.ffn_dim // tp_size\n        # merged expert weights, all of size  (ffn_dim * n_experts, model_dim)\n        self.w1 = nn.Parameter(\n            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n                        self.hidden_dim,\n                        device=torch.cuda.current_device()))\n        set_weight_attrs(self.w1, {\"weight_loader\": self.moe_weight_loader})\n        self.w2 = nn.Parameter(\n            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n                        self.hidden_dim,\n                        device=torch.cuda.current_device()))\n        set_weight_attrs(self.w2, {\"weight_loader\": self.moe_weight_loader})\n        self.w3 = nn.Parameter(\n            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n                        self.hidden_dim,\n                        device=torch.cuda.current_device()))\n        set_weight_attrs(self.w3, {\"weight_loader\": self.moe_weight_loader})\n\n        # Calculate the number of bits needed to represent the expert indices\n        # so that we can pass it to radix sort.\n        self.sort_end_bit = max(int(np.ceil(np.log2(self.num_experts))), 1)\n        self.blocking = 128\n        self.quantize_scatter_num_bits = -1\n\n        # Calculate the number of bits needed to represent the column indices\n        # in the intermediate sparse matrix.\n        max_column_index = (self.ffn_dim * self.num_experts) // self.blocking\n        self.transpose_sort_end_bit = max(\n            int(np.ceil(np.log2(max_column_index))), 1)\n\n    def moe_weight_loader(self, param: nn.Parameter,\n                          loaded_weight: torch.Tensor) -> None:\n        \"\"\"\n        Load the weights for the MoE linear layer.\n        \"\"\"\n        tp_rank = get_tensor_model_parallel_rank()\n        shard_size = self.ffn_dim_per_partition\n        loaded_weight = loaded_weight.view(self.num_experts, self.ffn_dim, -1)\n        loaded_weight = loaded_weight[:, shard_size * tp_rank:shard_size *\n                                      (tp_rank + 1)]\n        loaded_weight = loaded_weight.reshape_as(param)\n        param.data.copy_(loaded_weight)\n\n    def sparse_transpose(\n            self, size: int, row_indices,\n            column_indices) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        block_columns = size[1] // self.blocking\n\n        # Sort row indices by column indices to get the transposed matrix's\n        # column indices.\n        #\n        # NOTE: Our sort operation uses the same width indices as the input\n        # values. To avoid overflow when we have large activation matrices\n        # we cast to 32-bit before sorting.\n        _, gather_indices = ops.sort(column_indices.int(),\n                                     self.transpose_sort_end_bit)\n\n        # There are a constant number of blocks in every row of the sparse\n        # matrix. A blocks offset is:\n        #\n        # row_index * blocks_per_row + column_index % blocks_per_row\n        #\n        # Once we have the block offsets ordered for transposition we can\n        # divide by blocks_per_row to get the transposed column indices.\n        column_indices_t = row_indices.gather(0, gather_indices.long())\n        block_offsets_t = gather_indices.int()\n\n        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)\n        nnz_per_column = ops.histogram(column_indices, block_columns)\n        nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)\n        offsets_t = torch.cat([zero, nnz_per_column])\n        return column_indices_t, offsets_t, block_offsets_t\n\n    def topology(self, x: torch.Tensor,\n                 padded_bins: torch.Tensor) -> \"stk.Matrix\":\n        padded_tokens, _ = x.size()\n        assert padded_tokens % self.blocking == 0\n        assert self.ffn_dim_per_partition % self.blocking == 0\n\n        # Offsets for the sparse matrix. All rows have the\n        # same number of nonzero blocks dictated by the\n        # dimensionality of a single expert.\n        block_rows = padded_tokens // self.blocking\n        blocks_per_row = self.ffn_dim_per_partition // self.blocking\n        offsets = torch.arange(\n            0,\n            block_rows * blocks_per_row + 1,\n            blocks_per_row,\n            dtype=torch.int32,\n            device=x.device,\n        )\n\n        # Indices for the sparse matrix. The indices for\n        # the intermediate matrix are dynamic depending\n        # on the mapping of tokens to experts.\n        column_indices = ops.topology(padded_bins, self.blocking, block_rows,\n                                      blocks_per_row)\n\n        # TODO(tgale): This is unused. Remove the need for this in stk.\n        # For now, use meta init to save the device memory.\n        data = torch.empty(\n            column_indices.numel(),\n            self.blocking,\n            self.blocking,\n            dtype=x.dtype,\n            device=\"meta\",\n        )\n        shape = (padded_tokens, self.ffn_dim_per_partition * self.num_experts)\n        row_indices = stk.ops.row_indices(shape, data, offsets, column_indices)\n        column_indices_t, offsets_t, block_offsets_t = self.sparse_transpose(\n            shape, row_indices, column_indices)\n        return stk.Matrix(\n            shape,\n            data,\n            row_indices,\n            column_indices,\n            offsets,\n            column_indices_t,\n            offsets_t,\n            block_offsets_t,\n        )\n\n    def indices_and_padded_bins(\n        self, selected_experts: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n               torch.Tensor]:\n        # Sort the expert ids to produce the scatter/gather\n        # indices for the permutation.\n        selected_experts = selected_experts.int()\n        bin_ids, indices = ops.sort(selected_experts, self.sort_end_bit)\n\n        # Histogram the expert ids to identify the number of\n        # tokens routed to each expert.\n        tokens_per_expert = ops.histogram(selected_experts, self.num_experts)\n\n        # Round the token counts up to the block size used in\n        # the matrix muliplications. Caculate the starting\n        # position of each bin.\n        padded_tokens_per_expert = ops.round_up(tokens_per_expert,\n                                                self.blocking)\n        padded_bins = ops.inclusive_cumsum(padded_tokens_per_expert, 0)\n        padded_bins = promote_scalar(padded_bins)\n\n        # Calculate the bin bounds for the sorted tokens.\n        bins = ops.inclusive_cumsum(tokens_per_expert, 0)\n        bins = promote_scalar(bins)\n        return indices, bin_ids, bins, padded_bins, tokens_per_expert\n\n    @torch.inference_mode()\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: (sequence_length, model_dim)\n        gate_logits: (sequence_length, n_experts)\n        \"\"\"\n        # optional reshape\n        input_shape = x.shape\n        x = x.view(-1, input_shape[-1])\n\n        # gate_logits: (sequence_length, n_experts)\n        gate_logits = self.gate(x)\n        # all_probs: (sequence_length, n_experts) and upcast for softmax\n        all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)\n        # weights, selected_experts: (sequence_length, top-k)\n        weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)\n        weights /= weights.sum(dim=-1, keepdim=True)\n        weights = weights.flatten().to(x.dtype)\n        selected_experts = selected_experts.flatten()\n\n        (indices, bin_ids, bins, padded_bins,\n         _) = self.indices_and_padded_bins(selected_experts)\n\n        # Permute tokens and pad to prepare expert computation\n        # (top_k * sequence_length + padding, model_dim)\n        x = ops.padded_gather(x, indices, bin_ids, bins, padded_bins,\n                              self.top_k)\n\n        # Create the sparse matrix topology\n        with torch.no_grad():\n            topo = self.topology(x, padded_bins)\n\n        # Perform the expert computation\n        # First Dense x Dense -> Sparse for w1 and w3,\n        # (top_k * sequence_length + padding, ffn_dim * n_experts)\n        x = stk.Matrix(\n            topo.size(),\n            F.silu(stk.ops.sdd(x, self.w1.t(), topo).data) *\n            stk.ops.sdd(x, self.w3.t(), topo).data,\n            topo.row_indices,\n            topo.column_indices,\n            topo.offsets,\n            topo.column_indices_t,\n            topo.offsets_t,\n            topo.block_offsets_t,\n        )\n\n        # Then Sparse x Dense -> Dense for w2\n        # (top_k * sequence_length + padding, model_dim)\n        x = stk.ops.dsd(x, self.w2)\n\n        x = tensor_model_parallel_all_reduce(x)\n\n        # Permute back and remove padding\n        # (top_k * sequence_length, model_dim)\n        x = ops.padded_scatter(\n            x,\n            indices,\n            bin_ids,\n            weights,\n            bins,\n            padded_bins,\n            self.top_k,\n            self.quantize_scatter_num_bits,\n        )\n        return x.view(*input_shape)\n\n\nclass MixtralDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        # Requires transformers > 4.32.0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        self.attention = MixtralAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            max_position=config.max_position_embeddings,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            sliding_window=config.sliding_window)\n        self.block_sparse_moe = BlockSparseMoE(\n            hidden_dim=self.hidden_size,\n            ffn_dim=config.intermediate_size,\n            num_experts=config.num_local_experts,\n            top_k=config.num_experts_per_tok,\n        )\n        self.attention_norm = RMSNorm(config.hidden_size,\n                                      eps=config.rms_norm_eps)\n        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        x: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        r = self.attention(\n            positions=positions,\n            hidden_states=self.attention_norm(x),\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n            cache_event=cache_event,\n        )\n        h = x + r\n        r = self.block_sparse_moe(self.ffn_norm(h))\n        out = h + r\n        return out\n\n\nclass MixtralForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        assert linear_method is None\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.tok_embeddings = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.output = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n        self.layers = nn.ModuleList([\n            MixtralDecoderLayer(config)\n            for _ in range(config.num_hidden_layers)\n        ])\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> SamplerOutput:\n        hidden_states = self.tok_embeddings(input_ids)\n\n        # forward\n        for i in range(len(self.layers)):\n            cache_event = None if cache_events is None else cache_events[i]\n            layer = self.layers[i]\n            hidden_states = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                cache_event,\n            )\n        hidden_states = self.norm(hidden_states)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: Optional[torch.Tensor],\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.output.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"wqkv\", \"wq\", \"q\"),\n            (\"wqkv\", \"wk\", \"k\"),\n            (\"wqkv\", \"wv\", \"v\"),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                param = params_dict[name.replace(weight_name, param_name)]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 8e0a094c7..b11e3713f 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -31,22 +31,11 @@ import torch.nn.functional as F\n from torch import nn\n from transformers import MixtralConfig\n \n-try:\n-    import megablocks.ops as ops\n-except ImportError as e:\n-    raise ImportError(\"MegaBlocks not found. \"\n-                      \"Please install it by `pip install megablocks`.\") from e\n-try:\n-    import stk\n-except ImportError as e:\n-    raise ImportError(\n-        \"STK not found. \"\n-        \"Please install it by `pip install stanford-stk`.\") from e\n-\n from vllm.model_executor.input_metadata import InputMetadata\n from vllm.model_executor.layers.attention import PagedAttention\n from vllm.model_executor.layers.layernorm import RMSNorm\n from vllm.model_executor.layers.linear import (LinearMethodBase,\n+                                               ReplicatedLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.rotary_embedding import get_rope\n@@ -66,8 +55,134 @@ from vllm.sequence import SamplerOutput\n KVCache = Tuple[torch.Tensor, torch.Tensor]\n \n \n-def promote_scalar(x: torch.Tensor) -> torch.Tensor:\n-    return x.view(1) if len(x.size()) == 0 else x\n+class MixtralMLP(nn.Module):\n+\n+    def __init__(\n+        self,\n+        num_experts: int,\n+        hidden_size: int,\n+        intermediate_size: int,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.num_experts = num_experts\n+        self.ffn_dim = intermediate_size\n+        self.hidden_dim = hidden_size\n+\n+        self.w1 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w2 = ReplicatedLinear(self.ffn_dim,\n+                                   self.hidden_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w3 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+\n+        # TODO: Use vllm's SiluAndMul\n+        self.act_fn = nn.SiLU()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        w1_out, _ = self.w1(hidden_states)\n+        w1_out = self.act_fn(w1_out)\n+        w3_out, _ = self.w3(hidden_states)\n+        current_hidden_states = w1_out * w3_out\n+        current_hidden_states, _ = self.w2(current_hidden_states)\n+        return current_hidden_states\n+\n+\n+class DummyModule(nn.Module):\n+\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+        self.w1 = nn.Linear(0, 0, bias=False)\n+        self.w2 = nn.Linear(0, 0, bias=False)\n+        self.w3 = nn.Linear(0, 0, bias=False)\n+\n+        set_weight_attrs(self.w1.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w2.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w3.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+\n+    def forward(self, *args, **kwargs) -> None:\n+        raise NotImplementedError()\n+\n+    def dummy_weight_loader(self, *args, **kwargs) -> None:  # pylint: disable=unused-argument\n+        # Noop\n+        return\n+\n+\n+class MixtralMoE(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.rank = get_tensor_model_parallel_rank()\n+        self.tp_size = get_tensor_model_parallel_world_size()\n+        self.num_total_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+        if self.tp_size > self.num_total_experts:\n+            raise ValueError(\n+                f\"Tensor parallel size {self.tp_size} is greater than \"\n+                f\"the number of experts {self.num_total_experts}.\")\n+        # Split experts equally between ranks\n+        self.expert_indicies = np.array_split(range(\n+            self.num_total_experts), self.tp_size)[self.rank].tolist()\n+        if not self.expert_indicies:\n+            raise ValueError(\n+                f\"Rank {self.rank} has no experts assigned to it.\")\n+\n+        self.experts = nn.ModuleList([\n+            MixtralMLP(self.num_total_experts,\n+                       config.hidden_size,\n+                       config.intermediate_size,\n+                       linear_method=linear_method)\n+            if idx in self.expert_indicies else DummyModule()\n+            for idx in range(self.num_total_experts)\n+        ])\n+        self.gate = ReplicatedLinear(config.hidden_size,\n+                                     self.num_total_experts,\n+                                     bias=False,\n+                                     linear_method=linear_method)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits, _ = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights,\n+                                                       self.top_k,\n+                                                       dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+\n+        final_hidden_states = None\n+        for expert_idx in self.expert_indicies:\n+            expert_layer = self.experts[expert_idx]\n+            expert_mask = (selected_experts == expert_idx)\n+            expert_weights = (routing_weights * expert_mask).sum(dim=-1,\n+                                                                 keepdim=True)\n+\n+            current_hidden_states = expert_layer(hidden_states).mul_(\n+                expert_weights)\n+            if final_hidden_states is None:\n+                final_hidden_states = current_hidden_states\n+            else:\n+                final_hidden_states.add_(current_hidden_states)\n+\n+        return tensor_model_parallel_all_reduce(final_hidden_states).view(\n+            batch_size, sequence_length, hidden_dim)\n \n \n class MixtralAttention(nn.Module):\n@@ -78,6 +193,7 @@ class MixtralAttention(nn.Module):\n                  num_kv_heads: int,\n                  max_position: int = 4096 * 32,\n                  rope_theta: float = 10000,\n+                 linear_method: Optional[LinearMethodBase] = None,\n                  sliding_window: Optional[int] = None) -> None:\n         super().__init__()\n         self.hidden_size = hidden_size\n@@ -102,24 +218,26 @@ class MixtralAttention(nn.Module):\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n \n-        self.wqkv = QKVParallelLinear(\n+        self.qkv_proj = QKVParallelLinear(\n             hidden_size,\n             self.head_dim,\n             self.total_num_heads,\n             self.total_num_kv_heads,\n             bias=False,\n+            linear_method=linear_method,\n         )\n-        self.wo = RowParallelLinear(\n+        self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             hidden_size,\n             bias=False,\n+            linear_method=linear_method,\n         )\n         self.rotary_emb = get_rope(\n             self.head_dim,\n             rotary_dim=self.head_dim,\n             max_position=max_position,\n             base=int(self.rope_theta),\n-            is_neox_style=False,  # weights not in HF format\n+            is_neox_style=True,\n         )\n         self.attn = PagedAttention(\n             self.num_heads,\n@@ -137,310 +255,74 @@ class MixtralAttention(nn.Module):\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:\n-        qkv, _ = self.wqkv(hidden_states)\n+        qkv, _ = self.qkv_proj(hidden_states)\n         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n         q, k = self.rotary_emb(positions, q, k)\n         k_cache, v_cache = kv_cache\n         attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\n                                 cache_event)\n-        output, _ = self.wo(attn_output)\n+        output, _ = self.o_proj(attn_output)\n         return output\n \n \n-class BlockSparseMoE(nn.Module):\n-    \"\"\"\n-    Built on the paper and library Megablocks as described in\n-    https://arxiv.org/abs/2211.15841. This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-    \"\"\"\n-\n-    def __init__(self, hidden_dim: int, ffn_dim: int, num_experts: int,\n-                 top_k: int):\n-        super().__init__()\n-        self.hidden_dim = hidden_dim\n-        self.ffn_dim = ffn_dim\n-        self.num_experts = num_experts\n-        self.top_k = top_k\n-\n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim,\n-                              self.num_experts,\n-                              bias=False,\n-                              device=torch.cuda.current_device())\n-\n-        tp_size = get_tensor_model_parallel_world_size()\n-        assert self.ffn_dim % tp_size == 0\n-        self.ffn_dim_per_partition = self.ffn_dim // tp_size\n-        # merged expert weights, all of size  (ffn_dim * n_experts, model_dim)\n-        self.w1 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w1, {\"weight_loader\": self.moe_weight_loader})\n-        self.w2 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w2, {\"weight_loader\": self.moe_weight_loader})\n-        self.w3 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w3, {\"weight_loader\": self.moe_weight_loader})\n-\n-        # Calculate the number of bits needed to represent the expert indices\n-        # so that we can pass it to radix sort.\n-        self.sort_end_bit = max(int(np.ceil(np.log2(self.num_experts))), 1)\n-        self.blocking = 128\n-        self.quantize_scatter_num_bits = -1\n-\n-        # Calculate the number of bits needed to represent the column indices\n-        # in the intermediate sparse matrix.\n-        max_column_index = (self.ffn_dim * self.num_experts) // self.blocking\n-        self.transpose_sort_end_bit = max(\n-            int(np.ceil(np.log2(max_column_index))), 1)\n-\n-    def moe_weight_loader(self, param: nn.Parameter,\n-                          loaded_weight: torch.Tensor) -> None:\n-        \"\"\"\n-        Load the weights for the MoE linear layer.\n-        \"\"\"\n-        tp_rank = get_tensor_model_parallel_rank()\n-        shard_size = self.ffn_dim_per_partition\n-        loaded_weight = loaded_weight.view(self.num_experts, self.ffn_dim, -1)\n-        loaded_weight = loaded_weight[:, shard_size * tp_rank:shard_size *\n-                                      (tp_rank + 1)]\n-        loaded_weight = loaded_weight.reshape_as(param)\n-        param.data.copy_(loaded_weight)\n-\n-    def sparse_transpose(\n-            self, size: int, row_indices,\n-            column_indices) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        block_columns = size[1] // self.blocking\n-\n-        # Sort row indices by column indices to get the transposed matrix's\n-        # column indices.\n-        #\n-        # NOTE: Our sort operation uses the same width indices as the input\n-        # values. To avoid overflow when we have large activation matrices\n-        # we cast to 32-bit before sorting.\n-        _, gather_indices = ops.sort(column_indices.int(),\n-                                     self.transpose_sort_end_bit)\n-\n-        # There are a constant number of blocks in every row of the sparse\n-        # matrix. A blocks offset is:\n-        #\n-        # row_index * blocks_per_row + column_index % blocks_per_row\n-        #\n-        # Once we have the block offsets ordered for transposition we can\n-        # divide by blocks_per_row to get the transposed column indices.\n-        column_indices_t = row_indices.gather(0, gather_indices.long())\n-        block_offsets_t = gather_indices.int()\n-\n-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)\n-        nnz_per_column = ops.histogram(column_indices, block_columns)\n-        nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)\n-        offsets_t = torch.cat([zero, nnz_per_column])\n-        return column_indices_t, offsets_t, block_offsets_t\n-\n-    def topology(self, x: torch.Tensor,\n-                 padded_bins: torch.Tensor) -> \"stk.Matrix\":\n-        padded_tokens, _ = x.size()\n-        assert padded_tokens % self.blocking == 0\n-        assert self.ffn_dim_per_partition % self.blocking == 0\n-\n-        # Offsets for the sparse matrix. All rows have the\n-        # same number of nonzero blocks dictated by the\n-        # dimensionality of a single expert.\n-        block_rows = padded_tokens // self.blocking\n-        blocks_per_row = self.ffn_dim_per_partition // self.blocking\n-        offsets = torch.arange(\n-            0,\n-            block_rows * blocks_per_row + 1,\n-            blocks_per_row,\n-            dtype=torch.int32,\n-            device=x.device,\n-        )\n-\n-        # Indices for the sparse matrix. The indices for\n-        # the intermediate matrix are dynamic depending\n-        # on the mapping of tokens to experts.\n-        column_indices = ops.topology(padded_bins, self.blocking, block_rows,\n-                                      blocks_per_row)\n-\n-        # TODO(tgale): This is unused. Remove the need for this in stk.\n-        # For now, use meta init to save the device memory.\n-        data = torch.empty(\n-            column_indices.numel(),\n-            self.blocking,\n-            self.blocking,\n-            dtype=x.dtype,\n-            device=\"meta\",\n-        )\n-        shape = (padded_tokens, self.ffn_dim_per_partition * self.num_experts)\n-        row_indices = stk.ops.row_indices(shape, data, offsets, column_indices)\n-        column_indices_t, offsets_t, block_offsets_t = self.sparse_transpose(\n-            shape, row_indices, column_indices)\n-        return stk.Matrix(\n-            shape,\n-            data,\n-            row_indices,\n-            column_indices,\n-            offsets,\n-            column_indices_t,\n-            offsets_t,\n-            block_offsets_t,\n-        )\n-\n-    def indices_and_padded_bins(\n-        self, selected_experts: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n-               torch.Tensor]:\n-        # Sort the expert ids to produce the scatter/gather\n-        # indices for the permutation.\n-        selected_experts = selected_experts.int()\n-        bin_ids, indices = ops.sort(selected_experts, self.sort_end_bit)\n-\n-        # Histogram the expert ids to identify the number of\n-        # tokens routed to each expert.\n-        tokens_per_expert = ops.histogram(selected_experts, self.num_experts)\n-\n-        # Round the token counts up to the block size used in\n-        # the matrix muliplications. Caculate the starting\n-        # position of each bin.\n-        padded_tokens_per_expert = ops.round_up(tokens_per_expert,\n-                                                self.blocking)\n-        padded_bins = ops.inclusive_cumsum(padded_tokens_per_expert, 0)\n-        padded_bins = promote_scalar(padded_bins)\n-\n-        # Calculate the bin bounds for the sorted tokens.\n-        bins = ops.inclusive_cumsum(tokens_per_expert, 0)\n-        bins = promote_scalar(bins)\n-        return indices, bin_ids, bins, padded_bins, tokens_per_expert\n-\n-    @torch.inference_mode()\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n-        x: (sequence_length, model_dim)\n-        gate_logits: (sequence_length, n_experts)\n-        \"\"\"\n-        # optional reshape\n-        input_shape = x.shape\n-        x = x.view(-1, input_shape[-1])\n-\n-        # gate_logits: (sequence_length, n_experts)\n-        gate_logits = self.gate(x)\n-        # all_probs: (sequence_length, n_experts) and upcast for softmax\n-        all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)\n-        # weights, selected_experts: (sequence_length, top-k)\n-        weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)\n-        weights /= weights.sum(dim=-1, keepdim=True)\n-        weights = weights.flatten().to(x.dtype)\n-        selected_experts = selected_experts.flatten()\n-\n-        (indices, bin_ids, bins, padded_bins,\n-         _) = self.indices_and_padded_bins(selected_experts)\n-\n-        # Permute tokens and pad to prepare expert computation\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = ops.padded_gather(x, indices, bin_ids, bins, padded_bins,\n-                              self.top_k)\n-\n-        # Create the sparse matrix topology\n-        with torch.no_grad():\n-            topo = self.topology(x, padded_bins)\n-\n-        # Perform the expert computation\n-        # First Dense x Dense -> Sparse for w1 and w3,\n-        # (top_k * sequence_length + padding, ffn_dim * n_experts)\n-        x = stk.Matrix(\n-            topo.size(),\n-            F.silu(stk.ops.sdd(x, self.w1.t(), topo).data) *\n-            stk.ops.sdd(x, self.w3.t(), topo).data,\n-            topo.row_indices,\n-            topo.column_indices,\n-            topo.offsets,\n-            topo.column_indices_t,\n-            topo.offsets_t,\n-            topo.block_offsets_t,\n-        )\n-\n-        # Then Sparse x Dense -> Dense for w2\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = stk.ops.dsd(x, self.w2)\n-\n-        x = tensor_model_parallel_all_reduce(x)\n-\n-        # Permute back and remove padding\n-        # (top_k * sequence_length, model_dim)\n-        x = ops.padded_scatter(\n-            x,\n-            indices,\n-            bin_ids,\n-            weights,\n-            bins,\n-            padded_bins,\n-            self.top_k,\n-            self.quantize_scatter_num_bits,\n-        )\n-        return x.view(*input_shape)\n-\n-\n class MixtralDecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         # Requires transformers > 4.32.0\n         rope_theta = getattr(config, \"rope_theta\", 10000)\n-        self.attention = MixtralAttention(\n+        self.self_attn = MixtralAttention(\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n             max_position=config.max_position_embeddings,\n             num_kv_heads=config.num_key_value_heads,\n             rope_theta=rope_theta,\n-            sliding_window=config.sliding_window)\n-        self.block_sparse_moe = BlockSparseMoE(\n-            hidden_dim=self.hidden_size,\n-            ffn_dim=config.intermediate_size,\n-            num_experts=config.num_local_experts,\n-            top_k=config.num_experts_per_tok,\n-        )\n-        self.attention_norm = RMSNorm(config.hidden_size,\n-                                      eps=config.rms_norm_eps)\n-        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+            sliding_window=config.sliding_window,\n+            linear_method=linear_method)\n+        self.block_sparse_moe = MixtralMoE(config=config,\n+                                           linear_method=linear_method)\n+        self.input_layernorm = RMSNorm(config.hidden_size,\n+                                       eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n+                                                eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         positions: torch.Tensor,\n-        x: torch.Tensor,\n+        hidden_states: torch.Tensor,\n         kv_cache: KVCache,\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n+        residual: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n-        r = self.attention(\n+        # Self Attention\n+        if residual is None:\n+            residual = hidden_states\n+            hidden_states = self.input_layernorm(hidden_states)\n+        else:\n+            hidden_states, residual = self.input_layernorm(\n+                hidden_states, residual)\n+        hidden_states = self.self_attn(\n             positions=positions,\n-            hidden_states=self.attention_norm(x),\n+            hidden_states=hidden_states,\n             kv_cache=kv_cache,\n             input_metadata=input_metadata,\n             cache_event=cache_event,\n         )\n-        h = x + r\n-        r = self.block_sparse_moe(self.ffn_norm(h))\n-        out = h + r\n-        return out\n \n+        # Fully Connected\n+        hidden_states, residual = self.post_attention_layernorm(\n+            hidden_states, residual)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n+        return hidden_states, residual\n \n-class MixtralForCausalLM(nn.Module):\n+\n+class MixtralModel(nn.Module):\n \n     def __init__(\n         self,\n@@ -448,23 +330,18 @@ class MixtralForCausalLM(nn.Module):\n         linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n-        self.config = config\n-        assert linear_method is None\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n-        self.tok_embeddings = VocabParallelEmbedding(\n+\n+        self.embed_tokens = VocabParallelEmbedding(\n             config.vocab_size,\n             config.hidden_size,\n         )\n-\n-        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.output = ParallelLMHead(config.vocab_size, config.hidden_size)\n-        self.sampler = Sampler(config.vocab_size)\n-\n         self.layers = nn.ModuleList([\n-            MixtralDecoderLayer(config)\n+            MixtralDecoderLayer(config, linear_method=linear_method)\n             for _ in range(config.num_hidden_layers)\n         ])\n+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -474,20 +351,42 @@ class MixtralForCausalLM(nn.Module):\n         input_metadata: InputMetadata,\n         cache_events: Optional[List[torch.cuda.Event]],\n     ) -> SamplerOutput:\n-        hidden_states = self.tok_embeddings(input_ids)\n-\n-        # forward\n+        hidden_states = self.embed_tokens(input_ids)\n+        residual = None\n         for i in range(len(self.layers)):\n             cache_event = None if cache_events is None else cache_events[i]\n             layer = self.layers[i]\n-            hidden_states = layer(\n-                positions,\n-                hidden_states,\n-                kv_caches[i],\n-                input_metadata,\n-                cache_event,\n-            )\n-        hidden_states = self.norm(hidden_states)\n+            hidden_states, residual = layer(positions, hidden_states,\n+                                            kv_caches[i], input_metadata,\n+                                            cache_event, residual)\n+        hidden_states, _ = self.norm(hidden_states, residual)\n+        return hidden_states\n+\n+\n+class MixtralForCausalLM(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.linear_method = linear_method\n+        self.model = MixtralModel(config, linear_method)\n+        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n+        self.sampler = Sampler(config.vocab_size)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        positions: torch.Tensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n+    ) -> torch.Tensor:\n+        hidden_states = self.model(input_ids, positions, kv_caches,\n+                                   input_metadata, cache_events)\n         return hidden_states\n \n     def sample(\n@@ -495,7 +394,7 @@ class MixtralForCausalLM(nn.Module):\n         hidden_states: Optional[torch.Tensor],\n         sampling_metadata: SamplingMetadata,\n     ) -> SamplerOutput:\n-        next_tokens = self.sampler(self.output.weight, hidden_states,\n+        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens\n \n@@ -506,10 +405,11 @@ class MixtralForCausalLM(nn.Module):\n                      revision: Optional[str] = None):\n         stacked_params_mapping = [\n             # (param_name, shard_name, shard_id)\n-            (\"wqkv\", \"wq\", \"q\"),\n-            (\"wqkv\", \"wk\", \"k\"),\n-            (\"wqkv\", \"wv\", \"v\"),\n+            (\"qkv_proj\", \"q_proj\", \"q\"),\n+            (\"qkv_proj\", \"k_proj\", \"k\"),\n+            (\"qkv_proj\", \"v_proj\", \"v\"),\n         ]\n+\n         params_dict = dict(self.named_parameters())\n         for name, loaded_weight in hf_model_weights_iterator(\n                 model_name_or_path, cache_dir, load_format, revision):",
      "change_type": "modified",
      "lines_added": 208,
      "lines_removed": 308
    }
  ],
  "affected_apis": [
    "vllm.config.ModelConfig",
    "vllm.model_executor.models.mixtral.MixtralAttention.__init__",
    "vllm.model_executor.models.mixtral.MixtralMoE",
    "vllm.model_executor.models.mixtral.MixtralMLP",
    "vllm.model_executor.models.mixtral.BlockSparseMoE"
  ],
  "summary": {
    "total_files": 6,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 6
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_mixtral, test_expert_parallel)",
    "is_benchmark_actually_there": "",
    "sample_clues": "__init__, config, dockerfile"
  }
}