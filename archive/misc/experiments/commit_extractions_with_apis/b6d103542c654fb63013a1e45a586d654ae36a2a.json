{
  "commit_hash": "b6d103542c654fb63013a1e45a586d654ae36a2a",
  "parent_hash": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447",
  "message": "[Kernel] Layernorm performance optimization (#3662)",
  "author": "mawong-amd <156021403+mawong-amd@users.noreply.github.com>",
  "date": "2024-03-30 14:26:38 -0700",
  "files_changed": [
    {
      "file_path": "cmake/utils.cmake",
      "old_content": "#\n# Attempt to find the python package that uses the same python executable as\n# `EXECUTABLE` and is one of the `SUPPORTED_VERSIONS`.\n#\nmacro (find_python_from_executable EXECUTABLE SUPPORTED_VERSIONS)\n  file(REAL_PATH ${EXECUTABLE} EXECUTABLE)\n  set(Python_EXECUTABLE ${EXECUTABLE})\n  find_package(Python COMPONENTS Interpreter Development.Module)\n  if (NOT Python_FOUND)\n    message(FATAL_ERROR \"Unable to find python matching: ${EXECUTABLE}.\")\n  endif()\n  set(_VER \"${Python_VERSION_MAJOR}.${Python_VERSION_MINOR}\")\n  set(_SUPPORTED_VERSIONS_LIST ${SUPPORTED_VERSIONS} ${ARGN})\n  if (NOT _VER IN_LIST _SUPPORTED_VERSIONS_LIST)\n    message(FATAL_ERROR\n      \"Python version (${_VER}) is not one of the supported versions: \"\n      \"${_SUPPORTED_VERSIONS_LIST}.\")\n  endif()\n  message(STATUS \"Found python matching: ${EXECUTABLE}.\")\nendmacro()\n\n#\n# Run `EXPR` in python.  The standard output of python is stored in `OUT` and\n# has trailing whitespace stripped.  If an error is encountered when running\n# python, a fatal message `ERR_MSG` is issued.\n#\nfunction (run_python OUT EXPR ERR_MSG)\n  execute_process(\n    COMMAND\n    \"${Python_EXECUTABLE}\" \"-c\" \"${EXPR}\"\n    OUTPUT_VARIABLE PYTHON_OUT\n    RESULT_VARIABLE PYTHON_ERROR_CODE\n    ERROR_VARIABLE PYTHON_STDERR\n    OUTPUT_STRIP_TRAILING_WHITESPACE)\n\n  if(NOT PYTHON_ERROR_CODE EQUAL 0)\n    message(FATAL_ERROR \"${ERR_MSG}: ${PYTHON_STDERR}\")\n  endif()\n  set(${OUT} ${PYTHON_OUT} PARENT_SCOPE)\nendfunction()\n\n# Run `EXPR` in python after importing `PKG`. Use the result of this to extend\n# `CMAKE_PREFIX_PATH` so the torch cmake configuration can be imported.\nmacro (append_cmake_prefix_path PKG EXPR)\n  run_python(_PREFIX_PATH\n    \"import ${PKG}; print(${EXPR})\" \"Failed to locate ${PKG} path\")\n  list(APPEND CMAKE_PREFIX_PATH ${_PREFIX_PATH})\nendmacro()\n\n#\n# Add a target named `hipify${NAME}` that runs the hipify preprocessor on a set\n# of CUDA source files. The names of the corresponding \"hipified\" sources are\n# stored in `OUT_SRCS`.\n#\nfunction (hipify_sources_target OUT_SRCS NAME ORIG_SRCS)\n  #\n  # Split into C++ and non-C++ (i.e. CUDA) sources.\n  #\n  set(SRCS ${ORIG_SRCS})\n  set(CXX_SRCS ${ORIG_SRCS})\n  list(FILTER SRCS EXCLUDE REGEX \"\\.(cc)|(cpp)$\")\n  list(FILTER CXX_SRCS INCLUDE REGEX \"\\.(cc)|(cpp)$\")\n\n  #\n  # Generate ROCm/HIP source file names from CUDA file names.\n  # Since HIP files are generated code, they will appear in the build area\n  # `CMAKE_CURRENT_BINARY_DIR` directory rather than the original csrc dir.\n  #\n  set(HIP_SRCS)\n  foreach (SRC ${SRCS})\n    string(REGEX REPLACE \"\\.cu$\" \"\\.hip\" SRC ${SRC})\n    string(REGEX REPLACE \"cuda\" \"hip\" SRC ${SRC})\n    list(APPEND HIP_SRCS \"${CMAKE_CURRENT_BINARY_DIR}/${SRC}\")\n  endforeach()\n\n  set(CSRC_BUILD_DIR ${CMAKE_CURRENT_BINARY_DIR}/csrc)\n  add_custom_target(\n    hipify${NAME}\n    COMMAND ${CMAKE_SOURCE_DIR}/cmake/hipify.py -p ${CMAKE_SOURCE_DIR}/csrc -o ${CSRC_BUILD_DIR} ${SRCS}\n    DEPENDS ${CMAKE_SOURCE_DIR}/cmake/hipify.py ${SRCS}\n    BYPRODUCTS ${HIP_SRCS}\n    COMMENT \"Running hipify on ${NAME} extension source files.\")\n\n  # Swap out original extension sources with hipified sources.\n  list(APPEND HIP_SRCS ${CXX_SRCS})\n  set(${OUT_SRCS} ${HIP_SRCS} PARENT_SCOPE)\nendfunction()\n\n#\n# Get additional GPU compiler flags from torch.\n#\nfunction (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n  if (${GPU_LANG} STREQUAL \"CUDA\")\n    #\n    # Get common NVCC flags from torch.\n    #\n    run_python(GPU_FLAGS\n      \"from torch.utils.cpp_extension import COMMON_NVCC_FLAGS; print(';'.join(COMMON_NVCC_FLAGS))\"\n      \"Failed to determine torch nvcc compiler flags\")\n\n    if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n      list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n    endif()\n\n  elseif(${GPU_LANG} STREQUAL \"HIP\")\n    #\n    # Get common HIP/HIPCC flags from torch.\n    #\n    run_python(GPU_FLAGS\n      \"import torch.utils.cpp_extension as t; print(';'.join(t.COMMON_HIP_FLAGS + t.COMMON_HIPCC_FLAGS))\"\n      \"Failed to determine torch nvcc compiler flags\")\n\n    list(APPEND GPU_FLAGS\n      \"-DUSE_ROCM\"\n      \"-U__HIP_NO_HALF_CONVERSIONS__\"\n      \"-U__HIP_NO_HALF_OPERATORS__\"\n      \"-fno-gpu-rdc\")\n\n  endif()\n  set(${OUT_GPU_FLAGS} ${GPU_FLAGS} PARENT_SCOPE)\nendfunction()\n\n# Macro for converting a `gencode` version number to a cmake version number.\nmacro(string_to_ver OUT_VER IN_STR)\n  string(REGEX REPLACE \"\\([0-9]+\\)\\([0-9]\\)\" \"\\\\1.\\\\2\" ${OUT_VER} ${IN_STR})\nendmacro()\n\n#\n# Override the GPU architectures detected by cmake/torch and filter them by\n# `GPU_SUPPORTED_ARCHES`. Sets the final set of architectures in\n# `GPU_ARCHES`.\n#\n# Note: this is defined as a macro since it updates `CMAKE_CUDA_FLAGS`.\n#\nmacro(override_gpu_arches GPU_ARCHES GPU_LANG GPU_SUPPORTED_ARCHES)\n  set(_GPU_SUPPORTED_ARCHES_LIST ${GPU_SUPPORTED_ARCHES} ${ARGN})\n  message(STATUS \"${GPU_LANG} supported arches: ${_GPU_SUPPORTED_ARCHES_LIST}\")\n\n  if (${GPU_LANG} STREQUAL \"HIP\")\n    #\n    # `GPU_ARCHES` controls the `--offload-arch` flags.\n    # `CMAKE_HIP_ARCHITECTURES` is set up by torch and can be controlled\n    # via the `PYTORCH_ROCM_ARCH` env variable.\n    #\n\n    #\n    # Find the intersection of the supported + detected architectures to\n    # set the module architecture flags.\n    #\n    set(${GPU_ARCHES})\n    foreach (_ARCH ${CMAKE_HIP_ARCHITECTURES})\n      if (_ARCH IN_LIST _GPU_SUPPORTED_ARCHES_LIST)\n        list(APPEND ${GPU_ARCHES} ${_ARCH})\n      endif()\n    endforeach()\n\n    if(NOT ${GPU_ARCHES})\n      message(FATAL_ERROR\n        \"None of the detected ROCm architectures: ${CMAKE_HIP_ARCHITECTURES} is\"\n        \" supported. Supported ROCm architectures are: ${_GPU_SUPPORTED_ARCHES_LIST}.\")\n    endif()\n\n  elseif(${GPU_LANG} STREQUAL \"CUDA\")\n    #\n    # Setup/process CUDA arch flags.\n    #\n    # The torch cmake setup hardcodes the detected architecture flags in\n    # `CMAKE_CUDA_FLAGS`.  Since `CMAKE_CUDA_FLAGS` is a \"global\" variable, it\n    # can't modified on a per-target basis, e.g. for the `punica` extension.\n    # So, all the `-gencode` flags need to be extracted and removed from\n    # `CMAKE_CUDA_FLAGS` for processing so they can be passed by another method.\n    # Since it's not possible to use `target_compiler_options` for adding target\n    # specific `-gencode` arguments, the target's `CUDA_ARCHITECTURES` property\n    # must be used instead.  This requires repackaging the architecture flags\n    # into a format that cmake expects for `CUDA_ARCHITECTURES`.\n    #\n    # This is a bit fragile in that it depends on torch using `-gencode` as opposed\n    # to one of the other nvcc options to specify architectures.\n    #\n    # Note: torch uses the `TORCH_CUDA_ARCH_LIST` environment variable to override\n    # detected architectures.\n    #\n    message(DEBUG \"initial CMAKE_CUDA_FLAGS: ${CMAKE_CUDA_FLAGS}\")\n\n    # Extract all `-gencode` flags from `CMAKE_CUDA_FLAGS`\n    string(REGEX MATCHALL \"-gencode arch=[^ ]+\" _CUDA_ARCH_FLAGS\n      ${CMAKE_CUDA_FLAGS})\n\n    # Remove all `-gencode` flags from `CMAKE_CUDA_FLAGS` since they will be modified\n    # and passed back via the `CUDA_ARCHITECTURES` property.\n    string(REGEX REPLACE \"-gencode arch=[^ ]+ *\" \"\" CMAKE_CUDA_FLAGS\n      ${CMAKE_CUDA_FLAGS})\n\n    # If this error is triggered, it might mean that torch has changed how it sets\n    # up nvcc architecture code generation flags.\n    if (NOT _CUDA_ARCH_FLAGS)\n      message(FATAL_ERROR\n        \"Could not find any architecture related code generation flags in \"\n        \"CMAKE_CUDA_FLAGS. (${CMAKE_CUDA_FLAGS})\")\n    endif()\n\n    message(DEBUG \"final CMAKE_CUDA_FLAGS: ${CMAKE_CUDA_FLAGS}\")\n    message(DEBUG \"arch flags: ${_CUDA_ARCH_FLAGS}\")\n\n    # Initialize the architecture lists to empty.\n    set(${GPU_ARCHES})\n\n    # Process each `gencode` flag.\n    foreach(_ARCH ${_CUDA_ARCH_FLAGS})\n      # For each flag, extract the version number and whether it refers to PTX\n      # or native code.\n      # Note: if a regex matches then `CMAKE_MATCH_1` holds the binding\n      # for that match.\n\n      string(REGEX MATCH \"arch=compute_\\([0-9]+a?\\)\" _COMPUTE ${_ARCH})\n      if (_COMPUTE)\n        set(_COMPUTE ${CMAKE_MATCH_1})\n      endif()\n\n      string(REGEX MATCH \"code=sm_\\([0-9]+a?\\)\" _SM ${_ARCH})\n      if (_SM)\n        set(_SM ${CMAKE_MATCH_1})\n      endif()\n\n      string(REGEX MATCH \"code=compute_\\([0-9]+a?\\)\" _CODE ${_ARCH})\n      if (_CODE)\n        set(_CODE ${CMAKE_MATCH_1})\n      endif()\n\n      # Make sure the virtual architecture can be matched.\n      if (NOT _COMPUTE)\n        message(FATAL_ERROR\n          \"Could not determine virtual architecture from: ${_ARCH}.\")\n      endif()\n\n      # One of sm_ or compute_ must exist.\n      if ((NOT _SM) AND (NOT _CODE))\n        message(FATAL_ERROR\n          \"Could not determine a codegen architecture from: ${_ARCH}.\")\n      endif()\n\n      if (_SM)\n        # -real suffix let CMake to only generate elf code for the kernels.\n        # we want this, otherwise the added ptx (default) will increase binary size.\n        set(_VIRT \"-real\")\n        set(_CODE_ARCH ${_SM})\n      else()\n        # -virtual suffix let CMake to generate ptx code for the kernels.\n        set(_VIRT \"-virtual\")\n        set(_CODE_ARCH ${_CODE})\n      endif()\n\n      # Check if the current version is in the supported arch list.\n      string_to_ver(_CODE_VER ${_CODE_ARCH})\n      if (NOT _CODE_VER IN_LIST _GPU_SUPPORTED_ARCHES_LIST)\n        message(STATUS \"discarding unsupported CUDA arch ${_VER}.\")\n        continue()\n      endif()\n\n      # Add it to the arch list.\n      list(APPEND ${GPU_ARCHES} \"${_CODE_ARCH}${_VIRT}\")\n    endforeach()\n  endif()\n  message(STATUS \"${GPU_LANG} target arches: ${${GPU_ARCHES}}\")\nendmacro()\n\n#\n# Define a target named `GPU_MOD_NAME` for a single extension. The\n# arguments are:\n#\n# DESTINATION <dest>         - Module destination directory.\n# LANGUAGE <lang>            - The GPU language for this module, e.g CUDA, HIP,\n#                              etc.\n# SOURCES <sources>          - List of source files relative to CMakeLists.txt\n#                              directory.\n#\n# Optional arguments:\n#\n# ARCHITECTURES <arches>     - A list of target GPU architectures in cmake\n#                              format.\n#                              Refer `CMAKE_CUDA_ARCHITECTURES` documentation\n#                              and `CMAKE_HIP_ARCHITECTURES` for more info.\n#                              ARCHITECTURES will use cmake's defaults if\n#                              not provided.\n# COMPILE_FLAGS <flags>      - Extra compiler flags passed to NVCC/hip.\n# INCLUDE_DIRECTORIES <dirs> - Extra include directories.\n# LIBRARIES <libraries>      - Extra link libraries.\n# WITH_SOABI                 - Generate library with python SOABI suffix name.\n#\n# Note: optimization level/debug info is set via cmake build type.\n#\nfunction (define_gpu_extension_target GPU_MOD_NAME)\n  cmake_parse_arguments(PARSE_ARGV 1\n    GPU\n    \"WITH_SOABI\"\n    \"DESTINATION;LANGUAGE\"\n    \"SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES\")\n\n  # Add hipify preprocessing step when building with HIP/ROCm.\n  if (GPU_LANGUAGE STREQUAL \"HIP\")\n    hipify_sources_target(GPU_SOURCES ${GPU_MOD_NAME} \"${GPU_SOURCES}\")\n  endif()\n\n  if (GPU_WITH_SOABI)\n    set(GPU_WITH_SOABI WITH_SOABI)\n  else()\n    set(GPU_WITH_SOABI)\n  endif()\n\n  Python_add_library(${GPU_MOD_NAME} MODULE \"${GPU_SOURCES}\" ${GPU_WITH_SOABI})\n\n  if (GPU_LANGUAGE STREQUAL \"HIP\")\n    # Make this target dependent on the hipify preprocessor step.\n    add_dependencies(${GPU_MOD_NAME} hipify${GPU_MOD_NAME})\n  endif()\n\n  if (GPU_ARCHITECTURES)\n    set_target_properties(${GPU_MOD_NAME} PROPERTIES\n      ${GPU_LANGUAGE}_ARCHITECTURES \"${GPU_ARCHITECTURES}\")\n  endif()\n\n  set_property(TARGET ${GPU_MOD_NAME} PROPERTY CXX_STANDARD 17)\n\n  target_compile_options(${GPU_MOD_NAME} PRIVATE\n    $<$<COMPILE_LANGUAGE:${GPU_LANGUAGE}>:${GPU_COMPILE_FLAGS}>)\n\n  target_compile_definitions(${GPU_MOD_NAME} PRIVATE\n    \"-DTORCH_EXTENSION_NAME=${GPU_MOD_NAME}\")\n\n  target_include_directories(${GPU_MOD_NAME} PRIVATE csrc\n    ${GPU_INCLUDE_DIRECTORIES})\n\n  target_link_libraries(${GPU_MOD_NAME} PRIVATE torch ${torch_python_LIBRARY}\n    ${GPU_LIBRARIES})\n\n  # Don't use `TORCH_LIBRARIES` for CUDA since it pulls in a bunch of\n  # dependencies that are not necessary and may not be installed.\n  if (GPU_LANGUAGE STREQUAL \"CUDA\")\n    target_link_libraries(${GPU_MOD_NAME} PRIVATE ${CUDA_CUDA_LIB}\n      ${CUDA_LIBRARIES})\n  else()\n    target_link_libraries(${GPU_MOD_NAME} PRIVATE ${TORCH_LIBRARIES})\n  endif()\n\n  install(TARGETS ${GPU_MOD_NAME} LIBRARY DESTINATION ${GPU_DESTINATION})\nendfunction()\n",
      "diff": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex 6bf5d5130..c7d3d8538 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+      list(REMOVE_ITEM GPU_FLAGS\n+        \"-D__CUDA_NO_HALF_OPERATORS__\"\n+        \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+        \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\"\n+        \"-D__CUDA_NO_HALF2_OPERATORS__\")\n     endif()\n \n   elseif(${GPU_LANG} STREQUAL \"HIP\")",
      "change_type": "modified",
      "lines_added": 6,
      "lines_removed": 1
    },
    {
      "file_path": "csrc/layernorm_kernels.cu",
      "old_content": "#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n#include <c10/cuda/CUDAGuard.h>\n\n#include \"dispatch_utils.h\"\n#include \"reduction_utils.cuh\"\n\nnamespace vllm {\n\n// TODO(woosuk): Further optimize this kernel.\ntemplate<typename scalar_t>\n__global__ void rms_norm_kernel(\n  scalar_t* __restrict__ out,             // [..., hidden_size]\n  const scalar_t* __restrict__ input,     // [..., hidden_size]\n  const scalar_t* __restrict__ weight,    // [hidden_size]\n  const float epsilon,\n  const int num_tokens,\n  const int hidden_size) {\n  __shared__ float s_variance;\n  float variance = 0.0f;\n\n  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n    const float x = (float) input[blockIdx.x * hidden_size + idx];\n    variance += x * x;\n  }\n  variance = blockReduceSum<float>(variance);\n  if (threadIdx.x == 0) {\n    s_variance = rsqrtf(variance / hidden_size + epsilon);\n  }\n  __syncthreads();\n\n  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n    float x = (float) input[blockIdx.x * hidden_size + idx];\n    out[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n  }\n}\n\n// TODO: Further optimize this kernel.\ntemplate<typename scalar_t>\n__global__ void fused_add_rms_norm_kernel(\n  scalar_t* __restrict__ input,           // [..., hidden_size]\n  scalar_t* __restrict__ residual,        // [..., hidden_size]\n  const scalar_t* __restrict__ weight,    // [hidden_size]\n  const float epsilon,\n  const int num_tokens,\n  const int hidden_size) {\n  __shared__ float s_variance;\n  float variance = 0.0f;\n\n  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n    float x = (float) input[blockIdx.x * hidden_size + idx];\n    x += (float) residual[blockIdx.x * hidden_size + idx];\n    variance += x * x;\n    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n  }\n  variance = blockReduceSum<float>(variance);\n  if (threadIdx.x == 0) {\n    s_variance = rsqrtf(variance / hidden_size + epsilon);\n  }\n  __syncthreads();\n\n  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n    float x = (float) residual[blockIdx.x * hidden_size + idx];\n    input[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n  }\n}\n\n} // namespace vllm\n\nvoid rms_norm(\n  torch::Tensor& out,      // [..., hidden_size]\n  torch::Tensor& input,    // [..., hidden_size]\n  torch::Tensor& weight,   // [hidden_size]\n  float epsilon) {\n  int hidden_size = input.size(-1);\n  int num_tokens = input.numel() / hidden_size;\n\n  dim3 grid(num_tokens);\n  dim3 block(std::min(hidden_size, 1024));\n  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  VLLM_DISPATCH_FLOATING_TYPES(\n    input.scalar_type(),\n    \"rms_norm_kernel\",\n    [&] {\n      vllm::rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n        out.data_ptr<scalar_t>(),\n        input.data_ptr<scalar_t>(),\n        weight.data_ptr<scalar_t>(),\n        epsilon,\n        num_tokens,\n        hidden_size);\n    });\n}\n\nvoid fused_add_rms_norm(\n  torch::Tensor& input,    // [..., hidden_size]\n  torch::Tensor& residual, // [..., hidden_size]\n  torch::Tensor& weight,   // [hidden_size]\n  float epsilon) {\n  int hidden_size = input.size(-1);\n  int num_tokens = input.numel() / hidden_size;\n\n  dim3 grid(num_tokens);\n  dim3 block(std::min(hidden_size, 1024));\n  const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  VLLM_DISPATCH_FLOATING_TYPES(\n    input.scalar_type(),\n    \"fused_add_rms_norm_kernel\",\n    [&] {\n      vllm::fused_add_rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n        input.data_ptr<scalar_t>(),\n        residual.data_ptr<scalar_t>(),\n        weight.data_ptr<scalar_t>(),\n        epsilon,\n        num_tokens,\n        hidden_size);\n    });\n}\n",
      "diff": "diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex 6d34d014c..ea30fa274 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -4,6 +4,16 @@\n \n #include \"dispatch_utils.h\"\n #include \"reduction_utils.cuh\"\n+#ifndef USE_ROCM\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n+#else\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n+\n+  using __nv_bfloat16 = __hip_bfloat16;\n+  using __nv_bfloat162 = __hip_bfloat162;\n+#endif\n \n namespace vllm {\n \n@@ -35,9 +45,199 @@ __global__ void rms_norm_kernel(\n   }\n }\n \n-// TODO: Further optimize this kernel.\n-template<typename scalar_t>\n-__global__ void fused_add_rms_norm_kernel(\n+\n+/* Converter structs for the conversion from torch types to HIP/CUDA types,\n+   and the associated type conversions within HIP/CUDA. These helpers need\n+   to be implemented for now because the relevant type conversion\n+   operators/constructors are not consistently implemented by HIP/CUDA, so\n+   a generic conversion via type casts cannot be implemented.\n+\n+   Each struct should have the member static constexpr bool `exists`:\n+   If false, the optimized kernel is not used for the corresponding torch type.\n+   If true, the struct should be fully defined as shown in the examples below. \n+ */\n+template<typename torch_type>\n+struct _typeConvert { static constexpr bool exists = false; };\n+\n+template<>\n+struct _typeConvert<c10::Half> {\n+  static constexpr bool exists = true;\n+  using hip_type = __half;\n+  using packed_hip_type = __half2;\n+\n+  __device__ static inline float convert(hip_type x) { return __half2float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __half22float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2half_rn(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22half2_rn(x); }\n+};\n+\n+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800\n+// CUDA_ARCH < 800 does not have BF16 support\n+// TODO: Add in ROCm support once public headers handle bf16 maturely\n+template<>\n+struct _typeConvert<c10::BFloat16> {\n+  static constexpr bool exists = true;\n+  using hip_type = __nv_bfloat16;\n+  using packed_hip_type = __nv_bfloat162;\n+\n+  __device__ static inline float convert(hip_type x) { return __bfloat162float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __bfloat1622float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2bfloat16(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22bfloat162_rn(x); }\n+};\n+#endif\n+\n+\n+/* Vector POD struct to generate vectorized and packed FP16/BF16 ops\n+   for appropriate specializations of fused_add_rms_norm_kernel.\n+   Only functions that are necessary in that kernel are implemented.\n+   Alignment to 16 bytes is required to use 128-bit global memory ops.\n+ */\n+template<typename scalar_t, int width>\n+struct alignas(16) _f16Vec {\n+  /* Not theoretically necessary that width is a power of 2 but should \n+     almost always be the case for optimization purposes */ \n+  static_assert(width > 0 && (width & (width - 1)) == 0,\n+                \"Width is not a positive power of 2!\");\n+  using Converter = _typeConvert<scalar_t>;\n+  using T1 = typename Converter::hip_type;\n+  using T2 = typename Converter::packed_hip_type;\n+  T1 data[width];\n+\n+  __device__ _f16Vec& operator+=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp += T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] += other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp *= T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] *= other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const float scale) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 temp_f = Converter::convert(T2{data[i], data[i+1]});\n+        temp_f.x *= scale;\n+        temp_f.y *= scale;\n+        T2 temp = Converter::convert(temp_f);\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float temp = Converter::convert(data[i]) * scale;\n+        data[i] = Converter::convert(temp);\n+      }\n+    }\n+    return *this;\n+  }\n+\n+  __device__ float sum_squares() const {\n+    float result = 0.0f;\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 z = Converter::convert(T2{data[i], data[i+1]});\n+        result += z.x * z.x + z.y * z.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float x = Converter::convert(data[i]);\n+        result += x * x;\n+      }\n+    }\n+    return result;\n+  }\n+};\n+\n+/* Function specialization in the case of FP16/BF16 tensors.\n+   Additional optimizations we can make in this case are\n+   packed and vectorized operations, which help with the\n+   memory latency bottleneck. */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width > 0) && _typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n+  scalar_t* __restrict__ input,           // [..., hidden_size]\n+  scalar_t* __restrict__ residual,        // [..., hidden_size]\n+  const scalar_t* __restrict__ weight,    // [hidden_size]\n+  const float epsilon,\n+  const int num_tokens,\n+  const int hidden_size) {\n+  // Sanity checks on our vector struct and type-punned pointer arithmetic\n+  static_assert(std::is_pod_v<_f16Vec<scalar_t, width>>);\n+  static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);\n+\n+  const int vec_hidden_size = hidden_size / width;\n+  __shared__ float s_variance;\n+  float variance = 0.0f;\n+  /* These and the argument pointers are all declared `restrict` as they are\n+     not aliased in practice. Argument pointers should not be dereferenced\n+     in this kernel as that would be undefined behavior */\n+  auto* __restrict__ input_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(input);\n+  auto* __restrict__ residual_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(residual);\n+  auto* __restrict__ weight_v = reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = input_v[id];\n+    temp += residual_v[id];\n+    variance += temp.sum_squares();\n+    residual_v[id] = temp;\n+  }\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n+  if (threadIdx.x == 0) {\n+    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+  }\n+  __syncthreads();\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = residual_v[id];\n+    temp *= s_variance;\n+    temp *= weight_v[idx];\n+    input_v[id] = temp;\n+  }\n+}\n+\n+\n+/* Generic fused_add_rms_norm_kernel\n+   The width field is not used here but necessary for other specializations.\n+ */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width == 0) || !_typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n   scalar_t* __restrict__ input,           // [..., hidden_size]\n   scalar_t* __restrict__ residual,        // [..., hidden_size]\n   const scalar_t* __restrict__ weight,    // [hidden_size]\n@@ -48,12 +248,17 @@ __global__ void fused_add_rms_norm_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    x += (float) residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[blockIdx.x * hidden_size + idx];\n+    z += residual[blockIdx.x * hidden_size + idx];\n+    float x = (float) z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n+    residual[blockIdx.x * hidden_size + idx] = z;\n   }\n-  variance = blockReduceSum<float>(variance);\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n   }\n@@ -93,6 +298,21 @@ void rms_norm(\n     });\n }\n \n+#define LAUNCH_FUSED_ADD_RMS_NORM(width)              \\\n+  VLLM_DISPATCH_FLOATING_TYPES(                       \\\n+    input.scalar_type(),                              \\\n+    \"fused_add_rms_norm_kernel\",                      \\\n+    [&] {                                             \\\n+      vllm::fused_add_rms_norm_kernel                 \\\n+      <scalar_t, width><<<grid, block, 0, stream>>>(  \\\n+        input.data_ptr<scalar_t>(),                   \\\n+        residual.data_ptr<scalar_t>(),                \\\n+        weight.data_ptr<scalar_t>(),                  \\\n+        epsilon,                                      \\\n+        num_tokens,                                   \\\n+        hidden_size);                                 \\\n+    });\n+\n void fused_add_rms_norm(\n   torch::Tensor& input,    // [..., hidden_size]\n   torch::Tensor& residual, // [..., hidden_size]\n@@ -102,19 +322,29 @@ void fused_add_rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  /* This kernel is memory-latency bound in many scenarios.\n+     When num_tokens is large, a smaller block size allows\n+     for increased block occupancy on CUs and better latency\n+     hiding on global mem ops. */\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-  VLLM_DISPATCH_FLOATING_TYPES(\n-    input.scalar_type(),\n-    \"fused_add_rms_norm_kernel\",\n-    [&] {\n-      vllm::fused_add_rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        input.data_ptr<scalar_t>(),\n-        residual.data_ptr<scalar_t>(),\n-        weight.data_ptr<scalar_t>(),\n-        epsilon,\n-        num_tokens,\n-        hidden_size);\n-    });\n+  /*If the tensor types are FP16/BF16, try to use the optimized kernel\n+    with packed + vectorized ops.\n+    Max optimization is achieved with a width-8 vector of FP16/BF16s\n+    since we can load at most 128 bits at once in a global memory op.\n+    However, this requires each tensor's data to be aligned to 16\n+    bytes.\n+   */\n+  auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());\n+  auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());\n+  auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());\n+  bool ptrs_are_aligned = inp_ptr % 16 == 0 && res_ptr % 16 == 0 \\\n+                          && wt_ptr % 16 == 0;\n+  if (ptrs_are_aligned && hidden_size % 8 == 0) {\n+    LAUNCH_FUSED_ADD_RMS_NORM(8);\n+  } else {\n+    LAUNCH_FUSED_ADD_RMS_NORM(0);\n+  }\n }",
      "change_type": "modified",
      "lines_added": 251,
      "lines_removed": 21
    },
    {
      "file_path": "csrc/reduction_utils.cuh",
      "old_content": "/*\n * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/reduce_kernel_utils.cuh\n * Copyright (c) 2023, The vLLM team.\n * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n#pragma once\n\n#include \"cuda_compat.h\"\n\nnamespace vllm {\n\ntemplate<typename T>\n__inline__ __device__ T warpReduceSum(T val) {\n#pragma unroll\n  for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1)\n    val += VLLM_SHFL_XOR_SYNC(val, mask);\n  return val;\n}\n\n__inline__ __device__ constexpr int _calculateLaneMask(int warp_size) {\n  return warp_size - 1;\n}\n\n__inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n  return 5 + (warp_size >> 6);\n}\n\n/* Calculate the sum of all elements in a block */\ntemplate<typename T>\n__inline__ __device__ T blockReduceSum(T val) {\n  static __shared__ T shared[WARP_SIZE];\n  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n  int lane = threadIdx.x & LANE_MASK;\n  int wid = threadIdx.x >> WID_SHIFT;\n\n  val = warpReduceSum<T>(val);\n\n  if (lane == 0)\n    shared[wid] = val;\n\n  __syncthreads();\n\n  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n  // blockDim.x is not divided by 32\n  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n  val = warpReduceSum<T>(val);\n  return val;\n}\n\n} // namespace vllm\n",
      "diff": "diff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nindex c25464e86..bb5171f85 100644\n--- a/csrc/reduction_utils.cuh\n+++ b/csrc/reduction_utils.cuh\n@@ -20,43 +20,45 @@\n #include \"cuda_compat.h\"\n \n namespace vllm {\n-\n-template<typename T>\n+template<typename T, int numLanes = WARP_SIZE>\n __inline__ __device__ T warpReduceSum(T val) {\n-#pragma unroll\n-  for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1)\n+  static_assert(numLanes > 0 && (numLanes & (numLanes - 1)) == 0,\n+                \"numLanes is not a positive power of 2!\");\n+  static_assert(numLanes <= WARP_SIZE);\n+  #pragma unroll\n+  for (int mask = numLanes >> 1; mask > 0; mask >>= 1)\n     val += VLLM_SHFL_XOR_SYNC(val, mask);\n   return val;\n }\n \n-__inline__ __device__ constexpr int _calculateLaneMask(int warp_size) {\n-  return warp_size - 1;\n-}\n-\n-__inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n-  return 5 + (warp_size >> 6);\n+// Helper function to return the next largest power of 2\n+static constexpr int _nextPow2(unsigned int num) {\n+  if (num <= 1) return num;\n+  return 1 << (CHAR_BIT * sizeof(num) - __builtin_clz(num - 1));\n }\n \n /* Calculate the sum of all elements in a block */\n-template<typename T>\n+template<typename T, int maxBlockSize = 1024>\n __inline__ __device__ T blockReduceSum(T val) {\n-  static __shared__ T shared[WARP_SIZE];\n-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n-  int lane = threadIdx.x & LANE_MASK;\n-  int wid = threadIdx.x >> WID_SHIFT;\n-\n-  val = warpReduceSum<T>(val);\n-\n-  if (lane == 0)\n-    shared[wid] = val;\n+  static_assert(maxBlockSize <= 1024);\n+  if constexpr (maxBlockSize > WARP_SIZE) {\n+    val = warpReduceSum<T>(val);\n+    // Calculates max number of lanes that need to participate in the last warpReduce\n+    constexpr int maxActiveLanes = (maxBlockSize + WARP_SIZE - 1) / WARP_SIZE;\n+    static __shared__ T shared[maxActiveLanes];\n+    int lane = threadIdx.x % WARP_SIZE;\n+    int wid = threadIdx.x / WARP_SIZE;\n+    if (lane == 0)\n+      shared[wid] = val;\n \n-  __syncthreads();\n+    __syncthreads();\n \n-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n-  // blockDim.x is not divided by 32\n-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n-  val = warpReduceSum<T>(val);\n+    val = (threadIdx.x < blockDim.x / float(WARP_SIZE)) ? shared[lane] : (T)(0.0f);\n+    val = warpReduceSum<T, _nextPow2(maxActiveLanes)>(val);\n+  } else {\n+    // A single warpReduce is equal to blockReduce\n+    val = warpReduceSum<T, _nextPow2(maxBlockSize)>(val);\n+  }\n   return val;\n }",
      "change_type": "modified",
      "lines_added": 29,
      "lines_removed": 27
    },
    {
      "file_path": "tests/kernels/test_layernorm.py",
      "old_content": "import pytest\nimport torch\n\nfrom vllm.model_executor.layers.layernorm import RMSNorm\n\nDTYPES = [torch.half, torch.bfloat16, torch.float]\nNUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\nHIDDEN_SIZES = [768, 5120, 8192]  # Arbitrary values for testing\nADD_RESIDUAL = [False, True]\nSEEDS = [0]\nCUDA_DEVICES = [\n    f\"cuda:{i}\" for i in range(1 if torch.cuda.device_count() == 1 else 2)\n]\n\n\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\n@pytest.mark.parametrize(\"hidden_size\", HIDDEN_SIZES)\n@pytest.mark.parametrize(\"add_residual\", ADD_RESIDUAL)\n@pytest.mark.parametrize(\"dtype\", DTYPES)\n@pytest.mark.parametrize(\"seed\", SEEDS)\n@pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n@torch.inference_mode()\ndef test_rms_norm(\n    num_tokens: int,\n    hidden_size: int,\n    add_residual: bool,\n    dtype: torch.dtype,\n    seed: int,\n    device: str,\n) -> None:\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    torch.set_default_device(device)\n    layer = RMSNorm(hidden_size).to(dtype=dtype)\n    layer.weight.data.normal_(mean=1.0, std=0.1)\n    scale = 1 / (2 * hidden_size)\n    x = torch.randn(num_tokens, hidden_size, dtype=dtype)\n    x *= scale\n    residual = torch.randn_like(x) * scale if add_residual else None\n\n    # NOTE(woosuk): The reference implementation should be executed first\n    # because the custom kernel is in-place.\n    ref_out = layer._forward(x, residual)\n    out = layer(x, residual)\n    # NOTE(woosuk): LayerNorm operators (including RMS) typically have larger\n    # numerical errors than other operators because they involve reductions.\n    # Therefore, we use a larger tolerance.\n    if add_residual:\n        assert torch.allclose(out[0], ref_out[0], atol=1e-2, rtol=1e-2)\n        assert torch.allclose(out[1], ref_out[1], atol=1e-2, rtol=1e-2)\n    else:\n        assert torch.allclose(out, ref_out, atol=1e-2, rtol=1e-2)\n",
      "diff": "diff --git a/tests/kernels/test_layernorm.py b/tests/kernels/test_layernorm.py\nindex b1e3c1a7f..210d59e4f 100644\n--- a/tests/kernels/test_layernorm.py\n+++ b/tests/kernels/test_layernorm.py\n@@ -5,7 +5,8 @@ from vllm.model_executor.layers.layernorm import RMSNorm\n \n DTYPES = [torch.half, torch.bfloat16, torch.float]\n NUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\n-HIDDEN_SIZES = [768, 5120, 8192]  # Arbitrary values for testing\n+HIDDEN_SIZES = [768, 769, 770, 771, 5120, 5124, 5125, 5126, 8192,\n+                8199]  # Arbitrary values for testing\n ADD_RESIDUAL = [False, True]\n SEEDS = [0]\n CUDA_DEVICES = [",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    }
  ],
  "affected_apis": [
    "vllm.model_executor.layers.layernorm.RMSNorm"
  ],
  "summary": {
    "total_files": 4,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 4
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_layernorm)",
    "is_benchmark_actually_there": "",
    "sample_clues": "cuda, forward, layernorm_kernels"
  }
}