{
  "commit_hash": "ad932a221d2a4c1e6355021bb9e9c47f7a179e51",
  "parent_hash": "5510cf0e8a6a3ee56daefb86b145c7f2a000817f",
  "message": "[Core] Faster startup for LoRA enabled models (#4634)",
  "author": "Antoni Baum <antoni.baum@protonmail.com>",
  "date": "2024-05-08 10:33:18 -0700",
  "files_changed": [
    {
      "file_path": "vllm/lora/models.py",
      "old_content": "import copy\nimport json\nimport math\nimport os\nimport re\nfrom typing import Callable, Dict, List, Optional, Tuple, Type\n\nimport safetensors.torch\nimport torch\nfrom torch import nn\n\nfrom vllm.config import LoRAConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import BaseLayerWithLoRA, LoRAMapping\nfrom vllm.lora.lora import LoRALayerWeights, PackedLoRALayerWeights\nfrom vllm.lora.utils import (from_layer, from_layer_logits_processor,\n                             parse_fine_tuned_lora_name, replace_submodule)\nfrom vllm.utils import LRUCache, is_pin_memory_available\n\nlogger = init_logger(__name__)\n\n_GLOBAL_LORA_ID = 0\n\n\ndef convert_mapping(\n    mapping: LoRAMapping, lora_index_to_id: List[Optional[int]],\n    max_loras: int, vocab_size: int, extra_vocab_size: int\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, List[int]]:\n    \"\"\"Converts LoRAMapping to index tensors.\n\n    Args:\n        mapping: LoRAMapping mapping rows in a batch to LoRA ids.\n        lora_index_to_id: List mapping LoRA ids to LoRA indices.\n        max_loras: Maximum number of LoRAs.\n        vocab_size: Model vocab size.\n        extra_vocab_size: Extra vocab size each LoRA can have.\n\n    Returns:\n        A tuple of tensors:\n            base_indices: Tensor of shape [batch_size] mapping batch rows to\n                LoRA indices.\n            sampler_indices: Tensor of shape [batch_size] mapping requests to\n                LoRA indices for sampler. For generation, this will be the\n                same as base_indicies. For prefill, this will map requests\n                to LoRA indices.\n            sampler_indices_padded: Tensor of shape [batch_size] mapping\n                requests to LoRA indices for sampler with padding.\n                Same as sampler_indicies, but -1 is replaced with\n                max_loras.\n            embeddings_indices: Tensor of shape [2, batch_size] mapping\n                requests to embedding indices. First row is for embeddings\n                added by the LoRAs, second row is for the LoRA.lora_a\n                embeddings.\n            indices_len: List of lengths of the above tensors.\n    \"\"\"\n    index_mapping_indices: List[int] = list(mapping.index_mapping).copy()\n    embedding_indices = index_mapping_indices.copy()\n    lora_indices = index_mapping_indices.copy()\n    prompt_mapping: List[int] = [\n        lora_index_to_id.index(x) if x > 0 else -1\n        for x in mapping.prompt_mapping\n    ]\n    lora_idx = None\n    for i in range(len(index_mapping_indices)):\n        # TODO index can be slow. optimize\n        lora_idx = (lora_index_to_id.index(index_mapping_indices[i])\n                    if index_mapping_indices[i] > 0 else -1)\n        embedding_indices[i] = lora_idx if index_mapping_indices[i] > 0 else 0\n        index_mapping_indices[i] = i\n        lora_indices[i] = lora_idx\n\n    indices = torch.tensor(\n        [index_mapping_indices, lora_indices, embedding_indices],\n        dtype=torch.long,\n        device=\"cuda\")\n    prompt_mapping_tensor = torch.tensor(prompt_mapping,\n                                         device=\"cuda\",\n                                         dtype=torch.long)\n    embeddings_indices = torch.stack([\n        indices[2] * extra_vocab_size,\n        indices[2] * (vocab_size + extra_vocab_size)\n    ])\n    embeddings_indices[embeddings_indices == -1] = max_loras - 1\n    base_indices = indices[1]\n    sampler_indices = prompt_mapping_tensor\n    sampler_indices_padded = sampler_indices.clone()\n    sampler_indices_padded[sampler_indices_padded == -1] = max_loras - 1\n    sampler_indices_padded = (\n        torch.arange(\n            0, len(sampler_indices_padded), device=\"cuda\", dtype=torch.long) +\n        (sampler_indices_padded * len(sampler_indices_padded)))\n    indices_len = [\n        base_indices.shape[-1], sampler_indices.shape[-1],\n        sampler_indices_padded.shape[-1], embeddings_indices.shape[-1]\n    ]\n\n    return (base_indices, sampler_indices, sampler_indices_padded,\n            embeddings_indices, indices_len)\n\n\ndef get_lora_id():\n    global _GLOBAL_LORA_ID\n    _GLOBAL_LORA_ID += 1\n    return _GLOBAL_LORA_ID\n\n\nclass LoRAModel:\n    \"\"\"A LoRA fine-tuned model.\"\"\"\n\n    def __init__(\n        self,\n        lora_model_id: int,\n        rank: int,\n        loras: Dict[str, LoRALayerWeights],\n    ) -> None:\n        self.id = lora_model_id\n        assert (lora_model_id >\n                0), f\"a valid lora id should be greater than 0, got {self.id}\"\n        self.rank = rank\n        self.loras: Dict[str, LoRALayerWeights] = loras\n\n    @property\n    def extra_vocab_size(self) -> int:\n        return max(lora.extra_vocab_size\n                   for lora in self.loras.values()) if self.loras else 0\n\n    def get_lora(self, module_name: str) -> Optional[LoRALayerWeights]:\n        \"\"\"Get LoRA for a given module by name\"\"\"\n        return self.loras.get(module_name, None)\n\n    # (yard1): TODO see if we can derive target_embedding_padding automatically\n    @classmethod\n    def from_lora_tensors(\n        cls,\n        lora_model_id: int,\n        rank: int,\n        lora_alpha: int,\n        tensors: Dict[str, torch.Tensor],\n        device: str = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n        embeddings: Optional[Dict[str, torch.Tensor]] = None,\n        target_embedding_padding: Optional[int] = None,\n        embedding_modules: Optional[Dict[str, str]] = None,\n        embedding_padding_modules: Optional[List[str]] = None,\n    ) -> \"LoRAModel\":\n        \"\"\"Create a LoRAModel from a dictionary of tensors.\"\"\"\n        pin_memory = str(device) == \"cpu\" and is_pin_memory_available()\n        loras: Dict[str, LoRALayerWeights] = {}\n        for tensor_name, tensor in tensors.items():\n            module_name, is_lora_a = parse_fine_tuned_lora_name(tensor_name)\n            if module_name not in loras:\n                lora_embeddings_tensor = None\n                if embeddings:\n                    assert embedding_modules is not None\n                    embeddings_module = next(\n                        (k for k in embedding_modules if k in module_name),\n                        None)\n                    if embeddings_module:\n                        lora_embeddings_tensor = embeddings[\n                            embedding_modules[embeddings_module]].to(\n                                device=device, dtype=dtype)\n                        if pin_memory:\n                            lora_embeddings_tensor = (\n                                lora_embeddings_tensor.pin_memory())\n                loras[module_name] = LoRALayerWeights(module_name, rank,\n                                                      lora_alpha, None, None,\n                                                      lora_embeddings_tensor)\n            if is_lora_a:\n                loras[module_name].lora_a = tensor.to(device=device,\n                                                      dtype=dtype).t()\n                if pin_memory:\n                    loras[module_name].lora_a = loras[\n                        module_name].lora_a.pin_memory()\n            else:\n                loras[module_name].lora_b = tensor.to(device=device,\n                                                      dtype=dtype).t()\n                assert embedding_padding_modules is not None\n                if any(name in module_name\n                       for name in embedding_padding_modules\n                       ) and target_embedding_padding is not None:\n                    lora_b = loras[module_name].lora_b\n                    assert target_embedding_padding >= lora_b.shape[1]\n                    addition = target_embedding_padding - lora_b.shape[1]\n                    loras[module_name].lora_b = torch.nn.functional.pad(\n                        lora_b, (0, addition))\n                if pin_memory:\n                    loras[module_name].lora_b = loras[\n                        module_name].lora_b.pin_memory()\n\n        for lora in loras.values():\n            lora.optimize()\n        return cls(lora_model_id, rank, loras)\n\n    @classmethod\n    def from_local_checkpoint(\n        cls,\n        lora_dir: str,\n        expected_lora_modules: List[str],\n        lora_model_id: Optional[int] = None,\n        device: str = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n        target_embedding_padding: Optional[int] = None,\n        embedding_modules: Optional[Dict[str, str]] = None,\n        embedding_padding_modules: Optional[List[str]] = None,\n    ) -> \"LoRAModel\":\n        \"\"\"Create a LoRAModel from a local checkpoint.\"\"\"\n        lora_config_path = os.path.join(lora_dir, \"adapter_config.json\")\n        lora_tensor_path = os.path.join(lora_dir, \"adapter_model.safetensors\")\n        lora_bin_file_path = os.path.join(lora_dir, \"adapter_model.bin\")\n        new_embeddings_tensor_path = os.path.join(\n            lora_dir, \"new_embeddings.safetensors\")\n        new_embeddings_bin_file_path = os.path.join(lora_dir,\n                                                    \"new_embeddings.bin\")\n        with open(lora_config_path) as f:\n            config = json.load(f)\n        target_modules = config[\"target_modules\"]\n        unexpected_modules = []\n        for module in target_modules:\n            # Compatible with more modules, such as:layers.11.self_attn.k_proj\n            part_name = module.split(\".\")[-1]\n            if part_name not in expected_lora_modules:\n                unexpected_modules.append(module)\n        # loaded lora's target modules must be a subset of expected_lora_modules\n        if unexpected_modules:\n            raise ValueError(\n                f\"While loading {lora_dir}, expected\"\n                f\" target modules in {expected_lora_modules}\"\n                f\" but received {unexpected_modules}.\"\n                f\" Please verify that the loaded LoRA module is correct\")\n        if os.path.isfile(lora_tensor_path):\n            tensors = safetensors.torch.load_file(lora_tensor_path)\n        elif os.path.isfile(lora_bin_file_path):\n            tensors = torch.load(lora_bin_file_path)\n        else:\n            raise ValueError(f\"{lora_dir} doesn't contain tensors\")\n\n        embeddings = None\n        if os.path.isfile(new_embeddings_tensor_path):\n            embeddings = safetensors.torch.load_file(\n                new_embeddings_tensor_path)\n        elif os.path.isfile(new_embeddings_bin_file_path):\n            embeddings = torch.load(new_embeddings_bin_file_path)\n\n        rank = config[\"r\"]\n        lora_alpha = config[\"lora_alpha\"]\n        return cls.from_lora_tensors(\n            lora_model_id=get_lora_id()\n            if lora_model_id is None else lora_model_id,\n            rank=rank,\n            lora_alpha=lora_alpha,\n            tensors=tensors,\n            device=device,\n            dtype=dtype,\n            embeddings=embeddings,\n            target_embedding_padding=target_embedding_padding,\n            embedding_modules=embedding_modules,\n            embedding_padding_modules=embedding_padding_modules,\n        )\n\n\nclass LoRAModelManager:\n    \"\"\"A manager that manages multiple LoRA-fine-tuned models.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n    ):\n        \"\"\"Create a LoRAModelManager and adapter for a given model.\n\n        Args:\n            model: the model to be adapted.\n            max_num_seqs: the maximum number of sequences model can run in a\n                single batch.\n            max_num_batched_tokens: the maximum number of tokens model can run\n                in a single batch.\n            vocab_size: the vocab size of the model.\n            lora_config: the LoRA configuration.\n        \"\"\"\n        self.lora_config = lora_config\n        self.max_num_seqs = max_num_seqs\n        assert self.capacity >= self.lora_slots\n        self.max_num_batched_tokens = math.ceil(max_num_batched_tokens / 8) * 8\n        self.lora_index_to_id: List[Optional[int]] = [None] * self.lora_slots\n        self.vocab_size = vocab_size\n        self.base_indices = torch.empty(self.max_num_batched_tokens,\n                                        dtype=torch.long,\n                                        device=\"cuda\")\n        self.sampler_indices = torch.empty(self.max_num_batched_tokens,\n                                           dtype=torch.long,\n                                           device=\"cuda\")\n        self.sampler_indices_padded = torch.empty(self.max_num_batched_tokens,\n                                                  dtype=torch.long,\n                                                  device=\"cuda\")\n        self.embeddings_indices = torch.empty(2,\n                                              self.max_num_batched_tokens,\n                                              dtype=torch.long,\n                                              device=\"cuda\")\n        # 4 is the number of indicies tensors defined above\n        # base_indices, sampler_indices, sampler_indices_padded,\n        # embeddings_indices\n        self.indices_len: List[Optional[int]] = [None] * 4\n\n        self.model: nn.Module = model\n        if hasattr(self.model, \"supported_lora_modules\"):\n            self.supported_lora_modules = copy.deepcopy(\n                self.model.supported_lora_modules)\n            self.packed_modules_mapping = copy.deepcopy(\n                self.model.packed_modules_mapping)\n        self.packed_modules: Dict[str, List[str]] = {}\n        self.modules: Dict[str, \"BaseLayerWithLoRA\"] = {}\n        self._registered_loras: Dict[int, LoRAModel] = {}\n        # Dict instead of a Set for compatibility with LRUCache.\n        self._active_loras: Dict[int, None] = {}\n        self._last_mapping: Optional[LoRAMapping] = None\n        self._create_lora_modules()\n        self.model.lora_manager = self\n\n    @property\n    def capacity(self) -> int:\n        return self.lora_config.max_cpu_loras\n\n    @property\n    def lora_slots(self) -> int:\n        return self.lora_config.max_loras\n\n    def __len__(self) -> int:\n        return len(self._registered_loras)\n\n    def activate_lora(\n        self,\n        lora_id: int,\n    ) -> bool:\n        \"\"\"Move LoRA into a GPU buffer to be used in the forward pass.\"\"\"\n        if lora_id in self._active_loras:\n            return False\n        first_free_slot = next(\n            ((i, lora_id) for i, lora_id in enumerate(self.lora_index_to_id)\n             if lora_id is None), None)\n        if first_free_slot is None:\n            raise ValueError(\"No free lora slots\")\n        index, _ = first_free_slot\n        self._active_loras[lora_id] = None\n        lora_model = self._registered_loras[lora_id]\n        logger.debug(\"Activating LoRA. int id: %d, slot index: %d\",\n                     lora_model.id, index)\n        self.lora_index_to_id[index] = lora_model.id\n        for module_name, module in self.modules.items():\n            module_lora = lora_model.get_lora(module_name)\n            if module_lora:\n                module_lora.optimize()\n                module.set_lora(index, module_lora.lora_a, module_lora.lora_b,\n                                module_lora.embeddings_tensor)\n            else:\n                module.reset_lora(index)\n        return True\n\n    def _deactivate_lora(self, lora_id: int):\n        try:\n            index = self.lora_index_to_id.index(lora_id)\n            self.lora_index_to_id[index] = None\n        except ValueError:\n            pass\n\n    def deactivate_lora(self, lora_id: int) -> bool:\n        \"\"\"Remove a LoRA from a GPU buffer.\"\"\"\n        if lora_id in self._active_loras:\n            self._deactivate_lora(lora_id)\n            self._active_loras.pop(lora_id)\n            return True\n        return False\n\n    def _add_lora(self, lora: LoRAModel):\n        self._create_merged_loras_inplace(lora)\n        self._registered_loras[lora.id] = lora\n\n    def add_lora(self, lora: LoRAModel) -> bool:\n        \"\"\"Add a LoRAModel to the manager CPU cache.\"\"\"\n        if lora.id not in self._registered_loras:\n            if len(self._registered_loras) >= self.capacity:\n                raise RuntimeError(\"No free LoRA slots.\")\n            self._add_lora(lora)\n            return True\n        return False\n\n    def remove_lora(self, lora_id: int) -> bool:\n        \"\"\"Remove a LoRAModel from the manager CPU cache.\"\"\"\n        # TODO: should we check active lora?\n        self.deactivate_lora(lora_id)\n        return bool(self._registered_loras.pop(lora_id, None))\n\n    # TODO see if this can be vectorized\n    def _set_lora_mapping(self, mapping: LoRAMapping) -> None:\n        (base_indices, sampler_indices, sampler_indices_padded,\n         embeddings_indices,\n         indices_len) = convert_mapping(mapping, self.lora_index_to_id,\n                                        self.lora_slots + 1, self.vocab_size,\n                                        self.lora_config.lora_extra_vocab_size)\n        self.base_indices[:base_indices.shape[0]].copy_(base_indices)\n        self.sampler_indices[:sampler_indices.shape[0]].copy_(sampler_indices)\n        self.sampler_indices_padded[:sampler_indices_padded.shape[0]].copy_(\n            sampler_indices_padded)\n        self.embeddings_indices[:embeddings_indices.\n                                shape[0], :embeddings_indices.shape[1]].copy_(\n                                    embeddings_indices)\n        # Maintain the reference\n        self.indices_len[:] = indices_len\n\n    def set_lora_mapping(self, lora_mapping: LoRAMapping) -> None:\n        if self._last_mapping != lora_mapping:\n            self._set_lora_mapping(lora_mapping)\n        self._last_mapping = lora_mapping\n\n    def list_loras(self) -> Dict[int, LoRAModel]:\n        \"\"\"List all registered LoRAModels.\"\"\"\n        return dict(self._registered_loras)\n\n    def get_lora(self, lora_id: int) -> Optional[LoRAModel]:\n        return self._registered_loras.get(lora_id, None)\n\n    def remove_all_loras(self):\n        \"\"\"Remove all LoRAModels from the manager.\"\"\"\n        self._registered_loras.clear()\n        self.lora_index_to_id = [None] * self.lora_slots\n        self._active_loras.clear()\n\n    def _create_lora_modules(self):\n        for module_name, module in self.model.named_modules():\n            if not self._match_target_modules(module_name):\n                continue\n            parts = module_name.split(\".\")[-1]\n            packed_moduled_lst = self.packed_modules_mapping.get(parts, [])\n            new_module = replace_submodule(\n                self.model, module_name,\n                from_layer(module, self.lora_slots, self.lora_config,\n                           packed_moduled_lst, self.model.config))\n            # (yard1): TODO make this more robust\n            if \"lm_head\" in module_name:\n                logits_processor_module = self.model.get_submodule(\n                    \"logits_processor\")\n                new_module = replace_submodule(\n                    self.model, \"logits_processor\",\n                    from_layer_logits_processor(logits_processor_module,\n                                                module, self.lora_slots,\n                                                self.lora_config,\n                                                self.model.config))\n            self.register_module(module_name, new_module)\n            self._register_packed_modules(module_name)\n            new_module.set_mapping(self.base_indices, self.sampler_indices,\n                                   self.sampler_indices_padded,\n                                   self.embeddings_indices, self.indices_len)\n\n    def register_module(self, module_name: str, module: \"BaseLayerWithLoRA\"):\n        assert isinstance(module, BaseLayerWithLoRA)\n        self.modules[module_name] = module\n\n    def create_dummy_lora(\n            self,\n            lora_id: int,\n            rank: int,\n            embedding_modules: Optional[Dict[str, str]] = None) -> LoRAModel:\n        \"\"\"Create zero-initialized LoRAModel for warmup.\"\"\"\n        model = LoRAModel(lora_id, rank, {})\n        for module_name, module in self.model.named_modules():\n            if not self._match_target_modules(module_name) or not isinstance(\n                    module, BaseLayerWithLoRA):\n                continue\n            parts = module_name.split(\".\")\n            if module_name not in self.packed_modules:\n                assert embedding_modules is not None\n                if parts[-1] in embedding_modules:\n                    input_dim = (module.base_layer.org_vocab_size +\n                                 self.lora_config.lora_extra_vocab_size if\n                                 hasattr(module.base_layer, \"org_vocab_size\")\n                                 else module.base_layer.weight.shape[1])\n                    output_dim = module.base_layer.embedding_dim if hasattr(\n                        module.base_layer,\n                        \"embedding_dim\") else module.base_layer.weight.shape[0]\n                    embeddings_tensor_dim = (module.base_layer.embedding_dim if\n                                             hasattr(module.base_layer,\n                                                     \"embedding_dim\") else\n                                             module.base_layer.weight.shape[1])\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name,\n                        input_dim,\n                        output_dim,\n                        rank,\n                        module.lora_a_stacked.dtype,\n                        \"cpu\",\n                        embeddings_tensor_dim=embeddings_tensor_dim)\n                else:\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name,\n                        module.lora_a_stacked.shape[-1],\n                        module.lora_b_stacked.shape[-2],\n                        rank,\n                        module.lora_a_stacked.dtype,\n                        \"cpu\",\n                    )\n                lora.optimize()\n            else:\n                parts = module_name.split(\".\")\n                replacements = self.packed_modules_mapping[parts[-1]]\n                subloras: List[Optional[\"LoRALayerWeights\"]] = []\n                for i, r in enumerate(replacements):\n                    lora = LoRALayerWeights.create_dummy_lora_weights(\n                        module_name + \".\" + r,\n                        module.lora_a_stacked[i].shape[-1],\n                        module.lora_b_stacked[i].shape[-2],\n                        rank,\n                        module.lora_a_stacked[i].dtype,\n                        \"cpu\",\n                    )\n                    lora.optimize()\n                    subloras.append(lora)\n                lora = PackedLoRALayerWeights.pack(subloras)\n            model.loras[module_name] = lora\n        return model\n\n    def _match_target_modules(self, module_name: str):\n        return any(\n            re.match(\n                r\".*\\.{target_module}$\".format(target_module=target_module),\n                module_name) or target_module == module_name\n            for target_module in self.supported_lora_modules)\n\n    def _register_packed_modules(self, module_full_name: str) -> None:\n        parts = module_full_name.split(\".\")\n        module_name = parts[-1]\n        replacements = self.packed_modules_mapping.get(module_name, [])\n        # When replacements is less than or equal to 1, it indicates that this\n        # module is not a packed module.\n        if len(replacements) <= 1:\n            return\n        prefix = \".\".join(parts[:-1])\n        self.packed_modules[module_full_name] = [\n            prefix + \".\" + r if prefix else r for r in replacements\n        ]\n\n    def _create_merged_loras_inplace(self, lora_model: LoRAModel) -> None:\n        for module_name, new_module_names in self.packed_modules.items():\n            replacement_loras: List[Optional[LoRALayerWeights]] = []\n            has_replacement = False\n            for r in new_module_names:\n                lora = lora_model.get_lora(r)\n                replacement_loras.append(lora)\n                if lora:\n                    has_replacement = True\n            if not has_replacement:\n                continue\n            for i in range(len(replacement_loras)):\n                if replacement_loras[i]:\n                    continue\n                replacement_loras[i] = None\n            lora_model.loras[module_name] = PackedLoRALayerWeights.pack(\n                replacement_loras)\n\n\nclass LoRALRUCache(LRUCache[LoRAModel]):\n\n    def __init__(self, capacity: int, deactivate_lora_fn: Callable[[int],\n                                                                   bool]):\n        super().__init__(capacity)\n        self.deactivate_lora_fn = deactivate_lora_fn\n\n    def _on_remove(self, key: int, value: LoRAModel):\n        logger.debug(\"Removing LoRA. int id: %d\", key)\n        self.deactivate_lora_fn(key)\n        return super()._on_remove(key, value)\n\n\nclass LRUCacheLoRAModelManager(LoRAModelManager):\n    \"\"\"A model manager that manages multiple LoRAs with LRU cache.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n    ):\n        super().__init__(model, max_num_seqs, max_num_batched_tokens,\n                         vocab_size, lora_config)\n        self._registered_loras: LoRALRUCache = LoRALRUCache(\n            self.capacity, self.deactivate_lora)\n        self._active_loras: LoRALRUCache = LoRALRUCache(\n            self.lora_slots, self._deactivate_lora)\n\n    def list_loras(self) -> Dict[int, LoRAModel]:\n        \"\"\"List all registered LoRAModels.\"\"\"\n        return dict(self._registered_loras.cache)\n\n    def add_lora(self, lora: LoRAModel) -> bool:\n        \"\"\"Add a LoRAModel to the manager.\"\"\"\n        if lora.id not in self._registered_loras:\n            self._add_lora(lora)\n            was_added = True\n        else:\n            # We always touch to update the LRU cache order\n            self._registered_loras.touch(lora.id)\n            was_added = False\n        return was_added\n\n    def activate_lora(\n        self,\n        lora_id: int,\n    ) -> bool:\n        if lora_id not in self._active_loras and len(\n                self._active_loras) >= self.lora_slots:\n            self._active_loras.remove_oldest()\n        result = super().activate_lora(lora_id)\n        # We always touch to update the LRU cache order\n        self._active_loras.touch(lora_id)\n        return result\n\n    def remove_oldest_lora(self) -> bool:\n        if len(self._registered_loras) > 0:\n            self._registered_loras.remove_oldest()\n            return True\n        return False\n\n\ndef create_lora_manager(\n        model: nn.Module,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n        lora_manager_cls: Type[LoRAModelManager] = LoRAModelManager,\n        **kwargs) -> LoRAModelManager:\n    \"\"\"Create a LoRA adapter for a given model.\"\"\"\n    if not hasattr(model, \"supported_lora_modules\"):\n        raise ValueError(f\"Model {type(model)} is not supported for LoRA.\")\n    lora_manager = lora_manager_cls(\n        model=model,\n        max_num_seqs=max_num_seqs,\n        max_num_batched_tokens=max_num_batched_tokens,\n        vocab_size=vocab_size,\n        lora_config=lora_config,\n        **kwargs)\n    return lora_manager\n",
      "diff": "diff --git a/vllm/lora/models.py b/vllm/lora/models.py\nindex 50d7e9133..cd45040bc 100644\n--- a/vllm/lora/models.py\n+++ b/vllm/lora/models.py\n@@ -119,6 +119,16 @@ class LoRAModel:\n         self.rank = rank\n         self.loras: Dict[str, LoRALayerWeights] = loras\n \n+    def clone(self, lora_model_id: int) -> \"LoRAModel\":\n+        \"\"\"Return a copy of the object with different ids.\n+\n+        Will share the underlying tensors.\"\"\"\n+        return self.__class__(\n+            lora_model_id,\n+            rank=self.rank,\n+            loras=self.loras.copy(),\n+        )\n+\n     @property\n     def extra_vocab_size(self) -> int:\n         return max(lora.extra_vocab_size",
      "change_type": "modified",
      "lines_added": 11,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/lora/worker_manager.py",
      "old_content": "from abc import ABC, abstractmethod, abstractproperty\nfrom typing import Any, Dict, List, Set, Type\n\nimport torch\n\nfrom vllm.config import LoRAConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.models import (LoRAModel, LoRAModelManager,\n                              LRUCacheLoRAModelManager, create_lora_manager)\nfrom vllm.lora.request import LoRARequest\n\nlogger = init_logger(__name__)\n\n\nclass AbstractWorkerLoRAManager(ABC):\n    \"\"\"Abstract class for managing LoRA models on the worker side.\"\"\"\n\n    def __init__(self, max_num_seqs: int, max_num_batched_tokens: int,\n                 vocab_size: int, lora_config: LoRAConfig,\n                 device: torch.device):\n        self.max_num_seqs = max_num_seqs\n        self.max_num_batched_tokens = max_num_batched_tokens\n        self.vocab_size = vocab_size\n        self.device = device\n        self.lora_config = lora_config\n\n    @abstractproperty\n    def is_enabled(self) -> bool:\n        ...\n\n    @abstractmethod\n    def create_lora_manager(\n        self,\n        model: torch.nn.Module,\n    ) -> Any:\n        ...\n\n    @abstractmethod\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        ...\n\n    @abstractmethod\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        ...\n\n    @abstractmethod\n    def add_dummy_lora(self, lora_request: LoRARequest, rank: int) -> bool:\n        ...\n\n    @abstractmethod\n    def remove_lora(self, lora_id: int) -> bool:\n        ...\n\n    @abstractmethod\n    def remove_all_loras(self):\n        ...\n\n    @abstractmethod\n    def list_loras(self) -> Set[int]:\n        ...\n\n\nclass WorkerLoRAManager(AbstractWorkerLoRAManager):\n    \"\"\"WorkerLoRAManager that manages LoRA models on the worker side.\n\n    Every request, the requested LoRAs will be loaded (unless they are already\n    loaded), and every other LoRA will be unloaded.\"\"\"\n\n    _lora_manager_cls: Type[LoRAModelManager] = LoRAModelManager\n\n    def __init__(\n        self,\n        max_num_seqs: int,\n        max_num_batched_tokens: int,\n        vocab_size: int,\n        lora_config: LoRAConfig,\n        device: torch.device,\n        embedding_modules: Dict[str, str],\n        embedding_padding_modules: List[str],\n        lora_model_cls: Type[LoRAModel] = LoRAModel,\n    ):\n        self._lora_model_cls = lora_model_cls\n        self.embedding_modules = embedding_modules\n        self.embedding_padding_modules = embedding_padding_modules\n        # Lazily initialized by create_lora_manager.\n        self._lora_manager: LoRAModelManager\n        super().__init__(max_num_seqs, max_num_batched_tokens, vocab_size,\n                         lora_config, device)\n\n    @property\n    def is_enabled(self) -> bool:\n        return True\n\n    def create_lora_manager(\n        self,\n        model: torch.nn.Module,\n    ) -> Any:\n        lora_manager = create_lora_manager(\n            model,\n            max_num_seqs=self.max_num_seqs,\n            max_num_batched_tokens=self.max_num_batched_tokens,\n            vocab_size=self.vocab_size,\n            lora_config=self.lora_config,\n            lora_manager_cls=self._lora_manager_cls,\n        )\n        self._lora_manager = lora_manager\n        return lora_manager.model\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        self._apply_loras(lora_requests)\n        self._lora_manager.set_lora_mapping(lora_mapping)\n\n    def _apply_loras(self, lora_requests: Set[LoRARequest]) -> None:\n        loras_that_exist = self.list_loras()\n        loras_map = {\n            lora_request.lora_int_id: lora_request\n            for lora_request in lora_requests if lora_request\n        }\n        if len(loras_map) > self._lora_manager.lora_slots:\n            raise RuntimeError(\n                f\"Number of requested LoRAs ({len(loras_map)}) is greater \"\n                \"than the number of GPU LoRA slots \"\n                f\"({self._lora_manager.lora_slots}).\")\n\n        new_loras = set(loras_map)\n        loras_to_add = new_loras - loras_that_exist\n        loras_to_remove = loras_that_exist - new_loras\n\n        for lora_id in loras_to_remove:\n            self.remove_lora(lora_id)\n\n        for lora_id in loras_to_add:\n            self.add_lora(loras_map[lora_id])\n\n    def _load_lora(self, lora_request: LoRARequest) -> LoRAModel:\n        try:\n            model = self._lora_manager.model\n            supported_lora_modules = model.supported_lora_modules\n            packed_modules_mapping = model.packed_modules_mapping\n            expected_lora_modules = []\n            for module in supported_lora_modules:\n                if module in packed_modules_mapping:\n                    expected_lora_modules.extend(\n                        packed_modules_mapping[module])\n                else:\n                    expected_lora_modules.append(module)\n            lora = self._lora_model_cls.from_local_checkpoint(\n                lora_request.lora_local_path,\n                expected_lora_modules,\n                lora_model_id=lora_request.lora_int_id,\n                device=\"cpu\",\n                dtype=self.lora_config.lora_dtype,\n                target_embedding_padding=self.vocab_size +\n                self.lora_config.lora_extra_vocab_size,\n                embedding_modules=self.embedding_modules,\n                embedding_padding_modules=self.embedding_padding_modules,\n            )\n        except Exception as e:\n            raise RuntimeError(\n                f\"Loading lora {lora_request.lora_local_path} failed\") from e\n        if lora.rank > self.lora_config.max_lora_rank:\n            raise ValueError(\n                f\"LoRA rank {lora.rank} is greater than max_lora_rank \"\n                f\"{self.lora_config.max_lora_rank}.\")\n        if lora.extra_vocab_size > self.lora_config.lora_extra_vocab_size:\n            raise ValueError(f\"LoRA added vocab size {lora.extra_vocab_size} \"\n                             f\"is greater than lora_extra_vocab_size \"\n                             f\"{self.lora_config.lora_extra_vocab_size}.\")\n        return lora\n\n    def add_dummy_lora(self, lora_request: LoRARequest, rank: int) -> bool:\n        if lora_request.lora_int_id in self.list_loras():\n            return False\n        return self._lora_manager.add_lora(\n            self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n                                                 rank, self.embedding_modules))\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if lora_request.lora_int_id in self.list_loras():\n            return False\n        lora = self._load_lora(lora_request)\n        loaded = self._lora_manager.add_lora(lora)\n        self._lora_manager.activate_lora(lora.id)\n        return loaded\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self._lora_manager.remove_lora(lora_id)\n\n    def remove_all_loras(self):\n        self._lora_manager.remove_all_loras()\n\n    def list_loras(self) -> Set[int]:\n        return set(self._lora_manager.list_loras())\n\n\nclass LRUCacheWorkerLoRAManager(WorkerLoRAManager):\n    \"\"\"WorkerLoRAManager that manages LoRA models on the worker side.\n\n    Uses an LRU Cache. Every request, the requested LoRAs will be loaded\n    (unless they are already loaded) and least recently used LoRAs will\n    be unloaded if the cache is above capacity.\"\"\"\n\n    _lora_manager_cls: Type[\n        LRUCacheLoRAModelManager] = LRUCacheLoRAModelManager\n\n    def create_lora_manager(\n        self,\n        model: torch.nn.Module,\n    ) -> Any:\n        lora_manager = create_lora_manager(\n            model,\n            lora_manager_cls=self._lora_manager_cls,\n            max_num_seqs=self.max_num_seqs,\n            vocab_size=self.vocab_size,\n            lora_config=self.lora_config,\n            max_num_batched_tokens=self.max_num_batched_tokens,\n        )\n        self._lora_manager = lora_manager\n        return lora_manager.model\n\n    def _apply_loras(self, lora_requests: Set[LoRARequest]) -> None:\n        loras_map = {\n            lora_request.lora_int_id: lora_request\n            for lora_request in lora_requests if lora_request\n        }\n        if len(loras_map) > self._lora_manager.lora_slots:\n            raise RuntimeError(\n                f\"Number of requested LoRAs ({len(loras_map)}) is greater \"\n                \"than the number of GPU LoRA slots \"\n                f\"({self._lora_manager.lora_slots}).\")\n        for lora in loras_map.values():\n            self.add_lora(lora)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if lora_request.lora_int_id not in self.list_loras():\n            # Remove before we load the new lora to save memory\n            if len(self._lora_manager) + 1 > self._lora_manager.capacity:\n                assert isinstance(self._lora_manager, LRUCacheLoRAModelManager)\n                self._lora_manager.remove_oldest_lora()\n            lora = self._load_lora(lora_request)\n            loaded = self._lora_manager.add_lora(lora)\n        else:\n            # If the lora is already loaded, just touch it to\n            # update its position in the caches\n            loaded = self._lora_manager.get_lora(\n                lora_request.lora_int_id) is not None\n        self._lora_manager.activate_lora(lora_request.lora_int_id)\n        return loaded\n",
      "diff": "diff --git a/vllm/lora/worker_manager.py b/vllm/lora/worker_manager.py\nindex ec3c10c59..377f561cc 100644\n--- a/vllm/lora/worker_manager.py\n+++ b/vllm/lora/worker_manager.py\n@@ -1,5 +1,6 @@\n from abc import ABC, abstractmethod, abstractproperty\n-from typing import Any, Dict, List, Set, Type\n+from contextlib import contextmanager\n+from typing import Any, Dict, List, Literal, Set, Type, Union\n \n import torch\n \n@@ -25,6 +26,17 @@ class AbstractWorkerLoRAManager(ABC):\n         self.device = device\n         self.lora_config = lora_config\n \n+        # If False, do not cache. If None, cache is empty.\n+        self._cached_dummy_lora: Union[None, Literal[False], LoRAModel] = False\n+\n+    @contextmanager\n+    def dummy_lora_cache(self):\n+        \"\"\"Use this context manager to reuse the dummy lora model\n+        to avoid creating it repeatedly.\"\"\"\n+        self._cached_dummy_lora = None\n+        yield\n+        self._cached_dummy_lora = False\n+\n     @abstractproperty\n     def is_enabled(self) -> bool:\n         ...\n@@ -174,9 +186,15 @@ class WorkerLoRAManager(AbstractWorkerLoRAManager):\n     def add_dummy_lora(self, lora_request: LoRARequest, rank: int) -> bool:\n         if lora_request.lora_int_id in self.list_loras():\n             return False\n-        return self._lora_manager.add_lora(\n-            self._lora_manager.create_dummy_lora(lora_request.lora_int_id,\n-                                                 rank, self.embedding_modules))\n+        if isinstance(self._cached_dummy_lora, LoRAModel):\n+            dummy_lora = self._cached_dummy_lora.clone(\n+                lora_request.lora_int_id)\n+        else:\n+            dummy_lora = self._lora_manager.create_dummy_lora(\n+                lora_request.lora_int_id, rank, self.embedding_modules)\n+            if self._cached_dummy_lora is None:\n+                self._cached_dummy_lora = dummy_lora\n+        return self._lora_manager.add_lora(dummy_lora)\n \n     def add_lora(self, lora_request: LoRARequest) -> bool:\n         if lora_request.lora_int_id in self.list_loras():",
      "change_type": "modified",
      "lines_added": 23,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import contextlib\nimport time\nfrom enum import IntEnum\nfrom typing import Dict, List, NamedTuple, Optional, Set, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom vllm.attention import (AttentionMetadata, AttentionMetadataPerStage,\n                            get_attn_backend)\nfrom vllm.config import (DeviceConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig, VisionLanguageConfig)\nfrom vllm.distributed import broadcast_tensor_dict, with_pynccl_for_all_reduce\nfrom vllm.distributed.device_communicators import (custom_all_reduce,\n                                                   pynccl_utils)\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (MultiModalData, SamplerOutput, SequenceData,\n                           SequenceGroupMetadata)\nfrom vllm.utils import (CudaMemoryProfiler, get_kv_cache_torch_dtype, is_hip,\n                        is_pin_memory_available, make_tensor_with_pad)\n\nlogger = init_logger(__name__)\n\n_PAD_SLOT_ID = -1\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# Capture graphs for token size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)\n]\n\n\nclass PreparePromptMetadata(NamedTuple):\n    input_tokens: List[int]\n    input_positions: List[int]\n    attn_metadata: Optional[AttentionMetadataPerStage]\n    seq_lens: List[int]\n    query_lens: List[int]\n    lora_index_mapping: List[int]\n    lora_prompt_mapping: List[int]\n    lora_requests: Set[LoRARequest]\n    multi_modal_input: Optional[torch.Tensor]\n    slot_mapping: List[int]\n\n    @classmethod\n    def empty(cls):\n        return PreparePromptMetadata(\n            input_tokens=[],\n            input_positions=[],\n            attn_metadata=None,\n            seq_lens=[],\n            query_lens=[],\n            lora_index_mapping=[],\n            lora_prompt_mapping=[],\n            lora_requests=set(),\n            multi_modal_input=None,\n            slot_mapping=[],\n        )\n\n\nclass PrepareDecodeMetadata(NamedTuple):\n    input_tokens: List[int]\n    input_positions: List[int]\n    attn_metadata: Optional[AttentionMetadata]\n    lora_index_mapping: List[int]\n    lora_prompt_mapping: List[int]\n    lora_requests: Set[LoRARequest]\n    slot_mapping: List[int]\n\n    @classmethod\n    def empty(cls):\n        return PrepareDecodeMetadata(\n            input_tokens=[],\n            input_positions=[],\n            attn_metadata=None,\n            lora_index_mapping=[],\n            lora_prompt_mapping=[],\n            lora_requests=set(),\n            slot_mapping=[],\n        )\n\n\n# How batches are constructed.\nclass BatchType(IntEnum):\n    # Every batch is prefill.\n    PREFILL = 0\n    # Every batch is decode.\n    DECODE = 1\n    # Batch is a mixture of prefill and decode.\n    MIXED = 2\n\n\nclass ModelRunner:\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        vision_language_config: Optional[VisionLanguageConfig] = None,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n\n        # model_config can be None in tests/samplers/test_sampler.py.\n        # FIXME(woosuk): This is a hack to make the tests work. Refactor this.\n        self.sliding_window = (model_config.get_sliding_window()\n                               if model_config is not None else None)\n        self.device_config = (device_config\n                              if device_config is not None else DeviceConfig())\n        self.device = self.device_config.device\n\n        # Set after load_model.\n        self.lora_manager: LRUCacheWorkerLoRAManager = None\n\n        self.graph_runners: Dict[int, CUDAGraphRunner] = {}\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n\n        self.max_seq_len_to_capture = (self.model_config.max_seq_len_to_capture\n                                       if self.model_config is not None else 0)\n\n        self.pin_memory = is_pin_memory_available()\n        self.kv_cache_dtype = kv_cache_dtype\n        self.vision_language_config = vision_language_config\n\n        self.attn_backend = get_attn_backend(\n            self.model_config.dtype if model_config is not None else None)\n\n        # Lazy initialization\n        self.model: torch.nn.Module  # Set after load_model\n        self.block_size: int  # Set after initial profiling.\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables: torch.Tensor  # Set after initial profiling.\n\n        # Set if the backend is flashinfer.\n        self.flashinfer_workspace_buffer: torch.Tensor\n\n    def load_model(self) -> None:\n        with CudaMemoryProfiler() as m:\n            self.model = get_model(\n                model_config=self.model_config,\n                device_config=self.device_config,\n                load_config=self.load_config,\n                lora_config=self.lora_config,\n                vision_language_config=self.vision_language_config,\n                parallel_config=self.parallel_config,\n                scheduler_config=self.scheduler_config,\n            )\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert hasattr(self.model, \"supported_lora_modules\"\n                           ) and self.model.supported_lora_modules, (\n                               \"Model does not support LoRA\")\n            assert hasattr(\n                self.model,\n                \"embedding_modules\"), \"Model does not have embedding_modules\"\n            assert hasattr(self.model, \"embedding_padding_modules\"\n                           ), \"Model does not have embedding_padding_modules\"\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens, self.vocab_size,\n                self.lora_config, self.device, self.model.embedding_modules,\n                self.model.embedding_padding_modules)\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently scaled KV cache is only enabled on ROCm\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n        elif self.model_config.quantization_param_path is not None:\n            logger.warning(\"KV cache scaling factors provided, \"\n                           \"but the KV cache data type is not FP8. \"\n                           \"KV cache scaling factors will not be used.\")\n\n    def set_block_size(self, block_size: int) -> None:\n        self.block_size = block_size\n\n        self.graph_block_tables = np.zeros(\n            (max(_BATCH_SIZES_TO_CAPTURE), self.get_max_block_per_batch()),\n            dtype=np.int32)\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_prompt(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> PreparePromptMetadata:\n        input_tokens: List[int] = []\n        input_positions: List[int] = []\n        slot_mapping: List[int] = []\n        lora_index_mapping: List[int] = []\n        lora_prompt_mapping: List[int] = []\n        lora_requests: Set[LoRARequest] = set()\n\n        seq_lens: List[int] = []\n        context_lens: List[int] = []\n        query_lens: List[int] = []\n        prefix_block_tables: List[List[int]] = []\n        multi_modal_input_list: List[torch.Tensor] = []\n\n        if len(seq_group_metadata_list) == 0:\n            return PreparePromptMetadata.empty()\n\n        for seq_group_metadata in seq_group_metadata_list:\n            assert seq_group_metadata.is_prompt\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            assert len(seq_ids) == 1\n            seq_id = seq_ids[0]\n\n            computed_block_nums = seq_group_metadata.computed_block_nums\n            if (self.scheduler_config is not None\n                    and self.scheduler_config.chunked_prefill_enabled\n                    and not (computed_block_nums is None\n                             or computed_block_nums == [])):\n                raise RuntimeError(\n                    \"chunked prefill cannot be used with prefix caching \"\n                    \"now.\")\n\n            token_chunk_size = seq_group_metadata.token_chunk_size\n            seq_data = seq_group_metadata.seq_data[seq_id]\n            context_len = seq_data.get_num_computed_tokens()\n            # We should use get_len here because in case of preemption\n            # it contains output tokens.\n            seq_len = min(seq_data.get_len(), context_len + token_chunk_size)\n            prompt_tokens = seq_data.get_token_ids()[context_len:seq_len]\n            seq_lens.append(seq_len)\n\n            # NOTE: This only works for oooooooxxx style attention.\n            if computed_block_nums is not None and len(\n                    computed_block_nums) > 0 and self.sliding_window is None:\n                # Prefix is not supported with sliding_window\n                context_len = len(computed_block_nums) * self.block_size\n                prompt_tokens = prompt_tokens[context_len:]\n                prefix_block_tables.append(computed_block_nums)\n            elif self.scheduler_config.chunked_prefill_enabled:\n                if seq_group_metadata.block_tables is not None:\n                    # Prefill has chunked before.\n                    block_table = seq_group_metadata.block_tables[seq_id]\n                    prefix_block_tables.append(block_table)\n                else:\n                    # The first prefill.\n                    prefix_block_tables.append([])\n            else:\n                prefix_block_tables.append([])\n                # Right now, prefill start is always 0. However, this\n                # assumption can be changed once chunked prefill is introduced.\n                assert context_len == 0\n\n            # actual prompt lens\n            context_lens.append(context_len)\n            query_lens.append(seq_len - context_len)\n\n            input_tokens.extend(prompt_tokens)\n            # NOTE(woosuk): Here we assume that the first token in the prompt\n            # is always the first token in the sequence.\n            input_positions.extend(list(range(context_len, seq_len)))\n            lora_id = seq_group_metadata.lora_int_id\n\n            if lora_id > 0:\n                lora_requests.add(seq_group_metadata.lora_request)\n\n            lora_index_mapping += [lora_id] * (seq_len - context_len)\n            lora_prompt_mapping.extend(\n                [lora_id] *\n                (seq_len - context_len\n                 if seq_group_metadata.sampling_params.prompt_logprobs else 1))\n\n            if seq_group_metadata.multi_modal_data:\n                multi_modal_input_list.append(\n                    seq_group_metadata.multi_modal_data.data)\n\n            if seq_group_metadata.block_tables is None:\n                # During memory profiling, the block tables are not initialized\n                # yet. In this case, we just use a dummy slot mapping.\n                slot_mapping.extend([_PAD_SLOT_ID] * seq_len)\n                continue\n\n            # Compute the slot mapping.\n            block_table = seq_group_metadata.block_tables[seq_id]\n\n            # Mask the [0, start_idx) tokens of the prompt with _PAD_SLOT_ID,\n            # where start_idx is max(0, seq_len - sliding_window).\n            # For example, if the prompt len is 10, sliding window is 8, and\n            # block size is 4, the first two tokens are masked and the slot\n            # mapping will be [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n            start_idx = 0\n            if self.sliding_window is not None:\n                assert context_len == 0, (\n                    \"Prefix caching is currently not supported with \"\n                    \"sliding window attention\")\n                start_idx = max(0, seq_len - self.sliding_window)\n\n            for i in range(context_len, seq_len):\n                if i < start_idx:\n                    slot_mapping.append(_PAD_SLOT_ID)\n                    continue\n\n                block_number = block_table[i // self.block_size]\n                block_offset = i % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append(slot)\n\n        max_query_len = max(query_lens)\n        max_seq_len = max(seq_lens)\n        assert max_query_len > 0\n\n        context_lens_tensor = torch.tensor(context_lens,\n                                           dtype=torch.int,\n                                           device=self.device)\n\n        if multi_modal_input_list:\n            assert self.vision_language_config, (\n                \"Multi-modal inputs are only supported by \"\n                \"vision language models.\")\n            multi_modal_input = torch.cat(multi_modal_input_list,\n                                          dim=0).to(self.device)\n        else:\n            multi_modal_input = None\n\n        # Prepare prefix block tables\n        max_prompt_block_table_len = max(len(t) for t in prefix_block_tables)\n        block_tables = make_tensor_with_pad(\n            prefix_block_tables,\n            max_len=max_prompt_block_table_len,\n            pad=0,\n            dtype=torch.int,\n            device=self.device,\n        )\n\n        # Query length can be shorter than key (i.e., prompt) when prefill\n        # is chunked or prefix cached.\n        query_lens_tensor = torch.tensor(query_lens,\n                                         dtype=torch.long,\n                                         device=self.device)\n        subquery_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                         dtype=torch.int32,\n                                         device=self.device)\n\n        seq_lens_tensor = torch.tensor(seq_lens,\n                                       dtype=torch.int,\n                                       device=self.device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=self.device)\n\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=subquery_start_loc.dtype,\n                     out=subquery_start_loc[1:])\n\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n\n        if self.attn_backend.get_name() == \"flashinfer\":\n            attn_metadata = self.attn_backend.make_metadata(\n                is_prompt=True,\n                use_cuda_graph=False,\n                seq_start_loc=seq_start_loc,\n                max_seq_len=max_seq_len,\n                block_tables=block_tables)\n        else:\n            attn_metadata = self.attn_backend.make_metadata(\n                is_prompt=True,\n                seq_lens=seq_lens,\n                seq_lens_tensor=seq_lens_tensor,\n                max_query_len=max_query_len,\n                max_seq_len=max_seq_len,\n                subquery_start_loc=subquery_start_loc,\n                seq_start_loc=seq_start_loc,\n                context_lens_tensor=context_lens_tensor,\n                block_tables=block_tables,\n                use_cuda_graph=False,\n            )\n\n        return PreparePromptMetadata(\n            input_tokens=input_tokens,\n            input_positions=input_positions,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_index_mapping=lora_index_mapping,\n            lora_prompt_mapping=lora_prompt_mapping,\n            lora_requests=lora_requests,\n            multi_modal_input=multi_modal_input,\n            slot_mapping=slot_mapping,\n        )\n\n    def _prepare_decode(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> PrepareDecodeMetadata:\n        input_tokens: List[int] = []\n        input_positions: List[int] = []\n        slot_mapping: List[int] = []\n        seq_lens: List[int] = []\n        block_tables: List[List[int]] = []\n        lora_index_mapping: List[int] = []\n        lora_prompt_mapping: List[int] = []\n        lora_requests: Set[LoRARequest] = set()\n\n        # The following fields are only for flashinfer\n        # Please follow https://docs.flashinfer.ai/tutorials/kv_layout.html#page-layout\n        # for the precise definition of the following fields.\n        # An example:\n        # request 1, page indices [0, 5, 8]\n        # request 2, page indices [1, 6, 7]\n        # request 3, page indices [3, 4]\n        # paged_kv_indices is a concatenation of page indices of all requests:\n        # [0, 5, 8, 1, 6, 7, 3, 4]\n        # paged_kv_indptr is used to index into paged_kv_indices:\n        # [0, 3, 6, 8]\n        paged_kv_indices: List[int] = []\n        # 0 at the beginning of paged_kv_indptr indicates the start of the\n        # first requests page indices in the paged_kv_indices list.\n        paged_kv_indptr: List[int] = [0]\n        # paged_kv_last_page_len is the length of the last page of each request\n        paged_kv_last_page_len: List[int] = []\n\n        if len(seq_group_metadata_list) == 0:\n            return PrepareDecodeMetadata.empty()\n\n        for seq_group_metadata in seq_group_metadata_list:\n            assert not seq_group_metadata.is_prompt\n            assert seq_group_metadata.token_chunk_size == 1\n\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            lora_id = seq_group_metadata.lora_int_id\n\n            if lora_id > 0:\n                lora_requests.add(seq_group_metadata.lora_request)\n\n            for seq_id in seq_ids:\n                seq_data = seq_group_metadata.seq_data[seq_id]\n                generation_token = seq_data.get_last_token_id()\n                input_tokens.append(generation_token)\n\n                seq_len = seq_data.get_len()\n                position = seq_len - 1\n                input_positions.append(position)\n\n                seq_len = seq_len if self.sliding_window is None else min(\n                    seq_len, self.sliding_window)\n                seq_lens.append(seq_len)\n\n                block_table = seq_group_metadata.block_tables[seq_id]\n                block_number = block_table[position // self.block_size]\n                block_offset = position % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append(slot)\n                lora_index_mapping.append(lora_id)\n                lora_prompt_mapping.append(lora_id)\n\n                if self.sliding_window is not None:\n                    sliding_window_blocks = (self.sliding_window //\n                                             self.block_size)\n                    block_table = block_table[-sliding_window_blocks:]\n                block_tables.append(block_table)\n\n                paged_kv_indices.extend(block_table)\n                paged_kv_indptr.append(paged_kv_indptr[-1] + len(block_table))\n                last_page_len = seq_data.get_len() % self.block_size\n                if last_page_len == 0:\n                    last_page_len = self.block_size\n                paged_kv_last_page_len.append(last_page_len)\n\n        # vLLM uses cuda graph only for decoding requests.\n        # See `capture_model` API for more details.\n        # For decoding requests, batch_size == input_tokens.\n        batch_size = len(input_tokens)\n        max_seq_len = max(seq_lens)\n        use_captured_graph = (not self.model_config.enforce_eager\n                              and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n                              and max_seq_len <= self.max_seq_len_to_capture)\n        if use_captured_graph:\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            for _ in range(graph_batch_size - batch_size):\n                input_tokens.append(0)\n                input_positions.append(0)\n                slot_mapping.append(_PAD_SLOT_ID)\n                seq_lens.append(1)\n                block_tables.append([])\n                lora_index_mapping.append(0)\n            batch_size = graph_batch_size\n\n        seq_lens_tensor = torch.tensor(seq_lens,\n                                       dtype=torch.int,\n                                       device=self.device)\n\n        if use_captured_graph:\n            # When using cuda-graph all these tensors should be\n            # padded.\n            assert seq_lens_tensor.shape[0] == len(input_tokens)\n            assert seq_lens_tensor.shape[0] == len(input_positions)\n            assert seq_lens_tensor.shape[0] == len(slot_mapping)\n\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.tensor(input_block_tables, device=self.device)\n        else:\n            max_block_table_len = max(\n                len(block_table) for block_table in block_tables)\n            block_tables = make_tensor_with_pad(\n                block_tables,\n                max_len=max_block_table_len,\n                pad=0,\n                dtype=torch.int,\n                device=self.device,\n            )\n\n        if self.attn_backend.get_name() == \"flashinfer\":\n            if not hasattr(self, \"flashinfer_workspace_buffer\"):\n                # Allocate 16MB workspace buffer\n                # Follow the example of flashinfer: https://docs.flashinfer.ai/api/python/decode.html\n                self.flashinfer_workspace_buffer = torch.empty(\n                    16 * 1024 * 1024, dtype=torch.uint8, device=self.device)\n            paged_kv_indptr = torch.tensor(paged_kv_indptr,\n                                           dtype=torch.int,\n                                           device=self.device)\n            paged_kv_indices = torch.tensor(paged_kv_indices,\n                                            dtype=torch.int,\n                                            device=self.device)\n            paged_kv_last_page_len = torch.tensor(paged_kv_last_page_len,\n                                                  dtype=torch.int,\n                                                  device=self.device)\n            kv_cache_dtype = get_kv_cache_torch_dtype(self.kv_cache_dtype,\n                                                      self.model_config.dtype)\n\n            attn_metadata = self.attn_backend.make_metadata(\n                is_prompt=False,\n                use_cuda_graph=False,\n                workspace_buffer=self.flashinfer_workspace_buffer,\n                paged_kv_indptr=paged_kv_indptr,\n                paged_kv_indices=paged_kv_indices,\n                paged_kv_last_page_len=paged_kv_last_page_len,\n                num_qo_heads=self.model_config.get_num_attention_heads(\n                    self.parallel_config),\n                num_kv_heads=self.model_config.get_num_kv_heads(\n                    self.parallel_config),\n                head_dim=self.model_config.get_head_size(),\n                page_size=self.block_size,\n                data_type=kv_cache_dtype)\n        else:\n            attn_metadata = self.attn_backend.make_metadata(\n                is_prompt=False,\n                seq_lens=None,\n                seq_lens_tensor=seq_lens_tensor,\n                max_query_len=None,\n                max_seq_len=max_seq_len,\n                subquery_start_loc=None,\n                seq_start_loc=None,\n                context_lens_tensor=None,\n                block_tables=block_tables,\n                use_cuda_graph=use_captured_graph,\n            )\n        return PrepareDecodeMetadata(\n            input_tokens=input_tokens,\n            input_positions=input_positions,\n            attn_metadata=attn_metadata,\n            lora_index_mapping=lora_index_mapping,\n            lora_prompt_mapping=lora_prompt_mapping,\n            lora_requests=lora_requests,\n            slot_mapping=slot_mapping,\n        )\n\n    def prepare_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, SamplingMetadata,\n               Set[LoRARequest], LoRAMapping, torch.Tensor]:\n        if self.is_driver_worker:\n            prefill_reqs = []\n            decode_reqs = []\n            for seq_group_meta in seq_group_metadata_list:\n                if seq_group_meta.is_prompt:\n                    prefill_reqs.append(seq_group_meta)\n                else:\n                    decode_reqs.append(seq_group_meta)\n\n            # Prepare input tensors.\n            (\n                input_tokens,\n                input_positions,\n                prefill_attn_metadata,\n                seq_lens,\n                query_lens,\n                lora_index_mapping,\n                lora_prompt_mapping,\n                lora_requests,\n                multi_modal_input,\n                slot_mapping,\n            ) = self._prepare_prompt(prefill_reqs)\n            (\n                decode_input_tokens,\n                decode_input_positions,\n                decode_attn_metadata,\n                decode_lora_index_mapping,\n                decode_lora_prompt_mapping,\n                decode_lora_requests,\n                decode_slot_mapping,\n            ) = self._prepare_decode(decode_reqs)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, seq_lens, query_lens, self.device,\n                self.pin_memory)\n\n            if not self.scheduler_config.chunked_prefill_enabled:\n                assert (len(prefill_reqs) and len(decode_reqs)) == 0\n\n            num_prefills = len(seq_lens)\n            num_prefill_tokens = len(input_tokens)\n            num_decode_tokens = len(decode_input_tokens)\n\n            # Coalesce tensors. Note that attn_metadata is currently not\n            # coalesced for simplicity.\n            input_tokens.extend(decode_input_tokens)\n            input_positions.extend(decode_input_positions)\n            slot_mapping.extend(decode_slot_mapping)\n            lora_index_mapping.extend(decode_lora_index_mapping)\n            lora_prompt_mapping.extend(decode_lora_prompt_mapping)\n            lora_requests.update(decode_lora_requests)\n\n            input_tokens = torch.tensor(input_tokens,\n                                        dtype=torch.long,\n                                        device=self.device)\n            input_positions = torch.tensor(input_positions,\n                                           dtype=torch.long,\n                                           device=self.device)\n            slot_mapping = torch.tensor(slot_mapping,\n                                        dtype=torch.long,\n                                        device=self.device)\n\n            if self.lora_config:\n                lora_mapping = LoRAMapping(\n                    lora_index_mapping,\n                    lora_prompt_mapping,\n                )\n            else:\n                lora_mapping = None\n\n            # Broadcast the metadata.\n            # If batch contains both prefill and decode, it sends 2 broadcasts.\n            # If it only contains 1 type, it triggers a single broadcast.\n            if (prefill_attn_metadata is not None\n                    and decode_attn_metadata is not None):\n                batch_type = BatchType.MIXED\n            elif prefill_attn_metadata is not None:\n                batch_type = BatchType.PREFILL\n            else:\n                batch_type = BatchType.DECODE\n\n            metadata_dict = {\n                \"input_tokens\": input_tokens,\n                \"input_positions\": input_positions,\n                \"selected_token_indices\":\n                sampling_metadata.selected_token_indices,\n                \"lora_requests\": lora_requests,\n                \"lora_mapping\": lora_mapping,\n                \"multi_modal_input\": multi_modal_input,\n                \"num_prefill_tokens\": num_prefill_tokens,\n                \"num_decode_tokens\": num_decode_tokens,\n                \"slot_mapping\": slot_mapping,\n                \"num_prefills\": num_prefills,\n                \"batch_type\": batch_type,\n            }\n            if prefill_attn_metadata is not None:\n                metadata_dict.update(prefill_attn_metadata.asdict_zerocopy())\n            else:\n                assert decode_attn_metadata is not None\n                metadata_dict.update(decode_attn_metadata.asdict_zerocopy())\n            broadcast_tensor_dict(metadata_dict, src=0)\n\n            # Broadcast decode attn metadata for mixed batch type.\n            # The additional broadcast costs 300us overhead on 4 A10 GPUs.\n            # We can potentially reduce the overhead by coelescing tensors.\n            if batch_type == BatchType.MIXED:\n                assert decode_attn_metadata is not None\n                metadata_dict = decode_attn_metadata.asdict_zerocopy()\n                broadcast_tensor_dict(metadata_dict, src=0)\n        else:\n            metadata_dict = broadcast_tensor_dict(src=0)\n            input_tokens = metadata_dict.pop(\"input_tokens\")\n            input_positions = metadata_dict.pop(\"input_positions\")\n            slot_mapping = metadata_dict.pop(\"slot_mapping\")\n            num_prefills = metadata_dict.pop(\"num_prefills\")\n            selected_token_indices = metadata_dict.pop(\n                \"selected_token_indices\")\n            lora_mapping = metadata_dict.pop(\"lora_mapping\")\n            lora_requests = metadata_dict.pop(\"lora_requests\")\n            multi_modal_input = metadata_dict.pop(\"multi_modal_input\")\n            num_prefill_tokens = metadata_dict.pop(\"num_prefill_tokens\")\n            num_decode_tokens = metadata_dict.pop(\"num_decode_tokens\")\n            batch_type = metadata_dict.pop(\"batch_type\")\n\n            # Create an attention metadata.\n            prefill_attn_metadata = None\n            decode_attn_metadata = None\n            if batch_type == BatchType.PREFILL or batch_type == BatchType.MIXED:\n                prefill_attn_metadata = self.attn_backend.make_metadata(\n                    **metadata_dict)\n            else:\n                decode_attn_metadata = self.attn_backend.make_metadata(\n                    **metadata_dict)\n            sampling_metadata = SamplingMetadata(\n                seq_groups=None,\n                selected_token_indices=selected_token_indices,\n                categorized_sample_indices=None,\n                num_prompts=0,\n            )\n\n            # if it is a mixed batch, decode attn_metadata is broadcasted\n            # separately.\n            if batch_type == BatchType.MIXED:\n                metadata_dict = broadcast_tensor_dict(src=0)\n                decode_attn_metadata = self.attn_backend.make_metadata(\n                    **metadata_dict)\n\n        attn_metadata = AttentionMetadata(\n            num_prefills=num_prefills,\n            slot_mapping=slot_mapping,\n            num_prefill_tokens=num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            prefill_metadata=prefill_attn_metadata,\n            decode_metadata=decode_attn_metadata,\n            kv_cache_dtype=self.kv_cache_dtype,\n        )\n\n        return (input_tokens, input_positions, attn_metadata,\n                sampling_metadata, lora_requests, lora_mapping,\n                multi_modal_input)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        kv_caches: List[torch.Tensor],\n    ) -> Optional[SamplerOutput]:\n        (input_tokens, input_positions, attn_metadata, sampling_metadata,\n         lora_requests, lora_mapping, multi_modal_input\n         ) = self.prepare_input_tensors(seq_group_metadata_list)\n\n        if self.lora_config:\n            self.set_active_loras(lora_requests, lora_mapping)\n\n        # Currently cuda graph is only supported by the decode phase.\n        prefill_meta = attn_metadata.prefill_metadata\n        decode_meta = attn_metadata.decode_metadata\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            graph_batch_size = input_tokens.shape[0]\n            model_executable = self.graph_runners[graph_batch_size]\n        else:\n            model_executable = self.model\n        execute_model_kwargs = {\n            \"input_ids\": input_tokens,\n            \"positions\": input_positions,\n            \"kv_caches\": kv_caches,\n            \"attn_metadata\": attn_metadata,\n        }\n        if self.vision_language_config:\n            execute_model_kwargs.update({\"image_input\": multi_modal_input})\n        hidden_states = model_executable(**execute_model_kwargs)\n\n        # Compute the logits.\n        logits = self.model.compute_logits(hidden_states, sampling_metadata)\n\n        # Only perform sampling in the driver worker.\n        if not self.is_driver_worker:\n            return None\n\n        # Sample the next token.\n        output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        return output\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests = []\n        dummy_lora_requests_per_seq = []\n        if self.lora_config:\n            for idx in range(self.lora_config.max_loras):\n                lora_id = idx + 1\n                dummy_lora_request = LoRARequest(\n                    lora_name=f\"warmup_{lora_id}\",\n                    lora_int_id=lora_id,\n                    lora_local_path=\"/not/a/real/path\",\n                )\n                self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                 rank=LORA_WARMUP_RANK)\n                dummy_lora_requests.append(dummy_lora_request)\n            dummy_lora_requests_per_seq = [\n                dummy_lora_requests[idx % len(dummy_lora_requests)]\n                for idx in range(max_num_seqs)\n            ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for vision encoding, which needs\n        # to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n        if self.vision_language_config:\n            max_num_seqs = min(\n                max_num_seqs,\n                int(max_num_batched_tokens /\n                    self.vision_language_config.image_feature_size))\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            seq_data, fake_multi_modal_input = _prepare_fake_inputs(\n                seq_len, self.vision_language_config)\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=fake_multi_modal_input,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        self.execute_model(seqs, kv_caches)\n        torch.cuda.synchronize()\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_loras()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_loras(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_loras()\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[torch.Tensor]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        # NOTE(woosuk): This is a hack to ensure that the NCCL backend is never\n        # deleted before the CUDA graphs.\n        self.pynccl_backend = pynccl_utils.get_nccl_backend()\n\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping = torch.empty(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping.fill_(_PAD_SLOT_ID)\n        seq_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()\n        block_tables = torch.from_numpy(self.graph_block_tables).cuda()\n\n        graph_batch_size = _get_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        # NOTE(woosuk): There are 3 backends for all-reduce: custom all-reduce\n        # kernel, pynccl, and PyTorch NCCL. When using CUDA graph, we use\n        # either custom all-reduce kernel or pynccl. When not using CUDA\n        # graph, we use either custom all-reduce kernel or PyTorch NCCL.\n        # We always prioritize using custom all-reduce kernel but fall back\n        # to PyTorch or pynccl if it is disabled or not supported.\n        with custom_all_reduce.capture():\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for batch_size in reversed(batch_size_capture_list):\n                # Create dummy attn_metadata.\n                decode_metadata = self.attn_backend.make_metadata(\n                    is_prompt=False,\n                    seq_lens=None,\n                    seq_lens_tensor=seq_lens[:batch_size],\n                    max_query_len=None,\n                    max_seq_len=self.max_seq_len_to_capture,\n                    subquery_start_loc=None,\n                    seq_start_loc=None,\n                    context_lens_tensor=None,\n                    block_tables=block_tables[:batch_size],\n                    use_cuda_graph=True,\n                )\n                attn_metadata = AttentionMetadata(\n                    num_prefills=0,\n                    num_prefill_tokens=0,\n                    num_decode_tokens=batch_size,\n                    slot_mapping=slot_mapping[:batch_size],\n                    prefill_metadata=None,\n                    decode_metadata=decode_metadata,\n                    kv_cache_dtype=self.kv_cache_dtype,\n                )\n\n                if self.lora_config:\n                    lora_mapping = LoRAMapping(\n                        [0] * batch_size,\n                        [0] * batch_size,\n                    )\n                    self.set_active_loras(set(), lora_mapping)\n\n                graph_runner = CUDAGraphRunner(self.model)\n                graph_runner.capture(\n                    input_tokens[:batch_size],\n                    input_positions[:batch_size],\n                    kv_caches,\n                    attn_metadata,\n                    memory_pool=self.graph_memory_pool,\n                )\n                self.graph_memory_pool = graph_runner.graph.pool()\n                self.graph_runners[batch_size] = graph_runner\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    def __del__(self) -> None:\n        # Delete the CUDA graphs before deleting the pynccl communicator.\n        # NOTE(woosuk): This is necessary because otherwise deadlocks can\n        # happen.\n        # FIXME(woosuk): This is a bit hacky. Find a more robust solution.\n        # TODO(youkaichao): when we get enough user feedback that pynccl is\n        # more stable than cupy, we can remove this, e.g. in v0.4.1.\n        self.graph_runners.clear()\n        self.pynccl_backend = None\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module):\n        self.model = model\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool,\n        **kwargs,\n    ) -> None:\n        assert self._graph is None\n        # Run the model once without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        with _maybe_pynccl():\n            self.model(\n                input_ids,\n                positions,\n                kv_caches,\n                attn_metadata,\n                **kwargs,\n            )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        # NOTE(woosuk): Python 3.8 does not support multi-line with statements.\n        # https://stackoverflow.com/questions/31039022/python-multi-line-with-statement\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool):  # noqa: SIM117\n            with _maybe_pynccl():\n                hidden_states = self.model(\n                    input_ids,\n                    positions,\n                    kv_caches,\n                    attn_metadata,\n                    **kwargs,\n                )\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\": input_ids,\n            \"positions\": positions,\n            \"kv_caches\": kv_caches,\n            \"slot_mapping\": attn_metadata.slot_mapping,\n            \"seq_lens_tensor\": attn_metadata.decode_metadata.seq_lens_tensor,\n            \"block_tables\": attn_metadata.decode_metadata.block_tables,\n        }\n        self.output_buffers = {\"hidden_states\": hidden_states}\n        return\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(attn_metadata.slot_mapping,\n                                                 non_blocking=True)\n        self.input_buffers[\"seq_lens_tensor\"].copy_(\n            attn_metadata.decode_metadata.seq_lens_tensor, non_blocking=True)\n        self.input_buffers[\"block_tables\"].copy_(\n            attn_metadata.decode_metadata.block_tables, non_blocking=True)\n        # Run the graph.\n        self.graph.replay()\n\n        # Return the output tensor.\n        return self.output_buffers[\"hidden_states\"]\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\n@contextlib.contextmanager\ndef _maybe_pynccl():\n    if pynccl_utils.is_initialized(\n    ) and not custom_all_reduce.is_initialized():\n        with with_pynccl_for_all_reduce():\n            yield\n    else:\n        yield\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n\n\ndef _prepare_fake_inputs(\n        seq_len: int, vision_language_config: Optional[VisionLanguageConfig]):\n    \"\"\"Prepare fake inputs for profile run.\"\"\"\n    if vision_language_config:\n        prompt_tokens = [\n            vision_language_config.image_token_id\n        ] * vision_language_config.image_feature_size + [0] * (\n            seq_len - vision_language_config.image_feature_size)\n        fake_image_input = MultiModalData(\n            type=MultiModalData.Type.IMAGE,\n            data=torch.zeros(vision_language_config.image_input_shape,\n                             dtype=torch.float16))\n    else:\n        prompt_tokens = [0] * seq_len\n        fake_image_input = None\n    return SequenceData(prompt_tokens), fake_image_input\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex c96f13c59..46c673064 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -835,20 +835,21 @@ class ModelRunner:\n         dummy_lora_requests = []\n         dummy_lora_requests_per_seq = []\n         if self.lora_config:\n-            for idx in range(self.lora_config.max_loras):\n-                lora_id = idx + 1\n-                dummy_lora_request = LoRARequest(\n-                    lora_name=f\"warmup_{lora_id}\",\n-                    lora_int_id=lora_id,\n-                    lora_local_path=\"/not/a/real/path\",\n-                )\n-                self.lora_manager.add_dummy_lora(dummy_lora_request,\n-                                                 rank=LORA_WARMUP_RANK)\n-                dummy_lora_requests.append(dummy_lora_request)\n-            dummy_lora_requests_per_seq = [\n-                dummy_lora_requests[idx % len(dummy_lora_requests)]\n-                for idx in range(max_num_seqs)\n-            ]\n+            with self.lora_manager.dummy_lora_cache():\n+                for idx in range(self.lora_config.max_loras):\n+                    lora_id = idx + 1\n+                    dummy_lora_request = LoRARequest(\n+                        lora_name=f\"warmup_{lora_id}\",\n+                        lora_int_id=lora_id,\n+                        lora_local_path=\"/not/a/real/path\",\n+                    )\n+                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n+                                                     rank=LORA_WARMUP_RANK)\n+                    dummy_lora_requests.append(dummy_lora_request)\n+                dummy_lora_requests_per_seq = [\n+                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n+                    for idx in range(max_num_seqs)\n+                ]\n \n         # Profile memory usage with max_num_sequences sequences and the total\n         # number of tokens equal to max_num_batched_tokens.",
      "change_type": "modified",
      "lines_added": 16,
      "lines_removed": 15
    }
  ],
  "affected_apis": [
    "LoRAModel.clone",
    "AbstractWorkerLoRAManager.dummy_lora_cache",
    "WorkerLoRAManager.add_dummy_lora",
    "LoRAManager.create_dummy_lora",
    "LoRAManager.add_lora"
  ],
  "summary": {
    "total_files": 3,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 3
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "No (not needed)",
    "is_benchmark_actually_there": "",
    "sample_clues": "add, cache, clone"
  }
}