{
  "commit_hash": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
  "parent_hash": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
  "message": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "author": "youkaichao <youkaichao@gmail.com>",
  "date": "2024-08-28 16:10:12 -0700",
  "files_changed": [
    {
      "file_path": ".buildkite/run-tpu-test.sh",
      "old_content": "set -e\n\n# Build the docker image.\ndocker build -f Dockerfile.tpu -t vllm-tpu .\n\n# Set up cleanup.\nremove_docker_container() { docker rm -f tpu-test || true; }\ntrap remove_docker_container EXIT\n# Remove the container that might not be cleaned up in the previous run.\nremove_docker_container\n\n# For HF_TOKEN.\nsource /etc/environment\n# Run a simple end-to-end example.\ndocker run --privileged --net host --shm-size=16G -it -e HF_TOKEN=$HF_TOKEN --name tpu-test vllm-tpu /bin/bash -c \"python3 -m pip install git+https://github.com/thuml/depyf.git && python3 /workspace/vllm/tests/tpu/test_compilation.py && python3 /workspace/vllm/examples/offline_inference_tpu.py\"\n",
      "diff": "diff --git a/.buildkite/run-tpu-test.sh b/.buildkite/run-tpu-test.sh\nindex 335ffd83f..6989c94d4 100644\n--- a/.buildkite/run-tpu-test.sh\n+++ b/.buildkite/run-tpu-test.sh\n@@ -12,4 +12,4 @@ remove_docker_container\n # For HF_TOKEN.\n source /etc/environment\n # Run a simple end-to-end example.\n-docker run --privileged --net host --shm-size=16G -it -e HF_TOKEN=$HF_TOKEN --name tpu-test vllm-tpu /bin/bash -c \"python3 -m pip install git+https://github.com/thuml/depyf.git && python3 /workspace/vllm/tests/tpu/test_compilation.py && python3 /workspace/vllm/examples/offline_inference_tpu.py\"\n+docker run --privileged --net host --shm-size=16G -it -e HF_TOKEN=$HF_TOKEN --name tpu-test vllm-tpu /bin/bash -c \"python3 -m pip install git+https://github.com/thuml/depyf.git && python3 -m pip install pytest  && pytest -v -s /workspace/vllm/tests/tpu/test_custom_dispatcher.py && python3 /workspace/vllm/tests/tpu/test_compilation.py && python3 /workspace/vllm/examples/offline_inference_tpu.py\"",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": ".buildkite/test-pipeline.yaml",
      "old_content": "# In this file, you can add more tests to run either by adding a new step or\n# adding a new command to an existing step. See different options here for examples.\n\n# This script will be feed into Jinja template in `test-template-aws.j2` at\n# https://github.com/vllm-project/buildkite-ci/blob/main/scripts/test-template-aws.j2 \n# to generate the final pipeline yaml file.\n\n# Documentation\n# label(str): the name of the test. emoji allowed.\n# fast_check(bool): whether to run this on each commit on fastcheck pipeline.\n# fast_check_only(bool): run this test on fastcheck pipeline only\n# command(str): the single command to run for tests. incompatible with commands.\n# commands(list): the list of commands to run for test. incompatbile with command.\n# mirror_hardwares(list): the list of hardwares to run the test on as well. currently only supports [amd]\n# gpu(str): override the GPU selection for the test. default is on L4 GPUs. currently only supports a100\n# num_gpus(int): override the number of GPUs for the test. default to 1 GPU. currently support 2,4.\n# num_nodes(int): whether to simulate multi-node setup by launch multiple containers on one host, \n#     in this case, commands must be specified. the first command runs on first host, the second\n#     command runs on the second host.\n# working_dir(str): specify the place where command should execute, default to /vllm-workspace/tests\n# source_file_dependencies(list): the list of prefix to opt-in the test for, if empty, the test will always run.\n\n# When adding a test\n# - If the test belong to an existing group, add it there\n# - If the test is short, add to any existing step\n# - If the test takes more than 10min, then it is okay to create a new step. \n#   Note that all steps execute in parallel. \n\nsteps:\n##### fast check tests  #####\n\n- label: Documentation Build # 2min\n  working_dir: \"/vllm-workspace/test_docs/docs\"\n  fast_check: true\n  no_gpu: True\n  commands:\n  - pip install -r requirements-docs.txt\n  - SPHINXOPTS=\\\"-W\\\" make html\n  # Check API reference (if it fails, you may have missing mock imports)\n  - grep \\\"sig sig-object py\\\" build/html/dev/sampling_params.html\n\n- label: Async Engine, Inputs, Utils, Worker Test # 15min\n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/async_engine\n  - tests/test_inputs\n  - tests/multimodal\n  - tests/test_utils\n  - tests/worker\n  commands:\n  - pytest -v -s async_engine # Async Engine\n  - pytest -v -s test_inputs.py\n  - pytest -v -s multimodal\n  - pytest -v -s test_utils.py # Utils\n  - pytest -v -s worker # Worker\n\n- label: Basic Correctness Test # 30min\n  #mirror_hardwares: [amd]\n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/basic_correctness\n  commands:\n  - pytest -v -s basic_correctness/test_basic_correctness.py\n  - pytest -v -s basic_correctness/test_cpu_offload.py\n  - VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py\n  - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py\n  - VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest -v -s basic_correctness/test_preemption.py\n  \n- label: Core Test # 10min\n  mirror_hardwares: [amd]\n  fast_check: true\n  source_file_dependencies:\n  - vllm/core\n  - vllm/distributed\n  - tests/core\n  commands:\n  - pytest -v -s core\n\n- label: Entrypoints Test # 20min\n  working_dir: \"/vllm-workspace/tests\"\n  fast_check: true\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  commands:\n  - pip install -e ./plugins/vllm_add_dummy_model\n  - pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git@a4987bba6e9e9b3f22bd3a6c1ecf0abd04fd5622#egg=lm_eval[api]\n  - pytest -v -s entrypoints/llm --ignore=entrypoints/llm/test_lazy_outlines.py\n  - pytest -v -s entrypoints/llm/test_lazy_outlines.py # it needs a clean process\n  - pytest -v -s entrypoints/openai\n\n- label: Distributed Tests (4 GPUs) # 10min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  fast_check: true\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/core/\n  - tests/distributed\n  - tests/spec_decode/e2e/test_integration_dist_tp4\n  commands:\n  - pytest -v -s distributed/test_pynccl.py\n  - pytest -v -s spec_decode/e2e/test_integration_dist_tp4.py\n\n- label: Metrics, Tracing Test # 10min\n  num_gpus: 2 \n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/metrics\n  - tests/tracing\n  commands:\n  - pytest -v -s metrics \n  - \"pip install \\\n      'opentelemetry-sdk>=1.26.0,<1.27.0' \\\n      'opentelemetry-api>=1.26.0,<1.27.0' \\\n      'opentelemetry-exporter-otlp>=1.26.0,<1.27.0' \\\n      'opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0'\"\n  - pytest -v -s tracing\n\n##### fast check tests  #####\n#####  1 GPU test  #####\n\n- label: Regression Test # 5min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/test_regression\n  command: pytest -v -s test_regression.py\n  working_dir: \"/vllm-workspace/tests\" # optional\n\n- label: Engine Test # 10min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/engine\n  - tests/tokenization\n  commands:\n  - pytest -v -s engine test_sequence.py test_config.py test_logger.py\n  # OOM in the CI unless we run this separately\n  - pytest -v -s tokenization\n\n- label: Examples Test # 12min\n  working_dir: \"/vllm-workspace/examples\"\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/entrypoints\n  - examples/\n  commands:\n    - pip install awscli tensorizer # for llava example and tensorizer test\n    - python3 offline_inference.py\n    - python3 cpu_offload.py\n    - python3 offline_inference_chat.py\n    - python3 offline_inference_with_prefix.py\n    - python3 llm_engine_example.py\n    - python3 offline_inference_vision_language.py\n    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors\n    - python3 offline_inference_encoder_decoder.py\n\n- label: Models Test # 1hr10min\n  source_file_dependencies:\n  - vllm/\n  - tests/models\n  commands:\n    - pip install -e ./plugins/vllm_add_dummy_model\n    - pytest -v -s models/test_oot_registration.py # it needs a clean process\n    - pytest -v -s models -m \\\"not vlm\\\" --ignore=models/test_oot_registration.py\n\n- label: torch compile integration test\n  source_file_dependencies:\n  - vllm/\n  commands:\n    - pytest -v -s ./compile/test_full_graph.py\n\n\n- label: Vision Language Models Test # 42min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  commands:\n    - pytest -v -s models -m vlm\n\n- label: Prefix Caching Test # 7min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/prefix_caching\n  commands:\n    - pytest -v -s prefix_caching\n\n- label: Samplers Test # 18min\n  source_file_dependencies:\n  - vllm/model_executor/layers\n  - vllm/sampling_metadata.py\n  - tests/samplers\n  commands:\n    - pytest -v -s samplers\n    - VLLM_USE_FLASHINFER_SAMPLER=1 pytest -v -s samplers\n\n- label: LogitsProcessor Test # 5min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/model_executor/layers\n  - tests/test_logits_processor\n  command: pytest -v -s test_logits_processor.py\n\n- label: Speculative decoding tests # 22min\n  source_file_dependencies:\n  - vllm/spec_decode\n  - tests/spec_decode\n  commands:\n    # See https://github.com/vllm-project/vllm/issues/5152\n    - export VLLM_ATTENTION_BACKEND=XFORMERS\n    - pytest -v -s spec_decode\n\n- label: LoRA Test %N # 30min each\n  source_file_dependencies:\n  - vllm/lora\n  - csrc/punica\n  - tests/lora\n  command: pytest -v -s lora --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT --ignore=lora/test_long_context.py\n  parallelism: 4\n\n- label: Kernels Test %N # 30min each\n  source_file_dependencies:\n  - csrc/\n  - vllm/attention\n  - tests/kernels\n  commands:\n    - pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT\n  parallelism: 4\n\n- label: Tensorizer Test # 11min\n  mirror_hardwares: [amd]\n  soft_fail: true\n  source_file_dependencies:\n  - vllm/model_executor/model_loader\n  - tests/tensorizer_loader\n  commands:\n    - apt-get update && apt-get install -y curl libsodium23\n    - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n    - pytest -v -s tensorizer_loader\n\n- label: Benchmarks # 9min\n  working_dir: \"/vllm-workspace/.buildkite\"\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - benchmarks/\n  commands:\n  - pip install aiohttp\n  - bash run-benchmarks.sh\n\n- label: Quantization Test # 15min\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  - tests/quantization\n  command: pytest -v -s quantization\n\n- label: LM Eval Small Models # 53min\n  working_dir: \"/vllm-workspace/.buildkite/lm-eval-harness\"\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  commands:\n  - pip install lm-eval\n  - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n  - bash ./run-tests.sh -c configs/models-small.txt -t 1\n\n#####  1 GPU test  #####\n#####  multi gpus test  #####\n\n- label: Distributed Comm Ops Test # 7min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/distributed\n  - tests/distributed\n  commands:\n  - pytest -v -s distributed/test_comm_ops.py\n  - pytest -v -s distributed/test_shm_broadcast.py\n\n- label: 2 Node Tests (4 GPUs in total) # 16min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  num_nodes: 2\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  commands:\n  - # the following commands are for the first node, with ip 192.168.10.10 (ray environment already set up)\n    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py\n    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_multi_node_assignment.py\n    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_pipeline_parallel.py\n  - # the following commands are for the second node, with ip 192.168.10.11 (ray environment already set up)\n    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py\n\n- label: Distributed Tests (2 GPUs) # 28min\n  #mirror_hardwares: [amd]\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  commands:\n  - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py\n  - TARGET_TEST_SUITE=L4 pytest -v -s distributed/test_basic_distributed_correctness.py\n  - pytest -v -s distributed/test_basic_distributed_correctness_enc_dec.py\n  - pytest -v -s distributed/test_chunked_prefill_distributed.py\n  - pytest -v -s distributed/test_multimodal_broadcast.py\n  - pytest -v -s spec_decode/e2e/test_integration_dist_tp2.py\n  - pip install -e ./plugins/vllm_add_dummy_model\n  - pytest -v -s distributed/test_distributed_oot.py\n  - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s test_sharded_state_loader.py\n  - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s distributed/test_utils.py\n\n- label: Multi-step Tests (4 GPUs) # 21min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/model_executor/layers/sampler.py\n  - vllm/sequence.py\n  - vllm/worker/worker_base.py\n  - vllm/worker/worker.py\n  - vllm/worker/multi_step_worker.py\n  - vllm/worker/model_runner_base.py\n  - vllm/worker/model_runner.py\n  - vllm/worker/multi_step_model_runner.py\n  - vllm/engine\n  - tests/multi_step\n  commands:\n  - pytest -v -s multi_step/test_correctness_async_llm.py\n  - pytest -v -s multi_step/test_correctness_llm.py\n\n- label: Pipeline Parallelism Test # 23min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  commands:\n  - pytest -v -s distributed/test_pp_cudagraph.py\n  - pytest -v -s distributed/test_pipeline_parallel.py\n\n- label: LoRA Long Context (Distributed) # 11min\n  # This test runs llama 13B, so it is required to run on 4 GPUs.\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/lora\n  - csrc/punica\n  - tests/lora/test_long_context\n  commands:\n    # FIXIT: find out which code initialize cuda before running the test\n    # before the fix, we need to use spawn to test it\n    - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n    - pytest -v -s -x lora/test_long_context.py\n\n- label: Weight Loading Multiple GPU Test\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/\n  - tests/weight_loading\n  commands:\n    - bash weight_loading/run_model_weight_loading_test.sh\n\n\n##### multi gpus test #####\n##### A100 test #####\n\n- label: Distributed Tests (A100) # optional\n  gpu: a100\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/\n  commands: \n  # NOTE: don't test llama model here, it seems hf implementation is buggy\n  # see https://github.com/vllm-project/vllm/pull/5689 for details\n  - pytest -v -s distributed/test_custom_all_reduce.py\n  - TARGET_TEST_SUITE=A100 pytest -v -s distributed/test_basic_distributed_correctness.py\n  - pytest -v -s -x lora/test_mixtral.py\n\n- label: LM Eval Large Models # optional\n  gpu: a100\n  num_gpus: 4\n  working_dir: \"/vllm-workspace/.buildkite/lm-eval-harness\"\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  commands:\n  - pip install lm-eval\n  - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n  - bash ./run-tests.sh -c configs/models-large.txt -t 4\n",
      "diff": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 9f449ff65..235db72ee 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -173,6 +173,7 @@ steps:\n   - vllm/\n   commands:\n     - pytest -v -s ./compile/test_full_graph.py\n+    - pytest -v -s ./compile/test_wrapper.py\n \n \n - label: Vision Language Models Test # 42min",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 1
    },
    {
      "file_path": "tests/compile/test_wrapper.py",
      "old_content": "",
      "diff": "diff --git a/tests/compile/test_wrapper.py b/tests/compile/test_wrapper.py\nnew file mode 100644\nindex 000000000..cef516ade\n--- /dev/null\n+++ b/tests/compile/test_wrapper.py\n@@ -0,0 +1,59 @@\n+from typing import Optional\n+\n+import torch\n+\n+from vllm.compilation.wrapper import TorchCompileWrapperWithCustomDispacther\n+\n+\n+class MyMod(torch.nn.Module):\n+\n+    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        if cache is not None:\n+            return x + cache\n+        return x * 2\n+\n+\n+class MyWrapper(TorchCompileWrapperWithCustomDispacther):\n+\n+    def __init__(self, model):\n+        self.model = model\n+        compiled_callable = torch.compile(self.forward, backend=\"eager\")\n+        super().__init__(compiled_callable)\n+\n+    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        # this is the function to be compiled\n+        return self.model(x, cache)\n+\n+    def __call__(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        # let torch.compile compile twice\n+        if len(self.compiled_codes) == 2:\n+            dispatch_id = 0 if cache is None else 1\n+            with self.dispatch_to_code(dispatch_id):\n+                return self.forward(x, cache)\n+        else:\n+            return self.compiled_callable(x, cache)\n+\n+\n+def test_torch_compile_wrapper():\n+    mod = MyMod()\n+    wrappers = []\n+    for i in range(3):\n+        torch._dynamo.reset()\n+        wrapper = MyWrapper(mod)\n+        wrappers.append(wrapper)\n+        x = torch.tensor([1])\n+        wrapper(x, None)  # profile run, compile\n+        # create a cache tensor\n+        cache = torch.tensor([2])\n+        wrapper(x, cache)  # warm up with cache, recompile\n+\n+        # for new input, dispatch to the compiled code directly\n+        new_x = torch.tensor([3])\n+        assert wrapper(new_x,\n+                       None).item() == 6  # dispatch to the first compiled code\n+        assert wrapper(\n+            new_x, cache).item() == 5  # dispatch to the second compiled code\n+\n+    for wrapper in wrappers:\n+        # make sure they have independent compiled codes\n+        assert len(wrapper.compiled_codes) == 2",
      "change_type": "added",
      "lines_added": 60,
      "lines_removed": 1
    },
    {
      "file_path": "tests/tpu/__init__.py",
      "old_content": "",
      "diff": "diff --git a/tests/tpu/__init__.py b/tests/tpu/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb",
      "change_type": "added",
      "lines_added": 0,
      "lines_removed": 0
    },
    {
      "file_path": "tests/tpu/test_custom_dispatcher.py",
      "old_content": "",
      "diff": "diff --git a/tests/tpu/test_custom_dispatcher.py b/tests/tpu/test_custom_dispatcher.py\nnew file mode 100644\nindex 000000000..7f3fb5953\n--- /dev/null\n+++ b/tests/tpu/test_custom_dispatcher.py\n@@ -0,0 +1,9 @@\n+from ..utils import compare_two_settings\n+\n+\n+def test_custom_dispatcher():\n+    compare_two_settings(\"google/gemma-2b\",\n+                         arg1=[\"--enforce-eager\"],\n+                         arg2=[\"--enforce-eager\"],\n+                         env1={\"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\": \"0\"},\n+                         env2={})",
      "change_type": "added",
      "lines_added": 10,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/compilation/__init__.py",
      "old_content": "",
      "diff": "diff --git a/vllm/compilation/__init__.py b/vllm/compilation/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb",
      "change_type": "added",
      "lines_added": 0,
      "lines_removed": 0
    },
    {
      "file_path": "vllm/compilation/wrapper.py",
      "old_content": "",
      "diff": "diff --git a/vllm/compilation/wrapper.py b/vllm/compilation/wrapper.py\nnew file mode 100644\nindex 000000000..c3d863299\n--- /dev/null\n+++ b/vllm/compilation/wrapper.py\n@@ -0,0 +1,81 @@\n+import os\n+import sys\n+from abc import abstractmethod\n+from contextlib import contextmanager\n+from types import CodeType\n+from typing import Callable, List\n+\n+import torch\n+\n+import vllm.envs as envs\n+\n+\n+class TorchCompileWrapperWithCustomDispacther:\n+    \"\"\"\n+    A wrapper class for torch.compile, with a custom dispatch logic.\n+    Subclasses should:\n+    1. Implement the forward method\n+    2. Implement the dispatch logic in the __call__ method\n+        It can use `self.compiled_codes` to access the compiled bytecode,\n+        and `with self.dispatch_to_code(index):` to dispatch to\n+        the compiled code.\n+    3. Implement the `__init__` method to determine how to call\n+        `torch.compile` over the forward method.\n+    \"\"\"\n+\n+    def __init__(self, compiled_callable: Callable):\n+        self.compiled_callable = compiled_callable\n+        self.original_code_object = self.__class__.forward.__code__\n+        self.compiled_codes: List[CodeType] = []\n+        torch._dynamo.convert_frame.register_bytecode_hook(self.bytecode_hook)\n+\n+        # read the env var to determine whether to use the custom dispatcher\n+        # subclasses can use this to switch between the custom dispatcher\n+        # and the default Dynamo guard mechanism.\n+        self.use_custom_dispatcher: bool = \\\n+            envs.VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\n+\n+    def __call__(self, *args, **kwargs):\n+        \"\"\"Implement the dispatch logic here, beyond the torch.compile level.\n+        NOTE: this function can have additional arguments beyond the forward\n+         method, for directly dispatching to the compiled code.\n+        \"\"\"\n+        return self.compiled_callable(*args, **kwargs)\n+\n+    @abstractmethod\n+    def forward(self, *args, **kwargs):\n+        ...\n+\n+    def bytecode_hook(self, old_code: CodeType, new_code: CodeType):\n+        \"\"\"Hook to save the compiled bytecode for direct execution.\"\"\"\n+        if old_code is not self.original_code_object:\n+            return\n+        # code borrowed from https://github.com/thuml/depyf/blob/f4ad79fadee27ea113b4c75202db1eb1a11c0dbc/depyf/explain/enable_debugging.py#L25\n+        frame = sys._getframe()\n+        while True:\n+            frame = frame.f_back\n+            code_name = frame.f_code.co_name\n+            file_name = frame.f_code.co_filename.split(os.path.sep)[-1]\n+            if code_name == \"_compile\" and file_name == \"convert_frame.py\":\n+                break\n+        frame = frame.f_locals[\"frame\"]\n+        assert frame.f_code == old_code\n+\n+        if frame.f_locals[\"self\"] is not self:\n+            return\n+\n+        self.compiled_codes.append(new_code)\n+\n+    @contextmanager\n+    def dispatch_to_code(self, index: int):\n+        \"\"\"Context manager to dispatch to the compiled code.\n+        Why does this work? Because Dynamo guarantees that the compiled\n+        bytecode has exactly the same arguments, cell variables, and free\n+        variables as the original code. Therefore we can directly switch\n+        the code object in the function and call it.\n+\n+        See https://dev-discuss.pytorch.org/t/what-is-the-relationship-requirement-among-original-bytecode-transformed-bytecode-and-bytecode-returned-by-hooks-in-dynamo/1693/7 for more details.\n+        \"\"\" # noqa\n+        self.__class__.forward.__code__ = self.compiled_codes[index]\n+        yield\n+        self.__class__.forward.__code__ = self.original_code_object",
      "change_type": "added",
      "lines_added": 82,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/envs.py",
      "old_content": "import os\nimport tempfile\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n\nif TYPE_CHECKING:\n    VLLM_HOST_IP: str = \"\"\n    VLLM_PORT: Optional[int] = None\n    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()\n    VLLM_USE_MODELSCOPE: bool = False\n    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60\n    VLLM_INSTANCE_ID: Optional[str] = None\n    VLLM_NCCL_SO_PATH: Optional[str] = None\n    LD_LIBRARY_PATH: Optional[str] = None\n    VLLM_USE_TRITON_FLASH_ATTN: bool = False\n    LOCAL_RANK: int = 0\n    CUDA_VISIBLE_DEVICES: Optional[str] = None\n    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60\n    VLLM_API_KEY: Optional[str] = None\n    S3_ACCESS_KEY_ID: Optional[str] = None\n    S3_SECRET_ACCESS_KEY: Optional[str] = None\n    S3_ENDPOINT_URL: Optional[str] = None\n    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")\n    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")\n    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"\n    VLLM_NO_USAGE_STATS: bool = False\n    VLLM_DO_NOT_TRACK: bool = False\n    VLLM_USAGE_SOURCE: str = \"\"\n    VLLM_CONFIGURE_LOGGING: int = 1\n    VLLM_LOGGING_LEVEL: str = \"INFO\"\n    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None\n    VLLM_TRACE_FUNCTION: int = 0\n    VLLM_ATTENTION_BACKEND: Optional[str] = None\n    VLLM_USE_FLASHINFER_SAMPLER: bool = False\n    VLLM_PP_LAYER_PARTITION: Optional[str] = None\n    VLLM_CPU_KVCACHE_SPACE: int = 0\n    VLLM_CPU_OMP_THREADS_BIND: str = \"\"\n    VLLM_OPENVINO_KVCACHE_SPACE: int = 0\n    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None\n    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False\n    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")\n    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024\n    VLLM_USE_RAY_SPMD_WORKER: bool = False\n    VLLM_USE_RAY_COMPILED_DAG: bool = False\n    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True\n    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"\n    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")\n    VLLM_IMAGE_FETCH_TIMEOUT: int = 5\n    VLLM_AUDIO_FETCH_TIMEOUT: int = 5\n    VLLM_TARGET_DEVICE: str = \"cuda\"\n    MAX_JOBS: Optional[str] = None\n    NVCC_THREADS: Optional[str] = None\n    VLLM_USE_PRECOMPILED: bool = False\n    VLLM_NO_DEPRECATION_WARNING: bool = False\n    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False\n    CMAKE_BUILD_TYPE: Optional[str] = None\n    VERBOSE: bool = False\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False\n    VLLM_TEST_FORCE_FP8_MARLIN: bool = False\n    VLLM_RPC_GET_DATA_TIMEOUT_MS: int = 5000\n    VLLM_ALLOW_ENGINE_USE_RAY: bool = False\n    VLLM_PLUGINS: Optional[List[str]] = None\n    VLLM_TORCH_PROFILER_DIR: Optional[str] = None\n\n\ndef get_default_cache_root():\n    return os.getenv(\n        \"XDG_CACHE_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n    )\n\n\ndef get_default_config_root():\n    return os.getenv(\n        \"XDG_CONFIG_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n    )\n\n\n# The begin-* and end* here are used by the documentation generator\n# to extract the used env vars.\n\n# begin-env-vars-definition\n\nenvironment_variables: Dict[str, Callable[[], Any]] = {\n\n    # ================== Installation Time Env Vars ==================\n\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, neuron, cpu, openvino]\n    \"VLLM_TARGET_DEVICE\":\n    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),\n\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\":\n    lambda: os.getenv(\"MAX_JOBS\", None),\n\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\":\n    lambda: os.getenv(\"NVCC_THREADS\", None),\n\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\":\n    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")),\n\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\":\n    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),\n\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\":\n    lambda: bool(int(os.getenv('VERBOSE', '0'))),\n\n    # Root directory for VLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )),\n\n    # ================== Runtime Env Vars ==================\n\n    # Root directory for VLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )),\n\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    'VLLM_HOST_IP':\n    lambda: os.getenv('VLLM_HOST_IP', \"\") or os.getenv(\"HOST_IP\", \"\"),\n\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    # '0' is used to make mypy happy\n    'VLLM_PORT':\n    lambda: int(os.getenv('VLLM_PORT', '0'))\n    if 'VLLM_PORT' in os.environ else None,\n\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    'VLLM_RPC_BASE_PATH':\n    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),\n\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\":\n    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n\n    # Instance id represents an instance of the VLLM. All processes in the same\n    # instance should have the same instance id.\n    \"VLLM_INSTANCE_ID\":\n    lambda: os.environ.get(\"VLLM_INSTANCE_ID\", None),\n\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),\n\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\":\n    lambda: os.environ.get(\"CUDA_HOME\", None),\n\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\":\n    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\":\n    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n\n    # flag to control if vllm should use triton flash attention\n    \"VLLM_USE_TRITON_FLASH_ATTN\":\n    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Internal flag to enable Dynamo graph capture\n    \"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\":\n    lambda: int(os.environ.get(\"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\", \"0\")),\n\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\":\n    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\":\n    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":\n    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),\n\n    # API key for VLLM API server\n    \"VLLM_API_KEY\":\n    lambda: os.environ.get(\"VLLM_API_KEY\", None),\n\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\":\n    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\":\n    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\":\n    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\":\n    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),\n    \"VLLM_NO_USAGE_STATS\":\n    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DO_NOT_TRACK\":\n    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(\n        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",\n    \"VLLM_USAGE_SOURCE\":\n    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\":\n    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),\n    \"VLLM_LOGGING_CONFIG_PATH\":\n    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\":\n    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),\n\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\":\n    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n\n    # Backend for attention computation\n    # Available options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"XFORMERS\": use XFormers\n    # - \"ROCM_FLASH\": use ROCmFlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    \"VLLM_ATTENTION_BACKEND\":\n    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),\n\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_SAMPLER\", \"0\"))),\n\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\":\n    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n\n    # (CPU backend only) CPU key-value cache space.\n    # default is 4GB\n    \"VLLM_CPU_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),\n\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\":\n    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),\n\n    # OpenVINO key-value cache space\n    # default is 4GB\n    \"VLLM_OPENVINO_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),\n\n    # OpenVINO KV cache precision\n    # default is bf16 if natively supported by platform, otherwise f16\n    # To enable KV cache compression, please, explicitly specify u8\n    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":\n    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),\n\n    # Enables weights compression during model export via HF Optimum\n    # default is False\n    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":\n    lambda: bool(os.getenv(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", False)),\n\n    # If the env var is set, then all workers will execute as separate\n    # processes from the engine, and we use the same mechanism to trigger\n    # execution on all workers.\n    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.\n    \"VLLM_USE_RAY_SPMD_WORKER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),\n\n    # If the env var is set, it uses the Ray's compiled DAG API\n    # which optimizes the control plane overhead.\n    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n    \"VLLM_USE_RAY_COMPILED_DAG\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),\n\n    # If the env var is set, it uses NCCL for communication in\n    # Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))\n                 ),\n\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\":\n    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),\n\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )),\n\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"5\")),\n\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"65536\")),\n\n    # If set, vllm will skip the deprecation warnings.\n    \"VLLM_NO_DEPRECATION_WARNING\":\n    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),\n\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":\n    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),\n\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\":\n    lambda:\n    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_GET_DATA_TIMEOUT_MS\":\n    lambda: int(os.getenv(\"VLLM_RPC_GET_DATA_TIMEOUT_MS\", \"5000\")),\n\n    # If set, allow running the engine as a separate ray actor,\n    # which is a deprecated feature soon to be removed.\n    # See https://github.com/vllm-project/vllm/issues/7045\n    \"VLLM_ALLOW_ENGINE_USE_RAY\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_ENGINE_USE_RAY\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\":\n    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[\n        \"VLLM_PLUGINS\"].split(\",\"),\n\n    # Enables torch profiler if set. Path to the directory where torch profiler\n    # traces are saved. Note that it must be an absolute path.\n    \"VLLM_TORCH_PROFILER_DIR\":\n    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os\n             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),\n\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n}\n\n# end-env-vars-definition\n\n\ndef __getattr__(name: str):\n    # lazy evaluation of environment variables\n    if name in environment_variables:\n        return environment_variables[name]()\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef __dir__():\n    return list(environment_variables.keys())\n",
      "diff": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 4faafd9da..590698416 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -196,6 +196,10 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # Internal flag to enable Dynamo graph capture\n     \"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\":\n     lambda: int(os.environ.get(\"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\", \"0\")),\n+    \"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\":\n+    lambda:\n+    (os.environ.get(\"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\", \"True\").lower() in\n+     (\"true\", \"1\")),\n \n     # local rank of the process in the distributed setting, used to determine\n     # the GPU device id",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/worker/tpu_model_runner.py",
      "old_content": "import time\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type, Union\nfrom unittest.mock import patch\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch_xla.core.xla_model as xm\nimport torch_xla.runtime as xr\n\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                           Logprob, SamplerOutput, SequenceGroupMetadata,\n                           SequenceOutput)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase,\n    _add_attn_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\n# Here we utilize the behavior that out-of-bound index is ignored.\n# FIXME(woosuk): Find a more reliable way to prevent possible bugs.\n_PAD_SLOT_ID = 1_000_000_000\n# FIXME(woosuk): Temporarily disabled top-p sampling since it's too slow.\n_ENABLE_TOP_P = False\n# FIXME(woosuk): A temporary hack to support `n > 1`.\n# This can significantly affect the performance if too large.\n_MAX_NUM_SAMPLES = 128\n\n\n@dataclass(frozen=True)\nclass ModelInputForTPU(ModelRunnerInputBase):\n    token_ids: torch.Tensor\n    position_ids: torch.Tensor\n    attn_metadata: AttentionMetadata\n    input_lens: torch.Tensor\n    t: torch.Tensor\n    p: torch.Tensor\n    num_samples: int\n    best_of: List[int]\n    seq_groups: List[List[int]]\n    virtual_engine: int = 0\n\n    def as_broadcastable_tensor_dict(\n            self) -> Dict[str, Union[int, torch.Tensor]]:\n        tensor_dict = {\n            \"token_ids\": self.token_ids,\n            \"position_ids\": self.position_ids,\n            \"input_lens\": self.input_lens,\n            \"t\": self.t,\n            \"p\": self.p,\n            \"num_samples\": self.num_samples,\n            \"best_of\": self.best_of,\n            \"seq_groups\": self.seq_groups,\n            \"virtual_engine\": self.virtual_engine,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[\"ModelInputForTPU\"],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForTPU\":\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        is_driver_worker: bool = False,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n\n        self.block_size = self.cache_config.block_size\n        self.max_num_blocks_per_seq = (self.model_config.max_model_len //\n                                       self.block_size)\n        self.block_tables = np.zeros(\n            (self.scheduler_config.max_num_seqs, self.max_num_blocks_per_seq),\n            dtype=np.int32)\n        self.attn_backend = get_attn_backend(\n            self.model_config.get_num_attention_heads(self.parallel_config),\n            self.model_config.get_head_size(),\n            self.model_config.get_num_kv_heads(self.parallel_config),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.cache_config.cache_dtype,\n            self.block_size,\n            False,\n        )\n\n    def load_model(self) -> None:\n        self.device = self.device_config.device\n\n        # NOTE(woosuk): While the executor assigns the TP ranks to the worker\n        # process, the ranks can be different from the ranks internally assigned\n        # by the xm runtime. Therefore, there is a mismatch in the rank\n        # assignment between the gloo (cpu) runtime and the xm (tpu) runtime.\n        # This is not a problem in linear layers because all-reduce is\n        # rank-agnostic. However, it matters for all-gather as the ranks\n        # determine the order of concatenating the output tensors.\n        # As a workaround, we use the xm's rank assignment only when loading\n        # the embedding weights.\n        xm_tp_rank = xr.global_ordinal()\n        with patch(\n                \"vllm.model_executor.layers.vocab_parallel_embedding.\"\n                \"get_tensor_model_parallel_rank\",\n                return_value=xm_tp_rank):\n            model = get_model(\n                model_config=self.model_config,\n                load_config=self.load_config,\n                device_config=self.device_config,\n                parallel_config=self.parallel_config,\n                cache_config=self.cache_config,\n                scheduler_config=self.scheduler_config,\n                lora_config=None,\n            )\n        model = model.eval()\n        xm.wait_device_ops()\n        model = ModelWrapper(model)\n        self.model = torch.compile(model,\n                                   backend=\"openxla\",\n                                   fullgraph=True,\n                                   dynamic=False)\n\n    def _dummy_run(\n        self,\n        batch_size: int,\n        seq_len: int,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        is_prompt: bool,\n    ) -> None:\n        if is_prompt:\n            seq_len = (seq_len + 15) // 16 * 16\n            token_ids = torch.zeros((batch_size, seq_len),\n                                    dtype=torch.int32,\n                                    device=self.device)\n            position_ids = torch.zeros((batch_size, seq_len),\n                                       dtype=torch.int32,\n                                       device=self.device)\n            slot_mapping = torch.zeros((batch_size, seq_len),\n                                       dtype=torch.int64,\n                                       device=self.device)\n            attn_metadata = self.attn_backend.make_metadata(\n                num_prefills=batch_size,\n                num_prefill_tokens=batch_size * seq_len,\n                num_decode_tokens=0,\n                slot_mapping=slot_mapping,\n                block_tables=None,\n                context_lens=None,\n            )\n            input_lens = torch.ones((batch_size, ),\n                                    dtype=torch.int32,\n                                    device=self.device)\n        else:\n            assert seq_len == 1\n            token_ids = torch.zeros((batch_size, seq_len),\n                                    dtype=torch.int32,\n                                    device=self.device)\n            position_ids = torch.zeros((batch_size, seq_len),\n                                       dtype=torch.int32,\n                                       device=self.device)\n            slot_mapping = torch.zeros((batch_size, seq_len),\n                                       dtype=torch.int64,\n                                       device=self.device)\n            block_tables = torch.zeros(\n                (batch_size, self.max_num_blocks_per_seq),\n                dtype=torch.int32,\n                device=self.device)\n            context_lens = torch.ones((batch_size, ),\n                                      dtype=torch.int32,\n                                      device=self.device)\n            input_lens = torch.ones((batch_size, ),\n                                    dtype=torch.int32,\n                                    device=self.device)\n            attn_metadata = self.attn_backend.make_metadata(\n                num_prefills=0,\n                num_prefill_tokens=0,\n                num_decode_tokens=batch_size * seq_len,\n                slot_mapping=slot_mapping,\n                block_tables=block_tables,\n                context_lens=context_lens,\n            )\n        t = torch.ones((batch_size, ), dtype=torch.float32, device=self.device)\n        p = torch.ones((batch_size, ), dtype=torch.float32, device=self.device)\n        num_samples = _MAX_NUM_SAMPLES if is_prompt else 1\n\n        # NOTE(woosuk): There are two stages of compilation: torch.compile and\n        # XLA compilation. Using `mark_dynamic` can reduce the torch.compile\n        # overhead by reusing the FX graph for different shapes.\n        # However, the XLA graph will still require static shapes and needs to\n        # be re-compiled for every different shapes. This overhead is inevitable\n        # in the first run, but can be skipped afterwards as we cache the XLA\n        # graphs in the disk (VLLM_XLA_CACHE_PATH).\n        if is_prompt:\n            # Prefll\n            torch._dynamo.mark_dynamic(token_ids, 1)\n            torch._dynamo.mark_dynamic(position_ids, 1)\n            torch._dynamo.mark_dynamic(attn_metadata.slot_mapping, 1)\n        else:\n            # Decode\n            torch._dynamo.mark_dynamic(token_ids, 0)\n            torch._dynamo.mark_dynamic(position_ids, 0)\n            torch._dynamo.mark_dynamic(input_lens, 0)\n            torch._dynamo.mark_dynamic(attn_metadata.slot_mapping, 0)\n            torch._dynamo.mark_dynamic(attn_metadata.context_lens, 0)\n            torch._dynamo.mark_dynamic(attn_metadata.block_tables, 0)\n            torch._dynamo.mark_dynamic(t, 0)\n            torch._dynamo.mark_dynamic(p, 0)\n        # Dummy run.\n        self.model(token_ids, position_ids, attn_metadata, input_lens, t, p,\n                   num_samples, kv_caches)\n\n    def warmup_model(\n        self,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n    ) -> None:\n        # Prefill\n        logger.info(\"Compiling the model with different input shapes...\")\n        start = time.time()\n        for batch_size in [1]:\n            seq_len = 16\n            while True:\n                self._dummy_run(batch_size, seq_len, kv_caches, is_prompt=True)\n                xm.wait_device_ops()\n                logger.info(\"batch_size: %d, seq_len: %d\", batch_size, seq_len)\n\n                if seq_len >= self.model_config.max_model_len:\n                    break\n                num_tokens = batch_size * seq_len\n                if num_tokens >= self.scheduler_config.max_num_batched_tokens:\n                    break\n                seq_len = seq_len * 2\n\n        end = time.time()\n        logger.info(\"Compilation for prefill done in %.2f s.\", end - start)\n\n        # Decode\n        start = time.time()\n        seq_len = 1\n        batch_size = 8  # Must be in sync with _get_padded_batch_size()\n        while True:\n            self._dummy_run(batch_size, seq_len, kv_caches, is_prompt=False)\n            xm.wait_device_ops()\n            logger.info(\"batch_size: %d, seq_len: %d\", batch_size, seq_len)\n\n            if batch_size >= self.scheduler_config.max_num_seqs:\n                break\n            batch_size = batch_size + 16 if batch_size >= 16 else batch_size * 2\n\n        end = time.time()\n        logger.info(\"Compilation for decode done in %.2f s.\", end - start)\n\n    def _prepare_prompt(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, torch.Tensor]:\n        assert len(seq_group_metadata_list) > 0\n        input_tokens: List[int] = []\n        input_positions: List[int] = []\n        prompt_lens: List[int] = []\n        slot_mapping: List[int] = []\n\n        for seq_group_metadata in seq_group_metadata_list:\n            assert seq_group_metadata.is_prompt\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            assert len(seq_ids) == 1\n            seq_id = seq_ids[0]\n\n            seq_data = seq_group_metadata.seq_data[seq_id]\n            # Could include output tokens when a request is preempted.\n            prompt_tokens = seq_data.get_token_ids()\n            prompt_len = len(prompt_tokens)\n            prompt_lens.append(prompt_len)\n\n            input_tokens.extend(prompt_tokens)\n            input_positions.extend(list(range(prompt_len)))\n\n            assert seq_group_metadata.block_tables is not None\n            block_table = seq_group_metadata.block_tables[seq_id]\n            for i in range(prompt_len):\n                block_number = block_table[i // self.block_size]\n                block_offset = i % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append(slot)\n\n            # Add paddings to EACH prompt to the smallest power of 2 that is\n            # greater than or equal to the prompt length.\n            # We pad the seq_len to reduce the compilation overhead.\n            # We execute each prompt individually (i.e., with batch_size 1)\n            # because the FlashAttention kernel does not support ragged inputs.\n            # TODO(woosuk): Use SplashAttention to support ragged inputs.\n            padded_prompt_len = _get_padded_prefill_len(prompt_len)\n            num_paddings = padded_prompt_len - prompt_len\n            input_tokens += [0] * num_paddings\n            input_positions += [0] * num_paddings\n            slot_mapping += [_PAD_SLOT_ID] * num_paddings\n\n        assert len(prompt_lens) > 0\n        num_prefills = len(prompt_lens)\n        input_tokens = torch.tensor(input_tokens,\n                                    dtype=torch.int32,\n                                    device=\"cpu\")\n        input_positions = torch.tensor(input_positions,\n                                       dtype=torch.int32,\n                                       device=\"cpu\")\n        slot_mapping = torch.tensor(slot_mapping,\n                                    dtype=torch.int64,\n                                    device=\"cpu\")\n        prompt_lens = torch.tensor(prompt_lens,\n                                   dtype=torch.int32,\n                                   device=\"cpu\")\n        attn_metadata = self.attn_backend.make_metadata(\n            num_prefills=num_prefills,\n            num_prefill_tokens=0,  # NOTE: This is not used.\n            num_decode_tokens=0,\n            slot_mapping=slot_mapping,\n            block_tables=None,\n            context_lens=None,\n        )\n        return input_tokens, input_positions, attn_metadata, prompt_lens\n\n    def _prepare_decode(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, torch.Tensor]:\n        assert len(seq_group_metadata_list) > 0\n        input_tokens: List[List[int]] = []\n        input_positions: List[List[int]] = []\n        slot_mapping: List[List[int]] = []\n        context_lens: List[int] = []\n\n        batch_idx = 0\n        for seq_group_metadata in seq_group_metadata_list:\n            assert not seq_group_metadata.is_prompt\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            for seq_id in seq_ids:\n                seq_data = seq_group_metadata.seq_data[seq_id]\n                generation_token = seq_data.get_last_token_id()\n                input_tokens.append([generation_token])\n\n                seq_len = seq_data.get_len()\n                position = seq_len - 1\n                input_positions.append([position])\n                context_lens.append(seq_len)\n\n                assert seq_group_metadata.block_tables is not None\n                block_table = seq_group_metadata.block_tables[seq_id]\n                self.block_tables[batch_idx, :len(block_table)] = block_table\n                batch_idx += 1\n\n                block_number = block_table[position // self.block_size]\n                block_offset = position % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append([slot])\n\n        batch_size = _get_padded_batch_size(batch_idx)\n        num_paddings = batch_size - batch_idx\n        input_tokens = input_tokens + [[0]] * num_paddings\n        input_positions = input_positions + [[0]] * num_paddings\n        slot_mapping = slot_mapping + [[_PAD_SLOT_ID]] * num_paddings\n        context_lens = context_lens + [0] * num_paddings\n\n        input_tokens = torch.tensor(input_tokens,\n                                    dtype=torch.int32,\n                                    device=\"cpu\")\n        input_positions = torch.tensor(input_positions,\n                                       dtype=torch.int32,\n                                       device=\"cpu\")\n        slot_mapping = torch.tensor(slot_mapping,\n                                    dtype=torch.int64,\n                                    device=\"cpu\")\n        context_lens = torch.tensor(context_lens,\n                                    dtype=torch.int32,\n                                    device=\"cpu\")\n        block_tables = torch.tensor(self.block_tables[:batch_size],\n                                    dtype=torch.int32,\n                                    device=\"cpu\")\n        input_lens = torch.tensor([1] * batch_size,\n                                  dtype=torch.int32,\n                                  device=\"cpu\")\n        attn_metadata = self.attn_backend.make_metadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=batch_size,\n            slot_mapping=slot_mapping,\n            block_tables=block_tables,\n            context_lens=context_lens,\n        )\n        return input_tokens, input_positions, attn_metadata, input_lens\n\n    def _prepare_sample(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        padded_batch_size: int,\n    ) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:\n        assert len(seq_group_metadata_list) > 0\n        t = []\n        p = []\n        best_of = []\n        for seq_group_metadata in seq_group_metadata_list:\n            sampling_params = seq_group_metadata.sampling_params\n            t.append(sampling_params.temperature)\n            if sampling_params.top_p != 1 and not _ENABLE_TOP_P:\n                raise NotImplementedError(\n                    \"Top-p sampling is currently disabled for the TPU backend \"\n                    \"due to performance issues.\")\n            p.append(sampling_params.top_p)\n            if sampling_params.top_k != -1:\n                raise NotImplementedError(\n                    \"Top-k sampling is currently disabled for the TPU backend \"\n                    \"due to performance issues.\")\n            if sampling_params.best_of > _MAX_NUM_SAMPLES:\n                raise NotImplementedError(\n                    f\"Best of > {_MAX_NUM_SAMPLES} is not supported by the TPU \"\n                    \"backend.\")\n            best_of.append(sampling_params.best_of)\n            if sampling_params.use_beam_search:\n                raise NotImplementedError(\n                    \"Beam search is not supported by the TPU backend.\")\n            if sampling_params.logprobs is not None:\n                raise NotImplementedError(\n                    \"logprobs is not currently supported by the TPU backend.\")\n            if sampling_params.prompt_logprobs is not None:\n                raise NotImplementedError(\n                    \"prompt_logprobs is not currently supported by the TPU \"\n                    \"backend.\")\n\n            # Repeat the sampling params if the seq group has multiple seqs.\n            num_seqs = len(seq_group_metadata.seq_data)\n            t += [t[-1]] * (num_seqs - 1)\n            p += [p[-1]] * (num_seqs - 1)\n            best_of += [best_of[-1]] * (num_seqs - 1)\n\n        num_paddings = padded_batch_size - len(t)\n        t += [1.0] * num_paddings\n        p += [1.0] * num_paddings\n\n        t = torch.tensor(t, dtype=torch.float32, device=\"cpu\")\n        p = torch.tensor(p, dtype=torch.float32, device=\"cpu\")\n        return t, p, best_of\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None,\n    ) -> ModelInputForTPU:\n        del finished_requests_ids  # Unused.\n        assert virtual_engine == 0\n        assert len(seq_group_metadata_list) > 0\n        # NOTE: We assume that all sequences in the group are all prompts or\n        # all decodes.\n        is_prompt = seq_group_metadata_list[0].is_prompt\n        if is_prompt:\n            inputs = self._prepare_prompt(seq_group_metadata_list)\n        else:\n            inputs = self._prepare_decode(seq_group_metadata_list)\n        input_tokens, input_positions, attn_metadata, input_lens = inputs\n        padded_batch_size = input_tokens.shape[0]\n        t, p, best_of = self._prepare_sample(seq_group_metadata_list,\n                                             padded_batch_size)\n        num_samples = _MAX_NUM_SAMPLES if is_prompt else 1\n\n        seq_groups = [\n            list(metadata.seq_data.keys())\n            for metadata in seq_group_metadata_list\n        ]\n        return ModelInputForTPU(input_tokens, input_positions, attn_metadata,\n                                input_lens, t, p, num_samples, best_of,\n                                seq_groups)\n\n    def make_model_input_from_broadcasted_tensor_dict(\n            self, tensor_dict: Dict[str, Any]) -> ModelInputForTPU:\n        model_input = ModelInputForTPU.from_broadcasted_tensor_dict(\n            tensor_dict, attn_backend=self.attn_backend)\n        return model_input\n\n    @torch.no_grad()\n    def execute_model(\n        self,\n        model_input: ModelInputForTPU,\n        kv_caches: Optional[List[Any]],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> List[SamplerOutput]:\n        assert intermediate_tensors is None\n        if num_steps > 1:\n            raise ValueError(\n                \"TPUModelRunner does not support multi-step execution.\")\n\n        def _execute_model(*args):\n            \"\"\"Move input args from CPU to device and execute the model.\"\"\"\n\n            new_args = []\n            for arg in args:\n                if isinstance(arg, torch.Tensor):\n                    arg = arg.to(self.device)\n                elif isinstance(arg, AttentionMetadata):\n                    arg.slot_mapping = arg.slot_mapping.to(self.device)\n                    if getattr(arg, \"block_tables\", None) is not None:\n                        arg.block_tables = arg.block_tables.to(self.device)\n                    if getattr(arg, \"context_lens\", None) is not None:\n                        arg.context_lens = arg.context_lens.to(self.device)\n                new_args.append(arg)\n            return self.model(*new_args)\n\n        num_prefills = model_input.attn_metadata.num_prefills\n        is_prompt = num_prefills > 0\n        if is_prompt:\n            # NOTE(woosuk): Since the FlashAttention kernel does not support\n            # ragged inputs, we split the prompts into different batches and\n            # process them separately. This is a temporary hack that should be\n            # optimized by using SplashAttention.\n            next_token_ids = []\n            orig_slot_mapping = model_input.attn_metadata.slot_mapping\n            batch_size = model_input.input_lens.shape[0]\n            start_idx = 0\n            for i in range(batch_size):\n                # Get the actual prefill_len.\n                prefill_len = model_input.input_lens[i:i + 1].item()\n                prefill_len = _get_padded_prefill_len(prefill_len)\n                end_idx = start_idx + prefill_len\n\n                model_input.attn_metadata.slot_mapping = orig_slot_mapping[\n                    None, start_idx:end_idx]\n                model_input.attn_metadata.num_prefills = 1\n                output_token_ids = _execute_model(\n                    model_input.token_ids[None, start_idx:end_idx],\n                    model_input.position_ids[None, start_idx:end_idx],\n                    model_input.attn_metadata, model_input.input_lens[i:i + 1],\n                    model_input.t[i:i + 1], model_input.p[i:i + 1],\n                    model_input.num_samples, kv_caches)\n                # Retrieve the outputs to CPU.\n                next_token_ids += output_token_ids.cpu().tolist()\n                start_idx = end_idx\n        else:\n            # Execute the model.\n            output_token_ids = _execute_model(\n                model_input.token_ids, model_input.position_ids,\n                model_input.attn_metadata, model_input.input_lens,\n                model_input.t, model_input.p, model_input.num_samples,\n                kv_caches)\n            # Retrieve the outputs to CPU.\n            next_token_ids = output_token_ids.cpu().tolist()\n\n        # NOTE(woosuk): Minimal code to construct the sampler outputs.\n        # The TPU backend does not reuse the sampler, since the TPU backend\n        # does not support the advanced sampling parameters such as logprobs.\n        zero_logprob = Logprob(0.0)\n        batch_idx = 0\n        sampler_outputs = []\n        for seq_group in model_input.seq_groups:\n            seq_ids = seq_group\n            seq_outputs = []\n            if is_prompt:\n                assert len(seq_ids) == 1\n                seq_id = seq_ids[0]\n                for i in range(model_input.best_of[batch_idx]):\n                    next_token_id = next_token_ids[batch_idx][i]\n                    seq_outputs.append(\n                        SequenceOutput(seq_id, next_token_id,\n                                       {next_token_id: zero_logprob}))\n                batch_idx += 1\n            else:\n                for seq_id in seq_ids:\n                    next_token_id = next_token_ids[batch_idx][0]\n                    seq_outputs.append(\n                        SequenceOutput(seq_id, next_token_id,\n                                       {next_token_id: zero_logprob}))\n                    batch_idx += 1\n            sampler_outputs.append(\n                CompletionSequenceGroupOutput(seq_outputs, None))\n        return [SamplerOutput(sampler_outputs)]\n\n\nclass ModelWrapper(nn.Module):\n\n    def __init__(self, model: nn.Module):\n        super().__init__()\n        self.model = model\n\n    def forward(\n        self,\n        token_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        input_lens: torch.Tensor,\n        t: torch.Tensor,\n        p: torch.Tensor,\n        num_samples: int,\n        kv_caches: List[Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]],\n    ) -> torch.Tensor:\n        \"\"\"Executes the forward pass of the model and samples the next token.\n\n        Args:\n            token_ids: The input token IDs of shape [batch_size, seq_len].\n            position_ids: The input position IDs of shape [batch_size, seq_len].\n            attn_metadata: The Pallas attention metadata.\n            input_lens: The actual input lengths of shape [batch_size].\n            t: The sampling temperature of shape [batch_size].\n            p: The top-p probability of shape [batch_size].\n            num_samples: Number of samples to draw from each logits vector.\n            kv_caches: The key and value caches. They can be None during the\n                memory profiling at initialization.\n        \"\"\"\n        batch_size, seq_len = token_ids.shape\n        # Calculate the positions to sample from.\n        start_indicies = torch.arange(\n            batch_size, dtype=torch.int32, device=input_lens.device) * seq_len\n        logits_indices = start_indicies + input_lens - 1\n\n        # FIXME(woosuk): This is a temporary hack to avoid using the existing\n        # sampler and sampling metadata.\n        sampling_metadata = SamplingMetadata(\n            seq_groups=[],\n            selected_token_indices=logits_indices,\n            categorized_sample_indices={},\n            num_prompts=attn_metadata.num_prefills,\n        )\n\n        # Skip this in memory profiling at initialization.\n        if kv_caches[0][0] is not None:\n            # index_copy_(slot_mapping) only works when the inserted dimension\n            # is 0. However, the KV cache in the Pallas backend has the shape\n            # [num_kv_heads, num_blocks, block_size, head_size]. To make it\n            # work, we need to flatten the first three dimensions and modify\n            # the slot_mapping accordingly.\n            num_kv_heads, num_blocks, block_size, _ = kv_caches[0][0].shape\n            slot_mapping = attn_metadata.slot_mapping\n            slot_mapping = slot_mapping.flatten()\n            head_indicies = torch.arange(0,\n                                         num_kv_heads,\n                                         device=slot_mapping.device,\n                                         dtype=slot_mapping.dtype)\n            head_indicies *= block_size * num_blocks\n            slot_mapping = slot_mapping.repeat_interleave(num_kv_heads).view(\n                -1, num_kv_heads)\n            slot_mapping = slot_mapping + head_indicies.view(1, -1)\n            slot_mapping = slot_mapping.flatten()\n            attn_metadata.slot_mapping = slot_mapping\n\n        hidden_states = self.model(\n            token_ids,\n            position_ids,\n            kv_caches,\n            attn_metadata,\n        )\n        hidden_states = hidden_states.flatten(0, 1)\n        logits = self.model.compute_logits(hidden_states, sampling_metadata)\n\n        # Argmax sampling.\n        argmax_token_ids = torch.argmax(logits, dim=-1, keepdim=True)\n        argmax_token_ids = argmax_token_ids.repeat(1, num_samples)\n\n        # Zero temperature means greedy decoding. Avoid division by zero.\n        nonzero_t = torch.where(t != 0, t, 1.0)\n        logits = logits / nonzero_t.unsqueeze(dim=1)\n        if _ENABLE_TOP_P:\n            logits = _apply_top_p(logits, p.unsqueeze(dim=1))\n\n        # Random sampling.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float32)\n        sampled_token_ids = torch.multinomial(probs,\n                                              num_samples,\n                                              replacement=True)\n        next_token_ids = torch.where(t != 0, sampled_token_ids,\n                                     argmax_token_ids)\n        return next_token_ids\n\n\ndef _get_padded_prefill_len(x: int) -> int:\n    # NOTE(woosuk): The pallas FlashAttention kernel requires the sequence\n    # length to be a multiple of 16. We pad the prompt length to the nearest\n    # multiple of 16. This is also good for performance.\n    if x <= 16:\n        return 16\n    return 1 << (x - 1).bit_length()\n\n\ndef _get_padded_batch_size(batch_size: int) -> int:\n    # The GMM Pallas kernel requires num_tokens * topk to be a multiple of 16.\n    # To meet this requirement in the simplest way, we set the minimal batch\n    # size to 8.\n    if batch_size <= 8:\n        return 8\n    else:\n        return ((batch_size + 15) // 16) * 16\n\n\ndef _apply_top_p(logits: torch.Tensor, p: torch.Tensor) -> torch.Tensor:\n    logits_sorted = torch.sort(logits, dim=-1, descending=True).values\n    sorted_cum_probs = torch.cumsum(logits_sorted.softmax(dim=-1), dim=-1)\n    cutoff_index = torch.sum(sorted_cum_probs < p, dim=-1, keepdim=True)\n    cutoff_logit = torch.gather(logits_sorted, -1, cutoff_index)\n    logits = logits.masked_fill_(logits < cutoff_logit, -float(\"inf\"))\n    return logits\n",
      "diff": "diff --git a/vllm/worker/tpu_model_runner.py b/vllm/worker/tpu_model_runner.py\nindex 01daa64b5..a7ceb84ef 100644\n--- a/vllm/worker/tpu_model_runner.py\n+++ b/vllm/worker/tpu_model_runner.py\n@@ -10,6 +10,7 @@ import torch_xla.core.xla_model as xm\n import torch_xla.runtime as xr\n \n from vllm.attention import AttentionMetadata, get_attn_backend\n+from vllm.compilation.wrapper import TorchCompileWrapperWithCustomDispacther\n from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, ModelConfig,\n                          ParallelConfig, SchedulerConfig)\n from vllm.logger import init_logger\n@@ -144,11 +145,7 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n             )\n         model = model.eval()\n         xm.wait_device_ops()\n-        model = ModelWrapper(model)\n-        self.model = torch.compile(model,\n-                                   backend=\"openxla\",\n-                                   fullgraph=True,\n-                                   dynamic=False)\n+        self.model = ModelWrapper(model)\n \n     def _dummy_run(\n         self,\n@@ -235,8 +232,15 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n             torch._dynamo.mark_dynamic(t, 0)\n             torch._dynamo.mark_dynamic(p, 0)\n         # Dummy run.\n-        self.model(token_ids, position_ids, attn_metadata, input_lens, t, p,\n-                   num_samples, kv_caches)\n+        self.model(token_ids,\n+                   position_ids,\n+                   attn_metadata,\n+                   input_lens,\n+                   t,\n+                   p,\n+                   num_samples,\n+                   kv_caches,\n+                   is_prompt=is_prompt)\n \n     def warmup_model(\n         self,\n@@ -530,7 +534,7 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n                     if getattr(arg, \"context_lens\", None) is not None:\n                         arg.context_lens = arg.context_lens.to(self.device)\n                 new_args.append(arg)\n-            return self.model(*new_args)\n+            return self.model(*new_args, is_prompt=is_prompt)\n \n         num_prefills = model_input.attn_metadata.num_prefills\n         is_prompt = num_prefills > 0\n@@ -601,11 +605,32 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n         return [SamplerOutput(sampler_outputs)]\n \n \n-class ModelWrapper(nn.Module):\n+class ModelWrapper(TorchCompileWrapperWithCustomDispacther):\n \n     def __init__(self, model: nn.Module):\n-        super().__init__()\n         self.model = model\n+        compiled_callable = torch.compile(self.forward,\n+                                          backend=\"openxla\",\n+                                          fullgraph=True,\n+                                          dynamic=False)\n+        super().__init__(compiled_callable)\n+\n+    def __call__(self, *args, is_prompt: bool, **kwargs):\n+        if len(self.compiled_codes) < 3 or not self.use_custom_dispatcher:\n+            # not fully compiled yet, or not using the custom dispatcher,\n+            # let PyTorch handle it\n+            return self.compiled_callable(*args, **kwargs)\n+        # the 3 compiled codes are:\n+        # 0: for profiling\n+        # 1: for prompt\n+        # 2: for decode\n+        # dispatch to the compiled code directly, skip PyTorch\n+        if is_prompt:\n+            with self.dispatch_to_code(1):\n+                return self.forward(*args, **kwargs)\n+        else:\n+            with self.dispatch_to_code(2):\n+                return self.forward(*args, **kwargs)\n \n     def forward(\n         self,",
      "change_type": "modified",
      "lines_added": 36,
      "lines_removed": 11
    }
  ],
  "affected_apis": [
    "vllm.compilation.wrapper.TorchCompileWrapperWithCustomDispacther",
    "vllm.worker.tpu_model_runner.ModelWrapper",
    "vllm.worker.tpu_model_runner.ModelWrapper.__call__",
    "vllm.worker.tpu_model_runner.TPUModelRunner.warmup_model",
    "torch.compile"
  ],
  "summary": {
    "total_files": 9,
    "files_added": 5,
    "files_deleted": 0,
    "files_modified": 4
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (benchmark_throughput, test_wrapper, test_custom_dispatcher, test_tpu_model_runner)",
    "is_benchmark_actually_there": "",
    "sample_clues": "__init__, compilation, envs"
  }
}