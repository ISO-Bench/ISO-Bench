{
  "commit_hash": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
  "parent_hash": "6368e777a8ead7fb62054d3779c6237361ec0d86",
  "message": "Improve detokenization performance (#1338)",
  "author": "Antoni Baum <antoni.baum@protonmail.com>",
  "date": "2023-10-13 09:59:07 -0700",
  "files_changed": [
    {
      "file_path": "vllm/transformers_utils/tokenizer.py",
      "old_content": "from typing import List, Optional, Tuple, Union\n\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\n                          PreTrainedTokenizerFast)\n\nfrom vllm.logger import init_logger\n\nlogger = init_logger(__name__)\n\n# A fast LLaMA tokenizer with the pre-processed `tokenizer.json` file.\n_FAST_LLAMA_TOKENIZER = \"hf-internal-testing/llama-tokenizer\"\n\n\ndef get_tokenizer(\n    tokenizer_name: str,\n    *args,\n    tokenizer_mode: str = \"auto\",\n    trust_remote_code: bool = False,\n    tokenizer_revision: Optional[str] = None,\n    **kwargs,\n) -> Union[PreTrainedTokenizer, PreTrainedTokenizerFast]:\n    \"\"\"Gets a tokenizer for the given model name via Huggingface.\"\"\"\n    if tokenizer_mode == \"slow\":\n        if kwargs.get(\"use_fast\", False):\n            raise ValueError(\n                \"Cannot use the fast tokenizer in slow tokenizer mode.\")\n        kwargs[\"use_fast\"] = False\n\n    if (\"llama\" in tokenizer_name.lower() and kwargs.get(\"use_fast\", True)\n            and tokenizer_name != _FAST_LLAMA_TOKENIZER):\n        logger.info(\n            \"For some LLaMA V1 models, initializing the fast tokenizer may \"\n            \"take a long time. To reduce the initialization time, consider \"\n            f\"using '{_FAST_LLAMA_TOKENIZER}' instead of the original \"\n            \"tokenizer.\")\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_name,\n            *args,\n            trust_remote_code=trust_remote_code,\n            tokenizer_revision=tokenizer_revision,\n            **kwargs)\n    except TypeError as e:\n        # The LLaMA tokenizer causes a protobuf error in some environments.\n        err_msg = (\n            \"Failed to load the tokenizer. If you are using a LLaMA V1 model \"\n            f\"consider using '{_FAST_LLAMA_TOKENIZER}' instead of the \"\n            \"original tokenizer.\")\n        raise RuntimeError(err_msg) from e\n    except ValueError as e:\n        # If the error pertains to the tokenizer class not existing or not\n        # currently being imported, suggest using the --trust-remote-code flag.\n        if (not trust_remote_code and\n            (\"does not exist or is not currently imported.\" in str(e)\n             or \"requires you to execute the tokenizer file\" in str(e))):\n            err_msg = (\n                \"Failed to load the tokenizer. If the tokenizer is a custom \"\n                \"tokenizer not yet available in the HuggingFace transformers \"\n                \"library, consider setting `trust_remote_code=True` in LLM \"\n                \"or using the `--trust-remote-code` flag in the CLI.\")\n            raise RuntimeError(err_msg) from e\n        else:\n            raise e\n\n    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n        logger.warning(\n            \"Using a slow tokenizer. This might cause a significant \"\n            \"slowdown. Consider using a fast tokenizer instead.\")\n    return tokenizer\n\n\ndef _convert_tokens_to_string_with_added_encoders(\n    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n    output_tokens: List[str],\n    skip_special_tokens: bool,\n) -> str:\n    # Adapted from\n    # https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/tokenization_utils.py#L921\n    # NOTE(woosuk): The following code is slow because it runs a for loop over\n    # the output_tokens. In Python, running a for loop over a list can be slow\n    # even when the loop body is very simple.\n    sub_texts = []\n    current_sub_text = []\n    for token in output_tokens:\n        if skip_special_tokens and token in tokenizer.all_special_tokens:\n            continue\n        if token in tokenizer.added_tokens_encoder:\n            if current_sub_text:\n                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                sub_texts.append(sub_text)\n                current_sub_text = []\n            sub_texts.append(token)\n        else:\n            current_sub_text.append(token)\n    if current_sub_text:\n        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n        sub_texts.append(sub_text)\n    return \" \".join(sub_texts)\n\n\n# Based on\n# https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\n# under Apache 2.0 license\ndef detokenize_incrementally(\n    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n    all_input_ids: List[int],\n    prev_tokens: Optional[List[str]],\n    prefix_offset: int = 0,\n    read_offset: int = 0,\n    skip_special_tokens: bool = False,\n) -> Tuple[List[str], str, int, int]:\n    new_token_id = all_input_ids[-1]\n    # This is the first iteration for this sequence\n    if prev_tokens is None:\n        new_tokens = tokenizer.convert_ids_to_tokens(\n            all_input_ids, skip_special_tokens=skip_special_tokens)\n        output_tokens = new_tokens\n        # 5 is an arbitrary value that should work for all\n        # tokenizers (bigger = more conservative).\n        # Subtract 1 extra to account for the generated token.\n        prefix_offset = max(len(output_tokens) - 6, 0)\n        read_offset = max(len(output_tokens) - 1, 0)\n    else:\n        # Put new_token_id in a list so skip_special_tokens is respected\n        new_tokens = tokenizer.convert_ids_to_tokens(\n            [new_token_id], skip_special_tokens=skip_special_tokens)\n        output_tokens = prev_tokens + new_tokens\n\n    # The prefix text is necessary only to defeat cleanup algorithms in\n    # the decode which decide to add a space or not depending on the\n    # surrounding ids.\n    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n        prefix_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:read_offset])\n        new_text = tokenizer.convert_tokens_to_string(\n            output_tokens[prefix_offset:])\n    else:\n        prefix_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:read_offset],\n            skip_special_tokens=skip_special_tokens)\n        new_text = _convert_tokens_to_string_with_added_encoders(\n            tokenizer,\n            output_tokens[prefix_offset:],\n            skip_special_tokens=skip_special_tokens)\n\n    if len(new_text) > len(prefix_text) and not new_text.endswith(\"ï¿½\"):\n        # utf-8 char at the end means it's a potential unfinished byte sequence\n        # from byte fallback tokenization.\n        # If it's in the middle, it's probably a real invalid id generated\n        # by the model\n        new_text = new_text[len(prefix_text):]\n        return new_tokens, new_text, read_offset, len(output_tokens)\n    else:\n        return new_tokens, \"\", prefix_offset, read_offset\n",
      "diff": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..49e7007ae 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -81,10 +81,11 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n+    all_special_tokens = set(tokenizer.all_special_tokens)\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in tokenizer.get_added_vocab():\n             if current_sub_text:\n                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                 sub_texts.append(sub_text)\n@@ -129,7 +130,7 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n+    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n         prefix_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:read_offset])\n         new_text = tokenizer.convert_tokens_to_string(",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 4
    }
  ],
  "affected_apis": [
    "detokenize_incrementally"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_tokenizer)",
    "is_benchmark_actually_there": "",
    "sample_clues": "detokenize, detokenizer, get"
  }
}