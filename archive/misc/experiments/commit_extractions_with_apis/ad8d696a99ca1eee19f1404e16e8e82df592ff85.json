{
  "commit_hash": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
  "parent_hash": "3d925165f2b18379640a63fbb42de95440d63b64",
  "message": "[Core] Scheduler perf fix (#4270)",
  "author": "SangBin Cho <rkooo567@gmail.com>",
  "date": "2024-04-22 21:11:06 +0000",
  "files_changed": [
    {
      "file_path": "tests/core/test_scheduler.py",
      "old_content": "import time\nfrom collections import deque\nfrom typing import List\nfrom unittest.mock import MagicMock\n\nimport pytest  # noqa\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.core.policy import PolicyFactory\nfrom vllm.core.scheduler import Scheduler, SchedulingBudget\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import Logprob, SequenceGroup, SequenceStatus\n\nfrom .utils import create_dummy_prompt\n\n\ndef get_sequence_groups(scheduler_output):\n    return [s.seq_group for s in scheduler_output.scheduled_seq_groups]\n\n\ndef append_new_token(out, token_id: int):\n    seq_groups = get_sequence_groups(out)\n    for seq_group in seq_groups:\n        for seq in seq_group.get_seqs():\n            seq.append_token_id(token_id, {token_id: Logprob(token_id)})\n\n\ndef schedule_and_update_computed_tokens(scheduler):\n    metas, out = scheduler.schedule()\n    for s, meta in zip(out.scheduled_seq_groups, metas):\n        s.seq_group.update_num_computed_tokens(meta.token_chunk_size)\n    return metas, out\n\n\ndef append_new_token_seq_group(token_chunk_size, seq_group, token_id: int):\n    seq_group.update_num_computed_tokens(token_chunk_size)\n    for seq in seq_group.get_seqs():\n        seq.append_token_id(token_id, {token_id: Logprob(token_id)})\n\n\ndef test_scheduler_add_seq_group():\n    block_size = 4\n    scheduler_config = SchedulerConfig(100, 64, 1)\n    cache_config = CacheConfig(block_size, 1.0, 1, cache_dtype=\"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq group to scheduler.\n    num_seq_group = 4\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        scheduler.add_seq_group(seq_group)\n        assert scheduler.get_num_unfinished_seq_groups() == i + 1\n\n\ndef test_scheduler_abort_seq_group():\n    block_size = 4\n    scheduler_config = SchedulerConfig(100, 64, 1)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add multiple seq groups to scheduler.\n    num_seq_group = 4\n    request_ids = set()\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        scheduler.add_seq_group(seq_group)\n        request_ids.add(str(i))\n\n    # Abort all added seq groups.\n    assert scheduler.get_num_unfinished_seq_groups() == num_seq_group\n    scheduler.abort_seq_group(request_ids)\n    assert scheduler.get_num_unfinished_seq_groups() == 0\n\n\ndef test_scheduler_schedule_simple():\n    block_size = 4\n    num_seq_group = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, num_seq_group, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Schedule seq groups prompts.\n    num_tokens = block_size * num_seq_group\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_tokens\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_seq_group\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    append_new_token(out, 1)\n\n\ndef test_scheduler_prefill_prioritized():\n    \"\"\"Verify running batched tokens are not applied to prefill requests.\"\"\"\n    block_size = 4\n    max_model_len = 30\n    max_batched_num_tokens = 30\n    scheduler_config = SchedulerConfig(max_batched_num_tokens, 2,\n                                       max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 2\n    cache_config.num_gpu_blocks = 2\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    _, seq_group_a = create_dummy_prompt(\"1\", 1)\n    scheduler.add_seq_group(seq_group_a)\n\n    # Schedule seq groups prompts.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a]\n\n    # Add a new prefill request B.\n    _, seq_group_b = create_dummy_prompt(\"2\", 30)\n    scheduler.add_seq_group(seq_group_b)\n\n    # Verify prefill requests are prioritized. Since max_batched_num_tokens\n    # is 1, new prefill request has to be scheduled first.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_b]\n\n\ndef test_scheduler_schedule_preempt_abort():\n    block_size = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, 2, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 2\n    cache_config.num_gpu_blocks = 2\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    seq_a, seq_group_a = create_dummy_prompt(\"1\", block_size)\n    seq_b, seq_group_b = create_dummy_prompt(\"2\", block_size)\n    scheduler.add_seq_group(seq_group_a)\n    scheduler.add_seq_group(seq_group_b)\n\n    # Schedule seq groups prompts.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a, seq_group_b]\n    assert out.num_batched_tokens == block_size * 2  # seq_a and seq_b\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 2\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n\n    # Append \"generated\" tokens, allowing the sequence to mark prompt tokens as\n    # processed.\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation and preempt seq group b.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a]\n    assert out.num_batched_tokens == 1\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n\n    # Abort seq group a. Re-schedule seq group b prompt with recomputation.\n    scheduler.abort_seq_group(\"1\")\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_b]\n    assert out.num_batched_tokens == 5  # 4 prompt + 1 generation.\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 1\n\n\ndef test_scheduler_max_seqs():\n    block_size = 4\n    num_seq_group = 4\n    max_seq_group = 2\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, max_seq_group, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    all_seq_groups: List[SequenceGroup] = []\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size)\n        all_seq_groups.append(seq_group)\n\n    # Append 1 seq group\n    scheduler.add_seq_group(all_seq_groups[0])\n\n    # Schedule seq groups prompts.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[0]])\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[0]])\n    append_new_token(out, 1)\n\n    # Append 2 more seq group\n    scheduler.add_seq_group(all_seq_groups[1])\n    scheduler.add_seq_group(all_seq_groups[2])\n\n    # Schedule seq groups prompts.\n    # Only 1 seq group should be scheduled since max_seq_group is 2\n    # and one is prompting.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[1]])\n\n\ndef test_scheduler_delay_factor():\n    block_size = 4\n    scheduler_config = SchedulerConfig(100, 64, 16, delay_factor=0.5)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # schedule first prompt\n    seq_group_meta, seq_group = create_dummy_prompt(\"0\",\n                                                    prompt_length=block_size)\n    scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups > 0\n    assert seq_group_meta[0].request_id == '0'\n    append_new_token(out, 1)\n\n    # wait for a second before scheduling next prompt\n    time.sleep(1)\n    seq_group_meta, seq_group = create_dummy_prompt(\"1\",\n                                                    prompt_length=block_size)\n    scheduler.add_seq_group(seq_group)\n\n    # second prompt should *not* be scheduled\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups == 0\n    assert seq_group_meta[0].request_id == '0'\n    append_new_token(out, 1)\n\n    # wait for more than 0.5 second and try again\n    time.sleep(0.6)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups > 0\n    assert seq_group_meta[0].request_id == '1'\n    append_new_token(out, 1)\n\n\ndef test_swapped_out_prioritized():\n    scheduler = initialize_scheduler(max_num_seqs=6)\n    # best_of=2 * 3 == 6 sequences.\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n        scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 3\n    append_new_token(out, 1)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"2\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 2\n    assert out.num_batched_tokens == 2\n    assert out.blocks_to_swap_out != {}\n    assert out.blocks_to_swap_in == {}\n    append_new_token(out, 1)\n\n    # Add 1 more task. Swap should be prioritized over prefill.\n    _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n    scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    append_new_token(out, 1)\n    assert len(out.scheduled_seq_groups) == 3\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 3\n    assert out.blocks_to_swap_in != {}\n    assert out.blocks_to_swap_out == {}\n\n\ndef initialize_scheduler(*,\n                         max_num_seqs=1000,\n                         max_token_budget=1000,\n                         max_model_len=1000,\n                         lora_config=None):\n    block_size = 4\n    scheduler_config = SchedulerConfig(max_token_budget, max_num_seqs,\n                                       max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, lora_config)\n    return scheduler\n\n\ndef create_token_budget(token_budget: int = 10000,\n                        max_num_seqs: int = 10000) -> SchedulingBudget:\n    return SchedulingBudget(\n        token_budget=token_budget,\n        max_num_seqs=max_num_seqs,\n    )\n\n\ndef add_token_budget(budget: SchedulingBudget,\n                     num_batched_tokens: int = 0,\n                     num_curr_seqs: int = 0):\n    mock_seq_group = create_dummy_prompt('10', prompt_length=60)[1]\n    budget.add_num_batched_tokens(mock_seq_group.request_id,\n                                  num_batched_tokens)\n    budget.add_num_seqs(mock_seq_group.request_id, num_curr_seqs)\n\n\ndef test_prefill_schedule_max_prompt_len():\n    \"\"\"\n    Test prompt longer than max_prompt_len is aborted.\n    \"\"\"\n    scheduler = initialize_scheduler(max_model_len=30)\n    _, seq_group = create_dummy_prompt(0, prompt_length=60)\n    waiting = deque([seq_group])\n    budget = create_token_budget()\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 1\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 0\n\n\ndef test_prefill_schedule_token_budget():\n    \"\"\"\n    Test token budget respected.\n    \"\"\"\n    scheduler = initialize_scheduler()\n    waiting = deque()\n    budget = create_token_budget(token_budget=0)\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        waiting.append(seq_group)\n\n    # 0 token budget == nothing is scheduled.\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 2\n\n    # 60 token budget == 1 request scheduled.\n    budget = create_token_budget(token_budget=60)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 1\n    assert budget.num_batched_tokens == 60\n    assert budget.num_curr_seqs == 1\n    assert len(remaining_waiting) == 1\n\n    # Test when current_batched_tokens respected.\n    scheduler = initialize_scheduler()\n    waiting = deque()\n    budget = create_token_budget(token_budget=60)\n    add_token_budget(budget, 30, 0)\n    _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n    # Cannot schedule a prompt that doesn't fit the budget.\n    waiting.append(seq_group)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 30\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 1\n    budget = create_token_budget(token_budget=90)\n    add_token_budget(budget, 30, 0)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.seq_groups) == 1\n    assert budget.num_batched_tokens == 90\n    assert budget.num_curr_seqs == 1\n    assert len(remaining_waiting) == 0\n\n\ndef test_prefill_schedule_max_seqs():\n    \"\"\"\n    Test max seq respected.\n    \"\"\"\n    scheduler = initialize_scheduler()\n    waiting = deque()\n    budget = create_token_budget(max_num_seqs=2)\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        waiting.append(seq_group)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 2\n    assert budget.num_batched_tokens == 120\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 1\n\n    # Verify curr_num_seqs respected.\n    waiting = deque()\n    budget = create_token_budget(max_num_seqs=2)\n    add_token_budget(budget, 0, 2)\n    _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n    waiting.append(seq_group)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 1\n\n\ndef test_prefill_schedule_max_lora():\n    \"\"\"\n    Test max lora is respected and prioritized.\n    \"\"\"\n    lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n    scheduler = initialize_scheduler(lora_config=lora_config)\n    waiting = deque()\n    budget = create_token_budget(token_budget=120)\n    curr_loras = set()\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           lora_request=LoRARequest(\n                                               lora_name=str(i),\n                                               lora_int_id=i + 1,\n                                               lora_local_path=\"abc\"))\n        waiting.append(seq_group)\n    # Add two more requests to verify lora is prioritized.\n    # 0: Lora, 1: Lora, 2: regular, 3: regular\n    # In the first iteration, index 0, 2 is scheduled.\n    # If a request is not scheduled because it hits max lora, it is\n    # prioritized. Verify that.\n    for i in range(2, 4):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        waiting.append(seq_group)\n    # Schedule 2 requests (0 and 2)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, curr_loras)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 2\n    assert budget.num_batched_tokens == 120\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 2\n    assert len(curr_loras) == 1\n    # The second lora request is scheduled next as FCFS policy.\n    # Reset curr_loras so that it can be scheduled.\n    curr_loras = set()\n    budget = create_token_budget(token_budget=60)\n    remaining_waiting, output = scheduler._schedule_prefills(\n        remaining_waiting, budget, curr_loras)\n    assert len(output.seq_groups) == 1\n    assert output.seq_groups[0].seq_group.request_id == \"1\"\n    assert len(remaining_waiting) == 1\n    assert len(curr_loras) == 1\n    assert budget.num_batched_tokens == 60\n\n\ndef test_prefill_schedule_no_block_manager_capacity():\n    \"\"\"\n    Test sequence cannot be scheduled due to block manager has no capacity.\n    \"\"\"\n    scheduler = initialize_scheduler()\n    waiting = deque()\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        waiting.append(seq_group)\n    scheduler.block_manager.can_allocate = MagicMock()\n    scheduler.block_manager.can_allocate.return_value = AllocStatus.LATER\n    remainig_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remainig_waiting) == 3\n\n    scheduler = initialize_scheduler()\n    waiting = deque()\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        waiting.append(seq_group)\n    scheduler.block_manager.can_allocate = MagicMock()\n    scheduler.block_manager.can_allocate.return_value = AllocStatus.NEVER\n    remaining_waiting, output = scheduler._schedule_prefills(\n        waiting, budget, None)\n    assert len(output.ignored_seq_groups) == 3\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 0\n\n\ndef test_decode_schedule_preempted():\n    \"\"\"\n    Test decodes cannot be scheduled and preempted.\n    \"\"\"\n    scheduler = initialize_scheduler()\n    running = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        scheduler._allocate_and_set_running(seq_group, 60)\n        append_new_token_seq_group(60, seq_group, 1)\n        running.append(seq_group)\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # 1 cannot be scheduled, and the lowest priority (request 2)\n    # should be preempted. 1 will also be preempted.\n    budget = create_token_budget()\n    remainig_running, output = scheduler._schedule_running(\n        running, budget, curr_loras, policy)\n    assert len(remainig_running) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert output.decode_seq_groups[0].seq_group.request_id == \"0\"\n    assert len(output.preempted) == 2\n    # Verify budgets are updated.\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 1\n    # Both should be preempted, not swapped.\n    assert output.blocks_to_swap_out == {}\n    # Nothing is copied.\n    assert output.blocks_to_copy == {}\n\n\ndef test_decode_swap_beam_search():\n    \"\"\"\n    Test best_of > 1 swap out blocks\n    \"\"\"\n    scheduler = initialize_scheduler()\n    running = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n        scheduler._allocate_and_set_running(seq_group, 60)\n        running.append(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        budget.add_num_seqs(seq_group.request_id,\n                            seq_group.get_max_num_running_seqs())\n        budget.add_num_batched_tokens(\n            seq_group.request_id, seq_group.num_seqs(SequenceStatus.RUNNING))\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"2\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n    scheduler.block_manager.swap_out = MagicMock()\n    expected_swap_mapping = {\"5\": \"7\"}\n    scheduler.block_manager.swap_out.return_value = expected_swap_mapping\n\n    remainig_running, output = scheduler._schedule_running(\n        running, budget, curr_loras, policy)\n    assert len(remainig_running) == 0\n    assert len(output.decode_seq_groups) == 2\n    assert len(output.prefill_seq_groups) == 0\n    assert output.decode_seq_groups[0].seq_group.request_id == \"0\"\n    assert output.decode_seq_groups[1].seq_group.request_id == \"1\"\n    assert len(output.preempted) == 0\n    assert len(output.swapped_out) == 1\n    # Budget should refledct preempted requests.\n    assert budget.num_batched_tokens == 2\n    # since there are 2 sequences, 2 should be subtracted.\n    assert budget.num_curr_seqs == 4\n    # Both should be preempted, not swapped.\n    assert output.blocks_to_swap_out == expected_swap_mapping\n    # Nothing is copied.\n    assert output.blocks_to_copy == {}\n\n\ndef test_schedule_decode_blocks_to_copy_update():\n    \"\"\"\n    Verify blocks_to_copy is updated.\n    \"\"\"\n    scheduler = initialize_scheduler()\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n    running = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    scheduler._allocate_and_set_running(seq_group, 60)\n    append_new_token_seq_group(60, seq_group, 1)\n    running.append(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.append_slots = MagicMock()\n    scheduler.block_manager.append_slots.return_value = {2: [3]}\n\n    budget = create_token_budget()\n    remaining_running, output = scheduler._schedule_running(\n        running, budget, curr_loras, policy)\n    assert len(remaining_running) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert len(output.preempted) == 0\n    assert len(output.swapped_out) == 0\n    # Nothing is preempted.\n    assert output.blocks_to_swap_out == {}\n    # Since append_slot returns the source -> dist mapping, it should\n    # applied.\n    assert output.blocks_to_copy == {2: [3]}\n\n\ndef test_schedule_swapped_simple():\n    scheduler = initialize_scheduler()\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    blocks_to_swap_out = {}\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n    scheduler._allocate_and_set_running(seq_group, 60)\n    append_new_token_seq_group(60, seq_group, 1)\n    scheduler._swap_out(seq_group, blocks_to_swap_out)\n    swapped.append(seq_group)\n\n    budget = create_token_budget()\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 0\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    # swap in is the reverse of swap out\n    blocks_to_swap_in_reverse = {}\n    for swapin, swapout in output.blocks_to_swap_in.items():\n        blocks_to_swap_in_reverse[swapout] = swapin\n    assert blocks_to_swap_out == blocks_to_swap_in_reverse\n\n\ndef test_schedule_swapped_max_token_budget():\n    scheduler = initialize_scheduler()\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    blocks_to_swap_out = {}\n    for _ in range(2):\n        _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n        scheduler._allocate_and_set_running(seq_group, 60)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        swapped.append(seq_group)\n\n    budget = create_token_budget(token_budget=1)\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n\n    # Verify num_batched_tokens are respected.\n    budget = create_token_budget(token_budget=1)\n    add_token_budget(budget, 1, 0)\n    remaining_swapped, output = scheduler._schedule_swapped(\n        remaining_swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 0\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\ndef test_schedule_swapped_max_seqs():\n    scheduler = initialize_scheduler()\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    blocks_to_swap_out = {}\n    for i in range(4):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        scheduler._allocate_and_set_running(seq_group, 60)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        swapped.append(seq_group)\n\n    budget = create_token_budget(max_num_seqs=2)\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 2\n    assert len(output.prefill_seq_groups) == 0\n\n    # Verify num_curr_seqs are respected.\n    remaining_swapped, output = scheduler._schedule_swapped(\n        remaining_swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\ndef test_schedule_swapped_max_loras():\n    lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n    scheduler = initialize_scheduler(lora_config=lora_config)\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = set()\n    blocks_to_swap_out = {}\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           lora_request=LoRARequest(\n                                               lora_name=str(i),\n                                               lora_int_id=i + 1,\n                                               lora_local_path=\"abc\"))\n        scheduler._allocate_and_set_running(seq_group, 60)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        swapped.append(seq_group)\n\n    budget = create_token_budget()\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 1\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert len(curr_loras) == 1\n\n\ndef test_schedule_swapped_cannot_swap_in():\n    scheduler = initialize_scheduler()\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    blocks_to_swap_out = {}\n    for _ in range(2):\n        _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n        scheduler._allocate_and_set_running(seq_group, 60)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        swapped.append(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_swap_in = MagicMock()\n    scheduler.block_manager.can_swap_in.return_value = False\n    # Since we cannot swap in, none of the requests are swapped in.\n    budget = create_token_budget()\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\ndef test_schedule_swapped_blocks_to_copy():\n    scheduler = initialize_scheduler()\n    swapped = deque()\n    policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n    curr_loras = None\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n    scheduler._allocate_and_set_running(seq_group, 60)\n    append_new_token_seq_group(60, seq_group, 1)\n    blocks_to_swap_out = {}\n    scheduler._swap_out(seq_group, blocks_to_swap_out)\n    swapped.append(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.append_slots = MagicMock()\n    scheduler.block_manager.append_slots.return_value = {2: [3]}\n\n    budget = create_token_budget()\n    remaining_swapped, output = scheduler._schedule_swapped(\n        swapped, budget, curr_loras, policy)\n    assert len(remaining_swapped) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert output.blocks_to_copy == {2: [3]}\n\n\ndef test_scheduling_budget():\n    TOKEN_BUDGET = 4\n    MAX_SEQS = 4\n    budget = SchedulingBudget(token_budget=TOKEN_BUDGET, max_num_seqs=MAX_SEQS)\n    assert budget.can_schedule(num_new_tokens=1, num_new_seqs=1)\n    assert budget.can_schedule(num_new_tokens=4, num_new_seqs=4)\n    assert not budget.can_schedule(num_new_tokens=1, num_new_seqs=5)\n    assert not budget.can_schedule(num_new_tokens=5, num_new_seqs=1)\n    assert not budget.can_schedule(num_new_tokens=5, num_new_seqs=5)\n    assert budget.remaining_token_budget() == TOKEN_BUDGET\n\n    # Verify add/subtract num batched tokens.\n    _, seq_group = create_dummy_prompt(\"1\", 3)\n    budget.add_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.can_schedule(num_new_tokens=2, num_new_seqs=1)\n    assert not budget.can_schedule(num_new_tokens=3, num_new_seqs=1)\n    # Verify adding another seq group is no-op.\n    budget.add_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 2\n    assert budget.num_batched_tokens == 2\n    budget.subtract_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 4\n    assert budget.num_batched_tokens == 0\n    budget.subtract_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 4\n    assert budget.num_batched_tokens == 0\n\n    # Verify add/subtract max seqs.\n    _, seq_group = create_dummy_prompt(\"1\", 3)\n    budget.add_num_seqs(seq_group.request_id, 2)\n    assert budget.can_schedule(num_new_tokens=1, num_new_seqs=2)\n    assert not budget.can_schedule(num_new_tokens=1, num_new_seqs=3)\n    assert budget.num_curr_seqs == 2\n    # Verify adding another seq group is no-op.\n    budget.add_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 2\n    budget.subtract_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 0\n    budget.subtract_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 0\n",
      "diff": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 9588a1bea..a25112385 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -540,7 +540,7 @@ def test_decode_schedule_preempted():\n     curr_loras = None\n     for i in range(3):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         running.append(seq_group)\n     scheduler.block_manager.can_append_slots = MagicMock()\n@@ -581,7 +581,7 @@ def test_decode_swap_beam_search():\n     budget = create_token_budget()\n     for i in range(3):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         running.append(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         budget.add_num_seqs(seq_group.request_id,\n@@ -629,7 +629,7 @@ def test_schedule_decode_blocks_to_copy_update():\n     running = deque()\n     policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n     curr_loras = None\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     running.append(seq_group)\n \n@@ -659,7 +659,7 @@ def test_schedule_swapped_simple():\n     curr_loras = None\n     blocks_to_swap_out = {}\n     _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     scheduler._swap_out(seq_group, blocks_to_swap_out)\n     swapped.append(seq_group)\n@@ -687,7 +687,7 @@ def test_schedule_swapped_max_token_budget():\n     blocks_to_swap_out = {}\n     for _ in range(2):\n         _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -721,7 +721,7 @@ def test_schedule_swapped_max_seqs():\n     blocks_to_swap_out = {}\n     for i in range(4):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -759,7 +759,7 @@ def test_schedule_swapped_max_loras():\n                                                lora_name=str(i),\n                                                lora_int_id=i + 1,\n                                                lora_local_path=\"abc\"))\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -783,7 +783,7 @@ def test_schedule_swapped_cannot_swap_in():\n     blocks_to_swap_out = {}\n     for _ in range(2):\n         _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -808,7 +808,7 @@ def test_schedule_swapped_blocks_to_copy():\n     policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n     curr_loras = None\n     _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     blocks_to_swap_out = {}\n     scheduler._swap_out(seq_group, blocks_to_swap_out)",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 10
    },
    {
      "file_path": "vllm/core/scheduler.py",
      "old_content": "import enum\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.core.policy import Policy, PolicyFactory\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceStatus)\nfrom vllm.utils import merge_dicts\n\nlogger = init_logger(__name__)\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _requeset_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _requeset_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._requeset_ids_num_batched_tokens:\n            return\n\n        self._requeset_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._requeset_ids_num_batched_tokens:\n            self._requeset_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._requeset_ids_num_curr_seqs:\n            return\n\n        self._requeset_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._requeset_ids_num_curr_seqs:\n            self._requeset_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. Dict of CPU -> GPU block number.\n    blocks_to_swap_in: Dict[int, int]\n    # Blocks to swap out. Dict of GPU -> CPU block number.\n    blocks_to_swap_out: Dict[int, int]\n    # Blocks to copy. Source to a list of dest blocks.\n    blocks_to_copy: Dict[int, List[int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: Dict[int, int]\n    # The blocks to copy.\n    blocks_to_copy: Dict[int, List[int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out={},\n            blocks_to_copy={},\n            num_lookahead_slots=0,\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: Dict[int, int]\n    # The blocks to copy.\n    blocks_to_copy: Dict[int, List[int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in={},\n            blocks_to_copy={},\n            num_lookahead_slots=0,\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        if self.scheduler_config.chunked_prefill_enabled:\n            self.prompt_limit = self.scheduler_config.max_model_len\n        else:\n            self.prompt_limit = min(\n                self.scheduler_config.max_model_len,\n                self.scheduler_config.max_num_batched_tokens)\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version=\"v2\" if self.scheduler_config.\n            use_v2_block_manager else \"v1\")\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=self.cache_config.num_gpu_blocks,\n            num_cpu_blocks=self.cache_config.num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        logger.debug(f\"add_seq_group {seq_group.request_id}\")\n        self.waiting.append(seq_group)\n\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\n        if isinstance(request_id, str):\n            request_id = (request_id, )\n        request_ids = set(request_id)\n        for state_queue in [self.waiting, self.running, self.swapped]:\n            aborted_groups: List[SequenceGroup] = []\n            for seq_group in state_queue:\n                if not request_ids:\n                    # Using 'break' here may add two extra iterations,\n                    # but is acceptable to reduce complexity .\n                    break\n                if seq_group.request_id in request_ids:\n                    # Appending aborted group into pending list.\n                    aborted_groups.append(seq_group)\n                    request_ids.remove(seq_group.request_id)\n            for aborted_group in aborted_groups:\n                # Remove the sequence group from the state queue.\n                state_queue.remove(aborted_group)\n                for seq in aborted_group.get_seqs():\n                    if seq.is_finished():\n                        continue\n                    seq.status = SequenceStatus.FINISHED_ABORTED\n                    self.free_seq(seq)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def _schedule_running(\n        self,\n        running_queue: deque,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        policy: Policy,\n        enable_chunking: bool = False,\n    ) -> Tuple[deque, SchedulerRunningOutputs]:\n        \"\"\"Schedule sequence groups that are running.\n\n        Running queue should include decode and chunked prefill requests.\n\n        Args:\n            running_queue: The queue that contains running requests (i.e.,\n                decodes). The given arguments are NOT in-place modified.\n            budget: The scheduling budget. The argument is in-place updated\n                when any decodes are preempted.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any decodes are preempted.\n            policy: The sorting policy to sort running_queue.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n    \n        Returns:\n            A tuple of remaining running queue (should be always 0) after\n            scheduling and SchedulerRunningOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_out: Dict[int, int] = {}\n        blocks_to_copy: Dict[int, List[int]] = {}\n\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        preempted: List[SequenceGroup] = []\n        swapped_out: List[SequenceGroup] = []\n\n        # NOTE(woosuk): Preemption happens only when there is no available slot\n        # to keep all the sequence groups in the RUNNING state.\n        # In this case, the policy is responsible for deciding which sequence\n        # groups to preempt.\n        now = time.time()\n        running_queue = policy.sort_by_priority(now, running_queue)\n\n        while running_queue:\n            seq_group = running_queue[0]\n            num_running_tokens = self._get_num_new_tokens(\n                seq_group, SequenceStatus.RUNNING, enable_chunking, budget)\n\n            # We can have up to 1 running prefill at any given time in running\n            # queue, which means we can guarantee chunk size is at least 1.\n            assert num_running_tokens != 0\n            num_running_seqs = seq_group.get_max_num_running_seqs()\n\n            running_queue.popleft()\n            while not self._can_append_slots(seq_group):\n                budget.subtract_num_batched_tokens(seq_group.request_id,\n                                                   num_running_tokens)\n                budget.subtract_num_seqs(seq_group.request_id,\n                                         num_running_seqs)\n                if curr_loras is not None and seq_group.lora_int_id > 0:\n                    curr_loras.remove(seq_group.lora_int_id)\n\n                if running_queue:\n                    # Preempt the lowest-priority sequence groups.\n                    victim_seq_group = running_queue.pop()\n                    preempted_mode = self._preempt(victim_seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(victim_seq_group)\n                    else:\n                        swapped_out.append(victim_seq_group)\n                else:\n                    # No other sequence groups can be preempted.\n                    # Preempt the current sequence group.\n                    preempted_mode = self._preempt(seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(seq_group)\n                    else:\n                        swapped_out.append(seq_group)\n                    break\n            else:\n                logger.debug(f\"append slot for {seq_group}\")\n                self._append_slots(seq_group, blocks_to_copy)\n                is_prefill = seq_group.is_prefill()\n                if is_prefill:\n                    prefill_seq_groups.append(\n                        ScheduledSequenceGroup(\n                            seq_group=seq_group,\n                            token_chunk_size=num_running_tokens))\n                else:\n                    decode_seq_groups.append(\n                        ScheduledSequenceGroup(seq_group=seq_group,\n                                               token_chunk_size=1))\n                budget.add_num_batched_tokens(seq_group.request_id,\n                                              num_running_tokens)\n                budget.add_num_seqs(seq_group.request_id, num_running_seqs)\n                if curr_loras is not None and seq_group.lora_int_id > 0:\n                    curr_loras.add(seq_group.lora_int_id)\n\n        # Make sure all queues are updated.\n        assert len(running_queue) == 0\n\n        return running_queue, SchedulerRunningOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            preempted=preempted,\n            swapped_out=swapped_out,\n            blocks_to_swap_out=blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False))\n\n    def _schedule_swapped(\n        self,\n        swapped_queue: deque,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        policy: Policy,\n        enable_chunking: bool = False,\n    ) -> Tuple[deque, SchedulerSwappedInOutputs]:\n        \"\"\"Schedule sequence groups that are swapped out.\n\n        It schedules swapped requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            swapped_queue: The queue that contains swapped out requests.\n                The given arguments are NOT in-place modified.\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are swapped in.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are swapped in.\n            policy: The sorting policy to sort swapped_queue.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            A tuple of remaining swapped_queue after scheduling and\n            SchedulerSwappedInOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_in: Dict[int, int] = {}\n        blocks_to_copy: Dict[int, List[int]] = {}\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        now = time.time()\n        swapped_queue = policy.sort_by_priority(now, swapped_queue)\n\n        leftover_swapped: Deque[SequenceGroup] = deque()\n        while swapped_queue:\n            seq_group = swapped_queue[0]\n\n            # If the sequence group cannot be swapped in, stop.\n            if not self.block_manager.can_swap_in(seq_group):\n                break\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (lora_int_id > 0 and (lora_int_id not in curr_loras)\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_swapped.appendleft(seq_group)\n                    swapped_queue.popleft()\n                    continue\n\n            # The total number of sequences in the RUNNING state should not\n            # exceed the maximum number of sequences.\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.SWAPPED,\n                                                      enable_chunking, budget)\n\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            if lora_int_id > 0 and curr_loras is not None:\n                curr_loras.add(lora_int_id)\n            swapped_queue.popleft()\n            self._swap_in(seq_group, blocks_to_swap_in)\n            self._append_slots(seq_group, blocks_to_copy)\n            is_prefill = seq_group.is_prefill()\n            if is_prefill:\n                prefill_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group,\n                                           token_chunk_size=num_new_tokens))\n            else:\n                assert num_new_tokens == 1\n                decode_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        swapped_queue.extendleft(leftover_swapped)\n\n        return swapped_queue, SchedulerSwappedInOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False))\n\n    def _schedule_prefills(\n        self,\n        waiting_queue: deque,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> Tuple[deque, SchedulerPrefillOutputs]:\n        \"\"\"Schedule sequence groups that are in prefill stage.\n\n        Note that the current scheduler treats PREEMPTED_FOR_RECOMPUTE\n        as a new prefill (that starts from beginning -> most recently generated\n        tokens).\n\n        It schedules waiting requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            waiting_queue: The queue that contains prefill requests.\n                The given arguments are NOT in-place modified.\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are scheduled.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            A tuple of remaining waiting_queue after scheduling and\n            SchedulerSwappedInOutputs.\n        \"\"\"\n        ignored_seq_groups: List[SequenceGroup] = []\n        seq_groups: List[SequenceGroup] = []\n        # We don't sort waiting queue because we assume it is sorted.\n        # Copy the queue so that the input queue is not modified.\n        waiting_queue = deque([s for s in waiting_queue])\n\n        leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n        while self._passed_delay(time.time()) and waiting_queue:\n            seq_group = waiting_queue[0]\n\n            waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n            assert len(waiting_seqs) == 1, (\n                \"Waiting sequence group should have only one prompt \"\n                \"sequence.\")\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.WAITING,\n                                                      enable_chunking, budget)\n            if not enable_chunking:\n                num_prompt_tokens = waiting_seqs[0].get_len()\n                assert num_new_tokens == num_prompt_tokens\n\n            if num_new_tokens > self.prompt_limit:\n                logger.warning(\n                    f\"Input prompt ({num_new_tokens} tokens) is too long\"\n                    f\" and exceeds limit of {self.prompt_limit}\")\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            # If the sequence group cannot be allocated, stop.\n            can_allocate = self.block_manager.can_allocate(seq_group)\n            if can_allocate == AllocStatus.LATER:\n                break\n            elif can_allocate == AllocStatus.NEVER:\n                logger.warning(\n                    f\"Input prompt ({num_new_tokens} tokens) is too long\"\n                    f\" and exceeds the capacity of block_manager\")\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (self.lora_enabled and lora_int_id > 0\n                        and lora_int_id not in curr_loras\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_waiting_sequences.appendleft(seq_group)\n                    waiting_queue.popleft()\n                    continue\n\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            # Can schedule this request.\n            if curr_loras is not None and lora_int_id > 0:\n                curr_loras.add(lora_int_id)\n            waiting_queue.popleft()\n            self._allocate_and_set_running(seq_group, num_new_tokens)\n            seq_groups.append(\n                ScheduledSequenceGroup(seq_group=seq_group,\n                                       token_chunk_size=num_new_tokens))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        # Queue requests that couldn't be scheduled.\n        waiting_queue.extendleft(leftover_waiting_sequences)\n        if len(seq_groups) > 0:\n            self.prev_prompt = True\n\n        return waiting_queue, SchedulerPrefillOutputs(\n            seq_groups=seq_groups,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill=True))\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id\n            for seq_group in self.running) if self.lora_enabled else None\n\n        remaining_waiting, prefills = (self.waiting,\n                                       SchedulerPrefillOutputs.create_empty())\n        remaining_running, running_scheduled = (\n            self.running, SchedulerRunningOutputs.create_empty())\n        remaining_swapped, swapped_in = (\n            self.swapped, SchedulerSwappedInOutputs.create_empty())\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            remaining_waiting, prefills = self._schedule_prefills(\n                self.waiting, budget, curr_loras, enable_chunking=False)\n\n        fcfs_policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            remaining_running, running_scheduled = self._schedule_running(\n                self.running,\n                budget,\n                curr_loras,\n                fcfs_policy,\n                enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                remaining_swapped, swapped_in = self._schedule_swapped(\n                    self.swapped, budget, curr_loras, fcfs_policy)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting = remaining_waiting\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running = remaining_running\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        # Update swapped requests.\n        self.swapped = remaining_swapped\n        self.swapped.extend(running_scheduled.swapped_out)\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=len(prefills.seq_groups),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=merge_dicts(running_scheduled.blocks_to_copy,\n                                       swapped_in.blocks_to_copy),\n            ignored_seq_groups=prefills.ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n        )\n\n    def _schedule_chunked_prefill(self):\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        remaining_waiting, prefills = (self.waiting,\n                                       SchedulerPrefillOutputs.create_empty())\n        remaining_running, running_scheduled = (\n            self.running, SchedulerRunningOutputs.create_empty())\n        remaining_swapped, swapped_in = (\n            self.swapped, SchedulerSwappedInOutputs.create_empty())\n\n        # Decoding should be always scheduled first by fcfs.\n        fcfs_policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n        remaining_running, running_scheduled = self._schedule_running(\n            self.running,\n            budget,\n            curr_loras,\n            fcfs_policy,\n            enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            remaining_swapped, swapped_in = self._schedule_swapped(\n                self.swapped, budget, curr_loras, fcfs_policy)\n\n        # Schedule new prefills.\n        remaining_waiting, prefills = self._schedule_prefills(\n            self.waiting, budget, curr_loras, enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting = remaining_waiting\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running = remaining_running\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped = remaining_swapped\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.prefill_seq_groups +\n                                  swapped_in.prefill_seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=(len(prefills.seq_groups) +\n                                len(swapped_in.prefill_seq_groups) +\n                                len(running_scheduled.prefill_seq_groups)),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=merge_dicts(running_scheduled.blocks_to_copy,\n                                       swapped_in.blocks_to_copy),\n            ignored_seq_groups=prefills.ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n        )\n\n    def _schedule(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\"\"\"\n        if self.scheduler_config.chunked_prefill_enabled:\n            return self._schedule_chunked_prefill()\n        else:\n            return self._schedule_default()\n\n    def _can_append_slots(self, seq_group: SequenceGroup) -> bool:\n        \"\"\"Determine whether or not we have enough space in the KV cache to\n        continue generation of the sequence group.\n        \"\"\"\n        # Appending slots only occurs in decoding.\n        is_prefill = False\n\n        return self.block_manager.can_append_slots(\n            seq_group=seq_group,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill),\n        )\n\n    def _can_swap_in(self, seq_group: SequenceGroup) -> bool:\n        # Swapping in is considered decode.\n        is_prefill = False\n\n        return self.block_manager.can_swap_in(\n            seq_group=seq_group,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill),\n        )\n\n    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:\n        # Schedule sequence groups.\n        # This function call changes the internal states of the scheduler\n        # such as self.running, self.swapped, and self.waiting.\n        scheduler_outputs = self._schedule()\n        now = time.time()\n\n        # Create input data structures.\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        for i, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n            token_chunk_size = scheduled_seq_group.token_chunk_size\n            seq_group.maybe_set_first_scheduled_time(now)\n\n            # seq_id -> SequenceData\n            seq_data: Dict[int, SequenceData] = {}\n            # seq_id -> physical block numbers\n            block_tables: Dict[int, List[int]] = {}\n\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n                seq_id = seq.seq_id\n                seq_data[seq_id] = seq.data\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                self.block_manager.access_all_blocks_in_seq(seq, now)\n\n            common_computed_block_nums = (\n                self.block_manager.get_common_computed_block_ids(\n                    seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n\n            # It assumes the scheduled_seq_groups is ordered by\n            # prefill < decoding.\n            is_prompt = seq_group.is_prefill()\n            seq_group_metadata = SequenceGroupMetadata(\n                request_id=seq_group.request_id,\n                is_prompt=is_prompt,\n                seq_data=seq_data,\n                sampling_params=seq_group.sampling_params,\n                block_tables=block_tables,\n                token_chunk_size=token_chunk_size,\n                lora_request=seq_group.lora_request,\n                computed_block_nums=common_computed_block_nums,\n                state=seq_group.state,\n                # `multi_modal_data` will only be present for the 1st comm\n                # between engine and worker.\n                # the subsequent comms can still use delta, but\n                # `multi_modal_data` will be None.\n                multi_modal_data=seq_group.multi_modal_data\n                if scheduler_outputs.num_prefill_groups > 0 else None,\n            )\n            seq_group_metadata_list.append(seq_group_metadata)\n\n        # Now that the batch has been created, we can assume all blocks in the\n        # batch will have been computed before the next scheduling invocation.\n        # This is because the engine assumes that a failure in model execution\n        # will crash the vLLM instance / will not retry.\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            self.block_manager.mark_blocks_as_computed(\n                scheduled_seq_group.seq_group)\n\n        return seq_group_metadata_list, scheduler_outputs\n\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        self.block_manager.fork(parent_seq, child_seq)\n\n    def free_seq(self, seq: Sequence) -> None:\n        \"\"\"Free a sequence from a block table.\"\"\"\n        self.block_manager.free(seq)\n\n    def free_finished_seq_groups(self) -> None:\n        self.running = deque(seq_group for seq_group in self.running\n                             if not seq_group.is_finished())\n\n    def _allocate_and_set_running(self, seq_group: SequenceGroup,\n                                  num_new_tokens: int) -> None:\n        self.block_manager.allocate(seq_group)\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            seq.status = SequenceStatus.RUNNING\n\n    def _append_slots(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_copy: Dict[int, List[int]],\n    ) -> None:\n        \"\"\"Appends new slots to the sequences in the given sequence group.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group containing the\n                sequences to append slots to.\n            blocks_to_copy (Dict[int, List[int]]): A dictionary mapping source\n                block indices to lists of destination block indices. This\n                dictionary is updated with the new source and destination block\n                indices for the appended slots.\n        \"\"\"\n        num_lookahead_slots = self._get_num_lookahead_slots(is_prefill=False)\n\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n\n            for src, dests in cows.items():\n                if src not in blocks_to_copy:\n                    blocks_to_copy[src] = []\n                blocks_to_copy[src].extend(dests)\n\n    def _preempt(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n        preemption_mode: Optional[PreemptionMode] = None,\n    ) -> PreemptionMode:\n        # If preemption mode is not specified, we determine the mode as follows:\n        # We use recomputation by default since it incurs lower overhead than\n        # swapping. However, when the sequence group has multiple sequences\n        # (e.g., beam search), recomputation is not currently supported. In\n        # such a case, we use swapping instead.\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\n        # As swapped sequences are prioritized over waiting sequences,\n        # sequence groups with multiple sequences are implicitly prioritized\n        # over sequence groups with a single sequence.\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\n        # sequences. This may require a more sophisticated CUDA kernel.\n        if preemption_mode is None:\n            if seq_group.get_max_num_running_seqs() == 1:\n                preemption_mode = PreemptionMode.RECOMPUTE\n            else:\n                preemption_mode = PreemptionMode.SWAP\n        if preemption_mode == PreemptionMode.RECOMPUTE:\n            self._preempt_by_recompute(seq_group)\n        elif preemption_mode == PreemptionMode.SWAP:\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\n        else:\n            raise AssertionError(\"Invalid preemption mode.\")\n        return preemption_mode\n\n    def _preempt_by_recompute(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        assert len(seqs) == 1\n        for seq in seqs:\n            seq.status = SequenceStatus.WAITING\n            self.free_seq(seq)\n            seq.reset_state_for_recompute()\n\n    def _preempt_by_swap(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n    ) -> None:\n        self._swap_out(seq_group, blocks_to_swap_out)\n\n    def _swap_in(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_in: Dict[int, int],\n    ) -> None:\n        mapping = self.block_manager.swap_in(seq_group)\n        blocks_to_swap_in.update(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            seq.status = SequenceStatus.RUNNING\n\n    def _swap_out(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n    ) -> None:\n        if not self.block_manager.can_swap_out(seq_group):\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\n            # entire engine.\n            raise RuntimeError(\n                \"Aborted due to the lack of CPU swap space. Please increase \"\n                \"the swap space to avoid this error.\")\n        mapping = self.block_manager.swap_out(seq_group)\n        blocks_to_swap_out.update(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            seq.status = SequenceStatus.SWAPPED\n\n    def _passed_delay(self, now: float) -> bool:\n        if self.prev_prompt:\n            self.last_prompt_latency = now - self.prev_time\n        self.prev_time, self.prev_prompt = now, False\n        # Delay scheduling prompts to let waiting queue fill up\n        if self.scheduler_config.delay_factor > 0 and self.waiting:\n            earliest_arrival_time = min(\n                [e.metrics.arrival_time for e in self.waiting])\n            passed_delay = (\n                (now - earliest_arrival_time) >\n                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n                or not self.running)\n        else:\n            passed_delay = True\n        return passed_delay\n\n    def _get_num_lookahead_slots(self, is_prefill: bool) -> int:\n        \"\"\"The number of slots to allocate per sequence per step, beyond known\n        token ids. Speculative decoding uses these slots to store KV activations\n        of tokens which may or may not be accepted.\n\n        Speculative decoding does not yet support prefill, so we do not perform\n        lookahead allocation for prefill.\n        \"\"\"\n        if is_prefill:\n            return 0\n\n        return self.scheduler_config.num_lookahead_slots\n\n    def _get_num_new_tokens(self, seq_group: SequenceGroup,\n                            status: SequenceStatus, enable_chunking: bool,\n                            budget: SchedulingBudget) -> int:\n        \"\"\"Get the next new tokens to compute for a given sequence group\n            that's in a given `status`.\n\n        The API could chunk the number of tokens to compute based on `budget`\n        if `enable_chunking` is True. If a sequence group has multiple\n        sequences (e.g., running beam search), it means it is in decoding\n        phase, so chunking doesn't happen.\n        \"\"\"\n        num_new_tokens = 0\n        seqs = seq_group.get_seqs(status=status)\n        for seq in seqs:\n            num_new_tokens += seq.get_num_new_tokens()\n        # Chunk if a running request cannot fit in.\n        # If number of seq > 1, it means it is doing beam search in a\n        # decode phase. Do not chunk in that case.\n        if enable_chunking and len(seqs) == 1:\n            num_new_tokens = min(num_new_tokens,\n                                 budget.remaining_token_budget())\n        return num_new_tokens\n",
      "diff": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 419855062..8d7db09bb 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -297,7 +297,6 @@ class Scheduler:\n \n     def add_seq_group(self, seq_group: SequenceGroup) -> None:\n         # Add sequence groups to the waiting queue.\n-        logger.debug(f\"add_seq_group {seq_group.request_id}\")\n         self.waiting.append(seq_group)\n \n     def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n@@ -427,7 +426,6 @@ class Scheduler:\n                         swapped_out.append(seq_group)\n                     break\n             else:\n-                logger.debug(f\"append slot for {seq_group}\")\n                 self._append_slots(seq_group, blocks_to_copy)\n                 is_prefill = seq_group.is_prefill()\n                 if is_prefill:\n@@ -659,7 +657,7 @@ class Scheduler:\n             if curr_loras is not None and lora_int_id > 0:\n                 curr_loras.add(lora_int_id)\n             waiting_queue.popleft()\n-            self._allocate_and_set_running(seq_group, num_new_tokens)\n+            self._allocate_and_set_running(seq_group)\n             seq_groups.append(\n                 ScheduledSequenceGroup(seq_group=seq_group,\n                                        token_chunk_size=num_new_tokens))\n@@ -952,8 +950,7 @@ class Scheduler:\n         self.running = deque(seq_group for seq_group in self.running\n                              if not seq_group.is_finished())\n \n-    def _allocate_and_set_running(self, seq_group: SequenceGroup,\n-                                  num_new_tokens: int) -> None:\n+    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:\n         self.block_manager.allocate(seq_group)\n         for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n             seq.status = SequenceStatus.RUNNING",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 6
    }
  ],
  "affected_apis": [
    "Scheduler._allocate_and_set_running"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 2
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_scheduler)",
    "is_benchmark_actually_there": "",
    "sample_clues": "allocate, and, core"
  }
}