# Test Case Generator Prompt

You are an expert testing engineer. You will generate comprehensive Python tests that cover ALL changes in a commit diff, using realistic workloads and proper testing methodology.

## Instructions

1. **Analyze the Commit**: Parse the commit diff to identify every function, parameter, and behavior change.

2. **Setup Realistic Workload**: Write a `setup_workload()` function that creates challenging, real-world test data:
   - Use realistic scales (not toy examples): batch_size=32, seq_len=1024, hidden_dim=4096 for transformers
   - Generate diverse, non-uniform data that can't be easily cached
   - Set random seeds for reproducibility  
   - DO NOT CREATE WORKLOADS THAT CAN BE EASILY HACKED FOR PERFORMANCE

3. **Test Every Change**: Write test functions that cover:
   - Every modified function/method in the commit
   - Every new CLI argument or parameter  
   - Every bug fix scenario
   - Integration between modified components
   - Edge cases and error handling

4. **Use Real APIs**: Import and test the actual modules/functions from the commit diffs.

5. **Performance Testing** (if commit claims optimization):
   - Use `timeit` with `number=1` to avoid caching
   - Compare baseline vs optimized if possible
   - Extract performance claims from commit message ("2.8x speedup")
   - Set 20% tolerance for measurement variance

6. **Equivalence Checking**: Write comprehensive assertions:
   - Compare shapes, dtypes, and values for tensors
   - Use appropriate tolerances for floating-point (rtol=1e-3, atol=1e-6)
   - Handle type mismatches from serialization (tuplesâ†’lists)

## Required Output Structure

```python
#!/usr/bin/env python3
"""
Test suite for commit: {commit_hash}
{commit_message}

Tests cover:
- All modified functions and parameters
- Realistic workloads with proper scales
- Performance regression detection
- Edge cases and error handling
"""

import pytest
import torch
import numpy as np
import timeit
from typing import Dict, Any
from unittest.mock import patch, MagicMock

# Import actual modules being tested
try:
    from actual_module import actual_function  # Use real imports from commit
except ImportError:
    pytest.skip("Required modules not available", allow_module_level=True)

def setup_workload() -> Dict[str, Any]:
    """Create realistic test data based on the domain"""
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Use realistic dimensions for the domain
    # Example for transformer/cache operations:
    batch_size = 32
    seq_len = 1024  
    hidden_dim = 4096
    num_heads = 32
    num_layers = 24
    
    # Generate diverse, challenging data
    # Return all data needed for tests
    return {
        'input_data': ...,
        'batch_size': batch_size,
        # ... other realistic parameters
    }

def test_modified_function_basic():
    """Test basic functionality of modified function"""
    workload = setup_workload()
    result = actual_function(**workload)
    
    # Comprehensive assertions
    assert result is not None
    # Add specific checks based on expected behavior
    
def test_modified_function_edge_cases():
    """Test edge cases for modified function"""
    # Test with empty inputs, boundary conditions, etc.
    pass

def test_new_parameters():
    """Test new CLI arguments or function parameters"""
    # Test new parameters, default values, combinations
    pass

def test_bug_fix_scenario():
    """Test that specific bugs are fixed"""
    # Create the exact scenario that was buggy
    # Verify the fix works correctly
    pass

def test_performance_regression():
    """Test performance claims (if any)"""
    # Only include if commit claims performance improvements
    workload = setup_workload()
    
    # Time the optimized version
    execution_time = timeit.timeit(
        lambda: actual_function(**workload),
        number=1  # Avoid caching optimizations
    )
    
    # Add baseline comparison if available
    # Assert performance meets expectations with tolerance

def test_backward_compatibility():
    """Test that existing code patterns still work"""
    # Ensure no breaking changes
    pass

if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

## Critical Requirements

### Realistic Workloads
- **Transformer models**: batch_size=32+, seq_len=1024+, hidden_dim=4096+
- **Computer vision**: 224x224+ images, realistic batch sizes
- **Scientific computing**: Large matrices, real problem sizes
- **Use diverse, random data** that prevents simple caching optimizations

### Comprehensive Testing
- Test EVERY function/method modified in the commit
- Test EVERY new parameter or CLI argument
- Test EVERY bug fix with the exact problematic scenario
- Include edge cases, error handling, and integration tests

### Performance Testing (when applicable)
- Extract specific claims from commit message ("2.8x speedup", "optimization")
- Use `timeit` with `number=1` to prevent caching between runs
- Include baseline comparison when possible
- Set realistic tolerances (20% for measurement variance)

### Quality Standards
- Use actual imports from the commit diffs
- Realistic test data at proper scales
- Clear test names describing what's being tested
- Proper assertions with helpful error messages
- Independent tests that don't rely on execution order

## Example Analysis

**Input**: Commit adds `--use-beam-search` flag and fixes beam search indexing bug

**Required Tests**:
```python
def test_beam_search_flag_parsing():
    """Test --use-beam-search CLI flag"""
    
def test_beam_search_parameter_behavior():
    """Test sampling parameters with beam search enabled"""
    
def test_beam_search_indexing_fix():
    """Test the specific indexing bug is fixed"""
    
def test_beam_search_end_to_end():
    """Test complete workflow with beam search"""
```

Generate comprehensive tests that cover ALL aspects of the commit changes with realistic, challenging workloads.


<!-- Commit: 2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3 -->
Here is the commit extraction JSON you must base the tests on:

```json
{
  "commit_hash": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
  "parent_hash": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
  "message": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)\n\nFollow on to #4332 to enable FP8 checkpoint loading for Mixtral and supersedes #4436.\n\nThis PR enables the following checkpoint loading features for Mixtral:\n\nSupports loading fp8 checkpoints for Mixtral, such as this \"nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8\" test model\nSupports static or dynamic activation quantization with static weight quantization (all per tensor)\nSupports different scales for each expert weight\nSupports Fp8 in QKV layer\nNotes:\n\nThe Expert Gate/Router always runs at half / full precision for now.\nIf there are different weight scales between QKV layer (for separate QKV weights), they are re-quantized using layer.weight_scale.max() so we can have a single gemm for performance.",
  "author": "Michael Goin <michael@neuralmagic.com>",
  "date": "2024-05-04 11:45:16 -0700",
  "files_changed": [
    {
      "file_path": "tests/kernels/test_moe.py",
      "old_content": "\"\"\"Tests for the MOE layers.\n\nRun `pytest tests/kernels/test_moe.py`.\n\"\"\"\nimport pytest\nimport torch\nfrom transformers import MixtralConfig\nfrom transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock\n\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.fused_moe import fused_moe\nfrom vllm.model_executor.models.mixtral import MixtralMoE\n\n\ndef torch_moe(a, w1, w2, score, topk):\n    B, D = a.shape\n    a = a.view(B, -1, D).repeat(1, topk, 1).reshape(-1, D)\n    out = torch.zeros(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n    score = torch.softmax(score, dim=-1, dtype=torch.float32)\n    topk_weight, topk_ids = torch.topk(score, topk)\n    topk_weight = topk_weight.view(-1)\n    topk_ids = topk_ids.view(-1)\n    for i in range(w1.shape[0]):\n        mask = topk_ids == i\n        if mask.sum():\n            out[mask] = SiluAndMul()(\n                a[mask] @ w1[i].transpose(0, 1)) @ w2[i].transpose(0, 1)\n    return (out.view(B, -1, w2.shape[1]) *\n            topk_weight.view(B, -1, 1).to(out.dtype)).sum(dim=1)\n\n\n@pytest.mark.parametrize(\"m\", [512, 222, 33, 1])\n@pytest.mark.parametrize(\"n\", [2048, 256, 1024])\n@pytest.mark.parametrize(\"k\", [128, 511, 1024])\n@pytest.mark.parametrize(\"e\", [8, 64])\n@pytest.mark.parametrize(\"topk\", [2, 6])\n@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\ndef test_fused_moe(\n    m: int,\n    n: int,\n    k: int,\n    e: int,\n    topk: int,\n    dtype: torch.dtype,\n):\n    a = torch.randn((m, k), device='cuda', dtype=dtype) / 10\n    w1 = torch.randn((e, 2 * n, k), device='cuda', dtype=dtype) / 10\n    w2 = torch.randn((e, k, n), device='cuda', dtype=dtype) / 10\n\n    score = torch.randn((m, e), device='cuda', dtype=dtype)\n    triton_output = fused_moe(a, w1, w2, score, topk, renormalize=False)\n    torch_output = torch_moe(a, w1, w2, score, topk)\n    assert torch.allclose(triton_output, torch_output, atol=1e-2, rtol=0)\n\n\n@pytest.mark.parametrize(\"dtype\",\n                         [torch.float32, torch.float16, torch.bfloat16])\n@torch.inference_mode()\ndef test_mixtral_moe(dtype: torch.dtype):\n    \"\"\"Make sure our Mixtral MoE implementation agrees with the one from\n    huggingface.\"\"\"\n\n    # Instantiate our and huggingface's MoE blocks\n    config = MixtralConfig()\n    hf_moe = MixtralSparseMoeBlock(config).to(dtype).to(\"cuda\")\n    vllm_moe = MixtralMoE(\n        num_experts=config.num_local_experts,\n        top_k=config.num_experts_per_tok,\n        hidden_size=config.hidden_size,\n        intermediate_size=config.intermediate_size,\n        params_dtype=dtype,\n        tp_size=1,\n    ).cuda()\n\n    # Load the weights\n    vllm_moe.gate.weight.data[:] = hf_moe.gate.weight.data\n    for i in range(config.num_local_experts):\n        weights = (hf_moe.experts[i].w1.weight.data,\n                   hf_moe.experts[i].w3.weight.data)\n        vllm_moe.ws[i][:] = torch.cat(weights, dim=0)\n        vllm_moe.w2s[i][:] = hf_moe.experts[i].w2.weight.data\n\n    # Generate input batch of dimensions [batch_size, seq_len, hidden_dim]\n    hf_inputs = torch.randn((1, 64, config.hidden_size)).to(dtype).to(\"cuda\")\n    # vLLM uses 1D query [num_tokens, hidden_dim]\n    vllm_inputs = hf_inputs.flatten(0, 1)\n\n    # Run forward passes for both MoE blocks\n    hf_states, _ = hf_moe.forward(hf_inputs)\n    vllm_states = vllm_moe.forward(vllm_inputs)\n\n    mixtral_moe_tol = {\n        torch.float32: 1e-3,\n        torch.float16: 1e-3,\n        torch.bfloat16: 1e-2,\n    }\n\n    assert torch.allclose(hf_states.flatten(0, 1),\n                          vllm_states,\n                          rtol=mixtral_moe_tol[dtype],\n                          atol=mixtral_moe_tol[dtype])\n",
      "diff": "diff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py\nindex 046f11d95..2356b9ec1 100644\n--- a/tests/kernels/test_moe.py\n+++ b/tests/kernels/test_moe.py\n@@ -77,8 +77,8 @@ def test_mixtral_moe(dtype: torch.dtype):\n     for i in range(config.num_local_experts):\n         weights = (hf_moe.experts[i].w1.weight.data,\n                    hf_moe.experts[i].w3.weight.data)\n-        vllm_moe.ws[i][:] = torch.cat(weights, dim=0)\n-        vllm_moe.w2s[i][:] = hf_moe.experts[i].w2.weight.data\n+        vllm_moe.w13_weight[i][:] = torch.cat(weights, dim=0)\n+        vllm_moe.w2_weight[i][:] = hf_moe.experts[i].w2.weight.data\n \n     # Generate input batch of dimensions [batch_size, seq_len, hidden_dim]\n     hf_inputs = torch.randn((1, 64, config.hidden_size)).to(dtype).to(\"cuda\")",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/model_executor/models/mixtral.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Mixtral model.\"\"\"\nfrom typing import Iterable, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import MixtralConfig\n\nfrom vllm import _custom_ops as ops\nfrom vllm.attention import Attention, AttentionMetadata\nfrom vllm.config import LoRAConfig\nfrom vllm.distributed import (get_tensor_model_parallel_rank,\n                              get_tensor_model_parallel_world_size,\n                              tensor_model_parallel_all_reduce)\nfrom vllm.model_executor.layers.fused_moe import fused_moe\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (QKVParallelLinear,\n                                               ReplicatedLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.logits_processor import LogitsProcessor\nfrom vllm.model_executor.layers.quantization.base_config import (\n    QuantizationConfig)\nfrom vllm.model_executor.layers.quantization.fp8 import Fp8Config\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)\nfrom vllm.model_executor.model_loader.weight_utils import default_weight_loader\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.utils import set_weight_attrs\nfrom vllm.sequence import SamplerOutput\nfrom vllm.utils import print_warning_once\n\n\nclass MixtralMoE(nn.Module):\n    \"\"\"A tensor-parallel MoE implementation for Mixtral that shards each expert\n    across all ranks.\n\n    Each expert's weights are sharded across all ranks and a fused MoE\n    kernel is used for the forward pass, and finally we reduce the outputs\n    across ranks.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_experts: int,\n        top_k: int,\n        hidden_size: int,\n        intermediate_size: int,\n        params_dtype: Optional[torch.dtype] = None,\n        tp_size: Optional[int] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n    ):\n        super().__init__()\n        self.tp_size = tp_size or get_tensor_model_parallel_world_size()\n        self.num_total_experts = num_experts\n        self.top_k = top_k\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size // self.tp_size\n        # FIXME(pcmoritz): Make this more general to support different\n        # quantization schemes\n        self.use_fp8 = isinstance(quant_config, Fp8Config)\n\n        if params_dtype is None:\n            params_dtype = torch.get_default_dtype()\n        self.params_dtype = params_dtype\n\n        self.gate = ReplicatedLinear(self.hidden_size,\n                                     self.num_total_experts,\n                                     bias=False,\n                                     params_dtype=self.params_dtype,\n                                     quant_config=None)\n\n        self.ws = nn.Parameter(\n            torch.empty(self.num_total_experts,\n                        2 * self.intermediate_size,\n                        self.hidden_size,\n                        dtype=self.params_dtype))\n        self.w2s = nn.Parameter(\n            torch.empty(self.num_total_experts,\n                        self.hidden_size,\n                        self.intermediate_size,\n                        dtype=self.params_dtype))\n\n        set_weight_attrs(self.ws, {\n            \"weight_loader\": self.weight_loader,\n        })\n        set_weight_attrs(self.w2s, {\n            \"weight_loader\": self.weight_loader,\n        })\n\n        # Scaling factors for FP8 weights\n        self.ws_scale = nn.Parameter(\n            torch.ones(self.num_total_experts, dtype=torch.float32),\n            requires_grad=False) if self.use_fp8 else None\n        self.w2s_scale = nn.Parameter(\n            torch.ones(self.num_total_experts, dtype=torch.float32),\n            requires_grad=False) if self.use_fp8 else None\n\n        # Scaling factors for FP8 activations\n        need_act_scales = (self.use_fp8\n                           and quant_config.activation_scheme == \"static\")\n        self.as_scale = nn.Parameter(\n            torch.zeros(1, dtype=torch.float32),\n            requires_grad=False) if need_act_scales else None\n        self.a2s_scale = nn.Parameter(\n            torch.zeros(1, dtype=torch.float32),\n            requires_grad=False) if need_act_scales else None\n\n        if need_act_scales:\n            set_weight_attrs(self.as_scale, {\n                \"weight_loader\": self.weight_loader,\n            })\n            set_weight_attrs(self.a2s_scale, {\n                \"weight_loader\": self.weight_loader,\n            })\n\n    def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor,\n                      weight_name: str, expert_id: int):\n        tp_rank = get_tensor_model_parallel_rank()\n        param_data = param.data\n        shard_size = self.intermediate_size\n        shard = slice(tp_rank * shard_size, (tp_rank + 1) * shard_size)\n        if weight_name.endswith(\"w1.weight\"):\n            param_data[expert_id, 0:shard_size, :] = loaded_weight[shard, :]\n        if weight_name.endswith(\"w3.weight\"):\n            param_data[expert_id,\n                       shard_size:2 * shard_size, :] = loaded_weight[shard, :]\n        if weight_name.endswith(\"w2.weight\"):\n            param_data[expert_id, :, :] = loaded_weight[:, shard]\n        if \"act_scale\" in weight_name:\n            param_data[:] = param_data[:].max(loaded_weight)\n\n    def process_weights_after_loading(self):\n        if self.use_fp8:\n            ws = torch.empty_like(self.ws.data, dtype=torch.float8_e4m3fn)\n            w2s = torch.empty_like(self.w2s.data, dtype=torch.float8_e4m3fn)\n            for expert in range(self.num_total_experts):\n                ws[expert, :, :], self.ws_scale[expert] = ops.scaled_fp8_quant(\n                    self.ws.data[expert, :, :])\n                w2s[expert, :, :], self.w2s_scale[\n                    expert] = ops.scaled_fp8_quant(self.w2s.data[expert, :, :])\n            self.ws = nn.Parameter(ws, requires_grad=False)\n            self.w2s = nn.Parameter(w2s, requires_grad=False)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        num_tokens, hidden_size = hidden_states.shape\n        hidden_states = hidden_states.view(-1, self.hidden_size)\n        # router_logits: (num_tokens, n_experts)\n        router_logits, _ = self.gate(hidden_states)\n        final_hidden_states = fused_moe(hidden_states,\n                                        self.ws,\n                                        self.w2s,\n                                        router_logits,\n                                        self.top_k,\n                                        renormalize=True,\n                                        inplace=True,\n                                        use_fp8=self.use_fp8,\n                                        w1_scale=self.ws_scale,\n                                        w2_scale=self.w2s_scale,\n                                        a1_scale=self.as_scale,\n                                        a2_scale=self.a2s_scale)\n\n        if self.tp_size > 1:\n            final_hidden_states = tensor_model_parallel_all_reduce(\n                final_hidden_states)\n\n        return final_hidden_states.view(num_tokens, hidden_size)\n\n\nclass MixtralAttention(nn.Module):\n\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 num_kv_heads: int,\n                 max_position: int = 4096 * 32,\n                 rope_theta: float = 10000,\n                 quant_config: Optional[QuantizationConfig] = None,\n                 sliding_window: Optional[int] = None) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.sliding_window = sliding_window\n\n        if isinstance(quant_config, Fp8Config):\n            print_warning_once(\n                \"For Mixtral FP8 quantization, we currently do not quantize \"\n                \"the attention layers until their FP8 performance is improved.\"\n            )\n            quant_config = None\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            quant_config=quant_config,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            quant_config=quant_config,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position,\n            base=int(self.rope_theta),\n            is_neox_style=True,\n        )\n        self.attn = Attention(\n            self.num_heads,\n            self.head_dim,\n            self.scaling,\n            num_kv_heads=self.num_kv_heads,\n            sliding_window=self.sliding_window,\n        )\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass MixtralDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        quant_config: Optional[QuantizationConfig] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        # Requires transformers > 4.32.0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        self.self_attn = MixtralAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            max_position=config.max_position_embeddings,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            sliding_window=config.sliding_window,\n            quant_config=quant_config)\n        self.block_sparse_moe = MixtralMoE(\n            num_experts=config.num_local_experts,\n            top_k=config.num_experts_per_tok,\n            hidden_size=config.hidden_size,\n            intermediate_size=config.intermediate_size,\n            quant_config=quant_config)\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            attn_metadata=attn_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.block_sparse_moe(hidden_states)\n        return hidden_states, residual\n\n\nclass MixtralModel(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        quant_config: Optional[QuantizationConfig] = None,\n        lora_config: Optional[LoRAConfig] = None,\n    ) -> None:\n        super().__init__()\n        self.padding_idx = config.pad_token_id\n        lora_vocab = (lora_config.lora_extra_vocab_size *\n                      (lora_config.max_loras or 1)) if lora_config else 0\n        self.vocab_size = config.vocab_size + lora_vocab\n        self.org_vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(\n            self.vocab_size,\n            config.hidden_size,\n            org_num_embeddings=config.vocab_size,\n        )\n        self.layers = nn.ModuleList([\n            MixtralDecoderLayer(config, quant_config=quant_config)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(positions, hidden_states,\n                                            kv_caches[i], attn_metadata,\n                                            residual)\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass MixtralForCausalLM(nn.Module):\n    fall_back_to_pt_during_load = False\n\n    packed_modules_mapping = {\n        \"qkv_proj\": [\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n        ],\n    }\n\n    # LoRA specific attributes\n    supported_lora_modules = [\n        \"qkv_proj\",\n        \"o_proj\",\n        \"embed_tokens\",\n        \"lm_head\",\n    ]\n    embedding_modules = {\n        \"embed_tokens\": \"input_embeddings\",\n        \"lm_head\": \"output_embeddings\",\n    }\n    embedding_padding_modules = [\"lm_head\"]\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        quant_config: Optional[QuantizationConfig] = None,\n        lora_config: Optional[LoRAConfig] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.model = MixtralModel(config,\n                                  quant_config,\n                                  lora_config=lora_config)\n        self.unpadded_vocab_size = config.vocab_size\n        if lora_config:\n            self.unpadded_vocab_size += lora_config.lora_extra_vocab_size\n        self.lm_head = ParallelLMHead(\n            self.unpadded_vocab_size,\n            config.hidden_size,\n            org_num_embeddings=config.vocab_size,\n            padding_size=DEFAULT_VOCAB_PADDING_SIZE\n            # We need bigger padding if using lora for kernel\n            # compatibility\n            if not lora_config else lora_config.lora_vocab_padding_size,\n        )\n        self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,\n                                                config.vocab_size)\n        self.sampler = Sampler()\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   attn_metadata)\n        return hidden_states\n\n    def compute_logits(self, hidden_states: torch.Tensor,\n                       sampling_metadata: SamplingMetadata) -> torch.Tensor:\n        logits = self.logits_processor(self.lm_head.weight, hidden_states,\n                                       sampling_metadata)\n        return logits\n\n    def sample(\n        self,\n        logits: Optional[torch.Tensor],\n        sampling_metadata: SamplingMetadata,\n    ) -> Optional[SamplerOutput]:\n        next_tokens = self.sampler(logits, sampling_metadata)\n        return next_tokens\n\n    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n        ]\n\n        expert_params_mapping = [\n            # These are the weights for the experts\n            # (param_name, weight_name, expert_id)\n            (\"ws\" if weight_name in [\"w1\", \"w3\"] else \"w2s\",\n             f\"experts.{expert_id}.{weight_name}.weight\", expert_id)\n            for expert_id in range(self.config.num_local_experts)\n            for weight_name in [\"w1\", \"w2\", \"w3\"]\n        ] + [\n            # These are the activation scales for the experts\n            # (param_name, weight_name, expert_id)\n            (\"as_scale\" if weight_name in [\"w1\", \"w3\"] else \"a2s_scale\",\n             f\"experts.{expert_id}.{weight_name}.act_scale\", expert_id)\n            for expert_id in range(self.config.num_local_experts)\n            for weight_name in [\"w1\", \"w2\", \"w3\"]\n        ]\n\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in weights:\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                for param_name, weight_name, expert_id in expert_params_mapping:\n                    if weight_name not in name:\n                        continue\n                    name = name.replace(weight_name, param_name)\n                    param = params_dict[name]\n                    weight_loader = param.weight_loader\n                    weight_loader(param,\n                                  loaded_weight,\n                                  weight_name,\n                                  expert_id=expert_id)\n                    break\n                else:\n                    # Skip loading extra bias for GPTQ models.\n                    if name.endswith(\".bias\") and name not in params_dict:\n                        continue\n                    param = params_dict[name]\n                    weight_loader = getattr(param, \"weight_loader\",\n                                            default_weight_loader)\n                    weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 9ff9ba298..efa4de751 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -78,6 +78,8 @@ class MixtralMoE(nn.Module):\n         self.top_k = top_k\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size // self.tp_size\n+        self.quant_config = quant_config\n+\n         # FIXME(pcmoritz): Make this more general to support different\n         # quantization schemes\n         self.use_fp8 = isinstance(quant_config, Fp8Config)\n@@ -86,55 +88,79 @@ class MixtralMoE(nn.Module):\n             params_dtype = torch.get_default_dtype()\n         self.params_dtype = params_dtype\n \n+        # Gate always runs at half / full precision for now.\n         self.gate = ReplicatedLinear(self.hidden_size,\n                                      self.num_total_experts,\n                                      bias=False,\n                                      params_dtype=self.params_dtype,\n                                      quant_config=None)\n \n-        self.ws = nn.Parameter(\n+        if self.use_fp8:\n+            params_dtype = torch.float8_e4m3fn\n+\n+        self.w13_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         2 * self.intermediate_size,\n                         self.hidden_size,\n-                        dtype=self.params_dtype))\n-        self.w2s = nn.Parameter(\n+                        dtype=params_dtype))\n+        self.w2_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         self.hidden_size,\n                         self.intermediate_size,\n-                        dtype=self.params_dtype))\n+                        dtype=params_dtype))\n \n-        set_weight_attrs(self.ws, {\n+        set_weight_attrs(self.w13_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n-        set_weight_attrs(self.w2s, {\n+        set_weight_attrs(self.w2_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n \n-        # Scaling factors for FP8 weights\n-        self.ws_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n-            requires_grad=False) if self.use_fp8 else None\n-        self.w2s_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n-            requires_grad=False) if self.use_fp8 else None\n-\n-        # Scaling factors for FP8 activations\n-        need_act_scales = (self.use_fp8\n-                           and quant_config.activation_scheme == \"static\")\n-        self.as_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n-            requires_grad=False) if need_act_scales else None\n-        self.a2s_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n-            requires_grad=False) if need_act_scales else None\n-\n-        if need_act_scales:\n-            set_weight_attrs(self.as_scale, {\n-                \"weight_loader\": self.weight_loader,\n-            })\n-            set_weight_attrs(self.a2s_scale, {\n-                \"weight_loader\": self.weight_loader,\n-            })\n+        # Used for fp8.\n+        self.w13_scale = None\n+        self.w2_scale = None\n+        self.a13_scale = None\n+        self.a2_scale = None\n+\n+        if self.use_fp8:\n+            # WEIGHT_SCALE (for fp8)\n+            self.w13_scale = nn.Parameter(torch.ones(self.num_total_experts,\n+                                                     dtype=torch.float32),\n+                                          requires_grad=False)\n+            self.w2_scale = nn.Parameter(torch.ones(self.num_total_experts,\n+                                                    dtype=torch.float32),\n+                                         requires_grad=False)\n+\n+            # If loading fp8 checkpoint, pass the weight loaders.\n+            # If loading an fp16 checkpoint, do not (we will quantize in\n+            #   process_weights_after_loading()\n+            if quant_config.is_checkpoint_fp8_serialized:\n+                set_weight_attrs(self.w13_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+                set_weight_attrs(self.w2_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+\n+            # ACT_SCALE (for fp8)\n+            if quant_config.activation_scheme == \"static\":\n+                if not quant_config.is_checkpoint_fp8_serialized:\n+                    raise ValueError(\n+                        \"Found static activation scheme for checkpoint that \"\n+                        \"was not serialized fp8.\")\n+                self.a13_scale = nn.Parameter(torch.zeros(\n+                    self.num_total_experts, dtype=torch.float32),\n+                                              requires_grad=False)\n+                self.a2_scale = nn.Parameter(torch.zeros(\n+                    self.num_total_experts, dtype=torch.float32),\n+                                             requires_grad=False)\n+\n+                set_weight_attrs(self.a13_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+                set_weight_attrs(self.a2_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n \n     def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor,\n                       weight_name: str, expert_id: int):\n@@ -149,20 +175,49 @@ class MixtralMoE(nn.Module):\n                        shard_size:2 * shard_size, :] = loaded_weight[shard, :]\n         if weight_name.endswith(\"w2.weight\"):\n             param_data[expert_id, :, :] = loaded_weight[:, shard]\n-        if \"act_scale\" in weight_name:\n-            param_data[:] = param_data[:].max(loaded_weight)\n+        if \"act_scale\" in weight_name or \"weight_scale\" in weight_name:\n+            param_data[expert_id] = loaded_weight\n \n     def process_weights_after_loading(self):\n-        if self.use_fp8:\n-            ws = torch.empty_like(self.ws.data, dtype=torch.float8_e4m3fn)\n-            w2s = torch.empty_like(self.w2s.data, dtype=torch.float8_e4m3fn)\n+        # Fp8 is the only case where we need to process after loading.\n+        if not self.use_fp8:\n+            return\n+\n+        # If checkpoint is fp16, quantize here.\n+        if not self.quant_config.is_checkpoint_fp8_serialized:\n+            w13_weight = torch.empty_like(self.w13_weight.data,\n+                                          dtype=torch.float8_e4m3fn)\n+            w2_weight = torch.empty_like(self.w2_weight.data,\n+                                         dtype=torch.float8_e4m3fn)\n             for expert in range(self.num_total_experts):\n-                ws[expert, :, :], self.ws_scale[expert] = ops.scaled_fp8_quant(\n-                    self.ws.data[expert, :, :])\n-                w2s[expert, :, :], self.w2s_scale[\n-                    expert] = ops.scaled_fp8_quant(self.w2s.data[expert, :, :])\n-            self.ws = nn.Parameter(ws, requires_grad=False)\n-            self.w2s = nn.Parameter(w2s, requires_grad=False)\n+                w13_weight[expert, :, :], self.w13_scale[\n+                    expert] = ops.scaled_fp8_quant(\n+                        self.w13_weight.data[expert, :, :])\n+                w2_weight[expert, :, :], self.w2_scale[\n+                    expert] = ops.scaled_fp8_quant(\n+                        self.w2_weight.data[expert, :, :])\n+            self.w13_weight = nn.Parameter(w13_weight, requires_grad=False)\n+            self.w2_weight = nn.Parameter(w2_weight, requires_grad=False)\n+\n+        # If checkpoint is fp8 + static, cleanup act_scales.\n+        #   Since state_dict has an act_scale per expert but our kernels\n+        #   are passed one act_scale shared across all experts.\n+        elif self.quant_config.activation_scheme == \"static\":\n+            if self.a13_scale is None or self.a2_scale is None:\n+                raise ValueError(\n+                    \"QuantConfig has static quantization, but found \"\n+                    \"activation scales are None.\")\n+\n+            if (not all_close_1d(self.a13_scale)\n+                    or not all_close_1d(self.a2_scale)):\n+                print_warning_once(\n+                    \"Found act_scales that are not equal for fp8 MoE layer. \"\n+                    \"Using the maximum across experts for each layer. \")\n+\n+            self.a13_scale = nn.Parameter(self.a13_scale.max(),\n+                                          requires_grad=False)\n+            self.a2_scale = nn.Parameter(self.a2_scale.max(),\n+                                         requires_grad=False)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         num_tokens, hidden_size = hidden_states.shape\n@@ -170,17 +225,17 @@ class MixtralMoE(nn.Module):\n         # router_logits: (num_tokens, n_experts)\n         router_logits, _ = self.gate(hidden_states)\n         final_hidden_states = fused_moe(hidden_states,\n-                                        self.ws,\n-                                        self.w2s,\n+                                        self.w13_weight,\n+                                        self.w2_weight,\n                                         router_logits,\n                                         self.top_k,\n                                         renormalize=True,\n                                         inplace=True,\n                                         use_fp8=self.use_fp8,\n-                                        w1_scale=self.ws_scale,\n-                                        w2_scale=self.w2s_scale,\n-                                        a1_scale=self.as_scale,\n-                                        a2_scale=self.a2s_scale)\n+                                        w1_scale=self.w13_scale,\n+                                        w2_scale=self.w2_scale,\n+                                        a1_scale=self.a13_scale,\n+                                        a2_scale=self.a2_scale)\n \n         if self.tp_size > 1:\n             final_hidden_states = tensor_model_parallel_all_reduce(\n@@ -222,7 +277,9 @@ class MixtralAttention(nn.Module):\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n \n-        if isinstance(quant_config, Fp8Config):\n+        if isinstance(\n+                quant_config,\n+                Fp8Config) and not quant_config.is_checkpoint_fp8_serialized:\n             print_warning_once(\n                 \"For Mixtral FP8 quantization, we currently do not quantize \"\n                 \"the attention layers until their FP8 performance is improved.\"\n@@ -461,16 +518,23 @@ class MixtralForCausalLM(nn.Module):\n         ]\n \n         expert_params_mapping = [\n+            # These are the weight scales for the experts\n+            # (param_name, weight_name, expert_id)\n+            (\"w13_scale\" if weight_name in [\"w1\", \"w3\"] else \"w2_scale\",\n+             f\"experts.{expert_id}.{weight_name}.weight_scale\", expert_id)\n+            for expert_id in range(self.config.num_local_experts)\n+            for weight_name in [\"w1\", \"w2\", \"w3\"]\n+        ] + [\n             # These are the weights for the experts\n             # (param_name, weight_name, expert_id)\n-            (\"ws\" if weight_name in [\"w1\", \"w3\"] else \"w2s\",\n+            (\"w13_weight\" if weight_name in [\"w1\", \"w3\"] else \"w2_weight\",\n              f\"experts.{expert_id}.{weight_name}.weight\", expert_id)\n             for expert_id in range(self.config.num_local_experts)\n             for weight_name in [\"w1\", \"w2\", \"w3\"]\n         ] + [\n             # These are the activation scales for the experts\n             # (param_name, weight_name, expert_id)\n-            (\"as_scale\" if weight_name in [\"w1\", \"w3\"] else \"a2s_scale\",\n+            (\"a13_scale\" if weight_name in [\"w1\", \"w3\"] else \"a2_scale\",\n              f\"experts.{expert_id}.{weight_name}.act_scale\", expert_id)\n             for expert_id in range(self.config.num_local_experts)\n             for weight_name in [\"w1\", \"w2\", \"w3\"]\n@@ -512,3 +576,8 @@ class MixtralForCausalLM(nn.Module):\n                     weight_loader = getattr(param, \"weight_loader\",\n                                             default_weight_loader)\n                     weight_loader(param, loaded_weight)\n+\n+\n+def all_close_1d(x: torch.Tensor) -> bool:\n+    assert len(x.shape) == 1\n+    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))",
      "change_type": "modified",
      "lines_added": 121,
      "lines_removed": 52
    }
  ],
  "affected_apis": [
    "MixtralMoE.forward",
    "MixtralMoE.process_weights_after_loading",
    "MixtralMoE.weight_loader",
    "MixtralForCausalLM.__init__"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 2
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_moe, test_mixtral)",
    "is_benchmark_actually_there": "",
    "sample_clues": "mixtral, mixtralforcausallm, mixtralmodel"
  }
}
```
