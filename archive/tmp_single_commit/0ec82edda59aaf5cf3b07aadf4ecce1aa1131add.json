{
      "commit_hash": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
      "subject": "[perf] Speed up align sum kernels (#21079)",
      "message": "[perf] Speed up align sum kernels (#21079)\n\nSigned-off-by: Himanshu Jaju <hj@mistral.ai>",
      "date": "2025-07-21T11:19:23-07:00",
      "files_changed": [
        "benchmarks/kernels/benchmark_moe_align_block_size.py",
        "csrc/moe/moe_align_sum_kernels.cu",
        "vllm/model_executor/layers/fused_moe/moe_align_block_size.py"
      ],
      "functions_changed": [],
      "stats": {
        "num_test_files": 0,
        "num_non_test_files": 3,
        "only_test_files": 0,
        "only_non_test_files": 1,
        "num_files": 3,
        "num_hunks": 13,
        "num_edited_lines": 85,
        "num_non_test_edited_lines": 85,
        "commit_year": 2025
      },
      "affected_paths": [
        "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/moe_align_block_size.py",
        "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/kernels/benchmark_moe_align_block_size.py"
      ],
      "apis": [
        "vllm.model_executor.layers.fused_moe.moe_align_block_size"
      ],
      "diff_text": "diff --git a/benchmarks/kernels/benchmark_moe_align_block_size.py b/benchmarks/kernels/benchmark_moe_align_block_size.py\nindex 5170ac09d..1af5a21ca 100644\n--- a/benchmarks/kernels/benchmark_moe_align_block_size.py\n+++ b/benchmarks/kernels/benchmark_moe_align_block_size.py\n@@ -33,15 +33,13 @@ def check_correctness(num_tokens, num_experts=256, block_size=256, topk=8):\n     sorted_ids_triton = torch.empty(\n         (max_num_tokens_padded,), dtype=torch.int32, device=\"cuda\"\n     )\n-    sorted_ids_triton.fill_(topk_ids.numel())  # fill with sentinel value\n-    expert_ids_triton = torch.zeros(\n+    expert_ids_triton = torch.empty(\n         (max_num_tokens_padded // block_size,), dtype=torch.int32, device=\"cuda\"\n     )\n     num_tokens_post_pad_triton = torch.empty((1,), dtype=torch.int32, device=\"cuda\")\n \n     sorted_ids_vllm = torch.empty_like(sorted_ids_triton)\n-    sorted_ids_vllm.fill_(topk_ids.numel())\n-    expert_ids_vllm = torch.zeros_like(expert_ids_triton)\n+    expert_ids_vllm = torch.empty_like(expert_ids_triton)\n     num_tokens_post_pad_vllm = torch.empty_like(num_tokens_post_pad_triton)\n \n     # 2. run implementations\n@@ -102,7 +100,6 @@ def benchmark(num_tokens, num_experts, topk, provider):\n \n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     sorted_ids = torch.empty((max_num_tokens_padded,), dtype=torch.int32, device=\"cuda\")\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = max_num_tokens_padded // block_size\n     expert_ids = torch.empty((max_num_m_blocks,), dtype=torch.int32, device=\"cuda\")\n     num_tokens_post_pad = torch.empty((1,), dtype=torch.int32, device=\"cuda\")\ndiff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu\nindex 462dbd1f8..8bbcf5a67 100644\n--- a/csrc/moe/moe_align_sum_kernels.cu\n+++ b/csrc/moe/moe_align_sum_kernels.cu\n@@ -1,6 +1,7 @@\n #include <torch/all.h>\n #include <ATen/cuda/CUDAContext.h>\n #include <c10/cuda/CUDAGuard.h>\n+#include <cub/cub.cuh>\n \n #include <ATen/ATen.h>\n #include <ATen/cuda/Atomic.cuh>\n@@ -19,9 +20,14 @@ __global__ void moe_align_block_size_kernel(\n     int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,\n     int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,\n     int32_t padded_num_experts, int32_t experts_per_warp, int32_t block_size,\n-    size_t numel, int32_t* __restrict__ cumsum) {\n+    size_t numel, int32_t* __restrict__ cumsum, int32_t max_num_tokens_padded) {\n   extern __shared__ int32_t shared_counts[];\n \n+  // Initialize sorted_token_ids with numel\n+  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {\n+    sorted_token_ids[it] = numel;\n+  }\n+\n   const int warp_id = threadIdx.x / WARP_SIZE;\n   const int my_expert_start = warp_id * experts_per_warp;\n \n@@ -45,18 +51,27 @@ __global__ void moe_align_block_size_kernel(\n \n   __syncthreads();\n \n-  if (threadIdx.x == 0) {\n-    cumsum[0] = 0;\n-    for (int i = 1; i <= num_experts; ++i) {\n-      int expert_count = 0;\n-      int warp_idx = (i - 1) / experts_per_warp;\n-      int expert_offset = (i - 1) % experts_per_warp;\n-      expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset];\n+  // Compute prefix sum over token counts per expert\n+  using BlockScan = cub::BlockScan<int32_t, 1024>;\n+  __shared__ typename BlockScan::TempStorage temp_storage;\n \n-      cumsum[i] =\n-          cumsum[i - 1] + CEILDIV(expert_count, block_size) * block_size;\n-    }\n-    *total_tokens_post_pad = cumsum[num_experts];\n+  int expert_count = 0;\n+  int expert_id = threadIdx.x;\n+  if (expert_id < num_experts) {\n+    int warp_idx = expert_id / experts_per_warp;\n+    int expert_offset = expert_id % experts_per_warp;\n+    expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset];\n+    expert_count = CEILDIV(expert_count, block_size) * block_size;\n+  }\n+\n+  int cumsum_val;\n+  BlockScan(temp_storage).ExclusiveSum(expert_count, cumsum_val);\n+  if (expert_id <= num_experts) {\n+    cumsum[expert_id] = cumsum_val;\n+  }\n+\n+  if (expert_id == num_experts) {\n+    *total_tokens_post_pad = cumsum_val;\n   }\n \n   __syncthreads();\n@@ -67,6 +82,13 @@ __global__ void moe_align_block_size_kernel(\n       expert_ids[i / block_size] = threadIdx.x;\n     }\n   }\n+\n+  // Fill remaining expert_ids with 0\n+  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;\n+  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);\n+  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {\n+    expert_ids[i] = 0;\n+  }\n }\n \n template <typename scalar_t>\n@@ -105,7 +127,12 @@ __global__ void moe_align_block_size_small_batch_expert_kernel(\n     const scalar_t* __restrict__ topk_ids,\n     int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,\n     int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,\n-    int32_t block_size, size_t numel) {\n+    int32_t block_size, size_t numel, int32_t max_num_tokens_padded) {\n+  // Initialize sorted_token_ids with numel\n+  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {\n+    sorted_token_ids[it] = numel;\n+  }\n+\n   const size_t tid = threadIdx.x;\n   const size_t stride = blockDim.x;\n \n@@ -153,6 +180,13 @@ __global__ void moe_align_block_size_small_batch_expert_kernel(\n     }\n   }\n \n+  // Fill remaining expert_ids with 0\n+  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;\n+  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);\n+  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {\n+    expert_ids[i] = 0;\n+  }\n+\n   for (size_t i = tid; i < numel; i += stride) {\n     int32_t expert_id = topk_ids[i];\n     int32_t rank_post_pad =\n@@ -179,13 +213,17 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n   int threads = 1024;\n   threads = ((threads + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE;\n \n+  // BlockScan uses 1024 threads and assigns one thread per expert.\n+  TORCH_CHECK(padded_num_experts < 1024,\n+              \"padded_num_experts must be less than 1024\");\n+\n   VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(\n       topk_ids.scalar_type(), \"moe_align_block_size_kernel\", [&] {\n         // calc needed amount of shared mem for `cumsum` tensors\n         auto options_int =\n             torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());\n         torch::Tensor cumsum_buffer =\n-            torch::zeros({num_experts + 1}, options_int);\n+            torch::empty({num_experts + 1}, options_int);\n         bool small_batch_expert_mode =\n             (topk_ids.numel() < 1024) && (num_experts <= 64);\n \n@@ -203,7 +241,7 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n               sorted_token_ids.data_ptr<int32_t>(),\n               experts_ids.data_ptr<int32_t>(),\n               num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,\n-              topk_ids.numel());\n+              topk_ids.numel(), sorted_token_ids.size(0));\n         } else {\n           auto align_kernel = vllm::moe::moe_align_block_size_kernel<scalar_t>;\n \n@@ -217,7 +255,8 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n               experts_ids.data_ptr<int32_t>(),\n               num_tokens_post_pad.data_ptr<int32_t>(), num_experts,\n               padded_num_experts, experts_per_warp, block_size,\n-              topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>());\n+              topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>(),\n+              sorted_token_ids.size(0));\n \n           const int block_threads = std::min(256, (int)threads);\n           const int num_blocks =\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 3aae183df..2c9ad509f 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -111,6 +111,8 @@ def moe_align_block_size_triton(\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n     tokens_per_thread = cdiv(numel, num_experts)\n+    sorted_token_ids.fill_(numel)\n+    expert_ids.zero_()\n \n     moe_align_block_size_stage1[grid](\n         topk_ids,\n@@ -205,11 +207,8 @@ def moe_align_block_size(\n     sorted_ids = torch.empty((max_num_tokens_padded, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n-    # Expert ids must be zeroed out to prevent index out of bounds error while\n-    # mapping global expert ids to local expert ids in expert parallelism.\n-    expert_ids = torch.zeros((max_num_m_blocks, ),\n+    expert_ids = torch.empty((max_num_m_blocks, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n     num_tokens_post_pad = torch.empty((1),",
      "llm_reason": "The commit modifies non-test source files (C++ CUDA kernels and Python modules) and alters algorithmic behavior in the align sum kernels to speed them up. The changes include replacing filled arrays with more efficient initializations, adding a cub::BlockScan based prefix sum for computing cumsum, and additional logic to fill arrays without unnecessary initialization overhead. These modifications are carefully tuned to improve runtime performance on CPU accessible kernels (even though they run on CUDA, they donâ€™t involve GPU-exclusive workloads for training, and performance is testable without a GPU context running general algorithms). The changes are non-trivial modifications of internal performance-critical kernels rather than simple refactorings, bug fixes, or feature additions. Therefore, based on these performance optimization improvements, the commit satisfies the conditions for being performance related.",
      "llm_api_reason": "The commit refactors both the benchmark and CUDA kernel code used in the MoE alignment operation and updates the Python wrapper in vllm/model_executor/layers/fused_moe/moe_align_block_size.py. In doing so, it changes the initialization and memory filling routines used by the Python function that performs MoE block size alignment. This means that the Python API function moe_align_block_size is affected, as it now relies on the revised internal behavior for aligning tokens and mapping expert IDs.",
      "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm"
}