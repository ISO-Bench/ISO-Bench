{
  "instance": {
    "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "commit_subject": "Optimize tensor parallel execution speed (#17)",
    "repo": "vllm",
    "pr_url": "https://github.com/vllm-project/vllm/pull/17",
    "files_changed": [
      "benchmark/benchmark_latency.py",
      "cacheflow/parallel_utils/tensor_parallel/__init__.py",
      "cacheflow/parallel_utils/tensor_parallel/layers.py"
    ]
  },
  "result": {
    "status": "baseline_failed",
    "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "baseline_ms": null,
    "patched_ms": null,
    "speedup": null,
    "improvement": false,
    "baseline_output": null,
    "patched_output": null,
    "error_message": "Baseline test failed to produce JSON output",
    "stdout": "",
    "stderr": "[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/tmp/omniperf_test_j531bd2q.py\", line 559, in <module>\n[rank0]:     run_test(args.eqcheck, args.reference, args.prefix)\n[rank0]:   File \"/tmp/omniperf_test_j531bd2q.py\", line 495, in run_test\n[rank0]:     result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/tmp/omniperf_test_j531bd2q.py\", line 450, in time_gpu\n[rank0]:     _ = func()\n[rank0]:         ^^^^^^\n[rank0]:   File \"/tmp/omniperf_test_j531bd2q.py\", line 495, in <lambda>\n[rank0]:     result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n[rank0]:                                             ^^^^^^^^^^^^^^^^\n[rank0]:   File \"/tmp/omniperf_test_j531bd2q.py\", line 344, in experiment\n[rank0]:     column_layer = ColumnParallelLinear(\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/tensor_parallel/layers.py\", line 465, in __init__\n[rank0]:     world_size = get_tensor_model_parallel_world_size()\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/parallel_state.py\", line 276, in get_tensor_model_parallel_world_size\n[rank0]:     return torch.distributed.get_world_size(group=get_tensor_model_parallel_group())\n[rank0]:                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/parallel_state.py\", line 226, in get_tensor_model_parallel_group\n[rank0]:     assert _TENSOR_MODEL_PARALLEL_GROUP is not None, \\\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AssertionError: intra_layer_model parallel group is not initialized\n[rank0]:[W1224 19:18:04.498907849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
    "duration_s": 1.9552981853485107,
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-22_21-40-38/vllm_core-0071/model_patch.diff",
    "patch_stats": null
  }
}