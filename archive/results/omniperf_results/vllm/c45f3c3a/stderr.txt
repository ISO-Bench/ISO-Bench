[rank0]: Traceback (most recent call last):
[rank0]:   File "/tmp/omniperf_test_j531bd2q.py", line 559, in <module>
[rank0]:     run_test(args.eqcheck, args.reference, args.prefix)
[rank0]:   File "/tmp/omniperf_test_j531bd2q.py", line 495, in run_test
[rank0]:     result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/tmp/omniperf_test_j531bd2q.py", line 450, in time_gpu
[rank0]:     _ = func()
[rank0]:         ^^^^^^
[rank0]:   File "/tmp/omniperf_test_j531bd2q.py", line 495, in <lambda>
[rank0]:     result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)
[rank0]:                                             ^^^^^^^^^^^^^^^^
[rank0]:   File "/tmp/omniperf_test_j531bd2q.py", line 344, in experiment
[rank0]:     column_layer = ColumnParallelLinear(
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/tensor_parallel/layers.py", line 465, in __init__
[rank0]:     world_size = get_tensor_model_parallel_world_size()
[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/parallel_state.py", line 276, in get_tensor_model_parallel_world_size
[rank0]:     return torch.distributed.get_world_size(group=get_tensor_model_parallel_group())
[rank0]:                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/tmp/omniperf_worktree_cp4xbxam/worktree/cacheflow/parallel_utils/parallel_state.py", line 226, in get_tensor_model_parallel_group
[rank0]:     assert _TENSOR_MODEL_PARALLEL_GROUP is not None, \
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: AssertionError: intra_layer_model parallel group is not initialized
[rank0]:[W1224 19:18:04.498907849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
