{
  "instance": {
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)",
    "repo": "vllm",
    "pr_url": "https://github.com/vllm-project/vllm/pull/5602",
    "files_changed": [
      "benchmarks/benchmark_latency.py",
      "tests/conftest.py",
      "tests/core/block/test_block_table.py",
      "tests/core/block/test_cpu_gpu_block_allocator.py",
      "tests/core/block/test_naive_block.py",
      "tests/core/block/test_prefix_caching_block.py",
      "tests/spec_decode/test_batch_expansion.py",
      "vllm/core/block/block_table.py",
      "vllm/core/block/common.py",
      "vllm/core/block/cpu_gpu_block_allocator.py",
      "vllm/core/block/interfaces.py",
      "vllm/core/block/naive_block.py",
      "vllm/core/block/prefix_caching_block.py",
      "vllm/core/block_manager_v2.py",
      "vllm/engine/llm_engine.py",
      "vllm/entrypoints/openai/serving_completion.py",
      "vllm/model_executor/sampling_metadata.py",
      "vllm/outputs.py",
      "vllm/sequence.py"
    ]
  },
  "result": {
    "status": "baseline_failed",
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "baseline_ms": null,
    "patched_ms": null,
    "speedup": null,
    "improvement": false,
    "baseline_output": null,
    "patched_output": null,
    "error_message": "Baseline test failed to produce JSON output",
    "stdout": "WARNING 12-24 19:16:16 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
    "stderr": "Traceback (most recent call last):\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 348, in experiment\n    block = allocator.allocate_mutable_block(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'CpuGpuBlockAllocator' object has no attribute 'allocate_mutable_block'. Did you mean: 'allocate_mutable'?\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 538, in <module>\n    run_test(args.eqcheck, args.reference, args.prefix)\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 492, in run_test\n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 436, in time_gpu\n    _ = func()\n        ^^^^^^\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 492, in <lambda>\n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n                                            ^^^^^^^^^^^^^^^^\n  File \"/tmp/omniperf_test_v4_rt6_n.py\", line 356, in experiment\n    block = allocator.allocate_mutable(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/omniperf_worktree_7tssguy6/worktree/vllm/core/block/cpu_gpu_block_allocator.py\", line 131, in allocate_mutable\n    return self._allocators[device].allocate_mutable(prev_block)\n           ~~~~~~~~~~~~~~~~^^^^^^^^\nKeyError: device(type='cuda')\n",
    "duration_s": 10.106766700744629,
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0017/model_patch.diff",
    "patch_stats": null
  }
}