{
  "instance": {
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)",
    "repo": "vllm",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21116",
    "files_changed": [
      "csrc/layernorm_kernels.cu",
      "csrc/layernorm_quant_kernels.cu",
      "csrc/quantization/fp8/common.cu",
      "tests/kernels/core/test_layernorm.py",
      "vllm/model_executor/layers/linear.py",
      "vllm/model_executor/layers/quantization/fp8.py",
      "vllm/model_executor/models/deepseek_v2.py"
    ]
  },
  "result": {
    "status": "success",
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "baseline_ms": 0.352641921043396,
    "patched_ms": 0.35342592000961304,
    "speedup": 0.9977817162753775,
    "improvement": false,
    "baseline_output": {
      "impl_tag": "child",
      "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
      "device": "cuda",
      "dtype": "torch.float16",
      "iters": 50,
      "warmup": 5,
      "avg_ms": 0.352641921043396,
      "p50_ms": 0.3495680093765259,
      "p95_ms": 0.36111998558044434,
      "eq_level": "numeric",
      "opt_path_hit": true
    },
    "patched_output": {
      "impl_tag": "child",
      "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
      "device": "cuda",
      "dtype": "torch.float16",
      "iters": 50,
      "warmup": 5,
      "avg_ms": 0.35342592000961304,
      "p50_ms": 0.35040000081062317,
      "p95_ms": 0.35731199383735657,
      "eq_level": "numeric",
      "opt_path_hit": true
    },
    "error_message": null,
    "stdout": "INFO 12-24 20:11:44 [__init__.py:235] Automatically detected platform cuda.\nWARNING 12-24 20:11:45 [config.py:4940] Current vLLM config is not set.\nWARNING 12-24 20:11:45 [config.py:4940] Current vLLM config is not set.\nWARNING 12-24 20:11:45 [config.py:4940] Current vLLM config is not set.\n{\"impl_tag\": \"child\", \"commit_hash\": \"4fb56914c5f27ef062e10d44a0f79c6ceab382f9\", \"device\": \"cuda\", \"dtype\": \"torch.float16\", \"iters\": 50, \"warmup\": 5, \"avg_ms\": 0.352641921043396, \"p50_ms\": 0.3495680093765259, \"p95_ms\": 0.36111998558044434, \"eq_level\": \"numeric\", \"opt_path_hit\": true}\n\nINFO 12-24 20:11:50 [__init__.py:235] Automatically detected platform cuda.\nWARNING 12-24 20:11:50 [config.py:4940] Current vLLM config is not set.\nWARNING 12-24 20:11:50 [config.py:4940] Current vLLM config is not set.\nWARNING 12-24 20:11:50 [config.py:4940] Current vLLM config is not set.\n{\"impl_tag\": \"child\", \"commit_hash\": \"4fb56914c5f27ef062e10d44a0f79c6ceab382f9\", \"device\": \"cuda\", \"dtype\": \"torch.float16\", \"iters\": 50, \"warmup\": 5, \"avg_ms\": 0.35342592000961304, \"p50_ms\": 0.35040000081062317, \"p95_ms\": 0.35731199383735657, \"eq_level\": \"numeric\", \"opt_path_hit\": true}\n",
    "stderr": "\n",
    "duration_s": 31.41660451889038,
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0023/model_patch.diff",
    "patch_stats": null,
    "used_wheel": true,
    "used_compilation": false
  }
}