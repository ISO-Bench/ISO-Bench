{
  "timestamp": "20260101_102817",
  "total_commits": 1,
  "completed": 1,
  "success_count": 1,
  "error_count": 0,
  "results": [
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 4276.31,
        "ttft_median": 4098.8,
        "ttft_p99": 7095.17,
        "tpot_mean": 59.31,
        "tpot_median": 60.11,
        "tpot_p99": 77.02,
        "itl_mean": 59.31,
        "itl_median": 29.9,
        "itl_p99": 392.51
      },
      "human_metrics": {
        "ttft_mean": 2072.84,
        "ttft_median": 2010.32,
        "ttft_p99": 3623.47,
        "tpot_mean": 45.41,
        "tpot_median": 45.54,
        "tpot_p99": 55.91,
        "itl_mean": 45.41,
        "itl_median": 28.54,
        "itl_p99": 189.42
      },
      "agent_metrics": null,
      "human_improvement": {
        "ttft_mean": 51.52736822166774,
        "ttft_median": 50.9534497901825,
        "ttft_p99": 48.93046960115121,
        "tpot_mean": 23.436182768504477,
        "itl_mean": 23.436182768504477
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "server_logs": null,
      "duration_s": 5376.62214756012,
      "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "c_cuda",
      "baseline_version": "0.10.1.dev295+g88faa466d",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-01 11:12:41 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2acd43a58b80>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 11:12:48 [datasets.py:355] Sampling input_len from [512, 512] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  12.10     \nTotal input tokens:                      153250    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              24.79     \nOutput token throughput (tok/s):         3172.66   \nTotal Token throughput (tok/s):          15834.39  \n---------------Time to First Token----------------\nMean TTFT (ms):                          4276.31   \nMedian TTFT (ms):                        4098.80   \nP99 TTFT (ms):                           7095.17   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          59.31     \nMedian TPOT (ms):                        60.11     \nP99 TPOT (ms):                           77.02     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           59.31     \nMedian ITL (ms):                         29.90     \nP99 ITL (ms):                            392.51    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:11<57:57, 11.63s/it]\n  8",
      "agent_error": "Incremental build failed: Cache build timed out",
      "human_version": "0.10.1.dev296+geefbf4a68",
      "human_install_method": "wheel",
      "human_raw": "INFO 01-01 11:57:55 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2abd15404ae0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 11:58:00 [datasets.py:355] Sampling input_len from [512, 512] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  8.11      \nTotal input tokens:                      153250    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              37.01     \nOutput token throughput (tok/s):         4737.22   \nTotal Token throughput (tok/s):          23642.91  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2072.84   \nMedian TTFT (ms):                        2010.32   \nP99 TTFT (ms):                           3623.47   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          45.41     \nMedian TPOT (ms):                        45.54     \nP99 TPOT (ms):                           55.91     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           45.41     \nMedian ITL (ms):                         28.54     \nP99 ITL (ms):                            189.42    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:07<38:20,  7.69s/it]\n 16",
      "commit": "eefbf4a6",
      "full_commit": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
      "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9",
      "model": "Qwen/Qwen3-30B-A3B-FP8",
      "subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Ker",
      "has_agent_patch": true
    }
  ]
}