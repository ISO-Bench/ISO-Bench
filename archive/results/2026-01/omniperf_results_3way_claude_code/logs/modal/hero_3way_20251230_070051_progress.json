{
  "timestamp": "20251230_070051",
  "total_commits": 51,
  "completed": 6,
  "success_count": 0,
  "error_count": 6,
  "results": [
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 143.7060878276825,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:02:35 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev57+geb5741ad",
      "baseline_install_method": "wheel",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench serve\n\nFor help with the new command, run:\n    vllm bench serve --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench serve --help\n\n\n",
      "commit": "fc542144",
      "full_commit": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
      "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Feature] Fix guided decoding blocking bitmask mem",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 21.15184998512268,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:04:00 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797",
      "baseline_install_method": "wheel",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "6dd94dbe",
      "full_commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
      "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[perf] fix perf regression from #12253 (#12380)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 72.91526007652283,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:04:20 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev337+g7206ce4c",
      "baseline_install_method": "wheel",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench serve\n\nFor help with the new command, run:\n    vllm bench serve --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench serve --help\n\n\n",
      "commit": "aea94362",
      "full_commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
      "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Frontend][V1] Online serving performance improvem",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 433.87991428375244,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
      "baseline_version": "0.6.6.post2.dev145+ga732900e",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "310aca88",
      "full_commit": "310aca88c984983189a57f1b72e3b1dde89fb92f",
      "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834",
      "model": "meta-llama/Meta-Llama-3-70B",
      "subject": "[perf]fix current stream (#11870)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 86.33058786392212,
      "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
      "baseline_version": "0.6.6.dev17+g8936316d",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "f26c4aee",
      "full_commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
      "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Misc] Optimize ray worker initialization time (#1",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 62.49201941490173,
      "perf_command": "python benchmarks/benchmark_latency.py",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev182+g8c1e77fb",
      "baseline_install_method": "wheel",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "98f47f2a",
      "full_commit": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
      "parent_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
      "model": "unknown",
      "subject": "[V1] Optimize the CPU overheads in FlashAttention ",
      "has_agent_patch": true
    }
  ]
}