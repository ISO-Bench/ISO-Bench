{
  "timestamp": "20251231_061054",
  "total_commits": 37,
  "completed": 2,
  "success_count": 0,
  "error_count": 2,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "BASELINE server failed to start. Logs: easing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 12-31 06:13:36 model_runner.py:1518] Graph capturing finished in 15 secs, took 0.32 GiB\nINFO 12-31 06:13:37 launcher.py:19] Available routes are:\nINFO 12-31 06:13:37 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\nINFO 12-31 06:13:37 launcher.py:27] Route: /docs, Methods: GET, HEAD\nINFO 12-31 06:13:37 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 12-31 06:13:37 launcher.py:27] Route: /redoc, Methods: GET, HEAD\nINFO 12-31 06:13:37 launcher.py:27] Route: /health, Methods: GET\nINFO 12-31 06:13:37 launcher.py:27] Route: /tokenize, Methods: POST\nINFO 12-31 06:13:37 launcher.py:27] Route: /detokenize, Methods: POST\nINFO 12-31 06:13:37 launcher.py:27] Route: /v1/models, Methods: GET\nINFO 12-31 06:13:37 launcher.py:27] Route: /version, Methods: GET\nINFO 12-31 06:13:37 launcher.py:27] Route: /v1/chat/completions, Methods: POST\nINFO 12-31 06:13:37 launcher.py:27] Route: /v1/completions, Methods: POST\nINFO 12-31 06:13:37 launcher.py:27] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [102]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 29001): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n[rank0]:[W1231 06:13:37.441585857 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "server_logs": null,
      "duration_s": 692.3944916725159,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 622.7481055259705,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: args) as async_engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\n    if (model_is_embedding(args.model, args.trust_remote_code,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 71, in model_is_embedding\n    return ModelConfig(model=model_name,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 172, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/transformers_utils/config.py\", line 66, in get_config\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1250, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 649, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 708, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n    raise OSError(\nOSError: meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    }
  ]
}