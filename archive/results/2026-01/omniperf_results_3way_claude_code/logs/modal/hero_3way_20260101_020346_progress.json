{
  "timestamp": "20260101_020346",
  "total_commits": 31,
  "completed": 4,
  "success_count": 1,
  "error_count": 3,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:8",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Could not find vLLM installation",
      "duration_s": 5.213940382003784,
      "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8",
      "install_method": "docker_fallback",
      "baseline_raw": "",
      "human_raw": "",
      "agent_raw": "",
      "commit": "21d93c14",
      "full_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
      "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "subject": "Optimize Mixtral with expert parallelism (#2090)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 7a7929abe8e2",
      "server_logs": null,
      "duration_s": 36.38006925582886,
      "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "c45f3c3a",
      "full_commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
      "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
      "model": "facebook/opt-13b",
      "subject": "Optimize tensor parallel execution speed (#17)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for b56b6ca0d650",
      "server_logs": null,
      "duration_s": 23.023714303970337,
      "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "d4bc1a4d",
      "full_commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
      "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
      "model": "facebook/opt-125m",
      "subject": "Add unoptimized OPT Attention",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 34617.43,
        "ttft_median": 34876.07,
        "ttft_p99": 43173.79,
        "tpot_mean": 76.62,
        "tpot_median": 74.57,
        "tpot_p99": 297.59,
        "itl_mean": 76.62,
        "itl_median": 56.11,
        "itl_p99": 116.08
      },
      "human_metrics": {
        "ttft_mean": 8301.44,
        "ttft_median": 7594.62,
        "ttft_p99": 15538.75,
        "tpot_mean": 65.65,
        "tpot_median": 71.27,
        "tpot_p99": 105.97,
        "itl_mean": 65.65,
        "itl_median": 55.75,
        "itl_p99": 112.47
      },
      "agent_metrics": {
        "ttft_mean": 8281.1,
        "ttft_median": 7609.69,
        "ttft_p99": 15565.47,
        "tpot_mean": 66.75,
        "tpot_median": 72.29,
        "tpot_p99": 105.82,
        "itl_mean": 66.75,
        "itl_median": 59.27,
        "itl_p99": 126.55
      },
      "human_improvement": {
        "ttft_mean": 76.019479204551,
        "ttft_median": 78.22397993810655,
        "ttft_p99": 64.00883498993255,
        "tpot_mean": 14.317410597755153,
        "itl_mean": 14.317410597755153
      },
      "agent_improvement": {
        "ttft_mean": 76.07823573269305,
        "ttft_median": 78.18076979430309,
        "ttft_p99": 63.94694558897886,
        "tpot_mean": 12.881754111198127,
        "itl_mean": 12.881754111198127
      },
      "agent_vs_human": {
        "ttft_mean": 0.24501773186338932,
        "ttft_median": -0.19842994119521068,
        "ttft_p99": -0.17195720376477738,
        "tpot_mean": -1.6755521706016667,
        "itl_mean": -1.6755521706016667
      },
      "error": null,
      "server_logs": null,
      "duration_s": 565.9735832214355,
      "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.10.1.dev312+g25373b6c6",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-01 02:09:02 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ac90b638f40>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 02:09:08 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  46.62     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              6.43      \nOutput token throughput (tok/s):         823.61    \nTotal Token throughput (tok/s):          4108.60   \n---------------Time to First Token----------------\nMean TTFT (ms):                          34617.43  \nMedian TTFT (ms):                        34876.07  \nP99 TTFT (ms):                           43173.79  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          76.62     \nMedian TPOT (ms):                        74.57     \nP99 TPOT (ms):                           297.59    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           76.62     \nMedian ITL (ms):                         56.11     \nP99 ITL (ms):                            116.08    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:41<3:28:22, 41.81s/",
      "human_version": "0.10.1.dev313+gb690e3482",
      "human_install_method": "wheel",
      "human_raw": "INFO 01-01 02:11:51 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b5639ea0f40>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 02:11:57 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  18.83     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              15.93     \nOutput token throughput (tok/s):         2038.91   \nTotal Token throughput (tok/s):          10171.21  \n---------------Time to First Token----------------\nMean TTFT (ms):                          8301.44   \nMedian TTFT (ms):                        7594.62   \nP99 TTFT (ms):                           15538.75  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          65.65     \nMedian TPOT (ms):                        71.27     \nP99 TPOT (ms):                           105.97    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           65.65     \nMedian ITL (ms):                         55.75     \nP99 ITL (ms):                            112.47    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:10:40, 14.18s/",
      "agent_raw": "INFO 01-01 02:14:10 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae9677b4f40>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 02:14:16 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  19.02     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              15.77     \nOutput token throughput (tok/s):         2018.90   \nTotal Token throughput (tok/s):          10071.37  \n---------------Time to First Token----------------\nMean TTFT (ms):                          8281.10   \nMedian TTFT (ms):                        7609.69   \nP99 TTFT (ms):                           15565.47  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          66.75     \nMedian TPOT (ms):                        72.29     \nP99 TPOT (ms):                           105.82    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           66.75     \nMedian ITL (ms):                         59.27     \nP99 ITL (ms):                            126.55    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:10:46, 14.20s/",
      "commit": "b690e348",
      "full_commit": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
      "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157",
      "model": "ibm-ai-platform/Bamba-9B-v2",
      "subject": "[Model] Mamba2 preallocate SSM output tensor to av",
      "has_agent_patch": true
    }
  ]
}