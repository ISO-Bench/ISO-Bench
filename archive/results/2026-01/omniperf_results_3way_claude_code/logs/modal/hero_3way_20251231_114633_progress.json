{
  "timestamp": "20251231_114633",
  "total_commits": 41,
  "completed": 3,
  "success_count": 1,
  "error_count": 2,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 137.2130036354065,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=29001, endpoint='/v1/completions', dataset='ShareGPT_V3_unfiltered_cleaned_split.json', dataset_name='random', dataset_path=None, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=512, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/tmp/vllm-checkout/benchmarks/benchmark_serving.py:826: UserWarning: The '--dataset' argument will be deprecated in the next release. Please use '--dataset-name' and '--dataset-path' in the future runs.\n  main(args)\nTraceback (most recent call last):\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 826, in <module>\n    main(args)\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 518, in main\n    input_requests = sample_sharegpt_requests(\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 91, in sample_sharegpt_requests\n    with open(dataset_path) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Meta-Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "throughput": 50.51
      },
      "human_metrics": {
        "throughput": 50.11
      },
      "agent_metrics": {},
      "human_improvement": {
        "throughput": -0.79192239160562
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "duration_s": 243.800874710083,
      "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
      "install_method": "docker_fallback",
      "baseline_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 12-31 03:50:06 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 12-31 03:50:08 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 12-31 03:50:09 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 12-31 03:50:09 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 12-31 03:50:31 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 12-31 03:50:36 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 12-31 03:50:38 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 12-31 03:50:38 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 12-31 03:50:59 model_runner.py:1331] Graph capturing finished in 21 secs.\nThroughpu",
      "human_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 12-31 03:51:43 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 12-31 03:51:45 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 12-31 03:51:46 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 12-31 03:51:46 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 12-31 03:51:48 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 12-31 03:51:53 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 12-31 03:51:54 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 12-31 03:51:54 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 12-31 03:52:17 model_runner.py:1331] Graph capturing finished in 23 secs.\nThroughpu",
      "agent_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 12-31 03:53:04 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 12-31 03:53:06 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 12-31 03:53:06 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 12-31 03:53:07 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 12-31 03:53:08 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 12-31 03:53:14 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 12-31 03:53:16 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 12-31 03:53:16 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: U",
      "agent_error": "Agent benchmark produced no metrics",
      "commit": "ce6bf3a2",
      "full_commit": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
      "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
      "model": "google/gemma-2b",
      "subject": "[torch.compile] avoid Dynamo guard evaluation over",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 1922.85027551651,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
      "install_method": "docker_fallback",
      "baseline_raw": "Error: Command 'python /tmp/vllm-checkout/benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --host 127.0.0.1 --port 29001 --dataset-name random --random-input-len 512 --random-output-len 128 --num-prompts 100' timed out after 1800 seconds",
      "human_raw": "",
      "agent_raw": "",
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    }
  ]
}