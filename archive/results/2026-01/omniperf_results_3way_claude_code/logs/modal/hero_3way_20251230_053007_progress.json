{
  "timestamp": "20251230_053007",
  "total_commits": 5,
  "completed": 5,
  "success_count": 2,
  "error_count": 3,
  "results": [
    {
      "commit": "660470e5",
      "status": "no_baseline_wheel",
      "error": "Baseline wheel not available: 8d59dbb0"
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 2194.88,
        "ttft_median": 2134.71,
        "ttft_p99": 3955.59,
        "tpot_mean": 83.26,
        "tpot_median": 34.96,
        "tpot_p99": 202.51,
        "itl_mean": 29.83,
        "itl_median": 15.53,
        "itl_p99": 201.71
      },
      "human_metrics": {
        "ttft_mean": 2166.98,
        "ttft_median": 2284.63,
        "ttft_p99": 3906.37,
        "tpot_mean": 84.91,
        "tpot_median": 34.93,
        "tpot_p99": 199.6,
        "itl_mean": 29.78,
        "itl_median": 15.57,
        "itl_p99": 198.1
      },
      "agent_metrics": {
        "ttft_mean": 2194.78,
        "ttft_median": 2303.24,
        "ttft_p99": 3891.2,
        "tpot_mean": 84.08,
        "tpot_median": 35.13,
        "tpot_p99": 203.75,
        "itl_mean": 30.34,
        "itl_median": 15.81,
        "itl_p99": 199.64
      },
      "human_improvement": {
        "ttft_mean": 1.2711401078874511,
        "ttft_median": -7.022967990968332,
        "ttft_p99": 1.2443150073693243,
        "tpot_mean": -1.9817439346624925,
        "itl_mean": 0.16761649346294724
      },
      "agent_improvement": {
        "ttft_mean": 0.004556057734359466,
        "ttft_median": -7.894749169676431,
        "ttft_p99": 1.627822903789329,
        "tpot_mean": -0.9848666826807508,
        "itl_mean": -1.7096882333221641
      },
      "agent_vs_human": {
        "ttft_mean": -1.2828913972440994,
        "ttft_median": -0.8145739135002023,
        "ttft_p99": 0.38834007019304556,
        "tpot_mean": 0.9775055941585188,
        "itl_mean": -1.8804566823371345
      },
      "error": null,
      "server_logs": null,
      "duration_s": 612.2444472312927,
      "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.9.2rc2.dev356+g7d9457713",
      "baseline_raw": "INFO 12-30 05:34:28 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b7e35086ac0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:34:35 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.96      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21507     \nRequest throughput (req/s):              50.36     \nOutput token throughput (tok/s):         3610.21   \nTotal Token throughput (tok/s):          29333.04  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2194.88   \nMedian TTFT (ms):                        2134.71   \nP99 TTFT (ms):                           3955.59   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          83.26     \nMedian TPOT (ms):                        34.96     \nP99 TPOT (ms):                           202.51    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.83     \nMedian ITL (ms):                         15.53     \nP99 ITL (ms):                            201.71    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
      "human_version": "0.9.2rc2.dev357+g6d0734c56",
      "human_raw": "INFO 12-30 05:37:46 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b6271b863e0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:37:52 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.93      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21411     \nRequest throughput (req/s):              50.55     \nOutput token throughput (tok/s):         3607.82   \nTotal Token throughput (tok/s):          29428.88  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2166.98   \nMedian TTFT (ms):                        2284.63   \nP99 TTFT (ms):                           3906.37   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          84.91     \nMedian TPOT (ms):                        34.93     \nP99 TPOT (ms):                           199.60    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.78     \nMedian ITL (ms):                         15.57     \nP99 ITL (ms):                            198.10    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
      "agent_raw": "INFO 12-30 05:40:06 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b4cac46e340>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:40:12 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  6.00      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21631     \nRequest throughput (req/s):              50.01     \nOutput token throughput (tok/s):         3605.53   \nTotal Token throughput (tok/s):          29147.81  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2194.78   \nMedian TTFT (ms):                        2303.24   \nP99 TTFT (ms):                           3891.20   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          84.08     \nMedian TPOT (ms):                        35.13     \nP99 TPOT (ms):                           203.75    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           30.34     \nMedian ITL (ms):                         15.81     \nP99 ITL (ms):                            199.64    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
      "commit": "6d0734c5",
      "full_commit": "6d0734c562e759fdb7076d762222b3881e62ab1f",
      "parent_commit": "7d94577138e3d4c7bcfd781337ee1e5a2befa685",
      "model": "mistralai/Mistral-7B-Instruct-v0.3",
      "subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 b",
      "has_agent_patch": true
    },
    {
      "commit": "2deb029d",
      "status": "no_baseline_wheel",
      "error": "Baseline wheel not available: 029c71de"
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 35.75,
        "ttft_median": 32.32,
        "ttft_p99": 66.42,
        "tpot_mean": 0.0,
        "tpot_median": 0.0,
        "tpot_p99": 0.0,
        "itl_mean": 0.0,
        "itl_median": 0.0,
        "itl_p99": 0.0
      },
      "human_metrics": {
        "ttft_mean": 33.52,
        "ttft_median": 29.98,
        "ttft_p99": 64.89,
        "tpot_mean": 0.0,
        "tpot_median": 0.0,
        "tpot_p99": 0.0,
        "itl_mean": 0.0,
        "itl_median": 0.0,
        "itl_p99": 0.0
      },
      "agent_metrics": {
        "ttft_mean": 30.78,
        "ttft_median": 25.52,
        "ttft_p99": 63.29,
        "tpot_mean": 0.0,
        "tpot_median": 0.0,
        "tpot_p99": 0.0,
        "itl_mean": 0.0,
        "itl_median": 0.0,
        "itl_p99": 0.0
      },
      "human_improvement": {
        "ttft_mean": 6.237762237762229,
        "ttft_median": 7.240099009900989,
        "ttft_p99": 2.303523035230354
      },
      "agent_improvement": {
        "ttft_mean": 13.902097902097898,
        "ttft_median": 21.039603960396043,
        "ttft_p99": 4.712436013249025
      },
      "agent_vs_human": {
        "ttft_mean": 8.174224343675423,
        "ttft_median": 14.876584389593065,
        "ttft_p99": 2.4657112035752835
      },
      "error": null,
      "server_logs": null,
      "duration_s": 329.321843624115,
      "perf_command": "vllm bench serve --dataset-name random --model facebook/opt-125m --served-model-name facebook/opt-125m --random-input-len 700 --random-output-len 1 --endpoint /v1/completions --ignore-eos --host localhost --port 8000 --request-rate 200 --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.10.0rc2.dev36+gbc8a8ce5e",
      "baseline_raw": "INFO 12-30 05:42:11 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b2e6eb4e8e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:42:17 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              192.74    \nOutput token throughput (tok/s):         192.74    \nTotal Token throughput (tok/s):          134917.17 \n---------------Time to First Token----------------\nMean TTFT (ms):                          35.75     \nMedian TTFT (ms):                        32.32     \nP99 TTFT (ms):                           66.42     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 10%|\u2588         | 10/100 [00:00<00:00, 96.06it",
      "human_version": "0.10.0rc2.dev37+ga32237665",
      "human_raw": "INFO 12-30 05:44:06 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae159a728e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:44:12 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              193.28    \nOutput token throughput (tok/s):         193.28    \nTotal Token throughput (tok/s):          135295.36 \n---------------Time to First Token----------------\nMean TTFT (ms):                          33.52     \nMedian TTFT (ms):                        29.98     \nP99 TTFT (ms):                           64.89     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 11%|\u2588         | 11/100 [00:00<00:00, 103.16i",
      "agent_raw": "INFO 12-30 05:45:43 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2bab79e3e8e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:45:49 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              193.00    \nOutput token throughput (tok/s):         193.00    \nTotal Token throughput (tok/s):          135099.76 \n---------------Time to First Token----------------\nMean TTFT (ms):                          30.78     \nMedian TTFT (ms):                        25.52     \nP99 TTFT (ms):                           63.29     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 10%|\u2588         | 10/100 [00:00<00:00, 98.53it",
      "commit": "a3223766",
      "full_commit": "a32237665df876fcb51196dc209e8aff9fd89d29",
      "parent_commit": "bc8a8ce5ec374dd18e86f59be7cb0057a4b21992",
      "model": "facebook/opt-125m",
      "subject": "[Core] Optimize update checks in LogitsProcessor (",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 127.44298267364502,
      "perf_command": "python benchmark_guided.py --model meta-llama/Llama-3.1-8B-Instruct --dataset xgrammar_bench --async-engine --output-len 512 --num-prompts 20 --enable-chunked-prefill --guided-decoding-ratio 1",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev218+g3257d449",
      "baseline_raw": "\npython: can't open file '/opt/vllm-benchmarks/benchmark_guided.py': [Errno 2] No such file or directory\n",
      "commit": "9323a315",
      "full_commit": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
      "parent_commit": "3257d449fa0fd3e05aa20cc8c5fff79ad101984f",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core][Performance] Add XGrammar support for guide",
      "has_agent_patch": true
    }
  ]
}