{
  "timestamp": "20251231_092911",
  "total_commits": 37,
  "completed": 7,
  "success_count": 0,
  "error_count": 7,
  "results": [
    {
      "status": "version_bug",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "vLLM 0.6.3.post2.dev398+g4a18fd14 has known port binding bug (issue #8791) - serving benchmarks not supported",
      "server_logs": null,
      "duration_s": 80.33944201469421,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 132.27337765693665,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=29001, endpoint='/v1/completions', dataset='ShareGPT_V3_unfiltered_cleaned_split.json', dataset_name='random', dataset_path=None, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sharegpt_output_len=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, random_input_len=512, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/tmp/vllm-checkout/benchmarks/benchmark_serving.py:826: UserWarning: The '--dataset' argument will be deprecated in the next release. Please use '--dataset-name' and '--dataset-path' in the future runs.\n  main(args)\nTraceback (most recent call last):\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 826, in <module>\n    main(args)\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 518, in main\n    input_requests = sample_sharegpt_requests(\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_serving.py\", line 91, in sample_sharegpt_requests\n    with open(dataset_path) as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Meta-Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 1933.7246689796448,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
      "install_method": "docker_fallback",
      "baseline_raw": "Error: Command 'python /tmp/vllm-checkout/benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --host 127.0.0.1 --port 29001 --dataset-name random --random-input-len 512 --random-output-len 128 --num-prompts 100' timed out after 1800 seconds",
      "human_raw": "",
      "agent_raw": "",
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 625.0462093353271,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 625.1876468658447,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "7c01f706",
      "full_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539",
      "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize `SequenceStatus.is_finished` by sw",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 630.9518311023712,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "80aa7e91",
      "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
      "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Hardware][Intel] Optimize CPU backend and add mor",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 623.2831072807312,
      "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 25, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "8d75fe48",
      "full_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
      "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "subject": "[Kernel] Switch fp8 layers to use the CUTLASS kern",
      "has_agent_patch": true
    }
  ]
}