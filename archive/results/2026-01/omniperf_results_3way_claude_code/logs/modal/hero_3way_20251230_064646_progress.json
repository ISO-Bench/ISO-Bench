{
  "timestamp": "20251230_064646",
  "total_commits": 53,
  "completed": 4,
  "success_count": 0,
  "error_count": 4,
  "results": [
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 286.5832357406616,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 06:49:56 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev57+geb5741ad",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 06:51:54 __init__.py:183] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {serve,complete,chat} ...\nvllm: error: argument subparser: invalid choice: 'bench' (choose from 'serve', 'complete', 'chat')\n",
      "commit": "fc542144",
      "full_commit": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
      "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Feature] Fix guided decoding blocking bitmask mem",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 109.21945858001709,
      "perf_command": "VLLM_USE_V1=1 python3 benchmarks/benchmark_latency.py --model \"/data/users/ktong/llama/llm_8b_oss\" --tensor-parallel-size 1 --input_len 1000 --batch_size 32",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 06:53:36 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev379+g2a0309a6",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 06:53:43 __init__.py:183] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {serve,complete,chat} ...\nvllm: error: argument subparser: invalid choice: 'bench' (choose from 'serve', 'complete', 'chat')\n",
      "commit": "fa63e710",
      "full_commit": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
      "parent_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c",
      "model": "/data/users/ktong/llama/llm_8b_oss",
      "subject": "[V1][Perf] Reduce scheduling overhead in model run",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 31.40540099143982,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 06:54:10 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 06:54:17 __init__.py:183] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {serve,complete,chat} ...\nvllm: error: argument subparser: invalid choice: 'bench' (choose from 'serve', 'complete', 'chat')\n",
      "commit": "6dd94dbe",
      "full_commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
      "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[perf] fix perf regression from #12253 (#12380)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 74.31012487411499,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 06:54:37 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev337+g7206ce4c",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 06:55:29 __init__.py:183] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {serve,complete,chat} ...\nvllm: error: argument subparser: invalid choice: 'bench' (choose from 'serve', 'complete', 'chat')\n",
      "commit": "aea94362",
      "full_commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
      "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Frontend][V1] Online serving performance improvem",
      "has_agent_patch": true
    }
  ]
}