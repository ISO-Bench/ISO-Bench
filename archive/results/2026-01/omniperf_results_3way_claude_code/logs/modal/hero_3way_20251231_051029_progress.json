{
  "timestamp": "20251231_051029",
  "total_commits": 37,
  "completed": 4,
  "success_count": 0,
  "error_count": 4,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "BASELINE server failed to start. Logs: reasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 12-31 05:13:03 model_runner.py:1518] Graph capturing finished in 14 secs, took 0.32 GiB\nINFO 12-31 05:13:03 launcher.py:19] Available routes are:\nINFO 12-31 05:13:03 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\nINFO 12-31 05:13:03 launcher.py:27] Route: /docs, Methods: GET, HEAD\nINFO 12-31 05:13:03 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 12-31 05:13:03 launcher.py:27] Route: /redoc, Methods: GET, HEAD\nINFO 12-31 05:13:03 launcher.py:27] Route: /health, Methods: GET\nINFO 12-31 05:13:03 launcher.py:27] Route: /tokenize, Methods: POST\nINFO 12-31 05:13:03 launcher.py:27] Route: /detokenize, Methods: POST\nINFO 12-31 05:13:03 launcher.py:27] Route: /v1/models, Methods: GET\nINFO 12-31 05:13:03 launcher.py:27] Route: /version, Methods: GET\nINFO 12-31 05:13:03 launcher.py:27] Route: /v1/chat/completions, Methods: POST\nINFO 12-31 05:13:03 launcher.py:27] Route: /v1/completions, Methods: POST\nINFO 12-31 05:13:03 launcher.py:27] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [96]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 29001): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n[rank0]:[W1231 05:13:04.986664155 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "server_logs": null,
      "duration_s": 687.8299477100372,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 621.493908405304,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: args) as async_engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\n    if (model_is_embedding(args.model, args.trust_remote_code,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 71, in model_is_embedding\n    return ModelConfig(model=model_name,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 172, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/transformers_utils/config.py\", line 66, in get_config\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1250, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 649, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 708, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n    raise OSError(\nOSError: meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 118.54915356636047,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
      "install_method": "docker_fallback",
      "baseline_raw": "\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT] [--dataset DATASET]\n                            [--dataset-name {sharegpt,sonnet,random}]\n                            [--dataset-path DATASET_PATH] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--request-rate REQUEST_RATE] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME]\nbenchmark_serving.py: error: unrecognized arguments: --enable-prefix-caching --use-v2-block-manager\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 617.5387704372406,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    }
  ]
}