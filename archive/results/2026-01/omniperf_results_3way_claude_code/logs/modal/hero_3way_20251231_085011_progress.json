{
  "timestamp": "20251231_085011",
  "total_commits": 37,
  "completed": 2,
  "success_count": 0,
  "error_count": 2,
  "results": [
    {
      "status": "exception",
      "error": "cannot access local variable 're' where it is not associated with a value",
      "duration_s": 20.424194812774658,
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 666.0426180362701,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: args) as async_engine_client:\n  File \"/usr/lib/python3.10/contextlib.py\", line 199, in __aenter__\n    return await anext(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 117, in build_async_engine_client\n    if (model_is_embedding(args.model, args.trust_remote_code,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 71, in model_is_embedding\n    return ModelConfig(model=model_name,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 172, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/transformers_utils/config.py\", line 66, in get_config\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1250, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 649, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 708, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n    raise OSError(\nOSError: meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    }
  ]
}