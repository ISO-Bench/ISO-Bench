{
  "timestamp": "20251230_140812",
  "total_commits": 40,
  "completed": 40,
  "success_count": 0,
  "error_count": 40,
  "results": [
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 4.649162292480469e-05,
      "commit": "310aca88",
      "full_commit": "310aca88c984983189a57f1b72e3b1dde89fb92f",
      "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834",
      "model": "meta-llama/Meta-Llama-3-70B",
      "gpu_config": "H100:4",
      "subject": "[perf]fix current stream (#11870)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.600120544433594e-05,
      "commit": "f26c4aee",
      "full_commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
      "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:4",
      "subject": "[Misc] Optimize ray worker initialization time (#1",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.314018249511719e-05,
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.314018249511719e-05,
      "commit": "ce6bf3a2",
      "full_commit": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
      "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
      "model": "google/gemma-2b",
      "gpu_config": "H100:1",
      "subject": "[torch.compile] avoid Dynamo guard evaluation over",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.337860107421875e-05,
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "7c01f706",
      "full_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539",
      "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize `SequenceStatus.is_finished` by sw",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.218650817871094e-05,
      "commit": "80aa7e91",
      "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
      "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Hardware][Intel] Optimize CPU backend and add mor",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.3855438232421875e-05,
      "commit": "8d75fe48",
      "full_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
      "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "gpu_config": "H100:1",
      "subject": "[Kernel] Switch fp8 layers to use the CUTLASS kern",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.218650817871094e-05,
      "commit": "379da6dc",
      "full_commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
      "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
      "model": "meta-llama/Meta-Llama-3-70B",
      "gpu_config": "H100:4",
      "subject": "[Kernel] [FP8] Improve FP8 linear layer performanc",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.170967102050781e-05,
      "commit": "2a052011",
      "full_commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
      "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "gpu_config": "H100:1",
      "subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "bfdb1ba5",
      "full_commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
      "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "gpu_config": "H100:1",
      "subject": "[Core] Improve detokenization performance for pref",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.981590270996094e-05,
      "commit": "21d93c14",
      "full_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
      "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "gpu_config": "H100:8",
      "subject": "Optimize Mixtral with expert parallelism (#2090)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "c45f3c3a",
      "full_commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
      "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
      "model": "facebook/opt-13b",
      "gpu_config": "H100:1",
      "subject": "Optimize tensor parallel execution speed (#17)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "d4bc1a4d",
      "full_commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
      "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
      "model": "facebook/opt-125m",
      "gpu_config": "H100:1",
      "subject": "Add unoptimized OPT Attention",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "8aa1485f",
      "full_commit": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
      "parent_commit": "89ac266b262f08d25ebf25fc66122d1b2367ae64",
      "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
      "gpu_config": "H100:2",
      "subject": "[Perf] Disable chunked local attention by default ",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.361701965332031e-05,
      "commit": "61b8cea3",
      "full_commit": "61b8cea3b42feab021d506e9143551de18f9165c",
      "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b",
      "model": "meta-llama/Meta-Llama-3-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Attention] Optimize FlashInfer MetadataBuilder Bu",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "e7b20426",
      "full_commit": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
      "parent_commit": "90f1e55421f1b61394ba25abe34bf5abd82a71af",
      "model": "01-ai/Yi-1.5-9B-Chat",
      "gpu_config": "H100:1",
      "subject": "Revert \"[Performance] Performance improvements in ",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "0ec82edd",
      "full_commit": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
      "parent_commit": "005ae9be6c22dfa2c2c5580b50b41e67faee4a87",
      "model": "Qwen/Qwen3-30B-A3B",
      "gpu_config": "H100:1",
      "subject": "[perf] Speed up align sum kernels (#21079)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.337860107421875e-05,
      "commit": "dcc6cfb9",
      "full_commit": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
      "parent_commit": "dd572c0ab3effa539b74f9a1288bb61ce83ada76",
      "model": "Qwen/Qwen3-30B-A3B-FP8",
      "gpu_config": "H100:1",
      "subject": "[Kernel][Performance] Tweak MoE Batched silu_mul_f",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.361701965332031e-05,
      "commit": "c0569dbc",
      "full_commit": "c0569dbc82b5e945a77878190114d1b68027828b",
      "parent_commit": "8bb43b9c9ee878e07038d3f36aaf279ffb2fabab",
      "model": "Qwen/Qwen3-30B-A3B-FP8",
      "gpu_config": "H100:1",
      "subject": "[Misc] ModularKernel : Perform WeightAndReduce ins",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.790855407714844e-05,
      "commit": "22dd9c27",
      "full_commit": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b",
      "parent_commit": "a6d795d593046abd490b16349bcd9b40feedd334",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Kernel] Optimize Prefill Attention in Unified Tri",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.218650817871094e-05,
      "commit": "9a3b8832",
      "full_commit": "9a3b88328f7e434cac35b90ee463de6689f9a833",
      "parent_commit": "3014c920dae5a2360b9b4141395522cc52b59193",
      "model": "Qwen/Qwen2.5-VL-3B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.457069396972656e-05,
      "commit": "7661e92e",
      "full_commit": "7661e92ef85e552936195ae4b803e292b9a96776",
      "parent_commit": "f168b85725202915b5719c62b46d310a608b13dd",
      "model": "nvidia/Nemotron-4-340B-Instruct",
      "gpu_config": "H100:8",
      "subject": "[Model] Optimize nemotron_h implementation (#19249",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "e7523c2e",
      "full_commit": "e7523c2e031bc96740723ab63833d1cf94229ab4",
      "parent_commit": "a869baca73eb90ae7bd18402915dc4bfc36cf06b",
      "model": "google/gemma-3-12b-it",
      "gpu_config": "H100:1",
      "subject": "[V1][Sampler] Improve performance of FlashInfer sa",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.24249267578125e-05,
      "commit": "d55e446d",
      "full_commit": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
      "parent_commit": "ec82c3e388b962a30a02fa376c222cef787b3c14",
      "model": "meta-llama/Llama-3-8B",
      "gpu_config": "H100:1",
      "subject": "[V1][Spec Decode] Small refactors to improve eagle",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.314018249511719e-05,
      "commit": "67da5720",
      "full_commit": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
      "parent_commit": "5c04bb8b863bfdef8122b193631479315cc764f5",
      "model": "Qwen/Qwen2.5-7B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[PERF] Speed up Qwen2.5-VL model by speed up rotar",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.361701965332031e-05,
      "commit": "015069b0",
      "full_commit": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
      "parent_commit": "fbefc8a78d22b20eac042c586805c7dcbfc66b1c",
      "model": "Qwen/Qwen3-7B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Misc] Optimize the Qwen3_ReasoningParser extract_",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.147125244140625e-05,
      "commit": "bc7c4d20",
      "full_commit": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
      "parent_commit": "f67e9e9f221e9791733b827585d6eb6dbc23133c",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Kernel][ROCM] Upstream prefix prefill speed up fo",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.337860107421875e-05,
      "commit": "299ebb62",
      "full_commit": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
      "parent_commit": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9",
      "model": "Qwen/Qwen2.5-1.5B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Speed up decode by remove synchronizing ope",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.147125244140625e-05,
      "commit": "bd6028d6",
      "full_commit": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
      "parent_commit": "802329dee9e5b70c0c73df93c9db1ecdc4632664",
      "model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic",
      "gpu_config": "H100:2",
      "subject": "Optimized topk for topk=1 (Llama-4) (#16512)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.910064697265625e-05,
      "commit": "296f927f",
      "full_commit": "296f927f2493908984707354e3cc5d7b2e41650b",
      "parent_commit": "0032903a5bb7c7c655f52f4efdfcc221947e9ca8",
      "model": "ibm-ai-platform/Bamba-9B",
      "gpu_config": "H100:1",
      "subject": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fix",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.314018249511719e-05,
      "commit": "99abb8b6",
      "full_commit": "99abb8b650c66664cdc84d815b7f306f33bd9881",
      "parent_commit": "3a1e6481586ed7f079275b5d5072a6e246af691e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[V1][Spec Decode] Optimize Rejection Sampler with ",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.1948089599609375e-05,
      "commit": "ccf02fcb",
      "full_commit": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
      "parent_commit": "acaea3bb07883c80b71643ebee1cd08d555797bc",
      "model": "ibm-ai-platform/Bamba-9B",
      "gpu_config": "H100:1",
      "subject": "Revert \"[Model] Mamba2 Prefill Performance Tweaks:",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.170967102050781e-05,
      "commit": "fe66b347",
      "full_commit": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
      "parent_commit": "270a5da495d24e947a71e2fa0c56635f4fad2dc3",
      "model": "ibm-ai-platform/Bamba-9B",
      "gpu_config": "H100:1",
      "subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing ",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "70b808fe",
      "full_commit": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
      "parent_commit": "63d635d17962377df089cdc9d4a2684f0b007208",
      "model": "Qwen/Qwen2-VL-7B",
      "gpu_config": "H100:1",
      "subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.1948089599609375e-05,
      "commit": "ca7a2d5f",
      "full_commit": "ca7a2d5f28eac9621474563cdda0e08596222755",
      "parent_commit": "333681408feabb97193880303b23f6571ba39045",
      "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
      "gpu_config": "H100:1",
      "subject": "Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.2901763916015625e-05,
      "commit": "9badee53",
      "full_commit": "9badee53decb3d432dc805336abfb0eb81dfb48f",
      "parent_commit": "beebf4742af80296d3c3a657c66d512615c550c1",
      "model": "meta-llama/Llama-3.2-1B-Instruct",
      "gpu_config": "H100:1",
      "subject": "Fix performance when `--generation-config` is not ",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "[Errno 32] Broken pipe",
      "duration_s": 3.314018249511719e-05,
      "commit": "0d243f2a",
      "full_commit": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
      "parent_commit": "88f6ba3281f727d5641d362476ae68562b666081",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "gpu_config": "H100:1",
      "subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS",
      "has_agent_patch": true
    }
  ]
}