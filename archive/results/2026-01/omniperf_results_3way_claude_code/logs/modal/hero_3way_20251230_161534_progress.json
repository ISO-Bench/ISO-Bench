{
  "timestamp": "20251230_161534",
  "total_commits": 39,
  "completed": 2,
  "success_count": 0,
  "error_count": 2,
  "results": [
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 256.1589744091034,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
      "baseline_version": "0.6.6.post2.dev145+ga732900e",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "310aca88",
      "full_commit": "310aca88c984983189a57f1b72e3b1dde89fb92f",
      "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834",
      "model": "meta-llama/Meta-Llama-3-70B",
      "subject": "[perf]fix current stream (#11870)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 120.09447050094604,
      "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
      "baseline_version": "0.6.6.dev17+g8936316d",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "f26c4aee",
      "full_commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
      "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Misc] Optimize ray worker initialization time (#1",
      "has_agent_patch": true
    }
  ]
}