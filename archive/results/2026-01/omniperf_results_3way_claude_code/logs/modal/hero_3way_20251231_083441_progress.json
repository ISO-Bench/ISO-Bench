{
  "timestamp": "20251231_083441",
  "total_commits": 37,
  "completed": 1,
  "success_count": 0,
  "error_count": 1,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "BASELINE server failed to start. Logs: easing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 12-31 08:37:49 model_runner.py:1518] Graph capturing finished in 14 secs, took 0.32 GiB\nINFO 12-31 08:37:50 launcher.py:19] Available routes are:\nINFO 12-31 08:37:50 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\nINFO 12-31 08:37:50 launcher.py:27] Route: /docs, Methods: GET, HEAD\nINFO 12-31 08:37:50 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 12-31 08:37:50 launcher.py:27] Route: /redoc, Methods: GET, HEAD\nINFO 12-31 08:37:50 launcher.py:27] Route: /health, Methods: GET\nINFO 12-31 08:37:50 launcher.py:27] Route: /tokenize, Methods: POST\nINFO 12-31 08:37:50 launcher.py:27] Route: /detokenize, Methods: POST\nINFO 12-31 08:37:50 launcher.py:27] Route: /v1/models, Methods: GET\nINFO 12-31 08:37:50 launcher.py:27] Route: /version, Methods: GET\nINFO 12-31 08:37:50 launcher.py:27] Route: /v1/chat/completions, Methods: POST\nINFO 12-31 08:37:50 launcher.py:27] Route: /v1/completions, Methods: POST\nINFO 12-31 08:37:50 launcher.py:27] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [102]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 18001): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n[rank0]:[W1231 08:37:51.724473867 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "server_logs": null,
      "duration_s": 711.7694458961487,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    }
  ]
}