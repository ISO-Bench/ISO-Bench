{
  "timestamp": "20251231_023448",
  "total_commits": 37,
  "completed": 13,
  "success_count": 0,
  "error_count": 13,
  "results": [
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "BASELINE server failed to start. Logs: s` as needed to decrease memory usage.\nINFO 12-31 02:37:15 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.32 GiB\nINFO 12-31 02:37:15 api_server.py:248] vLLM to use /tmp/tmpt5ksnqxl as PROMETHEUS_MULTIPROC_DIR\nINFO 12-31 02:37:15 launcher.py:19] Available routes are:\nINFO 12-31 02:37:15 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\nINFO 12-31 02:37:15 launcher.py:27] Route: /docs, Methods: GET, HEAD\nINFO 12-31 02:37:15 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 12-31 02:37:15 launcher.py:27] Route: /redoc, Methods: GET, HEAD\nINFO 12-31 02:37:15 launcher.py:27] Route: /health, Methods: GET\nINFO 12-31 02:37:15 launcher.py:27] Route: /tokenize, Methods: POST\nINFO 12-31 02:37:15 launcher.py:27] Route: /detokenize, Methods: POST\nINFO 12-31 02:37:15 launcher.py:27] Route: /v1/models, Methods: GET\nINFO 12-31 02:37:15 launcher.py:27] Route: /version, Methods: GET\nINFO 12-31 02:37:15 launcher.py:27] Route: /v1/chat/completions, Methods: POST\nINFO 12-31 02:37:15 launcher.py:27] Route: /v1/completions, Methods: POST\nINFO 12-31 02:37:15 launcher.py:27] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [94]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('127.0.0.1', 9000): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n[rank0]:[W1231 02:37:16.858407156 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "server_logs": null,
      "duration_s": 679.1186044216156,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6848485469818115,
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6859526634216309,
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6651754379272461,
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.689974308013916,
      "commit": "7c01f706",
      "full_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539",
      "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Core] Optimize `SequenceStatus.is_finished` by sw",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6889023780822754,
      "commit": "80aa7e91",
      "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
      "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "gpu_config": "H100:1",
      "subject": "[Hardware][Intel] Optimize CPU backend and add mor",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6699867248535156,
      "commit": "8d75fe48",
      "full_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
      "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "gpu_config": "H100:1",
      "subject": "[Kernel] Switch fp8 layers to use the CUTLASS kern",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6674425601959229,
      "commit": "379da6dc",
      "full_commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
      "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
      "model": "meta-llama/Meta-Llama-3-70B",
      "gpu_config": "H100:4",
      "subject": "[Kernel] [FP8] Improve FP8 linear layer performanc",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 36fb68f94792",
      "server_logs": null,
      "duration_s": 14.714372873306274,
      "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --quantization fp8",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "2a052011",
      "full_commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
      "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6870954036712646,
      "commit": "bfdb1ba5",
      "full_commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
      "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "gpu_config": "H100:1",
      "subject": "[Core] Improve detokenization performance for pref",
      "has_agent_patch": true
    },
    {
      "status": "exception",
      "error": "name 'port' is not defined",
      "duration_s": 0.6753244400024414,
      "commit": "21d93c14",
      "full_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
      "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "gpu_config": "H100:8",
      "subject": "Optimize Mixtral with expert parallelism (#2090)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 7a7929abe8e2",
      "server_logs": null,
      "duration_s": 9.975142240524292,
      "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "c45f3c3a",
      "full_commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
      "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
      "model": "facebook/opt-13b",
      "subject": "Optimize tensor parallel execution speed (#17)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for b56b6ca0d650",
      "server_logs": null,
      "duration_s": 9.771373987197876,
      "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "d4bc1a4d",
      "full_commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
      "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
      "model": "facebook/opt-125m",
      "subject": "Add unoptimized OPT Attention",
      "has_agent_patch": true
    }
  ]
}