{
  "timestamp": "20251230_072306",
  "total_commits": 51,
  "completed": 22,
  "success_count": 6,
  "error_count": 16,
  "results": [
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 28.68,
        "ttft_median": 28.68,
        "ttft_p99": 28.68,
        "tpot_mean": 7.8,
        "tpot_median": 7.8,
        "tpot_p99": 7.8,
        "itl_mean": 7.8,
        "itl_median": 7.79,
        "itl_p99": 8.2
      },
      "human_metrics": {
        "ttft_mean": 28.4,
        "ttft_median": 28.4,
        "ttft_p99": 28.4,
        "tpot_mean": 7.75,
        "tpot_median": 7.75,
        "tpot_p99": 7.75,
        "itl_mean": 7.75,
        "itl_median": 7.72,
        "itl_p99": 8.34
      },
      "agent_metrics": {
        "ttft_mean": 27.44,
        "ttft_median": 27.44,
        "ttft_p99": 27.44,
        "tpot_mean": 7.7,
        "tpot_median": 7.7,
        "tpot_p99": 7.7,
        "itl_mean": 7.7,
        "itl_median": 7.69,
        "itl_p99": 8.11
      },
      "human_improvement": {
        "ttft_mean": 0.9762900976290136,
        "ttft_median": 0.9762900976290136,
        "ttft_p99": 0.9762900976290136,
        "tpot_mean": 0.6410256410256387,
        "itl_mean": 0.6410256410256387
      },
      "agent_improvement": {
        "ttft_mean": 4.323570432357037,
        "ttft_median": 4.323570432357037,
        "ttft_p99": 4.323570432357037,
        "tpot_mean": 1.2820512820512775,
        "itl_mean": 1.2820512820512775
      },
      "agent_vs_human": {
        "ttft_mean": 3.3802816901408357,
        "ttft_median": 3.3802816901408357,
        "ttft_p99": 3.3802816901408357,
        "tpot_mean": 0.6451612903225784,
        "itl_mean": 0.6451612903225784
      },
      "error": null,
      "server_logs": null,
      "duration_s": 355.4703526496887,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:24:34 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev57+geb5741ad",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 07:25:58 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.02      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.98      \nOutput token throughput (tok/s):         125.45    \nTotal Token throughput (tok/s):          627.23    \n---------------Time to First Token----------------\nMean TTFT (ms):                          28.68     \nMedian TTFT (ms):                        28.68     \nP99 TTFT (ms):                           28.68     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.80      \nMedian TPOT (ms):                        7.80      \nP99 TPOT (ms):                           7.80      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.80      \nMedian ITL (ms):                         7.79      \nP99 ITL (ms):                            8.20      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n",
      "human_version": "INFO 12-30 07:26:23 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev58+gfc542144",
      "human_install_method": "wheel",
      "human_raw": "INFO 12-30 07:27:36 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.01      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.99      \nOutput token throughput (tok/s):         126.23    \nTotal Token throughput (tok/s):          631.13    \n---------------Time to First Token----------------\nMean TTFT (ms):                          28.40     \nMedian TTFT (ms):                        28.40     \nP99 TTFT (ms):                           28.40     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.75      \nMedian TPOT (ms):                        7.75      \nP99 TPOT (ms):                           7.75      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.75      \nMedian ITL (ms):                         7.72      \nP99 ITL (ms):                            8.34      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.01s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.01s/it]\n",
      "agent_raw": "INFO 12-30 07:29:19 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.01      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.99      \nOutput token throughput (tok/s):         127.11    \nTotal Token throughput (tok/s):          635.57    \n---------------Time to First Token----------------\nMean TTFT (ms):                          27.44     \nMedian TTFT (ms):                        27.44     \nP99 TTFT (ms):                           27.44     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          7.70      \nMedian TPOT (ms):                        7.70      \nP99 TPOT (ms):                           7.70      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           7.70      \nMedian ITL (ms):                         7.69      \nP99 ITL (ms):                            8.11      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.01s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.01s/it]\n",
      "commit": "fc542144",
      "full_commit": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
      "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Feature] Fix guided decoding blocking bitmask mem",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 1602.7579123333303,
        "throughput": 204.8
      },
      "human_metrics": {
        "latency_avg": 1037.4960406333382,
        "throughput": 255.9
      },
      "agent_metrics": {},
      "human_improvement": {
        "throughput": 24.951171874999996,
        "latency_avg": 35.26807556838521
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "server_logs": null,
      "duration_s": 318.32646226882935,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:29:47 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 07:30:09 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 12-30 07:30:21 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "human_version": "INFO 12-30 07:32:06 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev365+g6dd94dbe",
      "human_install_method": "wheel",
      "human_raw": "INFO 12-30 07:32:28 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 12-30 07:32:39 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "agent_raw": "INFO 12-30 07:34:23 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 12-30 07:34:33 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "agent_error": "Agent benchmark produced no metrics",
      "commit": "6dd94dbe",
      "full_commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
      "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[perf] fix perf regression from #12253 (#12380)",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 3127.22,
        "ttft_median": 2672.25,
        "ttft_p99": 8029.42,
        "tpot_mean": 46.69,
        "tpot_median": 49.99,
        "tpot_p99": 63.85,
        "itl_mean": 46.56,
        "itl_median": 36.24,
        "itl_p99": 235.14
      },
      "human_metrics": {
        "ttft_mean": 3267.65,
        "ttft_median": 2896.64,
        "ttft_p99": 8135.29,
        "tpot_mean": 46.16,
        "tpot_median": 48.89,
        "tpot_p99": 63.5,
        "itl_mean": 45.85,
        "itl_median": 36.35,
        "itl_p99": 143.38
      },
      "agent_metrics": {},
      "human_improvement": {
        "ttft_mean": -4.490569899143658,
        "ttft_median": -8.39704368977453,
        "ttft_p99": -1.318526120192989,
        "tpot_mean": 1.1351467123581092,
        "itl_mean": 1.5249140893470807
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "server_logs": null,
      "duration_s": 319.5614125728607,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 12-30 07:35:01 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev337+g7206ce4c",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 07:36:20 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  10.78     \nTotal input tokens:                      153600    \nTotal generated tokens:                  36963     \nRequest throughput (req/s):              27.84     \nOutput token throughput (tok/s):         3430.05   \nTotal Token throughput (tok/s):          17683.64  \n---------------Time to First Token----------------\nMean TTFT (ms):                          3127.22   \nMedian TTFT (ms):                        2672.25   \nP99 TTFT (ms):                           8029.42   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          46.69     \nMedian TPOT (ms):                        49.99     \nP99 TPOT (ms):                           63.85     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           46.56     \nMedian ITL (ms):                         36.24     \nP99 ITL (ms):                            235.14    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:00<04:29,  1.11it/s]\n  1%|          | 2/300 [00:01<03:22,  1.47it/s]\n  1%|\u258f         | 4/300 [00:01<02:03,  2.40it/s]\n  2%|\u258f         | 5/300 [00:02<01:47,  2.75it/s]\n  2%|\u258f         | 6/300 [00:03<03:09,  1.56it/s]\n  2%|\u258f         | 7/300 [00:04<03:04,  1.58it/s]\n  3%|\u258e         | 8/300 [00:04<03:23,  1.43it/s]\n  3%|\u258e         | 9/300 [00:05<02:58,  1.63it/s]\n  3%|\u258e         | 10/300 [00:06<03:37,  1.33it/s]\n  4%|\u258e         | 11/300 [00:06<02:45,  1.75it/s]\n  4%|\u258d         | 12/300 [00:06<02:22,  2.02it/s]\n  ",
      "human_version": "INFO 12-30 07:36:54 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev338+gaea94362",
      "human_install_method": "wheel",
      "human_raw": "INFO 12-30 07:38:07 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  10.76     \nTotal input tokens:                      153600    \nTotal generated tokens:                  36963     \nRequest throughput (req/s):              27.88     \nOutput token throughput (tok/s):         3435.38   \nTotal Token throughput (tok/s):          17711.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          3267.65   \nMedian TTFT (ms):                        2896.64   \nP99 TTFT (ms):                           8135.29   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          46.16     \nMedian TPOT (ms):                        48.89     \nP99 TPOT (ms):                           63.50     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           45.85     \nMedian ITL (ms):                         36.35     \nP99 ITL (ms):                            143.38    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:00<04:53,  1.02it/s]\n  1%|          | 2/300 [00:01<03:10,  1.57it/s]\n  1%|\u258f         | 4/300 [00:02<03:18,  1.49it/s]\n  2%|\u258f         | 6/300 [00:04<03:15,  1.51it/s]\n  2%|\u258f         | 7/300 [00:05<04:11,  1.17it/s]\n  3%|\u258e         | 8/300 [00:06<04:33,  1.07it/s]\n  3%|\u258e         | 9/300 [00:06<03:40,  1.32it/s]\n  3%|\u258e         | 10/300 [00:07<03:08,  1.54it/s]\n  4%|\u258e         | 11/300 [00:07<02:23,  2.02it/s]\n  5%|\u258c         | 15/300 [00:07<00:58,  4.86it/s]\n  8%|\u258a         | 25/300 [00:07<00:20, 13.61it/s]\n ",
      "agent_raw": "INFO 12-30 07:39:59 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1248, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 888, in main\n    benchmark_result = asyncio.run(\n                       ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/base_events.py\", line 653, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 574, in benchmark\n    raise ValueError(\nValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Internal Server Error\n",
      "agent_error": "Agent benchmark produced no metrics",
      "commit": "aea94362",
      "full_commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
      "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Frontend][V1] Online serving performance improvem",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 227.71440505981445,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
      "baseline_version": "0.6.6.post2.dev145+ga732900e",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "310aca88",
      "full_commit": "310aca88c984983189a57f1b72e3b1dde89fb92f",
      "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834",
      "model": "meta-llama/Meta-Llama-3-70B",
      "subject": "[perf]fix current stream (#11870)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 97.48720240592957,
      "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
      "baseline_version": "0.6.6.dev17+g8936316d",
      "baseline_raw": "DEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench latency\n\nFor help with the new command, run:\n    vllm bench latency --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench latency --help\n\n\n",
      "commit": "f26c4aee",
      "full_commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
      "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Misc] Optimize ray worker initialization time (#1",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 250.92406366666702,
        "throughput": 1023.9
      },
      "human_metrics": {
        "latency_avg": 250.55025029999834,
        "throughput": 1023.7
      },
      "agent_metrics": {
        "latency_avg": 250.135358999997,
        "throughput": 1024.0
      },
      "human_improvement": {
        "throughput": -0.01953315753490886,
        "latency_avg": 0.14897469824387458
      },
      "agent_improvement": {
        "throughput": 0.00976657876745998,
        "latency_avg": 0.314320059680584
      },
      "agent_vs_human": {
        "throughput": 0.02930546058415107,
        "latency_avg": 0.16559205169603944
      },
      "error": null,
      "server_logs": null,
      "duration_s": 270.59115958213806,
      "perf_command": "python benchmarks/benchmark_latency.py",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev182+g8c1e77fb",
      "baseline_install_method": "wheel",
      "baseline_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 12-30 07:46:50 __init__.py:42] No plugins found.\nWARNING 12-30 07:47:00 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
      "human_version": "0.6.4.post2.dev183+g98f47f2a",
      "human_install_method": "wheel",
      "human_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 12-30 07:48:06 __init__.py:42] No plugins found.\nWARNING 12-30 07:48:16 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
      "agent_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 12-30 07:49:21 __init__.py:42] No plugins found.\nWARNING 12-30 07:49:30 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
      "commit": "98f47f2a",
      "full_commit": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
      "parent_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
      "model": "unknown",
      "subject": "[V1] Optimize the CPU overheads in FlashAttention ",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "BASELINE server failed to start. Logs: qs` as needed to decrease memory usage.\nINFO 12-30 07:51:04 model_runner.py:1518] Graph capturing finished in 10 secs, took 0.32 GiB\nINFO 12-30 07:51:05 api_server.py:248] vLLM to use /tmp/tmpxihsyy3s as PROMETHEUS_MULTIPROC_DIR\nINFO 12-30 07:51:05 launcher.py:19] Available routes are:\nINFO 12-30 07:51:05 launcher.py:27] Route: /openapi.json, Methods: GET, HEAD\nINFO 12-30 07:51:05 launcher.py:27] Route: /docs, Methods: GET, HEAD\nINFO 12-30 07:51:05 launcher.py:27] Route: /docs/oauth2-redirect, Methods: GET, HEAD\nINFO 12-30 07:51:05 launcher.py:27] Route: /redoc, Methods: GET, HEAD\nINFO 12-30 07:51:05 launcher.py:27] Route: /health, Methods: GET\nINFO 12-30 07:51:05 launcher.py:27] Route: /tokenize, Methods: POST\nINFO 12-30 07:51:05 launcher.py:27] Route: /detokenize, Methods: POST\nINFO 12-30 07:51:05 launcher.py:27] Route: /v1/models, Methods: GET\nINFO 12-30 07:51:05 launcher.py:27] Route: /version, Methods: GET\nINFO 12-30 07:51:05 launcher.py:27] Route: /v1/chat/completions, Methods: POST\nINFO 12-30 07:51:05 launcher.py:27] Route: /v1/completions, Methods: POST\nINFO 12-30 07:51:05 launcher.py:27] Route: /v1/embeddings, Methods: POST\nINFO:     Started server process [607]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8000): address already in use\nINFO:     Waiting for application shutdown.\nINFO:     Application shutdown complete.\n[rank0]:[W1230 07:51:05.078140105 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "server_logs": null,
      "duration_s": 616.3083562850952,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
      "baseline_install_method": "wheel",
      "commit": "b2e0ad3b",
      "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
      "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 95a178f86120",
      "server_logs": null,
      "duration_s": 13.714205503463745,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Llama-3-8B-Instruct",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 3cdfe1f38b2c",
      "server_logs": null,
      "duration_s": 9.121425867080688,
      "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "ce6bf3a2",
      "full_commit": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
      "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
      "model": "google/gemma-2b",
      "subject": "[torch.compile] avoid Dynamo guard evaluation over",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 8d59dbb00044",
      "server_logs": null,
      "duration_s": 9.111889600753784,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 54600709b6d4",
      "server_logs": null,
      "duration_s": 9.24731969833374,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 51e971d39e12",
      "server_logs": null,
      "duration_s": 8.989861011505127,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "7c01f706",
      "full_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539",
      "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize `SequenceStatus.is_finished` by sw",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for bd43973522ea",
      "server_logs": null,
      "duration_s": 9.03175663948059,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "80aa7e91",
      "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
      "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Hardware][Intel] Optimize CPU backend and add mor",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 388596c91437",
      "server_logs": null,
      "duration_s": 9.081268310546875,
      "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "8d75fe48",
      "full_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
      "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "subject": "[Kernel] Switch fp8 layers to use the CUTLASS kern",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local\n  \u00d7 Failed to download `vllm @\n  \u2502 https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u251c\u2500\u25b6 Failed to fetch:\n  \u2502   `https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u2570\u2500\u25b6 HTTP status client error (404 Not Found) for url\n      (https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)\n",
      "duration_s": 5.248615026473999,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-70B --input-len 1000 --output-len 50 --tensor-parallel-size 4 --quantization fp8",
      "commit": "379da6dc",
      "full_commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
      "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
      "model": "meta-llama/Meta-Llama-3-70B",
      "subject": "[Kernel] [FP8] Improve FP8 linear layer performanc",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 36fb68f94792",
      "server_logs": null,
      "duration_s": 9.212271928787231,
      "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --quantization fp8",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "2a052011",
      "full_commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
      "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for cf2f084d56a1",
      "server_logs": null,
      "duration_s": 9.07590126991272,
      "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "bfdb1ba5",
      "full_commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
      "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "subject": "[Core] Improve detokenization performance for pref",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:8",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local\n  \u00d7 Failed to download `vllm @\n  \u2502 https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u251c\u2500\u25b6 Failed to fetch:\n  \u2502   `https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u2570\u2500\u25b6 HTTP status client error (404 Not Found) for url\n      (https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)\n",
      "duration_s": 58.88585138320923,
      "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8",
      "commit": "21d93c14",
      "full_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
      "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "subject": "Optimize Mixtral with expert parallelism (#2090)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 7a7929abe8e2",
      "server_logs": null,
      "duration_s": 9.069486618041992,
      "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "c45f3c3a",
      "full_commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
      "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
      "model": "facebook/opt-13b",
      "subject": "Optimize tensor parallel execution speed (#17)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for b56b6ca0d650",
      "server_logs": null,
      "duration_s": 9.176289796829224,
      "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "d4bc1a4d",
      "full_commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
      "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
      "model": "facebook/opt-125m",
      "subject": "Add unoptimized OPT Attention",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 37982.99,
        "ttft_median": 37875.55,
        "ttft_p99": 44660.35,
        "tpot_mean": 58.62,
        "tpot_median": 59.37,
        "tpot_p99": 308.1,
        "itl_mean": 58.62,
        "itl_median": 41.29,
        "itl_p99": 123.92
      },
      "human_metrics": {
        "ttft_mean": 8529.66,
        "ttft_median": 8174.79,
        "ttft_p99": 14851.65,
        "tpot_mean": 56.09,
        "tpot_median": 58.62,
        "tpot_p99": 73.74,
        "itl_mean": 56.09,
        "itl_median": 41.0,
        "itl_p99": 128.47
      },
      "agent_metrics": {
        "ttft_mean": 8547.23,
        "ttft_median": 8170.18,
        "ttft_p99": 14983.99,
        "tpot_mean": 56.96,
        "tpot_median": 59.74,
        "tpot_p99": 75.48,
        "itl_mean": 56.96,
        "itl_median": 41.94,
        "itl_p99": 118.6
      },
      "human_improvement": {
        "ttft_mean": 77.54347406562779,
        "ttft_median": 78.41670946032467,
        "ttft_p99": 66.74533450812633,
        "tpot_mean": 4.315933128625033,
        "itl_mean": 4.315933128625033
      },
      "agent_improvement": {
        "ttft_mean": 77.49721651718309,
        "ttft_median": 78.4288809007394,
        "ttft_p99": 66.44900902030548,
        "tpot_mean": 2.8317980211531846,
        "itl_mean": 2.8317980211531846
      },
      "agent_vs_human": {
        "ttft_mean": -0.20598710851311436,
        "ttft_median": 0.05639288593345728,
        "ttft_p99": -0.8910794423515241,
        "tpot_mean": -1.5510786236405731,
        "itl_mean": -1.5510786236405731
      },
      "error": null,
      "server_logs": null,
      "duration_s": 541.0041761398315,
      "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.10.1.dev312+g25373b6c6",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 08:06:36 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  47.25     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              6.35      \nOutput token throughput (tok/s):         812.64    \nTotal Token throughput (tok/s):          4053.90   \n---------------Time to First Token----------------\nMean TTFT (ms):                          37982.99  \nMedian TTFT (ms):                        37875.55  \nP99 TTFT (ms):                           44660.35  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          58.62     \nMedian TPOT (ms):                        59.37     \nP99 TPOT (ms):                           308.10    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           58.62     \nMedian ITL (ms):                         41.29     \nP99 ITL (ms):                            123.92    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1298: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:43<3:36:08, 43.37s/it]\n  2%|\u258f         | 6/300 [00:43<26:11,  5.34s/it]",
      "human_version": "0.10.1.dev313+gb690e3482",
      "human_install_method": "wheel",
      "human_raw": "INFO 12-30 08:09:21 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  17.46     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              17.18     \nOutput token throughput (tok/s):         2199.44   \nTotal Token throughput (tok/s):          10972.00  \n---------------Time to First Token----------------\nMean TTFT (ms):                          8529.66   \nMedian TTFT (ms):                        8174.79   \nP99 TTFT (ms):                           14851.65  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          56.09     \nMedian TPOT (ms):                        58.62     \nP99 TPOT (ms):                           73.74     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           56.09     \nMedian ITL (ms):                         41.00     \nP99 ITL (ms):                            128.47    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1298: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:13<1:07:51, 13.62s/it]\n  1%|\u258f         | 4/300 [00:13<12:50,  2.60s/it]",
      "agent_raw": "INFO 12-30 08:11:41 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  17.60     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38333     \nRequest throughput (req/s):              17.05     \nOutput token throughput (tok/s):         2178.43   \nTotal Token throughput (tok/s):          10882.39  \n---------------Time to First Token----------------\nMean TTFT (ms):                          8547.23   \nMedian TTFT (ms):                        8170.18   \nP99 TTFT (ms):                           14983.99  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          56.96     \nMedian TPOT (ms):                        59.74     \nP99 TPOT (ms):                           75.48     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           56.96     \nMedian ITL (ms):                         41.94     \nP99 ITL (ms):                            118.60    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1298: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:13<1:05:01, 13.05s/it]\n  1%|          | 2/300 [00:13<28:32,  5.75s/it]",
      "commit": "b690e348",
      "full_commit": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
      "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157",
      "model": "ibm-ai-platform/Bamba-9B-v2",
      "subject": "[Model] Mamba2 preallocate SSM output tensor to av",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 2463.11,
        "ttft_median": 2342.47,
        "ttft_p99": 5641.26,
        "tpot_mean": 57.45,
        "tpot_median": 58.02,
        "tpot_p99": 70.78,
        "itl_mean": 57.45,
        "itl_median": 28.62,
        "itl_p99": 885.21
      },
      "human_metrics": {
        "ttft_mean": 2649.91,
        "ttft_median": 2548.12,
        "ttft_p99": 5881.76,
        "tpot_mean": 58.81,
        "tpot_median": 59.29,
        "tpot_p99": 73.29,
        "itl_mean": 58.81,
        "itl_median": 28.85,
        "itl_p99": 887.07
      },
      "agent_metrics": {
        "ttft_mean": 2448.65,
        "ttft_median": 2296.39,
        "ttft_p99": 5450.45,
        "tpot_mean": 57.31,
        "tpot_median": 57.99,
        "tpot_p99": 70.32,
        "itl_mean": 57.31,
        "itl_median": 29.0,
        "itl_p99": 947.58
      },
      "human_improvement": {
        "ttft_mean": -7.58390814864134,
        "ttft_median": -8.779194610816791,
        "ttft_p99": -4.263231972998939,
        "tpot_mean": -2.3672758920800683,
        "itl_mean": -2.3672758920800683
      },
      "agent_improvement": {
        "ttft_mean": 0.5870626971592838,
        "ttft_median": 1.9671543285506294,
        "ttft_p99": 3.382400385729436,
        "tpot_mean": 0.2436901653611846,
        "itl_mean": 0.2436901653611846
      },
      "agent_vs_human": {
        "ttft_mean": 7.594974923676645,
        "ttft_median": 9.87904808250789,
        "ttft_p99": 7.3330091673240725,
        "tpot_mean": 2.5505866349260327,
        "itl_mean": 2.5505866349260327
      },
      "error": null,
      "server_logs": null,
      "duration_s": 10289.889122247696,
      "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
      "benchmark_mode": "serving",
      "patch_type": "c_cuda",
      "baseline_version": "0.10.1.dev295+g88faa466d",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 12-30 08:18:37 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  10.03     \nTotal input tokens:                      153250    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              29.90     \nOutput token throughput (tok/s):         3827.04   \nTotal Token throughput (tok/s):          19100.30  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2463.11   \nMedian TTFT (ms):                        2342.47   \nP99 TTFT (ms):                           5641.26   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          57.45     \nMedian TPOT (ms):                        58.02     \nP99 TPOT (ms):                           70.78     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           57.45     \nMedian ITL (ms):                         28.62     \nP99 ITL (ms):                            885.21    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1299: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:09<47:57,  9.62s/it]\n 16%|\u2588\u258b        | 49/300 [00:09<00:35,  7.11it/s]\n 38%|",
      "agent_version": "0.10.1.dev295+g88faa466d.d20220101",
      "agent_raw": "INFO 12-30 10:58:39 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  10.00     \nTotal input tokens:                      153250    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              30.01     \nOutput token throughput (tok/s):         3840.99   \nTotal Token throughput (tok/s):          19169.94  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2448.65   \nMedian TTFT (ms):                        2296.39   \nP99 TTFT (ms):                           5450.45   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          57.31     \nMedian TPOT (ms):                        57.99     \nP99 TPOT (ms):                           70.32     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           57.31     \nMedian ITL (ms):                         29.00     \nP99 ITL (ms):                            947.58    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1299: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:09<47:46,  9.59s/it]\n 16%|\u2588\u258b        | 49/300 [00:09<00:35,  7.14it/s]\n 39%|",
      "human_version": "0.10.1.dev296+geefbf4a68",
      "human_install_method": "wheel",
      "human_raw": "INFO 12-30 11:03:18 [__init__.py:241] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, no_stream=False, max_concurrency=None, model='Qwen/Qwen3-30B-A3B-FP8', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf RPS.\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  10.39     \nTotal input tokens:                      153250    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              28.87     \nOutput token throughput (tok/s):         3695.62   \nTotal Token throughput (tok/s):          18444.43  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2649.91   \nMedian TTFT (ms):                        2548.12   \nP99 TTFT (ms):                           5881.76   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          58.81     \nMedian TPOT (ms):                        59.29     \nP99 TPOT (ms):                           73.29     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           58.81     \nMedian ITL (ms):                         28.85     \nP99 ITL (ms):                            887.07    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/opt/vllm-commit/benchmarks/benchmark_serving.py:1299: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:09<49:42,  9.98s/it]\n 17%|\u2588\u258b        | 50/300 [00:10<00:35,  7.00it/s]\n 38%|",
      "commit": "eefbf4a6",
      "full_commit": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
      "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9",
      "model": "Qwen/Qwen3-30B-A3B-FP8",
      "subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Ker",
      "has_agent_patch": true
    }
  ]
}