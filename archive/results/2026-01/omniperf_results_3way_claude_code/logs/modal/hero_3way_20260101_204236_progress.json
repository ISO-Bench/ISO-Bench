{
  "timestamp": "20260101_204236",
  "total_commits": 81,
  "completed": 41,
  "success_count": 6,
  "error_count": 35,
  "results": [
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 1331.7060970666603
      },
      "human_metrics": {
        "latency_avg": 1323.820436133345
      },
      "agent_metrics": {
        "latency_avg": 1329.9196108666706
      },
      "human_improvement": {
        "latency_avg": 0.5921472425999162
      },
      "agent_improvement": {
        "latency_avg": 0.13415018553453711
      },
      "agent_vs_human": {
        "latency_avg": -0.4607252288037025
      },
      "error": null,
      "server_logs": null,
      "duration_s": 755.4508941173553,
      "perf_command": "VLLM_USE_V1=1 python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --tensor-parallel-size 1 --input-len 1000 --batch-size 32",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 01-01 20:44:34 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev379+g2a0309a6",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-01 20:45:20 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:45:22 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:45:32 config.py:520] This model ",
      "human_version": "INFO 01-01 20:48:52 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev380+gfa63e710",
      "human_install_method": "wheel",
      "human_raw": "INFO 01-01 20:49:36 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:49:38 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:49:48 config.py:520] This model ",
      "agent_raw": "INFO 01-01 20:53:44 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:53:46 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:53:56 config.py:520] This model ",
      "commit": "fa63e710",
      "full_commit": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
      "parent_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[V1][Perf] Reduce scheduling overhead in model run",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 1486.6399703000034,
        "throughput": 204.5
      },
      "human_metrics": {
        "latency_avg": 1006.7062844333086,
        "throughput": 255.7
      },
      "agent_metrics": {},
      "human_improvement": {
        "throughput": 25.036674816625908,
        "latency_avg": 32.283114638027946
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "server_logs": null,
      "duration_s": 727.2735917568207,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "INFO 01-01 21:00:43 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-01 21:01:33 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-01 21:01:44 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "human_version": "INFO 01-01 21:03:28 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev365+g6dd94dbe",
      "human_install_method": "wheel",
      "human_raw": "INFO 01-01 21:04:13 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-01 21:04:24 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "agent_raw": "INFO 01-01 21:07:37 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-01 21:07:49 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
      "agent_error": "Agent benchmark produced no metrics",
      "commit": "6dd94dbe",
      "full_commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
      "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[perf] fix perf regression from #12253 (#12380)",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 155.1742353439331,
      "perf_command": "python benchmarks/benchmark_serving.py --backend vllm --model meta-llama/Llama-3.2-1B-Instruct --dataset-name sharegpt --num-prompts 6000 --request-rate inf --max-concurrency 400",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "INFO 01-01 21:08:23 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev337+g7206ce4c",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-01 21:10:23 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path=None, max_concurrency=400, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=6000, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1248, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 821, in main\n    input_requests = sample_sharegpt_requests(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 97, in sample_sharegpt_requests\n    with open(dataset_path, encoding='utf-8') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n",
      "commit": "aea94362",
      "full_commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
      "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691",
      "model": "meta-llama/Llama-3.2-1B-Instruct",
      "subject": "[Frontend][V1] Online serving performance improvem",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 1145.21,
        "ttft_median": 1103.73,
        "ttft_p99": 1921.62,
        "tpot_mean": 35.59,
        "tpot_median": 33.85,
        "tpot_p99": 72.67,
        "itl_mean": 45.87,
        "itl_median": 30.05,
        "itl_p99": 429.74
      },
      "human_metrics": {
        "ttft_mean": 1031.57,
        "ttft_median": 1037.16,
        "ttft_p99": 1775.55,
        "tpot_mean": 31.13,
        "tpot_median": 29.46,
        "tpot_p99": 80.48,
        "itl_mean": 39.84,
        "itl_median": 25.85,
        "itl_p99": 263.58
      },
      "agent_metrics": {
        "ttft_mean": 1056.14,
        "ttft_median": 1036.15,
        "ttft_p99": 1811.85,
        "tpot_mean": 30.92,
        "tpot_median": 29.82,
        "tpot_p99": 59.59,
        "itl_mean": 40.08,
        "itl_median": 25.15,
        "itl_p99": 297.41
      },
      "human_improvement": {
        "ttft_mean": 9.923070877830275,
        "ttft_median": 6.031366366774478,
        "ttft_p99": 7.601398819745836,
        "tpot_mean": 12.53161000280979,
        "itl_mean": 13.145846958796586
      },
      "agent_improvement": {
        "ttft_mean": 7.777612839566536,
        "ttft_median": 6.122874253667104,
        "ttft_p99": 5.712367689761763,
        "tpot_mean": 13.121663388592305,
        "itl_mean": 12.622629169391757
      },
      "agent_vs_human": {
        "ttft_mean": -2.3818063728103924,
        "ttft_median": 0.0973813105017539,
        "ttft_p99": -2.0444369350342124,
        "tpot_mean": 0.6745904272405953,
        "itl_mean": -0.6024096385542039
      },
      "error": null,
      "server_logs": null,
      "duration_s": 435.72700119018555,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.6.post2.dev52+g2f385183",
      "baseline_install_method": "wheel",
      "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  5.54      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11217     \nRequest throughput (req/s):              17.15     \nOutput token throughput (tok/s):         2024.57   \nTotal Token throughput (tok/s):          10803.67  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1145.21   \nMedian TTFT (ms):                        1103.73   \nP99 TTFT (ms):                           1921.62   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          35.59     \nMedian TPOT (ms):                        33.85     \nP99 TPOT (ms):                           72.67     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           45.87     \nMedian ITL (ms):                         30.05     \nP99 ITL (ms):                            429.74    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:23,  1.19it/s]\n  3%|\u258e         | 3/100 [00:01<00:41,  2.31it/s]\n  4%|\u258d         | 4/100 [00:01<00:31,  3.06it/s]\n  5%|\u258c         | 5/100 [00:03<01:33,  1.02it/s]\n  6%|\u258c         | 6/100 [00:04<01:11,  1.31it/s]\n  7%|\u258b         | 7/100 [00:04<01:11,  1.29it/s]\n  9%|\u2589         | 9/100 [00:05<00:39,  2.31it/s]\n 25%|\u2588\u2588\u258c       | 25/100 [00:05<00:05, 13.72it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:05<00:02, 27.89it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 63/100 [00:05<00:00, 48.17it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 86/100 [00:05<00:00, 73.65it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 18.05it/s]\n",
      "human_version": "0.6.6.post2.dev53+gb55ed6ef",
      "human_install_method": "wheel",
      "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  4.86      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11222     \nRequest throughput (req/s):              19.56     \nOutput token throughput (tok/s):         2310.42   \nTotal Token throughput (tok/s):          12324.58  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1031.57   \nMedian TTFT (ms):                        1037.16   \nP99 TTFT (ms):                           1775.55   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          31.13     \nMedian TPOT (ms):                        29.46     \nP99 TPOT (ms):                           80.48     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           39.84     \nMedian ITL (ms):                         25.85     \nP99 ITL (ms):                            263.58    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:38,  2.57it/s]\n  2%|\u258f         | 2/100 [00:00<00:49,  1.96it/s]\n  3%|\u258e         | 3/100 [00:01<00:48,  2.01it/s]\n  4%|\u258d         | 4/100 [00:01<00:42,  2.28it/s]\n  5%|\u258c         | 5/100 [00:03<01:29,  1.06it/s]\n  6%|\u258c         | 6/100 [00:03<01:03,  1.48it/s]\n  8%|\u258a         | 8/100 [00:03<00:35,  2.61it/s]\n 10%|\u2588         | 10/100 [00:04<00:28,  3.20it/s]\n 24%|\u2588\u2588\u258d       | 24/100 [00:04<00:05, 14.96it/s]\n 40%|\u2588\u2588\u2588\u2588      | 40/100 [00:04<00:01, 30.68it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 61/100 [00:04<00:00, 54.06it/s]\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 90/100 [00:04<00:00, 90.24it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 20.59it/s]\n",
      "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  4.93      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11265     \nRequest throughput (req/s):              19.25     \nOutput token throughput (tok/s):         2282.95   \nTotal Token throughput (tok/s):          12140.28  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1056.14   \nMedian TTFT (ms):                        1036.15   \nP99 TTFT (ms):                           1811.85   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30.92     \nMedian TPOT (ms):                        29.82     \nP99 TPOT (ms):                           59.59     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           40.08     \nMedian ITL (ms):                         25.15     \nP99 ITL (ms):                            297.41    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:09,  1.42it/s]\n  2%|\u258f         | 2/100 [00:01<00:54,  1.79it/s]\n  3%|\u258e         | 3/100 [00:01<00:52,  1.87it/s]\n  4%|\u258d         | 4/100 [00:01<00:35,  2.74it/s]\n  5%|\u258c         | 5/100 [00:03<01:20,  1.17it/s]\n  6%|\u258c         | 6/100 [00:03<01:00,  1.54it/s]\n  7%|\u258b         | 7/100 [00:03<00:44,  2.11it/s]\n  8%|\u258a         | 8/100 [00:04<00:46,  1.98it/s]\n 26%|\u2588\u2588\u258c       | 26/100 [00:04<00:04, 16.49it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:04<00:01, 31.47it/s]\n 57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 57/100 [00:04<00:00, 46.56it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 79/100 [00:04<00:00, 72.63it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 20.27it/s]\n",
      "commit": "b55ed6ef",
      "full_commit": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
      "parent_commit": "2f385183f35497e030ef22c9820d83b83bc4f6db",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)",
      "has_agent_patch": true
    },
    {
      "status": "version_bug",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "vLLM 0.6.4.post2.dev375+gd263bd9d has known port binding bug (issue #8791) - serving benchmarks not supported",
      "server_logs": null,
      "duration_s": 27.686750173568726,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev375+gd263bd9d",
      "baseline_install_method": "wheel",
      "commit": "25ebed2f",
      "full_commit": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
      "parent_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[V1][Minor] Cache np arange to reduce input prepar",
      "has_agent_patch": true
    },
    {
      "status": "version_bug",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "vLLM 0.6.4.post2.dev368+g6d917d0e has known port binding bug (issue #8791) - serving benchmarks not supported",
      "server_logs": null,
      "duration_s": 26.96428608894348,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --enable-prefix-caching",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev368+g6d917d0e",
      "baseline_install_method": "wheel",
      "commit": "88693683",
      "full_commit": "886936837ca89e5645bc1f71cc0e1492b65b1590",
      "parent_commit": "6d917d0eebd03990edf2443780a5f2506026ea78",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[Performance][Core] Optimize the performance of ev",
      "has_agent_patch": true
    },
    {
      "status": "version_bug",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "vLLM 0.6.4.post2.dev330+g1da8f0e1 has known port binding bug (issue #8791) - serving benchmarks not supported",
      "server_logs": null,
      "duration_s": 33.359105587005615,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev330+g1da8f0e1",
      "baseline_install_method": "wheel",
      "commit": "f092153f",
      "full_commit": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
      "parent_commit": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[V1] Use more persistent buffers to optimize input",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 1715.5026342333294,
        "throughput": 9824.4
      },
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "[Errno 2] No such file or directory: '/cache/build_edc4fa31888b4a41060acb7b16250540f051ad59'",
      "server_logs": null,
      "duration_s": 1151.3555328845978,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
      "benchmark_mode": "standalone",
      "patch_type": "c_cuda",
      "baseline_version": "0.6.4.post2.dev277+gedc4fa31",
      "baseline_install_method": "wheel",
      "baseline_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-01 21:36:37 config.py:405] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-01 21:36:37 arg_utils.py:1068] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you en",
      "commit": "3b61cb45",
      "full_commit": "3b61cb450d899dc423feb264c297d4d18d701678",
      "parent_commit": "edc4fa31888b4a41060acb7b16250540f051ad59",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[V1] Further reduce CPU overheads in flash-attn (#",
      "has_agent_patch": true
    },
    {
      "status": "version_bug",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "vLLM 0.6.4.post2.dev218+g3257d449 has known port binding bug (issue #8791) - serving benchmarks not supported",
      "server_logs": null,
      "duration_s": 26.236889839172363,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-3B-Instruct --guided-decoding-backend xgrammar",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev218+g3257d449",
      "baseline_install_method": "wheel",
      "commit": "9323a315",
      "full_commit": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
      "parent_commit": "3257d449fa0fd3e05aa20cc8c5fff79ad101984f",
      "model": "meta-llama/Llama-3.2-3B-Instruct",
      "subject": "[Core][Performance] Add XGrammar support for guide",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 1721.1104121333317,
        "throughput": 9828.9
      },
      "human_metrics": {
        "latency_avg": 1722.0939366666623,
        "throughput": 9825.5
      },
      "agent_metrics": null,
      "human_improvement": {
        "throughput": -0.03459186684165712,
        "latency_avg": -0.05714476691309711
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "server_logs": null,
      "duration_s": 346.8092987537384,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.post2.dev181+g5fc5ce0f",
      "baseline_install_method": "wheel",
      "baseline_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-01 21:40:03 __init__.py:42] No plugins found.\nINFO 01-01 21:40:14 config.py:373] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-01 21:40:14 arg_utils.py:1057] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some",
      "human_version": "0.6.4.post2.dev182+g8c1e77fb",
      "human_install_method": "wheel",
      "human_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-01 21:42:44 __init__.py:42] No plugins found.\nINFO 01-01 21:42:54 config.py:373] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-01 21:42:54 arg_utils.py:1057] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some",
      "agent_error": "Patch contains only non-Python files (skipped: ['CMakeLists.txt'])",
      "commit": "8c1e77fb",
      "full_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
      "parent_commit": "5fc5ce0fe45f974fc8840175e8321652238400f0",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Kernel] Update vllm-flash-attn version to reduce ",
      "has_agent_patch": true
    },
    {
      "status": "baseline_failed",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "server_logs": null,
      "duration_s": 102.24463534355164,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "baseline_version": "0.6.4.dev22+g5b8a1fde.d20241016",
      "baseline_install_method": "wheel",
      "baseline_raw": "Namespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model='[ngram]', num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-01 21:46:22 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n`torch_dtype` is deprecated! Use `dtype` instead!\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 1247, in maybe_create_spec_config\n    raise ValueError(f\"{ngram_prompt_lookup_max=} must be > 0\")\nValueError: ngram_prompt_lookup_max=None must be > 0\n",
      "commit": "83450458",
      "full_commit": "83450458339b07765b0e72a822e5fe93eeaf5258",
      "parent_commit": "5b8a1fde84224e24ec121e0dc149d775330d911b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Performance][Spec Decode] Optimize ngram lookup p",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 342.35026383399963,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --dataset-name sharegpt --multi-step",
      "install_method": "docker_fallback",
      "baseline_raw": "\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT] [--dataset DATASET]\n                            [--dataset-name {sharegpt,sonnet,random}]\n                            [--dataset-path DATASET_PATH] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--request-rate REQUEST_RATE] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm] [--profile]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME]\n                            [--percentile-metrics PERCENTILE_METRICS]\n                            [--metric-percentiles METRIC_PERCENTILES]\nbenchmark_serving.py: error: unrecognized arguments: --multi-step\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6d646d08",
      "full_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
      "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[Core] Optimize Async + Multi-step (#8050)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 2584.08620262146,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "Error: Command 'python /tmp/vllm-checkout/benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --host 127.0.0.1 --port 29001 --dataset-name random --random-input-len 512 --random-output-len 128' timed out after 1800 seconds",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6e36f4fa",
      "full_commit": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
      "parent_commit": "dd2a6a82e3f41b4673b1dbb24b2e99230ea96981",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "improve chunked prefill performance",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "throughput": 54.71
      },
      "human_metrics": {
        "throughput": 55.5
      },
      "agent_metrics": {},
      "human_improvement": {
        "throughput": 1.4439773350392966
      },
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": null,
      "duration_s": 522.2120008468628,
      "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
      "install_method": "docker_fallback",
      "baseline_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:40:58 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:41:01 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:41:01 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:41:01 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:41:23 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:41:28 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:41:29 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:41:29 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-01 14:41:52 model_runner.py:1331] Graph capturing finished in 23 secs.\nThroughpu",
      "human_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:42:31 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:42:33 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:42:33 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:42:33 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:42:34 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:42:39 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:42:40 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:42:40 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-01 14:43:00 model_runner.py:1331] Graph capturing finished in 19 secs.\nThroughpu",
      "agent_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:43:39 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:43:41 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:43:41 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:43:41 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:43:42 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:43:47 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:43:48 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:43:48 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: U",
      "agent_error": "Agent benchmark produced no metrics",
      "commit": "ce6bf3a2",
      "full_commit": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
      "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
      "model": "google/gemma-2b",
      "subject": "[torch.compile] avoid Dynamo guard evaluation over",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 549.3038637638092,
      "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048",
      "install_method": "docker_fallback",
      "baseline_raw": "\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT] [--dataset DATASET]\n                            [--dataset-name {sharegpt,sonnet,random}]\n                            [--dataset-path DATASET_PATH] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--request-rate REQUEST_RATE] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm] [--profile]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME]\nbenchmark_serving.py: error: unrecognized arguments: --max-num-batched-tokens 2048\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "e3580537",
      "full_commit": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
      "parent_commit": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "subject": "[Performance] Enable chunked prefill and prefix ca",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 2253.7604298591614,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "Error: Command 'python /tmp/vllm-checkout/benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --host 127.0.0.1 --port 29001 --dataset-name random --random-input-len 512 --random-output-len 128' timed out after 1800 seconds",
      "human_raw": "",
      "agent_raw": "",
      "commit": "fc7b8d1e",
      "full_commit": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
      "parent_commit": "67abdbb42fdbb59c274130368981c0d0ac3539e3",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Performance] e2e overheads reduction: Small follo",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 2218.1902825832367,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
      "install_method": "docker_fallback",
      "baseline_raw": "Error: Command 'python /tmp/vllm-checkout/benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --host 127.0.0.1 --port 29001 --dataset-name random --random-input-len 512 --random-output-len 128 --num-prompts 100' timed out after 1800 seconds",
      "human_raw": "",
      "agent_raw": "",
      "commit": "660470e5",
      "full_commit": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
      "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize evictor-v2 performance (#7193)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 1015.4045822620392,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: gits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 23, in <module>\n    from vllm.entrypoints.openai.cli_args import make_arg_parser\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/cli_args.py\", line 12, in <module>\n    from vllm.entrypoints.openai.serving_engine import (LoRAModulePath,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_engine.py\", line 28, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "6ce01f30",
      "full_commit": "6ce01f30667bbae33f112152e07a3b66b841078f",
      "parent_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5",
      "model": "meta-llama/Meta-Llama-3-8B",
      "subject": "[Performance] Optimize `get_seqs` (#7051)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 1019.6412377357483,
      "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 35, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 28, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "89a84b0b",
      "full_commit": "89a84b0bb7b30706a02836234a94493ea8f780bf",
      "parent_commit": "084a01fd3544557990f8af8af6fd3c1185bae848",
      "model": "Qwen/Qwen1.5-0.5B",
      "subject": "[Core] Use array to speedup padding (#6779)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 946.0534055233002,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 26, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "9ed82e70",
      "full_commit": "9ed82e7074a18e25680ab106fc846364ad97bc00",
      "parent_commit": "51f8aa90ad409cc77bfab208be7f5907bf7d5330",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Misc] Small perf improvements (#6520)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 854.0737025737762,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "3476ed08",
      "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
      "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize block_manager_v2 vs block_manager_",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 811.2599787712097,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 33, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "7c01f706",
      "full_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539",
      "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize `SequenceStatus.is_finished` by sw",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 827.849681854248,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 27, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "80aa7e91",
      "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
      "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Hardware][Intel] Optimize CPU backend and add mor",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 826.0435643196106,
      "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 25, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "8d75fe48",
      "full_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
      "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "subject": "[Kernel] Switch fp8 layers to use the CUTLASS kern",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 828.7949500083923,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 20, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 5, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "8bc68e19",
      "full_commit": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
      "parent_commit": "0fca3cdcf265cd375bca684d951702b6b7adf65a",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Frontend] [Core] perf: Automatically detect vLLM-",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:4",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 886.5852365493774,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-70B --dtype float8 --input-len 1000 --output-len 50",
      "install_method": "docker_fallback",
      "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 26, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 20, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 5, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "379da6dc",
      "full_commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
      "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
      "model": "meta-llama/Meta-Llama-3-70B",
      "subject": "[Kernel] [FP8] Improve FP8 linear layer performanc",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline benchmark produced no metrics",
      "duration_s": 334.1448130607605,
      "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.1-8B-Instruct --input-len 256 --output-len 256",
      "install_method": "docker_fallback",
      "baseline_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None)\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_throughput.py\", line 387, in <module>\n    main(args)\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_throughput.py\", line 221, in main\n    elapsed_time = run_vllm(\n  File \"/tmp/vllm-checkout/benchmarks/benchmark_throughput.py\", line 85, in run_vllm\n    llm = LLM(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py\", line 123, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 272, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py\", line 520, in create_engine_config\n    model_config = ModelConfig(\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 123, in __init__\n    self.max_model_len = _get_and_verify_max_len(self.hf_text_config,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/config.py\", line 1125, in _get_and_verify_max_len\n    if rope_scaling is not None and rope_scaling[\"type\"] != \"su\":\nKeyError: 'type'\n",
      "human_raw": "",
      "agent_raw": "",
      "commit": "d7740ea4",
      "full_commit": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
      "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Optimize sampler get_logprobs (#4594)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 36fb68f94792",
      "server_logs": null,
      "duration_s": 31.835895538330078,
      "perf_command": "python benchmarks/benchmark_throughput.py --model nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "2a052011",
      "full_commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
      "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
      "model": "nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
      "subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 3d925165f2b1",
      "server_logs": null,
      "duration_s": 24.063607454299927,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "ad8d696a",
      "full_commit": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
      "parent_commit": "3d925165f2b18379640a63fbb42de95440d63b64",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] Scheduler perf fix (#4270)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 95baec828f3e",
      "server_logs": null,
      "duration_s": 22.797927379608154,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "2f192835",
      "full_commit": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
      "parent_commit": "95baec828f3ee046074dace1d88202a920b7dc15",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[Core] latency optimization (#3890)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Could not find vLLM installation",
      "duration_s": 526.5989224910736,
      "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-2-70b-hf --dtype float16 --tensor-parallel-size 1",
      "install_method": "docker_fallback",
      "baseline_raw": "",
      "human_raw": "",
      "agent_raw": "",
      "commit": "b6d10354",
      "full_commit": "b6d103542c654fb63013a1e45a586d654ae36a2a",
      "parent_commit": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447",
      "model": "meta-llama/Llama-2-70b-hf",
      "subject": "[Kernel] Layernorm performance optimization (#3662",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 64172a976c8d",
      "server_logs": null,
      "duration_s": 31.203507661819458,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "3a243095",
      "full_commit": "3a243095e5e7b655b63ab08fbd5936cb40850415",
      "parent_commit": "64172a976c8d975b3aec946f1675716d2532d94f",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "Optimize `_get_ranks` in Sampler (#3623)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Could not find vLLM installation",
      "duration_s": 268.06788635253906,
      "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
      "install_method": "docker_fallback",
      "baseline_raw": "",
      "human_raw": "",
      "agent_raw": "",
      "commit": "bfdb1ba5",
      "full_commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
      "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "subject": "[Core] Improve detokenization performance for pref",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Could not find vLLM installation",
      "duration_s": 162.21688961982727,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "install_method": "docker_fallback",
      "baseline_raw": "",
      "human_raw": "",
      "agent_raw": "",
      "commit": "cf2f084d",
      "full_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
      "parent_commit": "f721096d48a7e3b98dffcb9b400bf58989cef64d",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "Dynamic scheduler delay to improve ITL performance",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 20478c4d3abc",
      "server_logs": null,
      "duration_s": 32.59974122047424,
      "perf_command": "python benchmarks/benchmark_throughput.py --model huggyllama/llama-7b --dataset-name sharegpt --num-prompts 2000",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "9474e89b",
      "full_commit": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
      "parent_commit": "20478c4d3abcd0aa8a1d9ace9c76ea3a2e04cb5e",
      "model": "huggyllama/llama-7b",
      "subject": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to blo",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:8",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Could not find vLLM installation",
      "duration_s": 20.69732093811035,
      "perf_command": "python benchmarks/benchmark_throughput.py --model mistralai/Mixtral-8x7B-v0.1 --tensor-parallel-size 8",
      "install_method": "docker_fallback",
      "baseline_raw": "",
      "human_raw": "",
      "agent_raw": "",
      "commit": "21d93c14",
      "full_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
      "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
      "model": "mistralai/Mixtral-8x7B-v0.1",
      "subject": "Optimize Mixtral with expert parallelism (#2090)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 6368e777a8ea",
      "server_logs": null,
      "duration_s": 21.716681241989136,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "ec3b5ce9",
      "full_commit": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
      "parent_commit": "6368e777a8ead7fb62054d3779c6237361ec0d86",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "Improve detokenization performance (#1338)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for 7a7929abe8e2",
      "server_logs": null,
      "duration_s": 21.025588274002075,
      "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
      "benchmark_mode": "standalone",
      "patch_type": "python_only",
      "commit": "c45f3c3a",
      "full_commit": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
      "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
      "model": "facebook/opt-13b",
      "subject": "Optimize tensor parallel execution speed (#17)",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {},
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Baseline install failed: No wheel available and no ancestor wheel found for b56b6ca0d650",
      "server_logs": null,
      "duration_s": 19.708415985107422,
      "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "commit": "d4bc1a4d",
      "full_commit": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
      "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
      "model": "facebook/opt-125m",
      "subject": "Add unoptimized OPT Attention",
      "has_agent_patch": true
    },
    {
      "status": "success",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "ttft_mean": 838.15,
        "ttft_median": 869.87,
        "ttft_p99": 1340.78,
        "tpot_mean": 19.56,
        "tpot_median": 17.09,
        "tpot_p99": 125.21,
        "itl_mean": 16.71,
        "itl_median": 12.75,
        "itl_p99": 196.23
      },
      "human_metrics": {
        "ttft_mean": 811.07,
        "ttft_median": 848.89,
        "ttft_p99": 1333.52,
        "tpot_mean": 20.41,
        "tpot_median": 16.38,
        "tpot_p99": 191.76,
        "itl_mean": 16.47,
        "itl_median": 12.52,
        "itl_p99": 199.49
      },
      "agent_metrics": {
        "ttft_mean": 835.48,
        "ttft_median": 869.46,
        "ttft_p99": 1339.83,
        "tpot_mean": 18.64,
        "tpot_median": 16.93,
        "tpot_p99": 35.83,
        "itl_mean": 16.69,
        "itl_median": 12.86,
        "itl_p99": 194.1
      },
      "human_improvement": {
        "ttft_mean": 3.230925252043182,
        "ttft_median": 2.4118546449469482,
        "ttft_p99": 0.5414758573367735,
        "tpot_mean": -4.345603271983648,
        "itl_mean": 1.4362657091562057
      },
      "agent_improvement": {
        "ttft_mean": 0.3185587305374884,
        "ttft_median": 0.04713347971535611,
        "ttft_p99": 0.07085427885261157,
        "tpot_mean": 4.703476482617578,
        "itl_mean": 0.11968880909634692
      },
      "agent_vs_human": {
        "ttft_mean": -3.0096045963973475,
        "ttft_median": -2.4231643675859122,
        "ttft_p99": -0.47318375427439746,
        "tpot_mean": 8.672219500244976,
        "itl_mean": -1.3357619914997112
      },
      "error": null,
      "server_logs": null,
      "duration_s": 564.3076100349426,
      "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
      "benchmark_mode": "serving",
      "patch_type": "python_only",
      "baseline_version": "0.10.1.dev310+g067c34a15",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-02 02:50:14 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b9d3fa28f40>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:50:22 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.01      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12340     \nRequest throughput (req/s):              33.18     \nOutput token throughput (tok/s):         4094.73   \nTotal Token throughput (tok/s):          21051.01  \n---------------Time to First Token----------------\nMean TTFT (ms):                          838.15    \nMedian TTFT (ms):                        869.87    \nP99 TTFT (ms):                           1340.78   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.56     \nMedian TPOT (ms):                        17.09     \nP99 TPOT (ms):                           125.21    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.71     \nMedian ITL (ms):                         12.75     \nP99 ITL (ms):                            196.23    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:14,  1.3",
      "human_version": "0.10.1.dev311+g58eee5f2e",
      "human_install_method": "wheel",
      "human_raw": "INFO 01-02 02:53:13 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b28c4838f40>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:53:20 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.96      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12301     \nRequest throughput (req/s):              33.81     \nOutput token throughput (tok/s):         4159.56   \nTotal Token throughput (tok/s):          21438.92  \n---------------Time to First Token----------------\nMean TTFT (ms):                          811.07    \nMedian TTFT (ms):                        848.89    \nP99 TTFT (ms):                           1333.52   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.41     \nMedian TPOT (ms):                        16.38     \nP99 TPOT (ms):                           191.76    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.47     \nMedian ITL (ms):                         12.52     \nP99 ITL (ms):                            199.49    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:13,  1.3",
      "agent_raw": "INFO 01-02 02:55:33 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2aed2ff04fe0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:55:39 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.01      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12340     \nRequest throughput (req/s):              33.24     \nOutput token throughput (tok/s):         4102.14   \nTotal Token throughput (tok/s):          21089.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          835.48    \nMedian TTFT (ms):                        869.46    \nP99 TTFT (ms):                           1339.83   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          18.64     \nMedian TPOT (ms):                        16.93     \nP99 TPOT (ms):                           35.83     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.69     \nMedian ITL (ms):                         12.86     \nP99 ITL (ms):                            194.10    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:13,  1.3",
      "commit": "58eee5f2",
      "full_commit": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
      "parent_commit": "067c34a1559400e956311f067ddd185f54207a2b",
      "model": "meta-llama/Llama-3.1-8B-Instruct",
      "subject": "[PERF] Use faster way of decode in tokenizer: avoi",
      "has_agent_patch": true
    },
    {
      "status": "error",
      "gpu_config": "H100:1",
      "baseline_metrics": {
        "latency_avg": 2029.7980098666696
      },
      "human_metrics": {},
      "agent_metrics": null,
      "human_improvement": {},
      "agent_improvement": null,
      "agent_vs_human": null,
      "error": "Command '['uv', 'pip', 'install', '--system', '-e', '.', '--no-build-isolation']' timed out after 2400 seconds",
      "server_logs": null,
      "duration_s": 11598.336044311523,
      "perf_command": "python benchmarks/benchmark_latency.py --model Qwen/Qwen3-30B-A3B-FP8 --batch-size 32 --input-len 512 --output-len 128",
      "benchmark_mode": "standalone",
      "patch_type": "c_cuda",
      "baseline_version": "0.10.1.dev295+g88faa466d",
      "baseline_install_method": "wheel",
      "baseline_raw": "INFO 01-02 04:27:45 [__init__.py:241] Automatically detected platform cuda.\nINFO 01-02 04:27:50 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-30B-A3B-FP8', 'enable_prefix_caching': False, 'enable_lora': None}\nINFO 01-02 04:28:01 [config.py:723] Resolved architecture: Qwen3MoeForCausalLM\nINFO 01-02 04:28:01 [config.py:1756] Using max model len 40960\nINFO 01-02 04:28:03 [config.py:2582] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 01-02 04:28:10 [__init__.py:241] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_0 pid=107)\u001b[0;0m INFO 01-02 04:28:15 [core.py:619] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_0 pid=107)\u001b[0;0m INFO 01-02 04:28:15 [core.py:71] Initializing a V1 LLM engine (v0.10.1.dev295+g88faa466d) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_0 pid=107)\u001b[0;0m INFO 01-02 04:28:18 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_0 pid=107)\u001b[0;0m WARNING 01-02 04:28:18 [topk_topp_sampler.py:60] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_0 pid=107)\u001b[0;0m INFO 01-02 04:28:18 [gpu_model_runner.py",
      "commit": "eefbf4a6",
      "full_commit": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
      "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9",
      "model": "Qwen/Qwen3-30B-A3B-FP8",
      "subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Ker",
      "has_agent_patch": true
    }
  ]
}