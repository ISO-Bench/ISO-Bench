2026-01-03 21:03:18,241 | INFO | ================================================================================
2026-01-03 21:03:18,241 | INFO | HERO BENCHMARK RUN
2026-01-03 21:03:18,241 | INFO | Start time: 2026-01-03T21:03:18.241490
2026-01-03 21:03:18,241 | INFO | Log file: omniperf_results/hero_run_logs/hero_run_20260103_210318.log
2026-01-03 21:03:18,241 | INFO | ================================================================================
2026-01-03 21:03:18,241 | INFO | Loaded 59 commits from plan
2026-01-03 21:03:18,241 | INFO | 
Loading HuggingFace dataset...
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-03 21:03:20,769 | INFO | 
*** PARALLEL MODE: 15 concurrent benchmarks ***
2026-01-03 21:03:20,769 | INFO | Spawning Modal containers in parallel...
2026-01-03 21:03:20,769 | INFO | [015069b0] Fixing model name: Qwen/Qwen3-7B-Instruct -> mergekit-community/Qwen3-7B-Instruct
2026-01-03 21:03:20,770 | INFO | [0ec82edd] Fixing model name: Qwen/Qwen3-30B-A3B -> Qwen/Qwen3-30B-A3B-Instruct-2507
2026-01-03 21:03:20,772 | INFO | [0d243f2a] Starting benchmark | Model: mistralai/Mixtral-8x7B-Instruct-v0.1 | GPU: H100:2
2026-01-03 21:03:20,773 | INFO | [21d93c14] Starting benchmark | Model: mistralai/Mixtral-8x7B-v0.1 | GPU: H100:8
2026-01-03 21:03:20,773 | INFO | [015069b0] Starting benchmark | Model: mergekit-community/Qwen3-7B-Instruct | GPU: H100:1
2026-01-03 21:03:20,773 | INFO | [22d33bac] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,774 | INFO | [0ec82edd] Starting benchmark | Model: Qwen/Qwen3-30B-A3B-Instruct-2507 | GPU: H100:2
2026-01-03 21:03:20,775 | INFO | [22dd9c27] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,775 | INFO | [296f927f] Starting benchmark | Model: ibm-ai-platform/Bamba-9B-v2 | GPU: H100:1
2026-01-03 21:03:20,776 | INFO | [2a052011] Starting benchmark | Model: nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8 | GPU: H100:2
2026-01-03 21:03:20,777 | INFO | [3092375e] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,779 | INFO | [2deb029d] Starting benchmark | Model: neuralmagic/Meta-Llama-3-8B-Instruct-FP8 | GPU: H100:1
2026-01-03 21:03:20,779 | INFO | [2f192835] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,779 | INFO | [35fad35a] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,780 | INFO | [3a243095] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,780 | INFO | [3476ed08] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:03:20,780 | INFO | [379da6dc] Starting benchmark | Model: meta-llama/Llama-3-70B | GPU: H100:4
2026-01-03 21:03:20,972 | INFO |   Baseline wheel: 88f6ba32
2026-01-03 21:03:20,972 | INFO |   Baseline wheel: f1c85201
2026-01-03 21:03:20,973 | INFO |   Baseline wheel: fbefc8a7
2026-01-03 21:03:20,973 | INFO |   Baseline wheel: b0e96aae
2026-01-03 21:03:20,973 | INFO |   Human wheel: 0d243f2a
2026-01-03 21:03:20,973 | INFO |   Baseline wheel: 005ae9be
2026-01-03 21:03:20,973 | INFO |   Human wheel: 21d93c14
2026-01-03 21:03:20,973 | INFO |   Human wheel: 015069b0
2026-01-03 21:03:20,973 | INFO |   Baseline wheel: a6d795d5
2026-01-03 21:03:20,973 | INFO |   Human wheel: 22d33bac
2026-01-03 21:03:20,973 | INFO |   Baseline wheel: 0032903a
2026-01-03 21:03:20,973 | INFO |   GPU config: H100:2
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 36fb68f9
2026-01-03 21:03:20,974 | INFO |   Human wheel: 0ec82edd
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 3cd91dc9
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 029c71de
2026-01-03 21:03:20,974 | INFO |   GPU config: H100:8
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 95baec82
2026-01-03 21:03:20,974 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 733e7c9e
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 64172a97
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: 54600709
2026-01-03 21:03:20,974 | INFO |   Baseline wheel: ebce310b
2026-01-03 21:03:20,975 | INFO |   Human wheel: 22dd9c27
2026-01-03 21:03:20,975 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,975 | INFO |   Human wheel: 296f927f
2026-01-03 21:03:20,975 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1...
Running 3-way benchmark on Modal with H100:2...
2026-01-03 21:03:20,975 | INFO |   Human wheel: 2a052011
2026-01-03 21:03:20,975 | INFO |   GPU config: H100:2
2026-01-03 21:03:20,975 | INFO |   Human wheel: 3092375e
2026-01-03 21:03:20,975 | INFO |   Human wheel: 2deb029d
2026-01-03 21:03:20,975 | INFO |   Command preview: python benchmarks/benchmark_throughput.py --model mistralai/Mixtral-8x7B-v0.1 --tensor-parallel-size...
Running 3-way benchmark on Modal with H100:8...
2026-01-03 21:03:20,975 | INFO |   Human wheel: 2f192835
2026-01-03 21:03:20,975 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mergekit-community/Qwen3-7B-Instruct --dataset-name s...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,975 | INFO |   Human wheel: 35fad35a
2026-01-03 21:03:20,975 | INFO |   Human wheel: 3a243095
2026-01-03 21:03:20,975 | INFO |   Human wheel: 3476ed08
2026-01-03 21:03:20,975 | INFO |   Human wheel: 379da6dc
2026-01-03 21:03:20,975 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,976 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,976 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,976 | INFO |   GPU config: H100:2
2026-01-03 21:03:20,976 | INFO |   Command preview: vllm bench throughput --model Qwen/Qwen3-30B-A3B-Instruct-2507 --load-format dummy --input-len 1000 ...
Running 3-way benchmark on Modal with H100:2...
2026-01-03 21:03:20,976 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,976 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,977 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,977 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,977 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,977 | INFO |   GPU config: H100:1
2026-01-03 21:03:20,977 | INFO |   GPU config: H100:4
2026-01-03 21:03:20,978 | INFO |   Command preview: VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1 VLLM_USE_V1=1 python benchmarks/benchmark_latency.py --mo...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,978 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dataset-name sharegpt...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,978 | INFO |   Command preview: python benchmarks/benchmark_throughput.py --model nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8...
Running 3-way benchmark on Modal with H100:2...
2026-01-03 21:03:20,978 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,978 | INFO |   Command preview: python benchmarks/benchmark_prefix_caching.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --out...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --nu...
Running 3-way benchmark on Modal with H100:1...
2026-01-03 21:03:20,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-70B --dtype float8 --input-len 100...
Running 3-way benchmark on Modal with H100:4...
[URL CHECK] Baseline wheel URL does not exist for 95baec82, will build from source
[URL CHECK] Baseline wheel URL does not exist for f1c85201, will build from source
[URL CHECK] Baseline wheel URL does not exist for ebce310b, will build from source
[URL CHECK] Baseline wheel URL does not exist for 029c71de, will build from source
[URL CHECK] Baseline wheel URL does not exist for 36fb68f9, will build from source
[URL CHECK] Baseline wheel URL does not exist for 54600709, will build from source
[URL CHECK] Baseline wheel URL does not exist for 64172a97, will build from source
[URL CHECK] Human wheel URL does not exist for 2f192835, will build from source
[CPU PRE-BUILD] Building baseline 95baec82 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 95baec82 on CPU-only instance...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[URL CHECK] Human wheel URL does not exist for 0d243f2a, will build from source
[CPU PRE-BUILD] Building human 0d243f2a on CPU...
[ENSURE BUILD] Checking/building vLLM 0d243f2a on CPU-only instance...
[URL CHECK] Human wheel URL does not exist for 2a052011, will build from source
[CPU PRE-BUILD] Building baseline 36fb68f9 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 36fb68f9 on CPU-only instance...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[URL CHECK] Human wheel URL does not exist for 21d93c14, will build from source
[CPU PRE-BUILD] Building baseline f1c85201 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM f1c85201 on CPU-only instance...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:2...
[PARALLEL] Starting 3-way parallel benchmark with H100:2...
[PARALLEL] Spawning baseline, human in parallel...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[URL CHECK] Human wheel URL does not exist for 3a243095, will build from source
[CPU PRE-BUILD] Building baseline 64172a97 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 64172a97 on CPU-only instance...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[URL CHECK] Human wheel URL does not exist for 3476ed08, will build from source
[CPU PRE-BUILD] Building baseline 54600709 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 54600709 on CPU-only instance...
[URL CHECK] Human wheel URL does not exist for 2deb029d, will build from source
[CPU PRE-BUILD] Building baseline 029c71de on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 029c71de on CPU-only instance...
[URL CHECK] Human wheel URL does not exist for 379da6dc, will build from source
[CPU PRE-BUILD] Building baseline ebce310b on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM ebce310b on CPU-only instance...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Cache HIT: vLLM 0.7.3.dev241+g0d243f2a5.d20220101.cu124 already cached
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=url, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:2...
[PARALLEL] Starting 3-way parallel benchmark with H100:2...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] FAILED: Wheel build failed with return code 1
[CPU PRE-BUILD] WARNING: Baseline build failed: Wheel build failed with return code 1
[CPU PRE-BUILD] Building human 21d93c14 on CPU...
[ENSURE BUILD] Checking/building vLLM 21d93c14 on CPU-only instance...
[ENSURE BUILD] FAILED: Wheel build failed with return code 1
[CPU PRE-BUILD] WARNING: Human build failed: Wheel build failed with return code 1
[CPU PRE-BUILD] Wheel sources: baseline=None, human=None, agent=None
Using Docker image fallback for 21d93c14
Using Docker image fallback: anonymous/vllm-bench:21d93c140d0a
  Human commit: 21d93c140d0a97af5f0c59e660cf04bd417fd424
  Base commit: f1c8520146031a650404a6ab120ee11e91c10bed
Creating Modal Sandbox with H100 x 8...
Running 3-way benchmark in sandbox...
Writing benchmark script to sandbox (22808 chars)...
[PARALLEL] baseline completed: status=no_metrics
[PARALLEL] Waiting for human...
Exception traceback: Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/src/eval/modal_benchmark.py", line 5728, in run_3way_benchmark_docker_fallback
    f = sb.open(script_path, "w")
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/modal/sandbox.py", line 1108, in open
    return await _FileIO.create(path, mode, self._client, task_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/modal/file_io.py", line 239, in create
    await self._open_file(path, mode)
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/modal/file_io.py", line 230, in _open_file
    await self._wait(resp.exec_id)
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/modal/file_io.py", line 201, in _wait
    raise data
modal.exception.FilesystemExecutionError: request cancelled due to internal error

2026-01-03 21:05:32,283 | INFO | 
2026-01-03 21:05:32,283 | INFO | ================================================================================
2026-01-03 21:05:32,283 | INFO | [1/59] COMPLETED: 21d93c14
2026-01-03 21:05:32,283 | INFO | Subject: Optimize Mixtral with expert parallelism (#2090)
2026-01-03 21:05:32,283 | ERROR |   ❌ ERROR: Docker fallback failed: request cancelled due to internal error
2026-01-03 21:05:32,283 | INFO |   Progress: 0 success, 1 errors (1/59)
2026-01-03 21:05:32,285 | INFO | [4fb56914] Starting benchmark | Model: deepseek-ai/DeepSeek-V3-0324 | GPU: H100:8
2026-01-03 21:05:32,285 | INFO |   Baseline wheel: 0df4d9b0
2026-01-03 21:05:32,285 | INFO |   Human wheel: 4fb56914
2026-01-03 21:05:32,285 | INFO |   GPU config: H100:8
2026-01-03 21:05:32,285 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt ...
[BLOCKED] Model deepseek-ai/DeepSeek-V3-0324 is blocked (too large/unstable), skipping benchmark
2026-01-03 21:05:32,285 | INFO | 
2026-01-03 21:05:32,285 | INFO | ================================================================================
2026-01-03 21:05:32,285 | INFO | [2/59] COMPLETED: 4fb56914
2026-01-03 21:05:32,285 | INFO | Subject: [perf] Add fused MLA QKV + strided layernorm (#21116)
2026-01-03 21:05:32,286 | ERROR |   ❌ BLOCKED_MODEL: Model deepseek-ai/DeepSeek-V3-0324 is blocked: too large or unstable for reliable benchmarking
2026-01-03 21:05:32,286 | INFO |   Progress: 0 success, 2 errors (2/59)
2026-01-03 21:05:32,287 | INFO | [526de822] Starting benchmark | Model: MODEL | GPU: H100:1
2026-01-03 21:05:32,287 | INFO |   Baseline wheel: 56fe4c29
2026-01-03 21:05:32,287 | INFO |   Human wheel: 526de822
2026-01-03 21:05:32,288 | INFO |   GPU config: H100:1
2026-01-03 21:05:32,288 | INFO |   Command preview: python benchmarks/benchmark_latency.py --dtype bfloat16 --enable-chunked-prefill False --load-format...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.3.3+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 3a243095 on CPU...
[ENSURE BUILD] Checking/building vLLM 3a243095 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.4.0.post1+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 2f192835 on CPU...
[ENSURE BUILD] Checking/building vLLM 2f192835 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.4.1+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 2a052011 on CPU...
[ENSURE BUILD] Checking/building vLLM 2a052011 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.4.2+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 379da6dc on CPU...
[ENSURE BUILD] Checking/building vLLM 379da6dc on CPU-only instance...
[PARALLEL] baseline completed: status=success
[PARALLEL] Waiting for human...
[ENSURE BUILD] Built and cached: vLLM 0.3.3+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] baseline completed: status=no_metrics
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=no_metrics
2026-01-03 21:11:41,178 | INFO | 
2026-01-03 21:11:41,178 | INFO | ================================================================================
2026-01-03 21:11:41,178 | INFO | [3/59] COMPLETED: 22dd9c27
2026-01-03 21:11:41,178 | INFO | Subject: [Kernel] Optimize Prefill Attention in Unified Triton Attent
2026-01-03 21:11:41,178 | ERROR |   ❌ BASELINE_FAILED: baseline: Benchmark produced no metrics. Output tail: f Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 17, in <module>
    from vllm import LLM, SamplingParams
  File "<frozen importlib._bootstrap>", line 1229, in _handle_fromlist
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 64, in __getattr__
    module = import_module(module_name, __package__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 20, in <module>
    from vllm.config import (CompilationConfig, ModelDType, TokenizerMode,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 37, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 33, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py", line 76, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.
; human: Benchmark produced no metrics. Output tail: f Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 17, in <module>
    from vllm import LLM, SamplingParams
  File "<frozen importlib._bootstrap>", line 1229, in _handle_fromlist
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 64, in __getattr__
    module = import_module(module_name, __package__)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 20, in <module>
    from vllm.config import (CompilationConfig, ModelDType, TokenizerMode,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 37, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 33, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py", line 76, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.

2026-01-03 21:11:41,179 | INFO |   Progress: 0 success, 3 errors (3/59)
2026-01-03 21:11:41,181 | INFO | [660470e5] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 21:11:41,181 | INFO |   Baseline wheel: 8d59dbb0
2026-01-03 21:11:41,181 | INFO |   Human wheel: 660470e5
2026-01-03 21:11:41,181 | INFO |   GPU config: H100:1
2026-01-03 21:11:41,181 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-si...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 8d59dbb0, will build from source
[URL CHECK] Human wheel URL does not exist for 660470e5, will build from source
[CPU PRE-BUILD] Building baseline 8d59dbb0 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 8d59dbb0 on CPU-only instance...
[PARALLEL] human completed: status=no_metrics
2026-01-03 21:11:50,968 | INFO | 
2026-01-03 21:11:50,968 | INFO | ================================================================================
2026-01-03 21:11:50,968 | INFO | [4/59] COMPLETED: 526de822
2026-01-03 21:11:50,968 | INFO | Subject: [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x 
2026-01-03 21:11:50,968 | ERROR |   ❌ BASELINE_FAILED: baseline: Benchmark produced no metrics. Output tail: _DRAFT_TENSOR_PARALLEL_SIZE]
                            [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]
                            [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]
                            [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]
                            [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]
                            [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]
                            [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]
                            [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]
                            [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]
                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                            [--ignore-patterns IGNORE_PATTERNS]
                            [--preemption-mode PREEMPTION_MODE]
                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]
                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]
                            [--disable-async-output-proc]
                            [--scheduling-policy {fcfs,priority}]
                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]
                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]
                            [--compilation-config COMPILATION_CONFIG]
                            [--kv-transfer-config KV_TRANSFER_CONFIG]
                            [--worker-cls WORKER_CLS]
                            [--generation-config GENERATION_CONFIG]
benchmark_latency.py: error: argument --batch-size: invalid int value: 'BS'
; human: Benchmark produced no metrics. Output tail: _DRAFT_TENSOR_PARALLEL_SIZE]
                            [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]
                            [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]
                            [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]
                            [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]
                            [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]
                            [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]
                            [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]
                            [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]
                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                            [--ignore-patterns IGNORE_PATTERNS]
                            [--preemption-mode PREEMPTION_MODE]
                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]
                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]
                            [--disable-async-output-proc]
                            [--scheduling-policy {fcfs,priority}]
                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]
                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]
                            [--compilation-config COMPILATION_CONFIG]
                            [--kv-transfer-config KV_TRANSFER_CONFIG]
                            [--worker-cls WORKER_CLS]
                            [--generation-config GENERATION_CONFIG]
benchmark_latency.py: error: argument --batch-size: invalid int value: 'BS'

2026-01-03 21:11:50,969 | INFO |   Progress: 0 success, 4 errors (4/59)
2026-01-03 21:11:50,970 | INFO | [67da5720] Starting benchmark | Model: Qwen/Qwen2.5-VL-3B-Instruct | GPU: H100:1
2026-01-03 21:11:50,970 | INFO |   Baseline wheel: 5c04bb8b
2026-01-03 21:11:50,970 | INFO |   Human wheel: 67da5720
2026-01-03 21:11:50,970 | INFO |   GPU config: H100:1
2026-01-03 21:11:50,971 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct --dataset-name sharegpt -...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.4.0.post1+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.4.1+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:2...
[PARALLEL] Starting 3-way parallel benchmark with H100:2...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.4.2+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:4...
[PARALLEL] Starting 3-way parallel benchmark with H100:4...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.5.0.post1+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 3476ed08 on CPU...
[ENSURE BUILD] Checking/building vLLM 3476ed08 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.5.5+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 2deb029d on CPU...
[ENSURE BUILD] Checking/building vLLM 2deb029d on CPU-only instance...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=error
2026-01-03 21:17:22,610 | INFO | 
2026-01-03 21:17:22,610 | INFO | ================================================================================
2026-01-03 21:17:22,611 | INFO | [5/59] COMPLETED: 2a052011
2026-01-03 21:17:22,611 | INFO | Subject: [Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Wei
2026-01-03 21:17:22,611 | ERROR |   ❌ BASELINE_FAILED: baseline: No metrics. Output tail: 
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_throughput.py", line 360, in <module>
    assert args.input_len is not None
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
; human: No metrics. Output tail: 
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_throughput.py", line 360, in <module>
    assert args.input_len is not None
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError

2026-01-03 21:17:22,611 | INFO |   Progress: 0 success, 5 errors (5/59)
2026-01-03 21:17:22,613 | INFO | [6ce01f30] Starting benchmark | Model: meta-llama/Llama-3-8B | GPU: H100:1
2026-01-03 21:17:22,613 | INFO |   Baseline wheel: 6a11fdfb
2026-01-03 21:17:22,613 | INFO |   Human wheel: 6ce01f30
2026-01-03 21:17:22,613 | INFO |   GPU config: H100:1
2026-01-03 21:17:22,613 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --backend vllm --num-prompts 10...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 6a11fdfb, will build from source
[URL CHECK] Human wheel URL does not exist for 6ce01f30, will build from source
[CPU PRE-BUILD] Building baseline 6a11fdfb on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 6a11fdfb on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.5.4+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 660470e5 on CPU...
[ENSURE BUILD] Checking/building vLLM 660470e5 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.5.0.post1+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.5.3.post1+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 6ce01f30 on CPU...
[ENSURE BUILD] Checking/building vLLM 6ce01f30 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.5.5+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] baseline completed: status=no_metrics
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=no_metrics
2026-01-03 21:33:38,208 | INFO | 
2026-01-03 21:33:38,208 | INFO | ================================================================================
2026-01-03 21:33:38,209 | INFO | [6/59] COMPLETED: 2deb029d
2026-01-03 21:33:38,209 | INFO | Subject: [Performance][BlockManagerV2] Mark prefix cache block as com
2026-01-03 21:33:38,209 | ERROR |   ❌ BASELINE_FAILED: baseline: Benchmark produced no metrics. Output tail: 
python: can't open file '/tmp/benchmarks/benchmark_prefix_caching.py': [Errno 2] No such file or directory
; human: Benchmark produced no metrics. Output tail: 
python: can't open file '/tmp/benchmarks/benchmark_prefix_caching.py': [Errno 2] No such file or directory

2026-01-03 21:33:38,209 | INFO |   Progress: 0 success, 6 errors (6/59)
2026-01-03 21:33:38,210 | INFO | [6d646d08] Starting benchmark | Model: meta-llama/Llama-3-8B | GPU: H100:1
2026-01-03 21:33:38,210 | INFO |   Baseline wheel: 95a178f8
2026-01-03 21:33:38,211 | INFO |   Human wheel: 6d646d08
2026-01-03 21:33:38,211 | INFO |   GPU config: H100:1
2026-01-03 21:33:38,211 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --dataset-name sharegpt --multi...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 95a178f8, will build from source
[URL CHECK] Human wheel URL does not exist for 6d646d08, will build from source
[CPU PRE-BUILD] Building baseline 95a178f8 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 95a178f8 on CPU-only instance...
[ENSURE BUILD] Built and cached: vLLM 0.5.4+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.5.3.post1+cu124
[CPU PRE-BUILD] Human wheel ready in volume
[CPU PRE-BUILD] Wheel sources: baseline=volume, human=volume, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[ENSURE BUILD] Built and cached: vLLM 0.5.5+cu124
[CPU PRE-BUILD] Baseline build ready (wheel: volume)
[CPU PRE-BUILD] Building human 6d646d08 on CPU...
[ENSURE BUILD] Checking/building vLLM 6d646d08 on CPU-only instance...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=error
2026-01-03 22:06:00,180 | INFO | 
2026-01-03 22:06:00,181 | INFO | ================================================================================
2026-01-03 22:06:00,181 | INFO | [7/59] COMPLETED: 015069b0
2026-01-03 22:06:00,181 | INFO | Subject: [Misc] Optimize the Qwen3_ReasoningParser extract_
2026-01-03 22:06:00,181 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: No server logs available; human: Server failed to start. Logs: _config={}, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto', load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, reasoning_parser=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, use_v2_block_manager=True, disable_log_stats=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', compilation_config=None, kv_transfer_config=None, kv_events_config=None, worker_cls='auto', worker_extension_cls='', additional_config=None, enable_reasoning=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
INFO 01-03 21:06:17 [config.py:749] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.
INFO 01-03 21:06:17 [config.py:2038] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 01-03 21:06:17 [api_server.py:171] V1 is enabled, but got --disable-frontend-multiprocessing. To disable frontend multiprocessing, set VLLM_USE_V1=0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1130, in <module>
    uvloop.run(run_server(args))
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
    return runner.run(wrapper())
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1078, in run_server
    async with build_async_engine_client(args) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 175, in build_async_engine_client_from_engine_args
    from vllm.v1.engine.async_llm import AsyncLLM
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 30, in <module>
    from vllm.v1.engine.output_processor import (OutputProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/output_processor.py", line 13, in <module>
    from vllm.v1.engine.detokenizer import IncrementalDetokenizer
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/detokenizer.py", line 8, in <module>
    from tokenizers.decoders import DecodeStream
ImportError: cannot import name 'DecodeStream' from 'tokenizers.decoders' (/usr/local/lib/python3.11/site-packages/tokenizers/decoders/__init__.py)

2026-01-03 22:06:00,182 | INFO |   Progress: 0 success, 7 errors (7/59)
2026-01-03 22:06:00,183 | INFO | [6e36f4fa] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:06:00,183 | INFO |   Baseline wheel: dd2a6a82
2026-01-03 22:06:00,183 | INFO |   Human wheel: 6e36f4fa
2026-01-03 22:06:00,183 | INFO |   GPU config: H100:1
2026-01-03 22:06:00,183 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for dd2a6a82, will build from source
[URL CHECK] Human wheel URL does not exist for 6e36f4fa, will build from source
[CPU PRE-BUILD] Building baseline dd2a6a82 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM dd2a6a82 on CPU-only instance...
[PARALLEL] human completed: status=error
2026-01-03 22:06:52,421 | INFO | 
2026-01-03 22:06:52,421 | INFO | ================================================================================
2026-01-03 22:06:52,421 | INFO | [8/59] COMPLETED: 0d243f2a
2026-01-03 22:06:52,421 | INFO | Subject: [ROCm][MoE] mi300 mixtral8x7B perf for specific BS
2026-01-03 22:06:52,421 | ERROR |   ❌ HUMAN_FAILED: human: Server failed to start. Logs: rchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

[rank0]: CPU: registered at /__modal/volumes/vo-UfUbViVHJ5wiHZp6WipoVa/build_0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/csrc/torch_bindings.cpp:513 [kernel]
[rank0]: Meta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]
[rank0]: BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
[rank0]: Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
[rank0]: FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]
[rank0]: Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]
[rank0]: Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
[rank0]: Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
[rank0]: Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
[rank0]: ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
[rank0]: ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]
[rank0]: AutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]
[rank0]: AutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]
[rank0]: AutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]
[rank0]: AutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]
[rank0]: AutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]
[rank0]: AutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]
[rank0]: AutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]
[rank0]: AutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]
[rank0]: AutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]
[rank0]: Tracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]
[rank0]: AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]
[rank0]: AutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]
[rank0]: AutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]
[rank0]: AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]
[rank0]: FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]
[rank0]: BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
[rank0]: FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
[rank0]: Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
[rank0]: VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
[rank0]: FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]
[rank0]: PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
[rank0]: FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]
[rank0]: PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
[rank0]: PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

ERROR 01-03 21:07:32 multiproc_worker_utils.py:124] Worker VllmWorkerProcess pid 139 died, exit code: -15
Exception ignored in: <function CustomAllreduce.__del__ at 0x2b2cd5883ce0>
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py", line 305, in __del__
  File "/usr/local/lib/python3.11/site-packages/vllm/distributed/device_communicators/custom_all_reduce.py", line 298, in close
AttributeError: 'CustomAllreduce' object has no attribute '_ptr'
[rank0]:[W103 21:07:32.479186224 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

2026-01-03 22:06:52,422 | INFO |   Progress: 0 success, 8 errors (8/59)
2026-01-03 22:06:52,423 | INFO | [7c01f706] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:06:52,423 | INFO |   Baseline wheel: 51e971d3
2026-01-03 22:06:52,423 | INFO |   Human wheel: 7c01f706
2026-01-03 22:06:52,423 | INFO |   GPU config: H100:1
2026-01-03 22:06:52,423 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --nu...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 51e971d3, will build from source
[URL CHECK] Human wheel URL does not exist for 7c01f706, will build from source
[CPU PRE-BUILD] Building baseline 51e971d3 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 51e971d3 on CPU-only instance...
[PARALLEL] human completed: status=error
2026-01-03 22:06:57,704 | INFO | 
2026-01-03 22:06:57,704 | INFO | ================================================================================
2026-01-03 22:06:57,704 | INFO | [9/59] COMPLETED: 22d33bac
2026-01-03 22:06:57,704 | INFO | Subject: [FrontEnd][Perf] `merge_async_iterators` fast-path
2026-01-03 22:06:57,704 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: 192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
ERROR 01-03 21:05:24 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 332, in run_engine_core
ERROR 01-03 21:05:24 [core.py:340]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 01-03 21:05:24 [core.py:340]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 287, in __init__
ERROR 01-03 21:05:24 [core.py:340]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 59, in __init__
ERROR 01-03 21:05:24 [core.py:340]     self.model_executor = executor_class(vllm_config)
ERROR 01-03 21:05:24 [core.py:340]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 01-03 21:05:24 [core.py:340]     self._init_executor()
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 45, in _init_executor
ERROR 01-03 21:05:24 [core.py:340]     self.collective_rpc("init_worker", args=([kwargs], ))
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 01-03 21:05:24 [core.py:340]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 01-03 21:05:24 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 2216, in run_method
ERROR 01-03 21:05:24 [core.py:340]     return func(*args, **kwargs)
ERROR 01-03 21:05:24 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 558, in init_worker
ERROR 01-03 21:05:24 [core.py:340]     worker_class = resolve_obj_by_qualname(
ERROR 01-03 21:05:24 [core.py:340]                    ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 1899, in resolve_obj_by_qualname
ERROR 01-03 21:05:24 [core.py:340]     module = importlib.import_module(module_name)
ERROR 01-03 21:05:24 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
ERROR 01-03 21:05:24 [core.py:340]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 01-03 21:05:24 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
ERROR 01-03 21:05:24 [core.py:340]   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 25, in <module>
ERROR 01-03 21:05:24 [core.py:340]     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 39, in <module>
ERROR 01-03 21:05:24 [core.py:340]     from vllm.v1.spec_decode.ngram_proposer import NgramProposer
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/spec_decode/ngram_proposer.py", line 5, in <module>
ERROR 01-03 21:05:24 [core.py:340]     from numba import jit
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
ERROR 01-03 21:05:24 [core.py:340]     _ensure_critical_deps()
ERROR 01-03 21:05:24 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
ERROR 01-03 21:05:24 [core.py:340]     raise ImportError(msg)
ERROR 01-03 21:05:24 [core.py:340] ImportError: Numba needs NumPy 2.0 or less. Got NumPy 2.4.
ERROR 01-03 21:05:24 [core.py:340] 
CRITICAL 01-03 21:05:24 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.
; human: Server failed to start. Logs: 192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
ERROR 01-03 21:07:18 [core.py:340] EngineCore hit an exception: Traceback (most recent call last):
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 332, in run_engine_core
ERROR 01-03 21:07:18 [core.py:340]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 01-03 21:07:18 [core.py:340]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 287, in __init__
ERROR 01-03 21:07:18 [core.py:340]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 59, in __init__
ERROR 01-03 21:07:18 [core.py:340]     self.model_executor = executor_class(vllm_config)
ERROR 01-03 21:07:18 [core.py:340]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 01-03 21:07:18 [core.py:340]     self._init_executor()
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 45, in _init_executor
ERROR 01-03 21:07:18 [core.py:340]     self.collective_rpc("init_worker", args=([kwargs], ))
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 01-03 21:07:18 [core.py:340]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 01-03 21:07:18 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 2221, in run_method
ERROR 01-03 21:07:18 [core.py:340]     return func(*args, **kwargs)
ERROR 01-03 21:07:18 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 558, in init_worker
ERROR 01-03 21:07:18 [core.py:340]     worker_class = resolve_obj_by_qualname(
ERROR 01-03 21:07:18 [core.py:340]                    ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 1904, in resolve_obj_by_qualname
ERROR 01-03 21:07:18 [core.py:340]     module = importlib.import_module(module_name)
ERROR 01-03 21:07:18 [core.py:340]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
ERROR 01-03 21:07:18 [core.py:340]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 01-03 21:07:18 [core.py:340]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
ERROR 01-03 21:07:18 [core.py:340]   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 25, in <module>
ERROR 01-03 21:07:18 [core.py:340]     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 39, in <module>
ERROR 01-03 21:07:18 [core.py:340]     from vllm.v1.spec_decode.ngram_proposer import NgramProposer
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/spec_decode/ngram_proposer.py", line 5, in <module>
ERROR 01-03 21:07:18 [core.py:340]     from numba import jit
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
ERROR 01-03 21:07:18 [core.py:340]     _ensure_critical_deps()
ERROR 01-03 21:07:18 [core.py:340]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
ERROR 01-03 21:07:18 [core.py:340]     raise ImportError(msg)
ERROR 01-03 21:07:18 [core.py:340] ImportError: Numba needs NumPy 2.0 or less. Got NumPy 2.4.
ERROR 01-03 21:07:18 [core.py:340] 
CRITICAL 01-03 21:07:18 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.

2026-01-03 22:06:57,705 | INFO |   Progress: 0 success, 9 errors (9/59)
2026-01-03 22:06:57,706 | INFO | [80aa7e91] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:06:57,706 | INFO |   Baseline wheel: bd439735
2026-01-03 22:06:57,706 | INFO |   Human wheel: 80aa7e91
2026-01-03 22:06:57,706 | INFO |   GPU config: H100:1
2026-01-03 22:06:57,706 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --nu...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for bd439735, will build from source
[URL CHECK] Human wheel URL does not exist for 80aa7e91, will build from source
[CPU PRE-BUILD] Building baseline bd439735 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM bd439735 on CPU-only instance...
[PARALLEL] human completed: status=error
2026-01-03 22:07:00,984 | INFO | 
2026-01-03 22:07:00,984 | INFO | ================================================================================
2026-01-03 22:07:00,985 | INFO | [10/59] COMPLETED: 35fad35a
2026-01-03 22:07:00,985 | INFO | Subject: [V1][Sampler] Faster top-k only implementation (#1
2026-01-03 22:07:00,985 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: No server logs available; human: Server failed to start. Logs: 192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
ERROR 01-03 21:07:26 [core.py:344] EngineCore hit an exception: Traceback (most recent call last):
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 336, in run_engine_core
ERROR 01-03 21:07:26 [core.py:344]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 01-03 21:07:26 [core.py:344]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 291, in __init__
ERROR 01-03 21:07:26 [core.py:344]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 61, in __init__
ERROR 01-03 21:07:26 [core.py:344]     self.model_executor = executor_class(vllm_config)
ERROR 01-03 21:07:26 [core.py:344]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 01-03 21:07:26 [core.py:344]     self._init_executor()
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 45, in _init_executor
ERROR 01-03 21:07:26 [core.py:344]     self.collective_rpc("init_worker", args=([kwargs], ))
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 01-03 21:07:26 [core.py:344]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 01-03 21:07:26 [core.py:344]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 2255, in run_method
ERROR 01-03 21:07:26 [core.py:344]     return func(*args, **kwargs)
ERROR 01-03 21:07:26 [core.py:344]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 558, in init_worker
ERROR 01-03 21:07:26 [core.py:344]     worker_class = resolve_obj_by_qualname(
ERROR 01-03 21:07:26 [core.py:344]                    ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 1905, in resolve_obj_by_qualname
ERROR 01-03 21:07:26 [core.py:344]     module = importlib.import_module(module_name)
ERROR 01-03 21:07:26 [core.py:344]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
ERROR 01-03 21:07:26 [core.py:344]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 01-03 21:07:26 [core.py:344]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
ERROR 01-03 21:07:26 [core.py:344]   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 25, in <module>
ERROR 01-03 21:07:26 [core.py:344]     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 39, in <module>
ERROR 01-03 21:07:26 [core.py:344]     from vllm.v1.spec_decode.ngram_proposer import NgramProposer
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/spec_decode/ngram_proposer.py", line 5, in <module>
ERROR 01-03 21:07:26 [core.py:344]     from numba import jit
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
ERROR 01-03 21:07:26 [core.py:344]     _ensure_critical_deps()
ERROR 01-03 21:07:26 [core.py:344]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
ERROR 01-03 21:07:26 [core.py:344]     raise ImportError(msg)
ERROR 01-03 21:07:26 [core.py:344] ImportError: Numba needs NumPy 2.0 or less. Got NumPy 2.4.
ERROR 01-03 21:07:26 [core.py:344] 
CRITICAL 01-03 21:07:26 [core_client.py:269] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.

2026-01-03 22:07:00,985 | INFO |   Progress: 0 success, 10 errors (10/59)
2026-01-03 22:07:00,987 | INFO | [83450458] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:07:00,987 | INFO |   Baseline wheel: 5b8a1fde
2026-01-03 22:07:00,987 | INFO |   Human wheel: 83450458
2026-01-03 22:07:00,987 | INFO |   GPU config: H100:1
2026-01-03 22:07:00,987 | INFO |   Command preview: python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model ...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] human completed: status=error
2026-01-03 22:07:05,569 | INFO | 
2026-01-03 22:07:05,569 | INFO | ================================================================================
2026-01-03 22:07:05,569 | INFO | [11/59] COMPLETED: 0ec82edd
2026-01-03 22:07:05,569 | INFO | Subject: [perf] Speed up align sum kernels (#21079)
2026-01-03 22:07:05,569 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: e e from None
ERROR 01-03 21:05:44 [core.py:613] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Process EngineCore_0:
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 617, in run_engine_core
    raise e
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 604, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 431, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 77, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 55, in __init__
    self._init_executor()
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 94, in _init_executor
    self.workers = WorkerProc.wait_for_ready(unready_workers)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 446, in wait_for_ready
    raise e from None
Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1857, in <module>
    uvloop.run(run_server(args))
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
    return runner.run(wrapper())
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1792, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1812, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 161, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 115, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 98, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 672, in __init__
    super().__init__(
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/local/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 653, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 703, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}
; human: Server failed to start. Logs: e e from None
ERROR 01-03 21:07:50 [core.py:613] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Process EngineCore_0:
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 617, in run_engine_core
    raise e
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 604, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 431, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 77, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 55, in __init__
    self._init_executor()
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 94, in _init_executor
    self.workers = WorkerProc.wait_for_ready(unready_workers)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/executor/multiproc_executor.py", line 446, in wait_for_ready
    raise e from None
Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1857, in <module>
    uvloop.run(run_server(args))
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
    return runner.run(wrapper())
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1792, in run_server
    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1812, in run_server_worker
    async with build_async_engine_client(args, client_config) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 194, in build_async_engine_client_from_engine_args
    async_llm = AsyncLLM.from_vllm_config(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 161, in from_vllm_config
    return cls(
           ^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 115, in __init__
    self.engine_core = EngineCoreClient.make_async_mp_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 98, in make_async_mp_client
    return AsyncMPClient(*client_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 672, in __init__
    super().__init__(
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 408, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/usr/local/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 653, in launch_core_engines
    wait_for_engine_startup(
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/utils.py", line 703, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {'EngineCore_0': 1}

2026-01-03 22:07:05,571 | INFO |   Progress: 0 success, 11 errors (11/59)
2026-01-03 22:07:05,572 | INFO | [89a84b0b] Starting benchmark | Model: Qwen/Qwen1.5-0.5B | GPU: H100:1
2026-01-03 22:07:05,572 | INFO |   Baseline wheel: 084a01fd
2026-01-03 22:07:05,572 | INFO |   Human wheel: 89a84b0b
2026-01-03 22:07:05,572 | INFO |   GPU config: H100:1
2026-01-03 22:07:05,572 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 -...
Running 3-way benchmark on Modal with H100:1...
[PARALLEL] human completed: status=error
2026-01-03 22:07:05,734 | INFO | 
2026-01-03 22:07:05,734 | INFO | ================================================================================
2026-01-03 22:07:05,734 | INFO | [12/59] COMPLETED: 296f927f
2026-01-03 22:07:05,734 | INFO | Subject: [Model] RE: Mamba2 Prefill Performance Tweaks: Fix
2026-01-03 22:07:05,734 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: No server logs available; human: Server failed to start. Logs:  use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
Traceback (most recent call last):
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1023, in from_pretrained
    config_class = CONFIG_MAPPING[config_dict["model_type"]]
                   ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 725, in __getitem__
    raise KeyError(key)
KeyError: 'bamba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1066, in <module>
    uvloop.run(run_server(args))
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
    return runner.run(wrapper())
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1016, in run_server
    async with build_async_engine_client(args) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 141, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 161, in build_async_engine_client_from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context=usage_context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 1211, in create_engine_config
    model_config = self.create_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 1132, in create_model_config
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 335, in __init__
    hf_config = get_config(self.hf_config_path or self.model,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 321, in get_config
    raise e
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 301, in get_config
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1025, in from_pretrained
    raise ValueError(
ValueError: The checkpoint you are trying to load has model type `bamba` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.

2026-01-03 22:07:05,735 | INFO |   Progress: 0 success, 12 errors (12/59)
2026-01-03 22:07:05,737 | INFO | [8bc68e19] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:07:05,737 | INFO |   Baseline wheel: 0fca3cdc
2026-01-03 22:07:05,737 | INFO |   Human wheel: 8bc68e19
2026-01-03 22:07:05,737 | INFO |   GPU config: H100:1
2026-01-03 22:07:05,737 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 084a01fd, will build from source
[URL CHECK] Baseline wheel URL does not exist for 0fca3cdc, will build from source
[URL CHECK] Human wheel URL does not exist for 89a84b0b, will build from source
[CPU PRE-BUILD] Building baseline 084a01fd on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 084a01fd on CPU-only instance...
[URL CHECK] Human wheel URL does not exist for 8bc68e19, will build from source
[CPU PRE-BUILD] Building baseline 0fca3cdc on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 0fca3cdc on CPU-only instance...
[PARALLEL] human completed: status=error
2026-01-03 22:07:07,969 | INFO | 
2026-01-03 22:07:07,969 | INFO | ================================================================================
2026-01-03 22:07:07,969 | INFO | [13/59] COMPLETED: 3092375e
2026-01-03 22:07:07,969 | INFO | Subject: [V1][Performance] Implement custom serializaton fo
2026-01-03 22:07:07,969 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: No server logs available; human: Server failed to start. Logs: 192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
ERROR 01-03 21:07:34 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 378, in run_engine_core
ERROR 01-03 21:07:34 [core.py:387]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 01-03 21:07:34 [core.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 320, in __init__
ERROR 01-03 21:07:34 [core.py:387]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 67, in __init__
ERROR 01-03 21:07:34 [core.py:387]     self.model_executor = executor_class(vllm_config)
ERROR 01-03 21:07:34 [core.py:387]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 52, in __init__
ERROR 01-03 21:07:34 [core.py:387]     self._init_executor()
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 45, in _init_executor
ERROR 01-03 21:07:34 [core.py:387]     self.collective_rpc("init_worker", args=([kwargs], ))
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 01-03 21:07:34 [core.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 01-03 21:07:34 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 2428, in run_method
ERROR 01-03 21:07:34 [core.py:387]     return func(*args, **kwargs)
ERROR 01-03 21:07:34 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/worker/worker_base.py", line 558, in init_worker
ERROR 01-03 21:07:34 [core.py:387]     worker_class = resolve_obj_by_qualname(
ERROR 01-03 21:07:34 [core.py:387]                    ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/utils.py", line 2059, in resolve_obj_by_qualname
ERROR 01-03 21:07:34 [core.py:387]     module = importlib.import_module(module_name)
ERROR 01-03 21:07:34 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/importlib/__init__.py", line 126, in import_module
ERROR 01-03 21:07:34 [core.py:387]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 01-03 21:07:34 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap_external>", line 940, in exec_module
ERROR 01-03 21:07:34 [core.py:387]   File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 25, in <module>
ERROR 01-03 21:07:34 [core.py:387]     from vllm.v1.worker.gpu_model_runner import GPUModelRunner
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 41, in <module>
ERROR 01-03 21:07:34 [core.py:387]     from vllm.v1.spec_decode.ngram_proposer import NgramProposer
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/vllm/v1/spec_decode/ngram_proposer.py", line 5, in <module>
ERROR 01-03 21:07:34 [core.py:387]     from numba import jit
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
ERROR 01-03 21:07:34 [core.py:387]     _ensure_critical_deps()
ERROR 01-03 21:07:34 [core.py:387]   File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
ERROR 01-03 21:07:34 [core.py:387]     raise ImportError(msg)
ERROR 01-03 21:07:34 [core.py:387] ImportError: Numba needs NumPy 2.2 or less. Got NumPy 2.4.
ERROR 01-03 21:07:34 [core.py:387] 
CRITICAL 01-03 21:07:34 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.

2026-01-03 22:07:07,970 | INFO |   Progress: 0 success, 13 errors (13/59)
2026-01-03 22:07:07,971 | INFO | [8d75fe48] Starting benchmark | Model: neuralmagic/Meta-Llama-3-8B-Instruct-FP8 | GPU: H100:1
2026-01-03 22:07:07,971 | INFO |   Baseline wheel: 388596c9
2026-01-03 22:07:07,971 | INFO |   Human wheel: 8d75fe48
2026-01-03 22:07:07,971 | INFO |   GPU config: H100:1
2026-01-03 22:07:07,971 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-na...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 388596c9, will build from source
[URL CHECK] Human wheel URL does not exist for 8d75fe48, will build from source
[CPU PRE-BUILD] Building baseline 388596c9 on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 388596c9 on CPU-only instance...
[PARALLEL] baseline completed: status=no_metrics
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=no_metrics
2026-01-03 22:08:40,479 | INFO | 
2026-01-03 22:08:40,479 | INFO | ================================================================================
2026-01-03 22:08:40,479 | INFO | [14/59] COMPLETED: 83450458
2026-01-03 22:08:40,480 | INFO | Subject: [Performance][Spec Decode] Optimize ngram lookup performance
2026-01-03 22:08:40,480 | ERROR |   ❌ BASELINE_FAILED: baseline: Benchmark produced no metrics. Output tail: force_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)
WARNING 01-03 22:08:27 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.

/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 284, in <module>
    main(args)
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 24, in main
    llm = LLM(
          ^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 177, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 571, in from_engine_args
    engine_config = engine_args.create_engine_config()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 976, in create_engine_config
    speculative_config = SpeculativeConfig.maybe_create_spec_config(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 1247, in maybe_create_spec_config
    raise ValueError(f"{ngram_prompt_lookup_max=} must be > 0")
ValueError: ngram_prompt_lookup_max=None must be > 0
; human: Benchmark produced no metrics. Output tail: force_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)
WARNING 01-03 22:08:39 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.

/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 284, in <module>
    main(args)
  File "/opt/vllm-commit/benchmarks/benchmark_latency.py", line 24, in main
    llm = LLM(
          ^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py", line 177, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/llm_engine.py", line 571, in from_engine_args
    engine_config = engine_args.create_engine_config()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 976, in create_engine_config
    speculative_config = SpeculativeConfig.maybe_create_spec_config(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 1247, in maybe_create_spec_config
    raise ValueError(f"{ngram_prompt_lookup_max=} must be > 0")
ValueError: ngram_prompt_lookup_max=None must be > 0

2026-01-03 22:08:40,481 | INFO |   Progress: 0 success, 14 errors (14/59)
2026-01-03 22:08:40,481 | INFO | [93e5f3c5] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:08:40,482 | INFO |   Baseline wheel: 70363bcc
2026-01-03 22:08:40,482 | INFO |   Human wheel: 93e5f3c5
2026-01-03 22:08:40,482 | INFO |   GPU config: H100:1
2026-01-03 22:08:40,482 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=error
2026-01-03 22:12:15,336 | INFO | 
2026-01-03 22:12:15,336 | INFO | ================================================================================
2026-01-03 22:12:15,336 | INFO | [15/59] COMPLETED: 3a243095
2026-01-03 22:12:15,336 | INFO | Subject: Optimize `_get_ranks` in Sampler (#3623)
2026-01-03 22:12:15,336 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.4.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, DeviceConfig, LoRAConfig, ModelConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 7, in <module>
    import torch
  File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/usr/local/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 22, in <module>
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 15, in <module>
    from vllm.model_executor.guided_decoding import (
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_decoding.py", line 15, in <module>
    from vllm.model_executor.guided_logits_processors import (CFGLogitsProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_logits_processors.py", line 22, in <module>
    from outlines.fsm.fsm import CFGFSM, RegexFSM
  File "/usr/local/lib/python3.11/site-packages/outlines/__init__.py", line 2, in <module>
    import outlines.generate
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/__init__.py", line 1, in <module>
    from .api import SequenceGenerator
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/api.py", line 5, in <module>
    from outlines.fsm.fsm import FSMState
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/fsm.py", line 9, in <module>
    from outlines.fsm.regex import create_fsm_index_tokenizer, make_deterministic_fsm
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/regex.py", line 5, in <module>
    import numba
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
    _ensure_critical_deps()
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
    raise ImportError(msg)
ImportError: Numba needs NumPy 2.3 or less. Got NumPy 2.4.
; human: Server failed to start. Logs: 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.4.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, DeviceConfig, LoRAConfig, ModelConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 7, in <module>
    import torch
  File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1382, in <module>
    from .functional import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/functional.py", line 7, in <module>
    import torch.nn.functional as F
  File "/usr/local/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 22, in <module>
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 15, in <module>
    from vllm.model_executor.guided_decoding import (
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_decoding.py", line 15, in <module>
    from vllm.model_executor.guided_logits_processors import (CFGLogitsProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_logits_processors.py", line 22, in <module>
    from outlines.fsm.fsm import CFGFSM, RegexFSM
  File "/usr/local/lib/python3.11/site-packages/outlines/__init__.py", line 2, in <module>
    import outlines.generate
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/__init__.py", line 1, in <module>
    from .api import SequenceGenerator
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/api.py", line 5, in <module>
    from outlines.fsm.fsm import FSMState
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/fsm.py", line 9, in <module>
    from outlines.fsm.regex import create_fsm_index_tokenizer, make_deterministic_fsm
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/regex.py", line 5, in <module>
    import numba
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
    _ensure_critical_deps()
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
    raise ImportError(msg)
ImportError: Numba needs NumPy 2.3 or less. Got NumPy 2.4.

2026-01-03 22:12:15,337 | INFO |   Progress: 0 success, 15 errors (15/59)
2026-01-03 22:12:15,338 | INFO | [9474e89b] Starting benchmark | Model: huggyllama/llama-7b | GPU: H100:1
2026-01-03 22:12:15,338 | INFO |   Baseline wheel: 20478c4d
2026-01-03 22:12:15,338 | INFO |   Human wheel: 9474e89b
2026-01-03 22:12:15,338 | INFO |   GPU config: H100:1
2026-01-03 22:12:15,338 | INFO |   Command preview: python benchmarks/benchmark_throughput.py --model huggyllama/llama-7b --dataset-name sharegpt --num-...
Running 3-way benchmark on Modal with H100:1...
[URL CHECK] Baseline wheel URL does not exist for 20478c4d, will build from source
[URL CHECK] Human wheel URL does not exist for 9474e89b, will build from source
[CPU PRE-BUILD] Building baseline 20478c4d on CPU (no S3 wheel)...
[ENSURE BUILD] Checking/building vLLM 20478c4d on CPU-only instance...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=error
2026-01-03 22:13:17,976 | INFO | 
2026-01-03 22:13:17,976 | INFO | ================================================================================
2026-01-03 22:13:17,976 | INFO | [16/59] COMPLETED: 67da5720
2026-01-03 22:13:17,976 | INFO | Subject: [PERF] Speed up Qwen2.5-VL model by speed up rotar
2026-01-03 22:13:17,976 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 01-03 21:13:20 [__init__.py:248] Automatically detected platform cuda.
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 12, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 20, in <module>
    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 38, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py", line 75, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.
; human: Server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 01-03 21:12:28 [__init__.py:248] Automatically detected platform cuda.
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 12, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 20, in <module>
    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 38, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py", line 75, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.

2026-01-03 22:13:17,978 | INFO |   Progress: 0 success, 16 errors (16/59)
2026-01-03 22:13:17,979 | INFO | [99abb8b6] Starting benchmark | Model: meta-llama/Llama-3.1-8B-Instruct | GPU: H100:1
2026-01-03 22:13:17,979 | INFO |   Baseline wheel: 3a1e6481
2026-01-03 22:13:17,979 | INFO |   Human wheel: 99abb8b6
2026-01-03 22:13:17,979 | INFO |   GPU config: H100:1
2026-01-03 22:13:17,979 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model ...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
[PARALLEL MODE] Using parallel execution for H100:1...
[PARALLEL] Starting 3-way parallel benchmark with H100:1...
[PARALLEL] Spawning baseline, human in parallel...
[PARALLEL] Waiting for all phases to complete...
[PARALLEL] Waiting for baseline...
[PARALLEL] baseline completed: status=error
[PARALLEL] Waiting for human...
[PARALLEL] human completed: status=error
2026-01-03 22:13:19,927 | INFO | 
2026-01-03 22:13:19,927 | INFO | ================================================================================
2026-01-03 22:13:19,927 | INFO | [17/59] COMPLETED: 2f192835
2026-01-03 22:13:19,927 | INFO | Subject: [Core] latency optimization (#3890)
2026-01-03 22:13:19,927 | ERROR |   ❌ BASELINE_FAILED: baseline: Server failed to start. Logs: 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.4.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, DeviceConfig, EngineConfig, LoRAConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 7, in <module>
    import torch
  File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/usr/local/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 22, in <module>
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 15, in <module>
    from vllm.model_executor.guided_decoding import (
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_decoding.py", line 15, in <module>
    from vllm.model_executor.guided_logits_processors import (CFGLogitsProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_logits_processors.py", line 22, in <module>
    from outlines.fsm.fsm import CFGFSM, RegexFSM
  File "/usr/local/lib/python3.11/site-packages/outlines/__init__.py", line 2, in <module>
    import outlines.generate
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/__init__.py", line 1, in <module>
    from .api import SequenceGenerator
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/api.py", line 5, in <module>
    from outlines.fsm.fsm import FSMState
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/fsm.py", line 9, in <module>
    from outlines.fsm.regex import create_fsm_index_tokenizer, make_deterministic_fsm
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/regex.py", line 5, in <module>
    import numba
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
    _ensure_critical_deps()
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
    raise ImportError(msg)
ImportError: Numba needs NumPy 2.3 or less. Got NumPy 2.4.
; human: Server failed to start. Logs: 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.4.0 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, DeviceConfig, EngineConfig, LoRAConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 7, in <module>
    import torch
  File "/usr/local/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/usr/local/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 22, in <module>
    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 15, in <module>
    from vllm.model_executor.guided_decoding import (
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_decoding.py", line 15, in <module>
    from vllm.model_executor.guided_logits_processors import (CFGLogitsProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/guided_logits_processors.py", line 22, in <module>
    from outlines.fsm.fsm import CFGFSM, RegexFSM
  File "/usr/local/lib/python3.11/site-packages/outlines/__init__.py", line 2, in <module>
    import outlines.generate
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/__init__.py", line 1, in <module>
    from .api import SequenceGenerator
  File "/usr/local/lib/python3.11/site-packages/outlines/generate/api.py", line 5, in <module>
    from outlines.fsm.fsm import FSMState
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/fsm.py", line 9, in <module>
    from outlines.fsm.regex import create_fsm_index_tokenizer, make_deterministic_fsm
  File "/usr/local/lib/python3.11/site-packages/outlines/fsm/regex.py", line 5, in <module>
    import numba
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 59, in <module>
    _ensure_critical_deps()
  File "/usr/local/lib/python3.11/site-packages/numba/__init__.py", line 45, in _ensure_critical_deps
    raise ImportError(msg)
ImportError: Numba needs NumPy 2.3 or less. Got NumPy 2.4.

2026-01-03 22:13:19,928 | INFO |   Progress: 0 success, 17 errors (17/59)
2026-01-03 22:13:19,929 | INFO | [9a3b8832] Starting benchmark | Model: Qwen/Qwen2.5-VL-3B-Instruct | GPU: H100:1
2026-01-03 22:13:19,929 | INFO |   Baseline wheel: 3014c920
2026-01-03 22:13:19,929 | INFO |   Human wheel: 9a3b8832
2026-01-03 22:13:19,929 | INFO |   GPU config: H100:1
2026-01-03 22:13:19,929 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct...
