2026-01-03 12:16:17,180 | INFO | ================================================================================
2026-01-03 12:16:17,180 | INFO | HERO BENCHMARK RUN
2026-01-03 12:16:17,180 | INFO | Start time: 2026-01-03T12:16:17.180484
2026-01-03 12:16:17,180 | INFO | Log file: omniperf_results/hero_run_logs/hero_run_20260103_121617.log
2026-01-03 12:16:17,180 | INFO | ================================================================================
2026-01-03 12:16:17,180 | INFO | Loaded 59 commits from plan
2026-01-03 12:16:17,180 | INFO | 
Loading HuggingFace dataset...
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-03 12:16:19,735 | INFO | 
================================================================================
2026-01-03 12:16:19,735 | INFO | [1/59] BENCHMARK: 015069b0
2026-01-03 12:16:19,735 | INFO | Subject: [Misc] Optimize the Qwen3_ReasoningParser extract_
2026-01-03 12:16:19,735 | INFO | Model: Qwen/Qwen3-7B-Instruct
2026-01-03 12:16:19,736 | INFO | ================================================================================
2026-01-03 12:16:19,936 | INFO |   Baseline wheel: fbefc8a7
2026-01-03 12:16:19,936 | INFO |   Human wheel: 015069b0
2026-01-03 12:16:19,936 | INFO |   GPU config: H100:1
2026-01-03 12:16:19,936 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --requ...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 12:28:36,059 | ERROR |   ❌ ERROR: BASELINE server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 01-03 12:18:38 [__init__.py:239] Automatically detected platform cuda.
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 12, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 18, in <module>
    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 37, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis2 import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis2.py", line 75, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.

2026-01-03 12:28:36,059 | INFO |   Progress: 0 success, 1 errors (1/59)
2026-01-03 12:28:36,059 | INFO | 
================================================================================
2026-01-03 12:28:36,059 | INFO | [2/59] BENCHMARK: 0d243f2a
2026-01-03 12:28:36,059 | INFO | Subject: [ROCm][MoE] mi300 mixtral8x7B perf for specific BS
2026-01-03 12:28:36,059 | INFO | Model: mistralai/Mixtral-8x7B-Instruct-v0.1
2026-01-03 12:28:36,059 | INFO | ================================================================================
2026-01-03 12:28:36,062 | INFO |   Baseline wheel: 88f6ba32
2026-01-03 12:28:36,062 | INFO |   Human wheel: 0d243f2a
2026-01-03 12:28:36,062 | INFO |   GPU config: H100:1
2026-01-03 12:28:36,062 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 12:40:08,246 | ERROR |   ❌ ERROR: BASELINE server failed to start. Logs:               ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/models/mixtral.py", line 87, in __init__
[rank0]:     self.experts = FusedMoE(num_experts=num_experts,
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 309, in __init__
[rank0]:     self.quant_method.create_weights(layer=self, **moe_quant_params)
[rank0]:   File "/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 83, in create_weights
[rank0]:     w2_weight = torch.nn.Parameter(torch.empty(
[rank0]:                                    ^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.11/site-packages/torch/utils/_device.py", line 106, in __torch_function__
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 812.56 MiB is free. Process 1 has 78.38 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 13.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W103 12:30:29.068934886 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

2026-01-03 12:40:08,247 | INFO |   Progress: 0 success, 2 errors (2/59)
2026-01-03 12:40:08,247 | INFO | 
================================================================================
2026-01-03 12:40:08,247 | INFO | [3/59] BENCHMARK: 0ec82edd
2026-01-03 12:40:08,247 | INFO | Subject: [perf] Speed up align sum kernels (#21079)
2026-01-03 12:40:08,247 | INFO | Model: Qwen/Qwen3-30B-A3B
2026-01-03 12:40:08,247 | INFO | ================================================================================
2026-01-03 12:40:08,249 | INFO |   Baseline wheel: 005ae9be
2026-01-03 12:40:08,249 | INFO |   Human wheel: 0ec82edd
2026-01-03 12:40:08,249 | INFO |   GPU config: H100:1
2026-01-03 12:40:08,249 | INFO |   Command preview: vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 1...
