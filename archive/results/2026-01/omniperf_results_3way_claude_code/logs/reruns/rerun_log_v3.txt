2026-01-03 13:15:30,490 | INFO | ================================================================================
2026-01-03 13:15:30,490 | INFO | HERO BENCHMARK RUN
2026-01-03 13:15:30,490 | INFO | Start time: 2026-01-03T13:15:30.490930
2026-01-03 13:15:30,490 | INFO | Log file: omniperf_results/hero_run_logs/hero_run_20260103_131530.log
2026-01-03 13:15:30,491 | INFO | ================================================================================
2026-01-03 13:15:30,491 | INFO | Loaded 59 commits from plan
2026-01-03 13:15:30,491 | INFO | 
Loading HuggingFace dataset...
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-03 13:15:33,137 | INFO | 
================================================================================
2026-01-03 13:15:33,137 | INFO | [1/59] BENCHMARK: 015069b0
2026-01-03 13:15:33,137 | INFO | Subject: [Misc] Optimize the Qwen3_ReasoningParser extract_
2026-01-03 13:15:33,137 | INFO | Model: Qwen/Qwen3-7B-Instruct
2026-01-03 13:15:33,138 | INFO | ================================================================================
2026-01-03 13:15:33,340 | INFO |   Baseline wheel: fbefc8a7
2026-01-03 13:15:33,340 | INFO |   Human wheel: 015069b0
2026-01-03 13:15:33,340 | INFO |   GPU config: H100:1
2026-01-03 13:15:33,340 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --requ...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 13:27:16,614 | ERROR |   ❌ ERROR: BASELINE server failed to start. Logs: un_server
    async with build_async_engine_client(args) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 166, in build_async_engine_client_from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context=usage_context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 941, in create_engine_config
    model_config = self.create_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 830, in create_model_config
    return ModelConfig(
           ^^^^^^^^^^^^
  File "<string>", line 41, in __init__
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 487, in __post_init__
    hf_config = get_config(self.hf_config_path or self.model,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 305, in get_config
    raise ValueError(error_message) from e
ValueError: Invalid repository ID or local directory specified: 'Qwen/Qwen3-7B-Instruct'.
Please verify the following requirements:
1. Provide a valid Hugging Face repository ID.
2. Specify a local directory that contains a recognized configuration file.
   - For Hugging Face models: ensure the presence of a 'config.json'.
   - For Mistral models: ensure the presence of a 'params.json'.


2026-01-03 13:27:16,614 | INFO |   Progress: 0 success, 1 errors (1/59)
2026-01-03 13:27:16,614 | INFO | 
================================================================================
2026-01-03 13:27:16,614 | INFO | [2/59] BENCHMARK: 0d243f2a
2026-01-03 13:27:16,615 | INFO | Subject: [ROCm][MoE] mi300 mixtral8x7B perf for specific BS
2026-01-03 13:27:16,615 | INFO | Model: mistralai/Mixtral-8x7B-Instruct-v0.1
2026-01-03 13:27:16,615 | INFO | ================================================================================
2026-01-03 13:27:16,617 | INFO |   Baseline wheel: 88f6ba32
2026-01-03 13:27:16,617 | INFO |   Human wheel: 0d243f2a
2026-01-03 13:27:16,617 | INFO |   GPU config: H100:2
2026-01-03 13:27:16,617 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1...
Running 3-way benchmark on Modal with H100:2...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 13:32:27,872 | ERROR |   ❌ ERROR: Human wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local
  × Failed to download `vllm @
  │ https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`
  ├─▶ Failed to fetch:
  │   `https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`
  ╰─▶ HTTP status client error (404 Not Found) for url
      (https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)

2026-01-03 13:32:27,872 | INFO |   Progress: 0 success, 2 errors (2/59)
2026-01-03 13:32:27,872 | INFO | 
================================================================================
2026-01-03 13:32:27,872 | INFO | [3/59] BENCHMARK: 0ec82edd
2026-01-03 13:32:27,872 | INFO | Subject: [perf] Speed up align sum kernels (#21079)
2026-01-03 13:32:27,873 | INFO | Model: Qwen/Qwen3-30B-A3B
2026-01-03 13:32:27,873 | INFO | ================================================================================
2026-01-03 13:32:27,875 | INFO |   Baseline wheel: 005ae9be
2026-01-03 13:32:27,875 | INFO |   Human wheel: 0ec82edd
2026-01-03 13:32:27,875 | INFO |   GPU config: H100:2
2026-01-03 13:32:27,875 | INFO |   Command preview: vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 1...
