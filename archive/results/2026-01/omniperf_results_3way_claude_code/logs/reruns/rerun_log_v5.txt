2026-01-03 13:53:03,854 | INFO | ================================================================================
2026-01-03 13:53:03,854 | INFO | HERO BENCHMARK RUN
2026-01-03 13:53:03,854 | INFO | Start time: 2026-01-03T13:53:03.854777
2026-01-03 13:53:03,854 | INFO | Log file: omniperf_results/hero_run_logs/hero_run_20260103_135303.log
2026-01-03 13:53:03,854 | INFO | ================================================================================
2026-01-03 13:53:03,854 | INFO | Loaded 59 commits from plan
2026-01-03 13:53:03,855 | INFO | 
Loading HuggingFace dataset...
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-03 13:53:06,306 | INFO | 
================================================================================
2026-01-03 13:53:06,307 | INFO | [1/59] BENCHMARK: 015069b0
2026-01-03 13:53:06,307 | INFO | Subject: [Misc] Optimize the Qwen3_ReasoningParser extract_
2026-01-03 13:53:06,307 | INFO | Model: Qwen/Qwen3-7B-Instruct
2026-01-03 13:53:06,307 | INFO | ================================================================================
2026-01-03 13:53:06,307 | INFO |   Fixing model name: Qwen/Qwen3-7B-Instruct -> mergekit-community/Qwen3-7B-Instruct
2026-01-03 13:53:06,504 | INFO |   Baseline wheel: fbefc8a7
2026-01-03 13:53:06,504 | INFO |   Human wheel: 015069b0
2026-01-03 13:53:06,504 | INFO |   GPU config: H100:1
2026-01-03 13:53:06,504 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mergekit-community/Qwen3-7B-Instruct --dataset-name s...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 14:05:05,415 | ERROR |   ❌ ERROR: BASELINE server failed to start. Logs: )
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 92, in run
    return runner.run(wrapper())
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
  File "/usr/local/lib/python3.11/site-packages/uvloop/__init__.py", line 48, in wrapper
    return await main
           ^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 1078, in run_server
    async with build_async_engine_client(args) as engine_client:
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
    async with build_async_engine_client_from_engine_args(
  File "/usr/local/lib/python3.11/contextlib.py", line 204, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py", line 175, in build_async_engine_client_from_engine_args
    from vllm.v1.engine.async_llm import AsyncLLM
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 30, in <module>
    from vllm.v1.engine.output_processor import (OutputProcessor,
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/output_processor.py", line 13, in <module>
    from vllm.v1.engine.detokenizer import IncrementalDetokenizer
  File "/usr/local/lib/python3.11/site-packages/vllm/v1/engine/detokenizer.py", line 8, in <module>
    from tokenizers.decoders import DecodeStream
ImportError: cannot import name 'DecodeStream' from 'tokenizers.decoders' (/usr/local/lib/python3.11/site-packages/tokenizers/decoders/__init__.py)

2026-01-03 14:05:05,415 | INFO |   Progress: 0 success, 1 errors (1/59)
2026-01-03 14:05:05,415 | INFO | 
================================================================================
2026-01-03 14:05:05,415 | INFO | [2/59] BENCHMARK: 0d243f2a
2026-01-03 14:05:05,415 | INFO | Subject: [ROCm][MoE] mi300 mixtral8x7B perf for specific BS
2026-01-03 14:05:05,415 | INFO | Model: mistralai/Mixtral-8x7B-Instruct-v0.1
2026-01-03 14:05:05,415 | INFO | ================================================================================
2026-01-03 14:05:05,417 | INFO |   Baseline wheel: 88f6ba32
2026-01-03 14:05:05,418 | INFO |   Human wheel: 0d243f2a
2026-01-03 14:05:05,418 | INFO |   GPU config: H100:2
2026-01-03 14:05:05,418 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1...
Running 3-way benchmark on Modal with H100:2...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 14:09:54,202 | ERROR |   ❌ ERROR: Human wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local
  × Failed to download `vllm @
  │ https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`
  ├─▶ Failed to fetch:
  │   `https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`
  ╰─▶ HTTP status client error (404 Not Found) for url
      (https://vllm-wheels.s3.us-west-2.amazonaws.com/0d243f2a54fbd1c56da8a571f0899c30b6aba5d9/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)

2026-01-03 14:09:54,203 | INFO |   Progress: 0 success, 2 errors (2/59)
2026-01-03 14:09:54,203 | INFO | 
================================================================================
2026-01-03 14:09:54,203 | INFO | [3/59] BENCHMARK: 0ec82edd
2026-01-03 14:09:54,203 | INFO | Subject: [perf] Speed up align sum kernels (#21079)
2026-01-03 14:09:54,203 | INFO | Model: Qwen/Qwen3-30B-A3B
2026-01-03 14:09:54,203 | INFO | ================================================================================
2026-01-03 14:09:54,203 | INFO |   Fixing model name: Qwen/Qwen3-30B-A3B -> Qwen/Qwen3-30B-A3B-Instruct-2507
2026-01-03 14:09:54,205 | INFO |   Baseline wheel: 005ae9be
2026-01-03 14:09:54,205 | INFO |   Human wheel: 0ec82edd
2026-01-03 14:09:54,205 | INFO |   GPU config: H100:2
2026-01-03 14:09:54,205 | INFO |   Command preview: vllm bench throughput --model Qwen/Qwen3-30B-A3B-Instruct-2507 --load-format dummy --input-len 1000 ...
