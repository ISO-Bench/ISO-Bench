2026-01-03 12:50:17,789 | INFO | ================================================================================
2026-01-03 12:50:17,789 | INFO | HERO BENCHMARK RUN
2026-01-03 12:50:17,789 | INFO | Start time: 2026-01-03T12:50:17.789970
2026-01-03 12:50:17,790 | INFO | Log file: omniperf_results/hero_run_logs/hero_run_20260103_125017.log
2026-01-03 12:50:17,790 | INFO | ================================================================================
2026-01-03 12:50:17,790 | INFO | Loaded 59 commits from plan
2026-01-03 12:50:17,790 | INFO | 
Loading HuggingFace dataset...
/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-03 12:50:20,748 | INFO | 
================================================================================
2026-01-03 12:50:20,748 | INFO | [1/59] BENCHMARK: 015069b0
2026-01-03 12:50:20,748 | INFO | Subject: [Misc] Optimize the Qwen3_ReasoningParser extract_
2026-01-03 12:50:20,748 | INFO | Model: Qwen/Qwen3-7B-Instruct
2026-01-03 12:50:20,748 | INFO | ================================================================================
2026-01-03 12:50:20,946 | INFO |   Baseline wheel: fbefc8a7
2026-01-03 12:50:20,946 | INFO |   Human wheel: 015069b0
2026-01-03 12:50:20,946 | INFO |   GPU config: H100:1
2026-01-03 12:50:20,946 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --requ...
Running 3-way benchmark on Modal with H100:1...
[CPU PRE-BUILD] Wheel sources: baseline=url, human=url, agent=None
2026-01-03 13:01:58,125 | ERROR |   ‚ùå ERROR: BASELINE server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 01-03 12:51:59 [__init__.py:239] Automatically detected platform cuda.
Traceback (most recent call last):
  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 112, in _get_module_details
  File "/usr/local/lib/python3.11/site-packages/vllm/__init__.py", line 12, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py", line 18, in <module>
    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,
  File "/usr/local/lib/python3.11/site-packages/vllm/config.py", line 37, in <module>
    from vllm.transformers_utils.config import (
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>
    from vllm.transformers_utils.configs.ovis2 import OvisConfig
  File "/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis2.py", line 75, in <module>
    AutoConfig.register("aimv2", AIMv2Config)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register
    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)
  File "/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register
    raise ValueError(f"'{key}' is already used by a Transformers config, pick another name.")
ValueError: 'aimv2' is already used by a Transformers config, pick another name.

2026-01-03 13:01:58,125 | INFO |   Progress: 0 success, 1 errors (1/59)
2026-01-03 13:01:58,125 | INFO | 
================================================================================
2026-01-03 13:01:58,125 | INFO | [2/59] BENCHMARK: 0d243f2a
2026-01-03 13:01:58,125 | INFO | Subject: [ROCm][MoE] mi300 mixtral8x7B perf for specific BS
2026-01-03 13:01:58,125 | INFO | Model: mistralai/Mixtral-8x7B-Instruct-v0.1
2026-01-03 13:01:58,125 | INFO | ================================================================================
2026-01-03 13:01:58,127 | INFO |   Baseline wheel: 88f6ba32
2026-01-03 13:01:58,128 | INFO |   Human wheel: 0d243f2a
2026-01-03 13:01:58,128 | INFO |   GPU config: H100:2
2026-01-03 13:01:58,128 | INFO |   Command preview: python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1...
