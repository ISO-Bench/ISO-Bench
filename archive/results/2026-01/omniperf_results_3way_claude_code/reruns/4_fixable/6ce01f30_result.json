{
  "commit_short": "6ce01f30",
  "commit_full": "6ce01f30667bbae33f112152e07a3b66b841078f",
  "parent_hash": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5",
  "task_id": "vllm_core-0030",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "benchmark_type": "throughput",
  "description": "Optimize get_seqs",
  "issue": "Only 100 prompts - insufficient",
  "fix": "Use throughput benchmark with 1000 prompts",
  "timestamp": "2026-01-16 13:08:00",
  "baseline_command": "python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000",
  "test_command": "python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000",
  "is_asymmetric": false,
  "num_trials": 1,
  "baseline": {
    "status": "success",
    "metrics": {
      "throughput_tok_s": 9.18
    },
    "duration_s": 217.71766209602356,
    "raw_output": "15 requests-2.32.5 safetensors-0.7.0 tokenizers-0.19.1 tqdm-4.67.1 transformers-4.44.2 typing-extensions-4.15.0 urllib3-2.6.3\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nUsing Python: /usr/bin/python3\nvLLM source: /opt/vllm_baseline\nBenchmarks dir: /opt/vllm_baseline/benchmarks\n=== Running BASELINE throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:09:09 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:09:15 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:09:16 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.62it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.35it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.84it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.58it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.70it/s]\n\nINFO 01-16 13:09:38 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:09:39 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:09:41 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:09:41 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:09:47 model_runner.py:1219] Graph capturing finished in 6 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:38:39, 27.55s/it, est. speed input: 37.17 toks/s, output: 9.29 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:55<02:16,  5.46it/s, est. speed input: 4760.53 toks/s, output: 1190.13 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:23<01:07,  7.17it/s, est. speed input: 6319.99 toks/s, output: 1580.00 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.27it/s, est. speed input: 7260.80 toks/s, output: 1815.20 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.22it/s, est. speed input: 9441.57 toks/s, output: 2360.39 toks/s]\nThroughput: 9.18 requests/s, 11745.11 tokens/s\n\nBENCHMARK_COMPLETE\n"
  },
  "human": {
    "status": "success",
    "metrics": {
      "throughput_tok_s": 9.21
    },
    "duration_s": 175.77143025398254,
    "raw_output": "typing-extensions-4.15.0 urllib3-2.6.3\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nNo benchmarks folder found, cloning compatible benchmark scripts...\nvLLM version: 0.5.3.post1\nUsing Python: /usr/bin/python3\nvLLM source: /workspace\nBenchmarks dir: /tmp/vllm-benchmarks/benchmarks\n=== Running HUMAN throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /tmp/vllm-benchmarks/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:12:27 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:12:30 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:12:30 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.73it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.48it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.00it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.71it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.83it/s]\n\nINFO 01-16 13:12:34 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:12:35 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:12:37 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:12:37 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:12:43 model_runner.py:1219] Graph capturing finished in 5 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:32:32, 27.18s/it, est. speed input: 37.68 toks/s, output: 9.42 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:54<02:15,  5.49it/s, est. speed input: 4796.03 toks/s, output: 1199.01 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:22<01:07,  7.18it/s, est. speed input: 6342.56 toks/s, output: 1585.64 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.28it/s, est. speed input: 7285.39 toks/s, output: 1821.35 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.25it/s, est. speed input: 9473.55 toks/s, output: 2368.39 toks/s]\nThroughput: 9.21 requests/s, 11786.74 tokens/s\n\nBENCHMARK_COMPLETE\n"
  },
  "agent": {
    "status": "success",
    "metrics": {
      "throughput_tok_s": 9.2
    },
    "duration_s": 161.73311924934387,
    "raw_output": ". It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nUsing Python: /usr/bin/python3\nvLLM source: /opt/vllm_baseline\nBenchmarks dir: /opt/vllm_baseline/benchmarks\nApplying agent patch...\nchecking file vllm/core/block_manager_v1.py\nchecking file vllm/sequence.py\nchecking file vllm/transformers_utils/detokenizer.py\npatching file vllm/core/block_manager_v1.py\npatching file vllm/sequence.py\npatching file vllm/transformers_utils/detokenizer.py\nAGENT_PATCH_APPLIED\n=== Running AGENT throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:15:02 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:15:11 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:15:11 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.78it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.51it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.03it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.85it/s]\n\nINFO 01-16 13:15:16 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:15:16 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:15:19 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:15:19 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:15:24 model_runner.py:1219] Graph capturing finished in 6 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:34:38, 27.31s/it, est. speed input: 37.50 toks/s, output: 9.38 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:54<02:15,  5.50it/s, est. speed input: 4793.09 toks/s, output: 1198.27 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:22<01:07,  7.19it/s, est. speed input: 6341.19 toks/s, output: 1585.30 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.27it/s, est. speed input: 7275.33 toks/s, output: 1818.83 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.24it/s, est. speed input: 9460.48 toks/s, output: 2365.12 toks/s]\nThroughput: 9.20 requests/s, 11770.39 tokens/s\n\nBENCHMARK_COMPLETE\n"
  },
  "analysis": {
    "primary_metric": "throughput_tok_s",
    "baseline_value": 9.18,
    "human_value": 9.21,
    "agent_value": 9.2,
    "human_improvement_pct": 0.33,
    "agent_improvement_pct": 0.22
  }
}