[
  {
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "commit_short": "3476ed08",
    "commit_subject": "Optimize block_manager_v2 vs v1",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model facebook/opt-125m --input-len 1536 --output-len 50 --batch-size 8 --use-v2-block-manager",
    "files_changed": null,
    "pr_url": null,
    "models": null,
    "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
    "gpu_config": null,
    "benchmark_mode": "standalone",
    "agent_name": "claude_code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-16",
    "model": "facebook/opt-125m",
    "has_agent_patch": true,
    "patch_path": null,
    "data_source": "claude_code_rerun_fixable",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 169.1986189332662,
    "baseline_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 175.43653616676238,
    "human_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 184.16436470006374,
    "agent_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -3.686742405359987,
    "human_improvement_throughput": null,
    "agent_improvement_latency_avg": -8.845075604724762,
    "agent_improvement_throughput": null,
    "agent_vs_human_latency_avg": -4.974920688701393,
    "agent_vs_human_throughput": null,
    "baseline_raw": "py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nINFO 01-16 13:02:49 weight_utils.py:218] Using model weights format ['*.bin']\nINFO 01-16 13:02:49 model_runner.py:234] Loading model weights took 0.2389 GB\nINFO 01-16 13:02:50 gpu_executor.py:83] # GPU blocks: 128016, # CPU blocks: 7281\nINFO 01-16 13:02:53 model_runner.py:864] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:02:53 model_runner.py:868] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:02:59 model_runner.py:1022] Graph capturing finished in 6 secs.\nSamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\nWarming up...\n\nWarmup iterations:   0%|          | 0/10 [00:00<?, ?it/s]\nWarmup iterations:  10%|\u2588         | 1/10 [00:00<00:01,  5.65it/s]\nWarmup iterations:  20%|\u2588\u2588        | 2/10 [00:00<00:01,  5.78it/s]\nWarmup iterations:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:01,  5.82it/s]\nWarmup iterations:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00<00:01,  5.85it/s]\nWarmup iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00<00:00,  5.86it/s]\nWarmup iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01<00:00,  5.83it/s]\nWarmup iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01<00:00,  5.84it/s]\nWarmup iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:01<00:00,  5.86it/s]\nWarmup iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:01<00:00,  5.88it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.89it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.85it/s]\n\nProfiling iterations:   0%|          | 0/30 [00:00<?, ?it/s]\nProfiling iterations:   3%|\u258e         | 1/30 [00:00<00:04,  5.82it/s]\nProfiling iterations:   7%|\u258b         | 2/30 [00:00<00:04,  5.87it/s]\nProfiling iterations:  10%|\u2588         | 3/30 [00:00<00:04,  5.88it/s]\nProfiling iterations:  13%|\u2588\u258e        | 4/30 [00:00<00:04,  5.90it/s]\nProfiling iterations:  17%|\u2588\u258b        | 5/30 [00:00<00:04,  5.89it/s]\nProfiling iterations:  20%|\u2588\u2588        | 6/30 [00:01<00:04,  5.89it/s]\nProfiling iterations:  23%|\u2588\u2588\u258e       | 7/30 [00:01<00:03,  5.88it/s]\nProfiling iterations:  27%|\u2588\u2588\u258b       | 8/30 [00:01<00:03,  5.88it/s]\nProfiling iterations:  30%|\u2588\u2588\u2588       | 9/30 [00:01<00:03,  5.88it/s]\nProfiling iterations:  33%|\u2588\u2588\u2588\u258e      | 10/30 [00:01<00:03,  5.89it/s]\nProfiling iterations:  37%|\u2588\u2588\u2588\u258b      | 11/30 [00:01<00:03,  5.91it/s]\nProfiling iterations:  40%|\u2588\u2588\u2588\u2588      | 12/30 [00:02<00:03,  5.91it/s]\nProfiling iterations:  43%|\u2588\u2588\u2588\u2588\u258e     | 13/30 [00:02<00:02,  5.88it/s]\nProfiling iterations:  47%|\u2588\u2588\u2588\u2588\u258b     | 14/30 [00:02<00:02,  5.90it/s]\nProfiling iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 15/30 [00:02<00:02,  5.91it/s]\nProfiling iterations:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 16/30 [00:02<00:02,  5.91it/s]\nProfiling iterations:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:02<00:02,  5.92it/s]\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:03<00:02,  5.91it/s]\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:03<00:01,  5.92it/s]\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:03<00:01,  5.93it/s]\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:03<00:01,  5.93it/s]\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:03<00:01,  5.90it/s]\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:03<00:01,  5.90it/s]\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:04<00:01,  5.90it/s]\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:04<00:00,  5.91it/s]\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:04<00:00,  5.91it/s]\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [00:04<00:00,  5.93it/s]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [00:04<00:00,  5.92it/s]\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:04<00:00,  5.93it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.92it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.90it/s]\nAvg latency: 0.1691986189332662 seconds\n10% percentile latency: 0.16786239770017347 seconds\n25% percentile latency: 0.1683596200005013 seconds\n50% percentile latency: 0.16895367499955682 seconds\n75% percentile latency: 0.1698795610000161 seconds\n90% percentile latency: 0.17052149300043312 seconds\n99% percentile latency: 0.17177463489039838 seconds\n\nBENCHMARK_COMPLETE\n",
    "human_raw": "dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nINFO 01-16 13:04:23 weight_utils.py:218] Using model weights format ['*.bin']\nINFO 01-16 13:04:24 model_runner.py:234] Loading model weights took 0.2389 GB\nINFO 01-16 13:04:24 gpu_executor.py:83] # GPU blocks: 128016, # CPU blocks: 7281\nINFO 01-16 13:04:28 model_runner.py:864] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:04:28 model_runner.py:868] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:04:35 model_runner.py:1022] Graph capturing finished in 7 secs.\nSamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\nWarming up...\n\nWarmup iterations:   0%|          | 0/10 [00:00<?, ?it/s]\nWarmup iterations:  10%|\u2588         | 1/10 [00:00<00:01,  5.55it/s]\nWarmup iterations:  20%|\u2588\u2588        | 2/10 [00:00<00:01,  5.66it/s]\nWarmup iterations:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:01,  5.71it/s]\nWarmup iterations:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00<00:01,  5.72it/s]\nWarmup iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00<00:00,  5.72it/s]\nWarmup iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01<00:00,  5.73it/s]\nWarmup iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01<00:00,  5.74it/s]\nWarmup iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:01<00:00,  5.75it/s]\nWarmup iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:01<00:00,  5.76it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.74it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.73it/s]\n\nProfiling iterations:   0%|          | 0/30 [00:00<?, ?it/s]\nProfiling iterations:   3%|\u258e         | 1/30 [00:00<00:05,  5.69it/s]\nProfiling iterations:   7%|\u258b         | 2/30 [00:00<00:04,  5.64it/s]\nProfiling iterations:  10%|\u2588         | 3/30 [00:00<00:04,  5.63it/s]\nProfiling iterations:  13%|\u2588\u258e        | 4/30 [00:00<00:04,  5.62it/s]\nProfiling iterations:  17%|\u2588\u258b        | 5/30 [00:00<00:04,  5.64it/s]\nProfiling iterations:  20%|\u2588\u2588        | 6/30 [00:01<00:04,  5.65it/s]\nProfiling iterations:  23%|\u2588\u2588\u258e       | 7/30 [00:01<00:04,  5.67it/s]\nProfiling iterations:  27%|\u2588\u2588\u258b       | 8/30 [00:01<00:03,  5.68it/s]\nProfiling iterations:  30%|\u2588\u2588\u2588       | 9/30 [00:01<00:03,  5.70it/s]\nProfiling iterations:  33%|\u2588\u2588\u2588\u258e      | 10/30 [00:01<00:03,  5.71it/s]\nProfiling iterations:  37%|\u2588\u2588\u2588\u258b      | 11/30 [00:01<00:03,  5.71it/s]\nProfiling iterations:  40%|\u2588\u2588\u2588\u2588      | 12/30 [00:02<00:03,  5.72it/s]\nProfiling iterations:  43%|\u2588\u2588\u2588\u2588\u258e     | 13/30 [00:02<00:02,  5.75it/s]\nProfiling iterations:  47%|\u2588\u2588\u2588\u2588\u258b     | 14/30 [00:02<00:02,  5.74it/s]\nProfiling iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 15/30 [00:02<00:02,  5.71it/s]\nProfiling iterations:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 16/30 [00:02<00:02,  5.72it/s]\nProfiling iterations:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:02<00:02,  5.72it/s]\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:03<00:02,  5.69it/s]\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:03<00:01,  5.70it/s]\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:03<00:01,  5.70it/s]\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:03<00:01,  5.70it/s]\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:03<00:01,  5.71it/s]\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:04<00:01,  5.70it/s]\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:04<00:01,  5.69it/s]\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:04<00:00,  5.70it/s]\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:04<00:00,  5.69it/s]\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [00:04<00:00,  5.70it/s]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [00:04<00:00,  5.69it/s]\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:05<00:00,  5.70it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.69it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.69it/s]\nAvg latency: 0.17543653616676239 seconds\n10% percentile latency: 0.17421639530030006 seconds\n25% percentile latency: 0.17458072649992573 seconds\n50% percentile latency: 0.17507054050020088 seconds\n75% percentile latency: 0.1760678955001822 seconds\n90% percentile latency: 0.1779423474997202 seconds\n\nBENCHMARK_COMPLETE\n",
    "agent_raw": "py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nINFO 01-16 13:05:35 weight_utils.py:218] Using model weights format ['*.bin']\nINFO 01-16 13:05:35 model_runner.py:234] Loading model weights took 0.2389 GB\nINFO 01-16 13:05:35 gpu_executor.py:83] # GPU blocks: 128016, # CPU blocks: 7281\nINFO 01-16 13:05:39 model_runner.py:864] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:05:39 model_runner.py:868] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:05:45 model_runner.py:1022] Graph capturing finished in 6 secs.\nSamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\nWarming up...\n\nWarmup iterations:   0%|          | 0/10 [00:00<?, ?it/s]\nWarmup iterations:  10%|\u2588         | 1/10 [00:00<00:01,  5.29it/s]\nWarmup iterations:  20%|\u2588\u2588        | 2/10 [00:00<00:01,  5.33it/s]\nWarmup iterations:  30%|\u2588\u2588\u2588       | 3/10 [00:00<00:01,  5.37it/s]\nWarmup iterations:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:00<00:01,  5.38it/s]\nWarmup iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:00<00:00,  5.32it/s]\nWarmup iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:01<00:00,  5.22it/s]\nWarmup iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:01<00:00,  5.25it/s]\nWarmup iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:01<00:00,  5.27it/s]\nWarmup iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:01<00:00,  5.31it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.34it/s]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  5.31it/s]\n\nProfiling iterations:   0%|          | 0/30 [00:00<?, ?it/s]\nProfiling iterations:   3%|\u258e         | 1/30 [00:00<00:05,  5.45it/s]\nProfiling iterations:   7%|\u258b         | 2/30 [00:00<00:05,  5.43it/s]\nProfiling iterations:  10%|\u2588         | 3/30 [00:00<00:04,  5.43it/s]\nProfiling iterations:  13%|\u2588\u258e        | 4/30 [00:00<00:04,  5.42it/s]\nProfiling iterations:  17%|\u2588\u258b        | 5/30 [00:00<00:04,  5.41it/s]\nProfiling iterations:  20%|\u2588\u2588        | 6/30 [00:01<00:04,  5.41it/s]\nProfiling iterations:  23%|\u2588\u2588\u258e       | 7/30 [00:01<00:04,  5.41it/s]\nProfiling iterations:  27%|\u2588\u2588\u258b       | 8/30 [00:01<00:04,  5.41it/s]\nProfiling iterations:  30%|\u2588\u2588\u2588       | 9/30 [00:01<00:03,  5.39it/s]\nProfiling iterations:  33%|\u2588\u2588\u2588\u258e      | 10/30 [00:01<00:03,  5.39it/s]\nProfiling iterations:  37%|\u2588\u2588\u2588\u258b      | 11/30 [00:02<00:03,  5.41it/s]\nProfiling iterations:  40%|\u2588\u2588\u2588\u2588      | 12/30 [00:02<00:03,  5.41it/s]\nProfiling iterations:  43%|\u2588\u2588\u2588\u2588\u258e     | 13/30 [00:02<00:03,  5.41it/s]\nProfiling iterations:  47%|\u2588\u2588\u2588\u2588\u258b     | 14/30 [00:02<00:02,  5.42it/s]\nProfiling iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 15/30 [00:02<00:02,  5.44it/s]\nProfiling iterations:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 16/30 [00:02<00:02,  5.43it/s]\nProfiling iterations:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:03<00:02,  5.41it/s]\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:03<00:02,  5.41it/s]\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:03<00:02,  5.43it/s]\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:03<00:01,  5.43it/s]\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:03<00:01,  5.44it/s]\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:04<00:01,  5.44it/s]\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:04<00:01,  5.45it/s]\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:04<00:01,  5.45it/s]\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:04<00:00,  5.45it/s]\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:04<00:00,  5.43it/s]\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [00:04<00:00,  5.44it/s]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [00:05<00:00,  5.43it/s]\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [00:05<00:00,  5.44it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.44it/s]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:05<00:00,  5.43it/s]\nAvg latency: 0.18416436470006375 seconds\n10% percentile latency: 0.18291490460014756 seconds\n25% percentile latency: 0.1832797660003962 seconds\n50% percentile latency: 0.18416823650022707 seconds\n75% percentile latency: 0.1849323657502282 seconds\n90% percentile latency: 0.1854262188998291 seconds\n99% percentile latency: 0.18634155841986286 seconds\n\nBENCHMARK_COMPLETE\n",
    "test_script": null
  },
  {
    "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "commit_short": "99abb8b6",
    "commit_subject": "Optimize Rejection Sampler with Triton Kernels",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model [ngram] --ngram-prompt-lookup-min 5 --ngram-prompt-lookup-max 10 --num-speculative-tokens 5 --input-len 550 --output-len 150",
    "files_changed": null,
    "pr_url": null,
    "models": null,
    "parent_commit": "3a1e6481586ed7f079275b5d5072a6e246af691e",
    "gpu_config": null,
    "benchmark_mode": "standalone",
    "agent_name": "claude_code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-16",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "data_source": "claude_code_rerun_fixable",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 2177.473214566726,
    "baseline_throughput": 2639.3,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 2231.1088848332774,
    "agent_throughput": 2635.0,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "agent_improvement_latency_avg": -2.4632068908009086,
    "agent_improvement_throughput": -0.16292198689047027,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "baseline_raw": " Number of emitted tokens: 5.\n\nProfiling iterations:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:36<00:28,  2.18s/it]INFO 01-16 13:19:51 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.13 verification_time_ms=0.19\n\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:39<00:26,  2.17s/it]\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:41<00:23,  2.17s/it]INFO 01-16 13:19:54 [metrics.py:481] Avg prompt throughput: 2639.8 tokens/s, Avg generation throughput: 538.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:19:54 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.640, System efficiency: 0.400, Number of speculative tokens: 5, Number of accepted tokens: 16, Number of draft tokens: 25, Number of emitted tokens: 12.\n\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:43<00:21,  2.17s/it]INFO 01-16 13:19:56 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.29 scoring_time_ms=11.69 verification_time_ms=0.21\n\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:45<00:19,  2.17s/it]INFO 01-16 13:19:59 [metrics.py:481] Avg prompt throughput: 1757.8 tokens/s, Avg generation throughput: 559.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:19:59 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.633, System efficiency: 0.417, Number of speculative tokens: 5, Number of accepted tokens: 19, Number of draft tokens: 30, Number of emitted tokens: 15.\n\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:47<00:17,  2.17s/it]INFO 01-16 13:20:01 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.07 verification_time_ms=0.19\n\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:50<00:15,  2.18s/it]INFO 01-16 13:20:04 [metrics.py:481] Avg prompt throughput: 1759.0 tokens/s, Avg generation throughput: 554.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:20:04 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.633, System efficiency: 0.417, Number of speculative tokens: 5, Number of accepted tokens: 19, Number of draft tokens: 30, Number of emitted tokens: 15.\n\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:52<00:13,  2.18s/it]INFO 01-16 13:20:06 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.18 verification_time_ms=0.19\n\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:54<00:10,  2.18s/it]INFO 01-16 13:20:09 [metrics.py:481] Avg prompt throughput: 1758.3 tokens/s, Avg generation throughput: 554.7 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:20:09 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.633, System efficiency: 0.417, Number of speculative tokens: 5, Number of accepted tokens: 19, Number of draft tokens: 30, Number of emitted tokens: 15.\n\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:56<00:08,  2.18s/it]INFO 01-16 13:20:11 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.33 scoring_time_ms=11.14 verification_time_ms=0.21\n\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [00:58<00:06,  2.18s/it]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [01:00<00:04,  2.18s/it]INFO 01-16 13:20:14 [metrics.py:481] Avg prompt throughput: 2636.5 tokens/s, Avg generation throughput: 536.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:20:14 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.633, System efficiency: 0.417, Number of speculative tokens: 5, Number of accepted tokens: 19, Number of draft tokens: 30, Number of emitted tokens: 15.\n\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [01:03<00:02,  2.18s/it]INFO 01-16 13:20:16 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.06 verification_time_ms=0.19\n\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:05<00:00,  2.18s/it]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:05<00:00,  2.18s/it]\nAvg latency: 2.1774732145667257 seconds\n10% percentile latency: 2.1689490835005927 seconds\n25% percentile latency: 2.1704049745001157 seconds\n50% percentile latency: 2.1797872639999696 seconds\n75% percentile latency: 2.182712282749435 seconds\n90% percentile latency: 2.1845483111004795 seconds\n99% percentile latency: 2.188432217169975 seconds\n[rank0]:[W116 13:20:18.907340449 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\nBENCHMARK_COMPLETE\n",
    "human_raw": null,
    "agent_raw": "speculative tokens: 5, Number of accepted tokens: 4, Number of draft tokens: 5, Number of emitted tokens: 1.\nINFO 01-16 13:23:11 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.28 scoring_time_ms=11.67 verification_time_ms=0.19\n\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:40<00:28,  2.35s/it]\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:42<00:25,  2.30s/it]INFO 01-16 13:23:16 [metrics.py:481] Avg prompt throughput: 1756.0 tokens/s, Avg generation throughput: 556.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:23:16 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.800, System efficiency: 0.167, Number of speculative tokens: 5, Number of accepted tokens: 4, Number of draft tokens: 5, Number of emitted tokens: 1.\nINFO 01-16 13:23:16 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.07 verification_time_ms=0.21\n\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:45<00:22,  2.26s/it]\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:47<00:20,  2.24s/it]INFO 01-16 13:23:21 [metrics.py:481] Avg prompt throughput: 1759.6 tokens/s, Avg generation throughput: 555.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:23:21 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.208, Number of speculative tokens: 5, Number of accepted tokens: 10, Number of draft tokens: 20, Number of emitted tokens: 5.\nINFO 01-16 13:23:21 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.15 verification_time_ms=0.19\n\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:49<00:17,  2.22s/it]\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:51<00:15,  2.21s/it]INFO 01-16 13:23:26 [metrics.py:481] Avg prompt throughput: 1756.2 tokens/s, Avg generation throughput: 554.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:23:26 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.440, System efficiency: 0.200, Number of speculative tokens: 5, Number of accepted tokens: 11, Number of draft tokens: 25, Number of emitted tokens: 6.\nINFO 01-16 13:23:26 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.32 scoring_time_ms=11.18 verification_time_ms=0.20\n\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:53<00:13,  2.20s/it]\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [00:56<00:10,  2.20s/it]\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [00:58<00:08,  2.19s/it]INFO 01-16 13:23:31 [metrics.py:481] Avg prompt throughput: 2636.6 tokens/s, Avg generation throughput: 536.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:23:31 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.400, System efficiency: 0.194, Number of speculative tokens: 5, Number of accepted tokens: 12, Number of draft tokens: 30, Number of emitted tokens: 7.\nINFO 01-16 13:23:31 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.29 scoring_time_ms=11.08 verification_time_ms=0.19\n\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [01:00<00:06,  2.19s/it]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [01:02<00:04,  2.19s/it]INFO 01-16 13:23:36 [metrics.py:481] Avg prompt throughput: 1756.9 tokens/s, Avg generation throughput: 555.8 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.\nINFO 01-16 13:23:36 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.400, System efficiency: 0.194, Number of speculative tokens: 5, Number of accepted tokens: 12, Number of draft tokens: 30, Number of emitted tokens: 7.\nINFO 01-16 13:23:36 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.05 verification_time_ms=0.20\n\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [01:04<00:02,  2.19s/it]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:06<00:00,  2.19s/it]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:06<00:00,  2.23s/it]\nAvg latency: 2.2311088848332776 seconds\n10% percentile latency: 2.166032633299801 seconds\n25% percentile latency: 2.170148110500122 seconds\n50% percentile latency: 2.1763458420009556 seconds\n75% percentile latency: 2.1824194007490405 seconds\n90% percentile latency: 2.184999966799842 seconds\n99% percentile latency: 3.360496487449001 seconds\n[rank0]:[W116 13:23:39.188803153 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n\nBENCHMARK_COMPLETE\n",
    "test_script": null
  },
  {
    "commit_hash": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "commit_short": "6ce01f30",
    "commit_subject": "Optimize get_seqs",
    "repo": "vllm-project/vllm",
    "perf_command": "python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000",
    "files_changed": null,
    "pr_url": null,
    "models": null,
    "parent_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5",
    "gpu_config": null,
    "benchmark_mode": "standalone",
    "agent_name": "claude_code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-16",
    "model": "meta-llama/Meta-Llama-3-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "data_source": "claude_code_rerun_fixable",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": 9.18,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": 9.21,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": 9.2,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": 0.32679738562092747,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": 0.21786492374727207,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": -0.1085776330076174,
    "baseline_raw": "15 requests-2.32.5 safetensors-0.7.0 tokenizers-0.19.1 tqdm-4.67.1 transformers-4.44.2 typing-extensions-4.15.0 urllib3-2.6.3\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nUsing Python: /usr/bin/python3\nvLLM source: /opt/vllm_baseline\nBenchmarks dir: /opt/vllm_baseline/benchmarks\n=== Running BASELINE throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:09:09 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:09:15 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:09:16 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.62it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.35it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.84it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.58it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.70it/s]\n\nINFO 01-16 13:09:38 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:09:39 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:09:41 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:09:41 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:09:47 model_runner.py:1219] Graph capturing finished in 6 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:38:39, 27.55s/it, est. speed input: 37.17 toks/s, output: 9.29 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:55<02:16,  5.46it/s, est. speed input: 4760.53 toks/s, output: 1190.13 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:23<01:07,  7.17it/s, est. speed input: 6319.99 toks/s, output: 1580.00 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.27it/s, est. speed input: 7260.80 toks/s, output: 1815.20 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.22it/s, est. speed input: 9441.57 toks/s, output: 2360.39 toks/s]\nThroughput: 9.18 requests/s, 11745.11 tokens/s\n\nBENCHMARK_COMPLETE\n",
    "human_raw": "typing-extensions-4.15.0 urllib3-2.6.3\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nNo benchmarks folder found, cloning compatible benchmark scripts...\nvLLM version: 0.5.3.post1\nUsing Python: /usr/bin/python3\nvLLM source: /workspace\nBenchmarks dir: /tmp/vllm-benchmarks/benchmarks\n=== Running HUMAN throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /tmp/vllm-benchmarks/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:12:27 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:12:30 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:12:30 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.73it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.48it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.00it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.71it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.83it/s]\n\nINFO 01-16 13:12:34 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:12:35 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:12:37 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:12:37 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:12:43 model_runner.py:1219] Graph capturing finished in 5 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:32:32, 27.18s/it, est. speed input: 37.68 toks/s, output: 9.42 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:54<02:15,  5.49it/s, est. speed input: 4796.03 toks/s, output: 1199.01 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:22<01:07,  7.18it/s, est. speed input: 6342.56 toks/s, output: 1585.64 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.28it/s, est. speed input: 7285.39 toks/s, output: 1821.35 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.25it/s, est. speed input: 9473.55 toks/s, output: 2368.39 toks/s]\nThroughput: 9.21 requests/s, 11786.74 tokens/s\n\nBENCHMARK_COMPLETE\n",
    "agent_raw": ". It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nVerifying fix...\nLogitsWarper OK\nCompatibility fixes done\nUsing Python: /usr/bin/python3\nvLLM source: /opt/vllm_baseline\nBenchmarks dir: /opt/vllm_baseline/benchmarks\nApplying agent patch...\nchecking file vllm/core/block_manager_v1.py\nchecking file vllm/sequence.py\nchecking file vllm/transformers_utils/detokenizer.py\npatching file vllm/core/block_manager_v1.py\npatching file vllm/sequence.py\npatching file vllm/transformers_utils/detokenizer.py\nAGENT_PATCH_APPLIED\n=== Running AGENT throughput benchmark ===\nCommand: python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --input-len 1024 --output-len 256 --num-prompts 1000\nNamespace(backend='vllm', dataset=None, input_len=1024, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto')\nINFO 01-16 13:15:02 llm_engine.py:175] Initializing an LLM engine (v0.5.3.post1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\nINFO 01-16 13:15:11 model_runner.py:719] Starting to load model meta-llama/Meta-Llama-3-8B-Instruct...\nINFO 01-16 13:15:11 weight_utils.py:225] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.78it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.51it/s]\n\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.03it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.73it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.85it/s]\n\nINFO 01-16 13:15:16 model_runner.py:731] Loading model weights took 14.9595 GB\nINFO 01-16 13:15:16 gpu_executor.py:102] # GPU blocks: 27901, # CPU blocks: 2048\nINFO 01-16 13:15:19 model_runner.py:1018] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-16 13:15:19 model_runner.py:1022] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-16 13:15:24 model_runner.py:1219] Graph capturing finished in 6 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   0%|          | 1/1000 [00:27<7:34:38, 27.31s/it, est. speed input: 37.50 toks/s, output: 9.38 toks/s]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:54<02:15,  5.50it/s, est. speed input: 4793.09 toks/s, output: 1198.27 toks/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [01:22<01:07,  7.19it/s, est. speed input: 6341.19 toks/s, output: 1585.30 toks/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:48<00:27,  8.27it/s, est. speed input: 7275.33 toks/s, output: 1818.83 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:48<00:00,  9.24it/s, est. speed input: 9460.48 toks/s, output: 2365.12 toks/s]\nThroughput: 9.20 requests/s, 11770.39 tokens/s\n\nBENCHMARK_COMPLETE\n",
    "test_script": null
  }
]