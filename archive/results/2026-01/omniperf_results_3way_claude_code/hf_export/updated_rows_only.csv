commit_hash,commit_short,commit_subject,repo,perf_command,files_changed,pr_url,models,parent_commit,gpu_config,benchmark_mode,agent_name,agent_model,benchmark_date,model,has_agent_patch,patch_path,baseline_ttft_mean,baseline_ttft_median,baseline_ttft_p99,baseline_tpot_mean,baseline_tpot_median,baseline_tpot_p99,baseline_itl_mean,baseline_itl_median,baseline_itl_p99,baseline_latency_avg,baseline_throughput,human_ttft_mean,human_ttft_median,human_ttft_p99,human_tpot_mean,human_tpot_median,human_tpot_p99,human_itl_mean,human_itl_median,human_itl_p99,human_latency_avg,human_throughput,agent_ttft_mean,agent_ttft_median,agent_ttft_p99,agent_tpot_mean,agent_tpot_median,agent_tpot_p99,agent_itl_mean,agent_itl_median,agent_itl_p99,agent_latency_avg,agent_throughput,human_improvement_ttft_mean,human_improvement_tpot_mean,human_improvement_itl_mean,agent_improvement_ttft_mean,agent_improvement_tpot_mean,agent_improvement_itl_mean,agent_vs_human_ttft_mean,agent_vs_human_tpot_mean,agent_vs_human_itl_mean,human_improvement_ttft_median,human_improvement_ttft_p99,agent_improvement_ttft_median,agent_improvement_ttft_p99,agent_vs_human_ttft_median,agent_vs_human_ttft_p99,human_improvement_latency_avg,human_improvement_throughput,agent_improvement_latency_avg,agent_improvement_throughput,agent_vs_human_latency_avg,agent_vs_human_throughput,baseline_raw,human_raw,agent_raw,test_script,data_source
22d33baca2c0c639cfd45c48e99803e56c3efa74,22d33bac,[FrontEnd][Perf] `merge_async_iterators` fast-path,vllm-project/vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100,[],,[],b0e96aaebbfbe8e70478e4192a5a13864ffdefa6,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,596.34,728.56,876.87,22.94,14.95,149.16,14.22,12.86,16.83,,2046.93,786.79,849.31,1265.03,19.96,19.56,23.2,19.96,16.94,114.13,,3946.1,651.12,701.84,1081.64,30.51,25.76,216.86,24.52,17.12,410.71,,,,,,,,,,,,,,,,,,,,,,,,"INFO 01-02 17:09:08 [__init__.py:256] Automatically detected platform cuda.

/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: vllm [-h] [-v] {chat,complete,serve,bench} ...
vllm: error: unrecognized arguments: --backend vllm
",,,,merged
3a243095e5e7b655b63ab08fbd5936cb40850415,3a243095,Optimize `_get_ranks` in Sampler (#3623),vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100,['vllm/model_executor/layers/sampler.py'],https://github.com/vllm-project/vllm/pull/3623,['N/A'],64172a976c8d975b3aec946f1675716d2532d94f,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,,,,,,,,,,,,,,,,,,,,,,2518.78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: 3a243095e5e7b655b63ab08fbd5936cb40850415
Message: Optimize `_get_ranks` in Sampler (#3623)

This script measures the actual performance impact of the optimization.
It supports cross-commit comparison with functional equivalence checking.
""""""

import os
import sys
import json
import time
import math
import importlib
from typing import Dict, Any, Tuple, Optional, List

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Disable TF32 for reproducibility unless required
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", """")
    symbol_name = os.getenv(""PROB_SYMBOL"", """")
    
    # Priority 2: Parse from commit metadata
    if not (module_path and symbol_name):
        # Based on the commit diff, the optimized function is _get_ranks
        module_path = ""vllm.model_executor.layers.sampler""
        symbol_name = ""_get_ranks""
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    device = torch.device(hw_info[""device""])
    dtype = torch.float32  # logprobs are float32 as per code
    
    # Realistic workload sizes for _get_ranks function
    # This function processes logprobs during sampling
    batch_size = 64  # Number of sequences being processed
    vocab_size = 32000  # Typical vocab size for LLMs like Llama
    
    # Create logprobs tensor (2D: [batch_size, vocab_size])
    # Use realistic distribution - log of softmax outputs
    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)
    logprobs = torch.log_softmax(logits, dim=-1)
    
    # Create indices tensor - chosen token indices for each sequence
    # These would be the sampled tokens
    indices = torch.randint(0, vocab_size, (batch_size,), device=device, dtype=torch.long)
    
    data = {
        ""device"": device,
        ""dtype"": dtype,
        ""hw_info"": hw_info,
        ""logprobs"": logprobs,
        ""indices"": indices,
        ""batch_size"": batch_size,
        ""vocab_size"": vocab_size
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    target, fq_name = resolve_target()
    
    # Call the optimized _get_ranks function
    with torch.no_grad():
        result = target(data[""logprobs""], data[""indices""])
    
    return result

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    if isinstance(result, torch.Tensor):
        torch.save({""type"": ""tensor"", ""data"": result.cpu()}, filepath)
    else:
        torch.save({""type"": ""generic"", ""data"": result}, filepath)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    data = torch.load(filepath)
    return data.get(""data"", data)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    if isinstance(current_result, torch.Tensor):
        assert current_result.shape == reference_result.shape, f""Shape mismatch: {current_result.shape} vs {reference_result.shape}""
        assert current_result.dtype == reference_result.dtype, f""Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}""
        
        # Ranks should be exact integers
        rtol, atol = 0, 0
        
        torch.testing.assert_close(
            current_result.cpu(),
            reference_result.cpu(),
            rtol=rtol, atol=atol
        )

# =======================
# Timing Implementation
# =======================
def time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:
    """"""Time GPU operations with CUDA events.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
        torch.cuda.synchronize()
    
    # Clear cache
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        
        torch.cuda.synchronize()
        start.record()
        result = func()
        end.record()
        torch.cuda.synchronize()
        
        times_ms.append(start.elapsed_time(end))
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95)],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99)],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms)
    }
    
    return result, stats

def time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:
    """"""Time CPU operations.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        start = time.perf_counter()
        result = func()
        end = time.perf_counter()
        times_ms.append((end - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms) if len(times_ms) > 1 else 0.0
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # Timing
    if hw_info[""device""] == ""cuda"":
        warmup = 5
        iters = 50
        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)
        avg_ms = timing_stats[""avg_ms""]
        p50_ms = timing_stats[""p50_ms""]
        p95_ms = timing_stats[""p95_ms""]
    else:
        warmup = 3
        iters = 10
        result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)
        avg_ms = timing_stats[""avg_ms""]
        p50_ms = timing_stats[""p50_ms""]
        p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""3a243095e5e7b655b63ab08fbd5936cb40850415"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pt""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": str(hw_info[""device""]),
        ""dtype"": str(data[""dtype""]),
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""exact""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
6e36f4fa6ce64619b9ea94c88a157f5783a63a65,6e36f4fa,improve chunked prefill performance,vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100,"['tests/basic_correctness/test_chunked_prefill.py', 'vllm/core/scheduler.py']",https://github.com/vllm-project/vllm/pull/7874,['N/A'],dd2a6a82e3f41b4673b1dbb24b2e99230ea96981,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,,,,,,,,,,,,,,,,,,,,,,2413.58,1011.46,1119.57,1398.78,30.52,27.71,71.21,33.53,22.23,189.79,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: 6e36f4fa6ce64619b9ea94c88a157f5783a63a65
Message: improve chunked prefill performance

This script measures the actual performance impact of the optimization.
It supports cross-commit comparison with functional equivalence checking.
""""""

import os
import sys
import json
import time
import math
import importlib
from typing import Dict, Any, Tuple, Optional, List
from collections import deque
from dataclasses import dataclass, field

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Disable TF32 for reproducibility unless required
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", """")
    symbol_name = os.getenv(""PROB_SYMBOL"", """")
    
    # Priority 2: Parse from commit metadata
    if not (module_path and symbol_name):
        # Based on the diff, the target is the Scheduler class
        module_path = ""vllm.core.scheduler""
        symbol_name = ""Scheduler""
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Mock Classes for Testing
# =======================
@dataclass
class SequenceData:
    """"""Mock sequence data""""""
    prompt_token_ids: List[int] = field(default_factory=list)
    output_token_ids: List[int] = field(default_factory=list)
    
    def get_len(self):
        return len(self.prompt_token_ids) + len(self.output_token_ids)
    
    def get_num_computed_tokens(self):
        return 0

class Sequence:
    """"""Mock sequence""""""
    def __init__(self, seq_id, prompt_tokens):
        self.seq_id = seq_id
        self.data = SequenceData(prompt_token_ids=prompt_tokens)
        self.status = ""WAITING""
    
    def get_num_new_tokens(self):
        return len(self.data.prompt_token_ids)
    
    def is_finished(self):
        return False

class SequenceGroup:
    """"""Mock sequence group""""""
    def __init__(self, request_id, seqs, is_prefill=True):
        self.request_id = request_id
        self.seqs = seqs
        self._is_prefill = is_prefill
        self.lora_int_id = 0
        self.sampling_params = None
        self.pooling_params = None
        self.lora_request = None
        self.prompt_adapter_request = None
        self.multi_modal_data = None
        self.state = None
        self.metrics = None
    
    def is_prefill(self):
        return self._is_prefill
    
    def get_seqs(self, status=None):
        if status:
            return [s for s in self.seqs if s.status == status]
        return self.seqs
    
    def get_max_num_running_seqs(self):
        return len(self.seqs)
    
    def is_encoder_decoder(self):
        return False
    
    def get_encoder_seq(self):
        return None
    
    def is_finished(self):
        return all(s.is_finished() for s in self.seqs)
    
    def init_multi_step(self, num_scheduler_steps):
        pass
    
    def maybe_set_first_scheduled_time(self, now):
        pass

@dataclass
class ScheduledSequenceGroup:
    seq_group: SequenceGroup
    token_chunk_size: int

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    # Create a mix of prefill and decode requests to test chunked prefill scheduling
    num_prefill_requests = 8
    num_decode_requests = 16
    prefill_seq_len = 512  # Tokens per prefill request
    
    # Create prefill sequence groups
    prefill_groups = []
    for i in range(num_prefill_requests):
        seq = Sequence(f""prefill_{i}"", list(range(prefill_seq_len)))
        seq.status = ""WAITING""
        group = SequenceGroup(f""prefill_req_{i}"", [seq], is_prefill=True)
        prefill_groups.append(group)
    
    # Create decode sequence groups (already running)
    decode_groups = []
    for i in range(num_decode_requests):
        seq = Sequence(f""decode_{i}"", [0])  # Single token for decode
        seq.status = ""RUNNING""
        group = SequenceGroup(f""decode_req_{i}"", [seq], is_prefill=False)
        decode_groups.append(group)
    
    # Create swapped sequence groups
    swapped_groups = []
    for i in range(4):
        seq = Sequence(f""swapped_{i}"", [0])
        seq.status = ""SWAPPED""
        group = SequenceGroup(f""swapped_req_{i}"", [seq], is_prefill=False)
        swapped_groups.append(group)
    
    data = {
        ""device"": hw_info[""device""],
        ""dtype"": torch.float16 if hw_info[""device""] == ""cuda"" else torch.float32,
        ""hw_info"": hw_info,
        ""prefill_groups"": prefill_groups,
        ""decode_groups"": decode_groups,
        ""swapped_groups"": swapped_groups,
        ""max_num_batched_tokens"": 2048,
        ""max_num_seqs"": 256,
        ""enable_chunking"": True,
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    
    # Import necessary vLLM components
    try:
        from vllm.core.scheduler import Scheduler, SchedulingBudget
        from vllm.core.scheduler import SchedulerPrefillOutputs, SchedulerSwappedInOutputs
        from vllm.config import SchedulerConfig, CacheConfig
    except ImportError as e:
        # Fallback: simulate the scheduling behavior
        return simulate_chunked_prefill_scheduling(data)
    
    # Create scheduler config
    scheduler_config = SchedulerConfig(
        max_num_batched_tokens=data[""max_num_batched_tokens""],
        max_num_seqs=data[""max_num_seqs""],
        max_model_len=2048,
        chunked_prefill_enabled=data[""enable_chunking""],
    )
    
    cache_config = CacheConfig(
        block_size=16,
        gpu_memory_utilization=0.9,
        cache_dtype=""auto"",
    )
    
    # Create scheduler instance
    scheduler = Scheduler(
        scheduler_config=scheduler_config,
        cache_config=cache_config,
        lora_config=None,
    )
    
    # Add sequence groups to scheduler queues
    for group in data[""prefill_groups""]:
        scheduler.waiting.append(group)
    
    for group in data[""decode_groups""]:
        scheduler.running.append(group)
    
    for group in data[""swapped_groups""]:
        scheduler.swapped.append(group)
    
    # Execute the chunked prefill scheduling
    with torch.no_grad():
        result = scheduler._schedule_chunked_prefill()
    
    # Extract scheduling order for comparison
    scheduled_order = []
    for seq_group in result.scheduled_seq_groups:
        scheduled_order.append({
            ""request_id"": seq_group.seq_group.request_id,
            ""is_prefill"": seq_group.seq_group.is_prefill(),
            ""token_chunk_size"": seq_group.token_chunk_size,
        })
    
    return {
        ""scheduled_order"": scheduled_order,
        ""num_prefill_groups"": result.num_prefill_groups,
        ""num_batched_tokens"": result.num_batched_tokens,
        ""preempted"": result.preempted,
    }

def simulate_chunked_prefill_scheduling(data: Dict[str, Any]) -> Any:
    """"""Simulate the scheduling behavior when vLLM is not available.""""""
    
    # Simulate the optimized scheduling order:
    # 1. Decode requests first (from running)
    # 2. Swapped-in decode requests
    # 3. Swapped-in prefill requests  
    # 4. Running prefill requests (chunked)
    # 5. New prefill requests
    
    scheduled_order = []
    
    # Schedule decode requests first (optimization)
    for group in data[""decode_groups""]:
        scheduled_order.append({
            ""request_id"": group.request_id,
            ""is_prefill"": False,
            ""token_chunk_size"": 1,
        })
    
    # Schedule swapped requests
    for group in data[""swapped_groups""]:
        scheduled_order.append({
            ""request_id"": group.request_id,
            ""is_prefill"": group.is_prefill(),
            ""token_chunk_size"": 1 if not group.is_prefill() else 512,
        })
    
    # Schedule new prefill requests (chunked)
    token_budget = data[""max_num_batched_tokens""] - len(data[""decode_groups""])
    for group in data[""prefill_groups""]:
        if token_budget > 0:
            chunk_size = min(512, token_budget)
            scheduled_order.append({
                ""request_id"": group.request_id,
                ""is_prefill"": True,
                ""token_chunk_size"": chunk_size,
            })
            token_budget -= chunk_size
    
    return {
        ""scheduled_order"": scheduled_order,
        ""num_prefill_groups"": len([s for s in scheduled_order if s[""is_prefill""]]),
        ""num_batched_tokens"": sum(s[""token_chunk_size""] for s in scheduled_order),
        ""preempted"": 0,
    }

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    torch.save({""type"": ""dict"", ""data"": result}, filepath)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    data = torch.load(filepath)
    return data.get(""data"", data)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    
    # Check that both results have the same structure
    assert set(current_result.keys()) == set(reference_result.keys()), \
        f""Result keys mismatch: {current_result.keys()} vs {reference_result.keys()}""
    
    # Check scheduling order maintains decode-first priority
    current_order = current_result[""scheduled_order""]
    reference_order = reference_result[""scheduled_order""]
    
    # Verify decode requests are scheduled before prefills
    def get_first_prefill_index(order):
        for i, item in enumerate(order):
            if item[""is_prefill""]:
                return i
        return len(order)
    
    current_first_prefill = get_first_prefill_index(current_order)
    reference_first_prefill = get_first_prefill_index(reference_order)
    
    # The optimization should schedule decodes first
    assert current_first_prefill > 0, ""No decode requests scheduled before prefills""
    
    # Check numerical values
    assert abs(current_result[""num_batched_tokens""] - reference_result[""num_batched_tokens""]) <= 512, \
        f""Token count mismatch: {current_result['num_batched_tokens']} vs {reference_result['num_batched_tokens']}""

# =======================
# Timing Implementation
# =======================
def time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:
    """"""Time GPU operations with CUDA events.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
        if torch.cuda.is_available():
            torch.cuda.synchronize()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        if torch.cuda.is_available():
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            
            torch.cuda.synchronize()
            start.record()
            result = func()
            end.record()
            torch.cuda.synchronize()
            
            times_ms.append(start.elapsed_time(end))
        else:
            start = time.perf_counter()
            result = func()
            times_ms.append((time.perf_counter() - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95)],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99)],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms)
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # Timing
    if hw_info[""device""] == ""cuda"":
        warmup = 5
        iters = 50
    else:
        warmup = 3
        iters = 10
    
    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)
    avg_ms = timing_stats[""avg_ms""]
    p50_ms = timing_stats[""p50_ms""]
    p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""6e36f4fa6ce64619b9ea94c88a157f5783a63a65"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pt""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": str(hw_info[""device""]),
        ""dtype"": str(data[""dtype""]),
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""behavioral""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
7c01f706418d593b3cf23d2ec9110dca7151c539,7c01f706,[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974),vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0,['vllm/sequence.py'],https://github.com/vllm-project/vllm/pull/5974,"['meta-llama/Llama-3.1-8B-Instruct', 'Qwen/Qwen2.5-7B-Instruct']",51e971d39e1272f1c5b070a5da6b38ccfa92fc14,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,,,,,,,,,,,,,,,,,,,,,,2229.44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: 7c01f706418d593b3cf23d2ec9110dca7151c539
Message: [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)

This script measures the actual performance impact of the optimization.
It supports cross-commit comparison with functional equivalence checking.
""""""

import os
import sys
import json
import time
import importlib
from typing import Dict, Any, Tuple, Optional, List

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", """")
    symbol_name = os.getenv(""PROB_SYMBOL"", """")
    
    # Priority 2: Parse from commit metadata
    if not (module_path and symbol_name):
        # Based on the commit diff, the optimization is in SequenceStatus.is_finished
        module_path = ""vllm.sequence""
        symbol_name = ""SequenceStatus""
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    # The optimization is for SequenceStatus.is_finished which checks if a status is finished
    # We need to create a workload that tests this method with various status values
    
    SequenceStatus, _ = resolve_target()
    
    # Create all possible status values to test
    all_statuses = [
        SequenceStatus.WAITING,
        SequenceStatus.RUNNING,
        SequenceStatus.SWAPPED,
        SequenceStatus.FINISHED_STOPPED,
        SequenceStatus.FINISHED_LENGTH_CAPPED,
        SequenceStatus.FINISHED_ABORTED,
        SequenceStatus.FINISHED_IGNORED,
    ]
    
    # Create a large test set with repeated status checks to measure performance
    # Simulate realistic usage patterns with more finished statuses (common in batch processing)
    test_statuses = []
    # 30% waiting/running/swapped, 70% finished (realistic for batch inference)
    for _ in range(10000):
        if np.random.random() < 0.3:
            test_statuses.append(np.random.choice(all_statuses[:3]))
        else:
            test_statuses.append(np.random.choice(all_statuses[3:]))
    
    data = {
        ""device"": hw_info[""device""],
        ""dtype"": None,  # Not applicable for this optimization
        ""hw_info"": hw_info,
        ""SequenceStatus"": SequenceStatus,
        ""test_statuses"": test_statuses,
        ""all_statuses"": all_statuses,
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    SequenceStatus = data[""SequenceStatus""]
    test_statuses = data[""test_statuses""]
    
    # The optimization is in the is_finished static method
    # We'll call it many times to measure the performance improvement
    results = []
    for status in test_statuses:
        result = SequenceStatus.is_finished(status)
        results.append(result)
    
    return results

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    # Store as JSON since results are boolean values
    import pickle
    with open(filepath, 'wb') as f:
        pickle.dump(result, f)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    import pickle
    with open(filepath, 'rb') as f:
        return pickle.load(f)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    assert isinstance(current_result, list), f""Expected list, got {type(current_result)}""
    assert isinstance(reference_result, list), f""Expected list, got {type(reference_result)}""
    assert len(current_result) == len(reference_result), f""Length mismatch: {len(current_result)} vs {len(reference_result)}""
    
    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):
        assert curr == ref, f""Mismatch at index {i}: {curr} vs {ref}""

# =======================
# Timing Implementation
# =======================
def time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:
    """"""Time CPU operations.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        start = time.perf_counter()
        result = func()
        end = time.perf_counter()
        times_ms.append((end - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95)],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99)],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms)
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # This is a CPU-only optimization (enum comparison)
    warmup = 5
    iters = 100
    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)
    avg_ms = timing_stats[""avg_ms""]
    p50_ms = timing_stats[""p50_ms""]
    p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""7c01f706418d593b3cf23d2ec9110dca7151c539"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pkl""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": ""cpu"",  # This is a CPU-only optimization
        ""dtype"": ""None"",  # Not applicable
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""exact""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
89a84b0bb7b30706a02836234a94493ea8f780bf,89a84b0b,[Core] Use array to speedup padding (#6779),vllm,python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024,"['vllm/model_executor/layers/sampler.py', 'vllm/model_executor/sampling_metadata.py', 'vllm/sequence.py']",https://github.com/vllm-project/vllm/pull/6779,['N/A'],084a01fd3544557990f8af8af6fd3c1185bae848,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,Qwen/Qwen1.5-0.5B,True,,,,,,,,,,,,,,,,,,,,,,,3558.51,356.05,349.72,407.54,23.73,23.36,38.75,28.48,19.34,348.87,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: 89a84b0bb7b30706a02836234a94493ea8f780bf
Message: [Core] Use array to speedup padding (#6779)

This script measures the actual performance impact of using arrays instead of lists
for token storage in vLLM's sampling metadata preparation.
""""""

import os
import sys
import json
import time
import importlib
from array import array
from typing import Dict, Any, Tuple, Optional, List
import random

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", ""vllm.model_executor.sampling_metadata"")
    symbol_name = os.getenv(""PROB_SYMBOL"", ""SamplingTensors.from_lists"")
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    device = torch.device(hw_info[""device""])
    dtype = torch.float16 if hw_info[""device""] == ""cuda"" else torch.float32
    
    # Realistic vLLM workload parameters
    batch_size = 32  # Number of sequences
    vocab_size = 32000  # Llama vocab size
    max_prompt_len = 2048
    max_output_len = 512
    
    # Generate token lists that would be used in sampling
    prompt_tokens = []
    output_tokens = []
    
    for _ in range(batch_size):
        # Generate varying length prompts and outputs
        prompt_len = random.randint(128, max_prompt_len)
        output_len = random.randint(1, max_output_len)
        
        # Use arrays as per the optimization
        prompt_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(prompt_len)])
        output_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(output_len)])
        
        prompt_tokens.append(prompt_seq)
        output_tokens.append(output_seq)
    
    # Other sampling parameters
    temperatures = [0.7] * batch_size
    top_ps = [0.9] * batch_size
    top_ks = [40] * batch_size
    min_ps = [0.0] * batch_size
    presence_penalties = [0.0] * batch_size
    frequency_penalties = [0.0] * batch_size
    repetition_penalties = [1.0] * batch_size
    sampling_seeds = [random.randint(0, 2**31-1) for _ in range(batch_size)]
    sample_indices = list(range(batch_size))
    
    data = {
        ""device"": device,
        ""dtype"": dtype,
        ""hw_info"": hw_info,
        ""temperatures"": temperatures,
        ""top_ps"": top_ps,
        ""top_ks"": top_ks,
        ""min_ps"": min_ps,
        ""presence_penalties"": presence_penalties,
        ""frequency_penalties"": frequency_penalties,
        ""repetition_penalties"": repetition_penalties,
        ""sampling_seeds"": sampling_seeds,
        ""sample_indices"": sample_indices,
        ""prompt_tokens"": prompt_tokens,
        ""output_tokens"": output_tokens,
        ""vocab_size"": vocab_size,
        ""extra_seeds_to_generate"": 0,
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    target, fq_name = resolve_target()
    
    # Call SamplingTensors.from_lists with the prepared data
    result = target(
        temperatures=data[""temperatures""],
        top_ps=data[""top_ps""],
        top_ks=data[""top_ks""],
        min_ps=data[""min_ps""],
        presence_penalties=data[""presence_penalties""],
        frequency_penalties=data[""frequency_penalties""],
        repetition_penalties=data[""repetition_penalties""],
        sampling_seeds=data[""sampling_seeds""],
        sample_indices=data[""sample_indices""],
        prompt_tokens=data[""prompt_tokens""],
        output_tokens=data[""output_tokens""],
        vocab_size=data[""vocab_size""],
        extra_seeds_to_generate=data[""extra_seeds_to_generate""],
        device=data[""device""],
        dtype=data[""dtype""]
    )
    
    return result

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    # Store the tensor attributes of SamplingTensors
    tensors_dict = {
        ""temperatures"": result.temperatures.cpu(),
        ""top_ps"": result.top_ps.cpu(),
        ""top_ks"": result.top_ks.cpu(),
        ""min_ps"": result.min_ps.cpu(),
        ""presence_penalties"": result.presence_penalties.cpu(),
        ""frequency_penalties"": result.frequency_penalties.cpu(),
        ""repetition_penalties"": result.repetition_penalties.cpu(),
        ""prompt_tokens"": result.prompt_tokens.cpu(),
        ""output_tokens"": result.output_tokens.cpu(),
        ""sampling_seeds"": result.sampling_seeds.cpu(),
        ""sample_indices"": result.sample_indices.cpu(),
    }
    torch.save({""type"": ""sampling_tensors"", ""data"": tensors_dict}, filepath)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    data = torch.load(filepath)
    return data.get(""data"", data)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    # Check each tensor attribute
    attrs_to_check = [
        ""temperatures"", ""top_ps"", ""top_ks"", ""min_ps"",
        ""presence_penalties"", ""frequency_penalties"", ""repetition_penalties"",
        ""prompt_tokens"", ""output_tokens"", ""sampling_seeds"", ""sample_indices""
    ]
    
    for attr in attrs_to_check:
        current_tensor = getattr(current_result, attr).cpu()
        ref_tensor = reference_result[attr]
        
        assert current_tensor.shape == ref_tensor.shape, f""{attr} shape mismatch""
        assert current_tensor.dtype == ref_tensor.dtype, f""{attr} dtype mismatch""
        
        if current_tensor.dtype in (torch.float16, torch.bfloat16):
            rtol, atol = 1e-3, 1e-4
        else:
            rtol, atol = 1e-5, 1e-7
        
        torch.testing.assert_close(
            current_tensor,
            ref_tensor,
            rtol=rtol, atol=atol
        )

# =======================
# Timing Implementation
# =======================
def time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:
    """"""Time CPU operations.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        start = time.perf_counter()
        result = func()
        end = time.perf_counter()
        times_ms.append((end - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms) if len(times_ms) > 1 else 0.0
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # This optimization primarily affects CPU operations (array vs list)
    # so we time on CPU
    warmup = 5
    iters = 20
    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)
    avg_ms = timing_stats[""avg_ms""]
    p50_ms = timing_stats[""p50_ms""]
    p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""89a84b0bb7b30706a02836234a94493ea8f780bf"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pt""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": ""cpu"",  # This optimization affects CPU operations
        ""dtype"": str(data[""dtype""]),
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""numeric""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
9badee53decb3d432dc805336abfb0eb81dfb48f,9badee53,Fix performance when `--generation-config` is not ,vllm-project/vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json,[],,[],beebf4742af80296d3c3a657c66d512615c550c1,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.2-1B-Instruct,True,,2894.68,2744.14,5268.14,18.21,19.07,19.69,18.19,14.86,31.15,,10588.27,168.63,163.24,219.03,7.83,7.95,9.19,7.83,6.79,51.59,,3424.18,174.68,177.73,236.2,9.85,8.09,56.03,7.98,6.42,79.35,,,94.17448560808103,57.001647446457994,56.95437053326003,,,,,,,,,,,,,,-11.494417879408074,,,,,"INFO 01-02 16:41:24 [__init__.py:253] Automatically detected platform cuda.
Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', max_concurrency=None, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)

/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File ""/opt/vllm-commit/benchmarks/benchmark_serving.py"", line 1315, in <module>
    main(args)
  File ""/opt/vllm-commit/benchmarks/benchmark_serving.py"", line 876, in main
    input_requests = sample_sharegpt_requests(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/vllm-commit/benchmarks/benchmark_serving.py"", line 102, in sample_sharegpt_requests
    with open(dataset_path, encoding='utf-8') as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'
",,,,merged
e206b5433109d298e53451015465b2bf8f03ef0a,e206b543,[v0][Core] Use xgrammar shared context to avoid co,vllm-project/vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --guided-decoding-backend xgrammar,[],,[],1d35662e6dc199431bfe4004cc84d66fd9b297b1,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,576.76,569.57,993.18,22.04,22.19,27.97,22.03,17.04,84.57,,3116.22,574.18,533.96,920.66,22.76,23.42,30.25,22.66,16.98,273.01,,3105.08,669.91,705.32,1099.28,30.88,23.74,225.1,24.56,17.15,288.98,,,0.44732644427492213,-3.2667876588021887,-2.8597367226509256,,,,,,,,,,,,,,1.452079763302976,,,,,"INFO 01-02 17:04:36 [__init__.py:207] Automatically detected platform cuda.

/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: benchmark_serving.py [-h]
                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}]
                            [--base-url BASE_URL] [--host HOST] [--port PORT]
                            [--endpoint ENDPOINT]
                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf}]
                            [--dataset-path DATASET_PATH]
                            [--max-concurrency MAX_CONCURRENCY] --model MODEL
                            [--tokenizer TOKENIZER] [--best-of BEST_OF]
                            [--use-beam-search] [--num-prompts NUM_PROMPTS]
                            [--logprobs LOGPROBS]
                            [--request-rate REQUEST_RATE]
                            [--burstiness BURSTINESS] [--seed SEED]
                            [--trust-remote-code] [--disable-tqdm] [--profile]
                            [--save-result] [--metadata [KEY=VALUE ...]]
                            [--result-dir RESULT_DIR]
                            [--result-filename RESULT_FILENAME] [--ignore-eos]
                            [--percentile-metrics PERCENTILE_METRICS]
                            [--metric-percentiles METRIC_PERCENTILES]
                            [--goodput GOODPUT [GOODPUT ...]]
                            [--sonnet-input-len SONNET_INPUT_LEN]
                            [--sonnet-output-len SONNET_OUTPUT_LEN]
                            [--sonnet-prefix-len SONNET_PREFIX_LEN]
                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]
                            [--random-input-len RANDOM_INPUT_LEN]
                            [--random-output-len RANDOM_OUTPUT_LEN]
                            [--random-range-ratio RANDOM_RANGE_RATIO]
                            [--random-prefix-len RANDOM_PREFIX_LEN]
                            [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]
                            [--hf-output-len HF_OUTPUT_LEN]
                            [--tokenizer-mode {auto,slow,mistral,custom}]
                            [--served-model-name SERVED_MODEL_NAME]
                            [--lora-modules LORA_MODULES [LORA_MODULES ...]]
benchmark_serving.py: error: unrecognized arguments: --guided-decoding-backend xgrammar
",,,,merged
e3580537a41a46b0f3cd750b86b633c1857a8c90,e3580537,[Performance] Enable chunked prefill and prefix caching together (#7753),vllm,python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048,"['tests/basic_correctness/test_chunked_prefill.py', 'tests/core/test_block_manager.py', 'tests/core/test_chunked_prefill_scheduler.py', 'vllm/core/block_manager_v1.py', 'vllm/core/block_manager_v2.py', 'vllm/core/embedding_model_block_manager.py', 'vllm/core/interfaces.py', 'vllm/core/scheduler.py', 'vllm/worker/model_runner.py']",https://github.com/vllm-project/vllm/pull/7753,['N/A'],f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,neuralmagic/Meta-Llama-3-8B-Instruct-FP8,True,,,,,,,,,,,,,,,,,,,,,,,2496.89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: e3580537a41a46b0f3cd750b86b633c1857a8c90
Message: [Performance] Enable chunked prefill and prefix caching together (#7753)

This script measures the actual performance impact of the optimization.
It supports cross-commit comparison with functional equivalence checking.
""""""

import os
import sys
import json
import time
import math
import importlib
from typing import Dict, Any, Tuple, Optional, List

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Disable TF32 for reproducibility unless required
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", """")
    symbol_name = os.getenv(""PROB_SYMBOL"", """")
    
    # Priority 2: Parse from commit metadata - target mark_blocks_as_computed
    if not (module_path and symbol_name):
        # Based on the diff, the key change is in mark_blocks_as_computed method
        module_path = ""vllm.core.block_manager_v1""
        symbol_name = ""BlockSpaceManagerV1.mark_blocks_as_computed""
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    device = torch.device(hw_info[""device""])
    dtype = torch.float16 if hw_info[""device""] == ""cuda"" else torch.float32
    
    # Create a block manager setup that exercises chunked prefill + prefix caching
    block_size = 16
    num_gpu_blocks = 256
    num_cpu_blocks = 0
    
    # Import the block manager class
    from vllm.core.block_manager_v1 import BlockSpaceManagerV1
    from vllm.compilation.backends import Sequence
    from vllm.core.block.utils import SequenceGroup
    from vllm.core.block_manager import SequenceStatus
    from vllm import SamplingParams
    
    # Create block manager with prefix caching enabled
    block_manager = BlockSpaceManagerV1(
        block_size=block_size,
        num_gpu_blocks=num_gpu_blocks,
        num_cpu_blocks=num_cpu_blocks,
        watermark=0.01,
        enable_caching=True  # Enable prefix caching
    )
    
    # Create a sequence group with a long prompt to test chunked prefill
    prompt_length = 512  # Long enough to require multiple chunks
    token_chunk_size = 64  # Chunk size for chunked prefill
    
    # Create sequence
    seq = Sequence(
        seq_id=0,
        inputs={""prompt_token_ids"": list(range(prompt_length))},
        block_size=block_size
    )
    
    # Create sequence group
    seq_group = SequenceGroup(
        request_id=""test_request"",
        seqs=[seq],
        arrival_time=time.time(),
        sampling_params=SamplingParams()
    )
    
    # Allocate blocks for the sequence
    block_manager.allocate(seq_group)
    
    data = {
        ""device"": device,
        ""dtype"": dtype,
        ""hw_info"": hw_info,
        ""block_manager"": block_manager,
        ""seq_group"": seq_group,
        ""token_chunk_size"": token_chunk_size,
        ""block_size"": block_size,
        ""prompt_length"": prompt_length
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    
    block_manager = data[""block_manager""]
    seq_group = data[""seq_group""]
    token_chunk_size = data[""token_chunk_size""]
    prompt_length = data[""prompt_length""]
    
    # Simulate chunked prefill by marking blocks as computed in chunks
    results = []
    num_chunks = (prompt_length + token_chunk_size - 1) // token_chunk_size
    
    for chunk_idx in range(num_chunks):
        # Update the number of computed tokens for the sequence
        for seq in seq_group.get_seqs():
            current_computed = min((chunk_idx + 1) * token_chunk_size, prompt_length)
            seq.data.update_num_computed_tokens(current_computed)
        
        # Call the optimized function with token_chunk_size
        with torch.no_grad():
            block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)
            
            # Get computed blocks for verification
            computed_blocks = []
            for seq in seq_group.get_seqs():
                blocks = block_manager.get_all_computed_blocks(seq)
                computed_blocks.append(len(blocks))
        
        results.append({
            ""chunk_idx"": chunk_idx,
            ""computed_blocks"": computed_blocks[0] if computed_blocks else 0
        })
    
    return results

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    torch.save({""type"": ""list"", ""data"": result}, filepath)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    data = torch.load(filepath)
    return data.get(""data"", data)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    assert type(current_result) == type(reference_result), f""Type mismatch""
    assert len(current_result) == len(reference_result), f""Length mismatch""
    
    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):
        assert curr[""chunk_idx""] == ref[""chunk_idx""], f""Chunk index mismatch at {i}""
        assert curr[""computed_blocks""] == ref[""computed_blocks""], f""Computed blocks mismatch at {i}""

# =======================
# Timing Implementation
# =======================
def time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:
    """"""Time GPU operations with CUDA events.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
        if torch.cuda.is_available():
            torch.cuda.synchronize()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        if torch.cuda.is_available():
            start = torch.cuda.Event(enable_timing=True)
            end = torch.cuda.Event(enable_timing=True)
            
            torch.cuda.synchronize()
            start.record()
            result = func()
            end.record()
            torch.cuda.synchronize()
            
            times_ms.append(start.elapsed_time(end))
        else:
            start = time.perf_counter()
            result = func()
            times_ms.append((time.perf_counter() - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95)],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99)],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms) if len(times_ms) > 1 else 0.0
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # Timing
    if hw_info[""device""] == ""cuda"":
        warmup = 5
        iters = 50
    else:
        warmup = 3
        iters = 10
    
    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)
    avg_ms = timing_stats[""avg_ms""]
    p50_ms = timing_stats[""p50_ms""]
    p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""e3580537a41a46b0f3cd750b86b633c1857a8c90"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pt""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": str(hw_info[""device""]),
        ""dtype"": str(data[""dtype""]),
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""exact""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
fc7b8d1eefcbe837a56b7c080509417fe5167e6c,fc7b8d1e,[Performance] e2e overheads reduction: Small followup diff (#7364),vllm,python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100,"['vllm/core/block_manager_v1.py', 'vllm/sequence.py']",https://github.com/vllm-project/vllm/pull/7364,['N/A'],67abdbb42fdbb59c274130368981c0d0ac3539e3,H100:1,serving,claude-code,sonnet-4.5,2026-01-14,meta-llama/Llama-3.1-8B-Instruct,True,,,,,,,,,,,,,,,,,,,,,,,2214.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"#!/usr/bin/env python3
""""""
Performance test for commit: fc7b8d1eefcbe837a56b7c080509417fe5167e6c
Message: [Performance] e2e overheads reduction: Small followup diff (#7364)

This script measures the actual performance impact of the optimization.
It supports cross-commit comparison with functional equivalence checking.
""""""

import os
import sys
import json
import time
import math
import importlib
from typing import Dict, Any, Tuple, Optional, List

import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm.sampling_params import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import inspect
import logging

# API Probing helpers - auto-generated for compatibility
def safe_create_object(cls, **kwargs):
    """"""Create object with only valid arguments based on signature.""""""
    try:
        if not callable(cls):
            raise TypeError(f""{cls} is not callable"")
        sig = inspect.signature(cls)
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters and k != ""self""}
        return cls(**valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}"")
        raise

def safe_call_function(func, *args, **kwargs):
    """"""Call function with only valid arguments based on signature.""""""
    try:
        if not callable(func):
            raise TypeError(f""{func} is not callable"")
        sig = inspect.signature(func)
        # Filter kwargs to only valid parameters
        valid_kwargs = {k: v for k, v in kwargs.items() 
                       if k in sig.parameters}
        return func(*args, **valid_kwargs)
    except Exception as e:
        logging.warning(f""Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}"")
        raise

# Specific helpers for common vllm classes
def safe_create_engine_output(**kwargs):
    """"""Create EngineCoreOutput with compatible arguments.""""""
    try:
        from vllm.v1.engine import EngineCoreOutput
        return safe_create_object(EngineCoreOutput, **kwargs)
    except ImportError:
        try:
            from vllm.engine import EngineCoreOutput  
            return safe_create_object(EngineCoreOutput, **kwargs)
        except ImportError:
            raise ImportError(""EngineCoreOutput not found in vllm"")

def safe_create_sampling_params(**kwargs):
    """"""Create SamplingParams with compatible arguments.""""""
    try:
        from vllm import SamplingParams
        return safe_create_object(SamplingParams, **kwargs)
    except ImportError:
        try:
            from vllm import SamplingParams
            return safe_create_object(SamplingParams, **kwargs)
        except ImportError:
            raise ImportError(""SamplingParams not found in vllm"")

def safe_create_llm(**kwargs):
    """"""Create LLM with compatible arguments.""""""
    try:
        from vllm import LLM
        return safe_create_object(LLM, **kwargs)
    except ImportError:
        raise ImportError(""LLM not found in vllm"")



import numpy as np
import torch

# =======================
# Determinism Setup
# =======================
def ensure_determinism():
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        # Disable TF32 for reproducibility unless required
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

# =======================
# Hardware Detection
# =======================
def detect_hardware() -> Dict[str, Any]:
    hw_info = {}
    if torch.cuda.is_available():
        hw_info[""device""] = ""cuda""
        hw_info[""device_name""] = torch.cuda.get_device_name()
        hw_info[""capability""] = torch.cuda.get_device_capability()
        hw_info[""memory_gb""] = torch.cuda.get_device_properties(0).total_memory / 1e9
    else:
        hw_info[""device""] = ""cpu""
        hw_info[""device_name""] = ""CPU""
        hw_info[""memory_gb""] = 0
    return hw_info

# =======================
# Import Resolution
# =======================
def resolve_target() -> Tuple[Any, str]:
    """"""Resolve the optimization target from environment or metadata.""""""
    
    # Priority 1: Environment variables
    module_path = os.getenv(""PROB_MODULE"", """")
    symbol_name = os.getenv(""PROB_SYMBOL"", """")
    
    # Priority 2: Parse from commit metadata - use SequenceGroup.get_finished_seqs
    if not (module_path and symbol_name):
        module_path = ""vllm.sequence""
        symbol_name = ""SequenceGroup""
    
    # Import with error handling
    try:
        module = importlib.import_module(module_path)
        target = module
        for attr in symbol_name.split("".""):
            target = getattr(target, attr)
        
        fq_name = f""{module_path}.{symbol_name}""
        return target, fq_name
        
    except (ImportError, AttributeError) as e:
        error_data = {
            ""target_resolved"": False,
            ""error"": str(e),
            ""attempted_module"": module_path,
            ""attempted_symbol"": symbol_name
        }
        print(json.dumps(error_data))
        sys.exit(1)

# =======================
# Workload Setup
# =======================
def setup() -> Dict[str, Any]:
    """"""Create realistic workload for the optimization.""""""
    ensure_determinism()
    hw_info = detect_hardware()
    
    # Import required vLLM components
    from vllm.compilation.backends import Sequence
    from vllm.core.block.utils import SequenceGroup
    from vllm.core.block_manager import SequenceStatus
    from vllm.core.scheduler import SequenceData
    from vllm import SamplingParams
    from vllm.beam_search import LoRARequest
    
    # Create sampling params
    sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)
    
    # Number of test scenarios
    num_single_seq_groups = 500  # Single sequence groups (fast path)
    num_multi_seq_groups = 100   # Multi-sequence groups (beam search)
    
    single_seq_groups = []
    multi_seq_groups = []
    
    # Create single sequence groups (common case)
    for i in range(num_single_seq_groups):
        seq_id = f""single_{i}""
        seq_data = SequenceData([1, 2, 3, 4, 5])  # Mock prompt tokens
        seq = Sequence(
            seq_id=seq_id,
            inputs={""prompt_token_ids"": [1, 2, 3, 4, 5]},
            block_size=16
        )
        # Mark some as finished
        if i % 3 == 0:
            seq.status = SequenceStatus.FINISHED_STOPPED
        
        seq_group = SequenceGroup(
            request_id=f""req_single_{i}"",
            seqs=[seq],
            sampling_params=sampling_params,
            arrival_time=time.time()
        )
        single_seq_groups.append(seq_group)
    
    # Create multi-sequence groups (beam search case)
    for i in range(num_multi_seq_groups):
        seqs = []
        beam_width = 4
        for j in range(beam_width):
            seq_id = f""multi_{i}_{j}""
            seq_data = SequenceData([1, 2, 3, 4, 5])
            seq = Sequence(
                seq_id=seq_id,
                inputs={""prompt_token_ids"": [1, 2, 3, 4, 5]},
                block_size=16
            )
            # Mark some beams as finished
            if j < 2 and i % 2 == 0:
                seq.status = SequenceStatus.FINISHED_STOPPED
            seqs.append(seq)
        
        seq_group = SequenceGroup(
            request_id=f""req_multi_{i}"",
            seqs=seqs,
            sampling_params=sampling_params,
            arrival_time=time.time()
        )
        multi_seq_groups.append(seq_group)
    
    data = {
        ""device"": hw_info[""device""],
        ""dtype"": torch.float32,  # CPU optimization
        ""hw_info"": hw_info,
        ""single_seq_groups"": single_seq_groups,
        ""multi_seq_groups"": multi_seq_groups,
        ""all_seq_groups"": single_seq_groups + multi_seq_groups
    }
    
    return data

# =======================
# Experiment Execution
# =======================
def experiment(data: Dict[str, Any]) -> Any:
    """"""Execute the optimized operation.""""""
    
    # Test get_finished_seqs() which was optimized
    results = {
        ""single_finished"": [],
        ""multi_finished"": []
    }
    
    # Test single sequence groups (optimized fast path)
    for seq_group in data[""single_seq_groups""]:
        finished = seq_group.get_finished_seqs()
        results[""single_finished""].append(len(finished))
    
    # Test multi-sequence groups
    for seq_group in data[""multi_seq_groups""]:
        finished = seq_group.get_finished_seqs()
        results[""multi_finished""].append(len(finished))
    
    return results

# =======================
# Result I/O
# =======================
def store_result(result: Any, filepath: str) -> None:
    """"""Store result for reference comparison.""""""
    if isinstance(result, torch.Tensor):
        torch.save({""type"": ""tensor"", ""data"": result.cpu()}, filepath)
    else:
        torch.save({""type"": ""generic"", ""data"": result}, filepath)

def load_result(filepath: str) -> Any:
    """"""Load reference result.""""""
    data = torch.load(filepath)
    return data.get(""data"", data)

# =======================
# Equivalence Checking
# =======================
def check_equivalence(current_result: Any, reference_result: Any) -> None:
    """"""Verify functional equivalence.""""""
    if isinstance(current_result, dict):
        assert current_result.keys() == reference_result.keys(), f""Keys mismatch""
        for key in current_result:
            if isinstance(current_result[key], list):
                assert len(current_result[key]) == len(reference_result[key]), f""Length mismatch for {key}""
                assert current_result[key] == reference_result[key], f""Value mismatch for {key}""
    elif isinstance(current_result, torch.Tensor):
        assert current_result.shape == reference_result.shape
        assert current_result.dtype == reference_result.dtype
        
        # Determine tolerances based on dtype
        if current_result.dtype in (torch.float16, torch.bfloat16):
            rtol, atol = 1e-3, 1e-4
        else:
            rtol, atol = 1e-5, 1e-7
        
        torch.testing.assert_close(
            current_result.cpu(),
            reference_result.cpu(),
            rtol=rtol, atol=atol
        )
    else:
        assert current_result == reference_result, f""Value mismatch""

# =======================
# Timing Implementation
# =======================
def time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:
    """"""Time CPU operations with high precision.""""""
    # Warmup
    for _ in range(warmup):
        _ = func()
    
    # Timing
    times_ms = []
    for _ in range(iterations):
        start = time.perf_counter()
        result = func()
        end = time.perf_counter()
        times_ms.append((end - start) * 1000)
    
    # Statistics
    times_ms.sort()
    stats = {
        ""avg_ms"": sum(times_ms) / len(times_ms),
        ""p50_ms"": times_ms[len(times_ms) // 2],
        ""p95_ms"": times_ms[int(len(times_ms) * 0.95)],
        ""p99_ms"": times_ms[int(len(times_ms) * 0.99)],
        ""min_ms"": times_ms[0],
        ""max_ms"": times_ms[-1],
        ""std_ms"": np.std(times_ms)
    }
    
    return result, stats

# =======================
# Main Test Function
# =======================
def run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:
    """"""Main test entry point.""""""
    
    # Setup
    data = setup()
    hw_info = data[""hw_info""]
    
    # This is a CPU optimization - use CPU timing
    warmup = 5
    iters = 100  # More iterations for CPU timing stability
    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)
    avg_ms = timing_stats[""avg_ms""]
    p50_ms = timing_stats[""p50_ms""]
    p95_ms = timing_stats[""p95_ms""]
    
    # Reference handling
    commit_hash = os.getenv(""COMMIT_HASH"", ""fc7b8d1eefcbe837a56b7c080509417fe5167e6c"")
    impl_tag = os.getenv(""IMPL_TAG"", ""child"")
    ref_file = f""{prefix}_{impl_tag}_{commit_hash}_reference.pt""
    
    if reference:
        store_result(result, ref_file)
    
    if eqcheck and os.path.exists(ref_file):
        ref_result = load_result(ref_file)
        check_equivalence(result, ref_result)
    
    # Output compact JSON schema
    summary = {
        ""impl_tag"": impl_tag,
        ""commit_hash"": commit_hash,
        ""device"": ""cpu"",  # This is a CPU optimization
        ""dtype"": ""torch.float32"",
        ""iters"": iters,
        ""warmup"": warmup,
        ""avg_ms"": avg_ms,
        ""p50_ms"": p50_ms,
        ""p95_ms"": p95_ms,
        ""eq_level"": os.getenv(""PROB_EQ_LEVEL"", ""exact""),
        ""opt_path_hit"": True
    }
    print(json.dumps(summary))
    
    return avg_ms / 1000.0

# =======================
# Entry Point
# =======================
if __name__ == ""__main__"":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument(""--eqcheck"", action=""store_true"")
    parser.add_argument(""--reference"", action=""store_true"")
    parser.add_argument(""--prefix"", type=str, default="""")
    args = parser.parse_args()
    
    run_test(args.eqcheck, args.reference, args.prefix)",merged
19d98e0c,19d98e0c,,vllm-project/vllm,,[],,[],2b04c209ee98174f29f1fc98f0dc3222d652a7bd,,serving,claude-code,sonnet-4.5,2026-01-10,deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct,True,,,,,,,,,,,,,,,,,,,,,,,2358.41,1099.58,1141.58,1339.15,35.95,35.18,42.74,35.89,31.97,126.39,,,,,,,,,,,,,,,,,,,,,,,,,,,,separate
99abb8b650c66664cdc84d815b7f306f33bd9881,99abb8b6,Optimize Rejection Sampler with Triton Kernels,vllm-project/vllm,python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model [ngram] --ngram-prompt-lookup-min 5 --ngram-prompt-lookup-max 10 --num-speculative-tokens 5 --input-len 550 --output-len 150,,,,3a1e6481586ed7f079275b5d5072a6e246af691e,,serving,claude_code,claude-sonnet-4-20250514,2026-01-16,meta-llama/Llama-3.1-8B-Instruct,True,,7804.06,7591.49,14809.46,55.57,58.47,60.91,55.54,28.46,128.99,2174.039090000345,2635.1,573.16,541.18,923.83,22.72,23.25,30.07,22.65,16.94,266.1,2179.971499000385,2634.0,656.64,719.77,1087.64,30.98,25.54,210.1,24.46,17.03,397.91,,,92.65561771693196,59.114630196149,59.218581202736765,,,,,,,,,,,,,-0.27287499232771767,-0.04174414633220406,-0.577067527938958,0.01897461196918523,-0.30336472912990214,0.06074411541381583," tokens: 10, Number of emitted tokens: 3.

Profiling iterations:  57%|    | 17/30 [00:36<00:28,  2.17s/it]INFO 01-16 18:19:20 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.28 scoring_time_ms=11.63 verification_time_ms=0.19

Profiling iterations:  60%|    | 18/30 [00:39<00:26,  2.17s/it]
Profiling iterations:  63%|   | 19/30 [00:41<00:23,  2.18s/it]INFO 01-16 18:19:25 [metrics.py:481] Avg prompt throughput: 2635.4 tokens/s, Avg generation throughput: 536.7 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:19:25 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.250, Number of speculative tokens: 5, Number of accepted tokens: 5, Number of draft tokens: 10, Number of emitted tokens: 3.
INFO 01-16 18:19:25 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.63 scoring_time_ms=11.41 verification_time_ms=0.45

Profiling iterations:  67%|   | 20/30 [00:43<00:21,  2.19s/it]
Profiling iterations:  70%|   | 21/30 [00:45<00:19,  2.18s/it]INFO 01-16 18:19:30 [metrics.py:481] Avg prompt throughput: 1756.5 tokens/s, Avg generation throughput: 554.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:19:30 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.250, Number of speculative tokens: 5, Number of accepted tokens: 5, Number of draft tokens: 10, Number of emitted tokens: 3.
INFO 01-16 18:19:30 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.12 verification_time_ms=0.21

Profiling iterations:  73%|  | 22/30 [00:47<00:17,  2.18s/it]
Profiling iterations:  77%|  | 23/30 [00:50<00:15,  2.17s/it]INFO 01-16 18:19:35 [metrics.py:481] Avg prompt throughput: 1757.8 tokens/s, Avg generation throughput: 559.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 01-16 18:19:35 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.250, Number of speculative tokens: 5, Number of accepted tokens: 5, Number of draft tokens: 10, Number of emitted tokens: 3.
INFO 01-16 18:19:35 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.33 scoring_time_ms=11.15 verification_time_ms=0.19

Profiling iterations:  80%|  | 24/30 [00:52<00:13,  2.17s/it]
Profiling iterations:  83%| | 25/30 [00:54<00:10,  2.17s/it]
Profiling iterations:  87%| | 26/30 [00:56<00:08,  2.17s/it]INFO 01-16 18:19:40 [metrics.py:481] Avg prompt throughput: 2637.3 tokens/s, Avg generation throughput: 540.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 01-16 18:19:40 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.250, Number of speculative tokens: 5, Number of accepted tokens: 5, Number of draft tokens: 10, Number of emitted tokens: 3.
INFO 01-16 18:19:40 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.29 scoring_time_ms=11.06 verification_time_ms=0.21

Profiling iterations:  90%| | 27/30 [00:58<00:06,  2.17s/it]
Profiling iterations:  93%|| 28/30 [01:00<00:04,  2.17s/it]INFO 01-16 18:19:45 [metrics.py:481] Avg prompt throughput: 1758.8 tokens/s, Avg generation throughput: 559.6 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:19:45 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.500, System efficiency: 0.250, Number of speculative tokens: 5, Number of accepted tokens: 5, Number of draft tokens: 10, Number of emitted tokens: 3.
INFO 01-16 18:19:45 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.09 verification_time_ms=0.21

Profiling iterations:  97%|| 29/30 [01:03<00:02,  2.17s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.17s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.17s/it]
Avg latency: 2.174039090000345 seconds
10% percentile latency: 2.16461203880026 seconds
25% percentile latency: 2.1657746500013673 seconds
50% percentile latency: 2.168114684000102 seconds
75% percentile latency: 2.1754512775005423 seconds
90% percentile latency: 2.1886876833017594 seconds
99% percentile latency: 2.2230718714207613 seconds
[rank0]:[W116 18:19:49.499356817 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

BENCHMARK_COMPLETE
","ons:  77%|  | 23/30 [00:50<00:15,  2.18s/it]INFO 01-16 18:22:03 [metrics.py:481] Avg prompt throughput: 1757.5 tokens/s, Avg generation throughput: 556.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 01-16 18:22:03 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.800, System efficiency: 0.667, Number of speculative tokens: 5, Number of accepted tokens: 12, Number of draft tokens: 15, Number of emitted tokens: 12.
INFO 01-16 18:22:03 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.32 scoring_time_ms=11.14 verification_time_ms=0.19

Profiling iterations:  80%|  | 24/30 [00:52<00:13,  2.18s/it]
Profiling iterations:  83%| | 25/30 [00:54<00:10,  2.18s/it]INFO 01-16 18:22:08 [metrics.py:481] Avg prompt throughput: 1759.3 tokens/s, Avg generation throughput: 555.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 01-16 18:22:08 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.800, System efficiency: 0.667, Number of speculative tokens: 5, Number of accepted tokens: 12, Number of draft tokens: 15, Number of emitted tokens: 12.

Profiling iterations:  87%| | 26/30 [00:56<00:08,  2.18s/it]INFO 01-16 18:22:08 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.29 scoring_time_ms=11.10 verification_time_ms=0.19

Profiling iterations:  90%| | 27/30 [00:58<00:06,  2.18s/it]
Profiling iterations:  93%|| 28/30 [01:01<00:04,  2.18s/it]INFO 01-16 18:22:13 [metrics.py:481] Avg prompt throughput: 2639.5 tokens/s, Avg generation throughput: 537.5 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:22:13 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.800, System efficiency: 0.667, Number of speculative tokens: 5, Number of accepted tokens: 12, Number of draft tokens: 15, Number of emitted tokens: 12.
INFO 01-16 18:22:13 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.05 verification_time_ms=0.18

Profiling iterations:  97%|| 29/30 [01:03<00:02,  2.18s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.18s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.18s/it]
Avg latency: 2.179971499000385 seconds
10% percentile latency: 2.177658427701317 seconds
25% percentile latency: 2.178150117248151 seconds
50% percentile latency: 2.1797193690017593 seconds
75% percentile latency: 2.181337725000958 seconds
90% percentile latency: 2.1834473690996674 seconds
99% percentile latency: 2.1843047734810535 seconds
[rank0]:[W116 18:22:17.175714656 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

BENCHMARK_COMPLETE
Unable to find image 'anonymous/vllm-baseline:human-99abb8b650c66664cdc84d815b7f306f33bd9881' locally
human-99abb8b650c66664cdc84d815b7f306f33bd9881: Pulling from anonymous/vllm-baseline
3c645031de29: Already exists
0d6448aff889: Already exists
0a7674e3e8fe: Already exists
b71b637b97c5: Already exists
56dc85502937: Already exists
ec6d5f6c9ed9: Already exists
47b8539d532f: Already exists
fd9cc1ad8dee: Already exists
83525caeeb35: Already exists
8e79813a7b9d: Already exists
312a542960e3: Already exists
949691b47390: Already exists
7cbb09265719: Already exists
c1418d32082b: Already exists
7b190e861cd3: Already exists
a2174ed770d5: Already exists
4f4fb700ef54: Already exists
b652f1805f42: Already exists
c37de3041c01: Already exists
0fe2e5d2db60: Already exists
e265b8391010: Already exists
955a4eae3855: Already exists
477046108da7: Already exists
ec400c1e0947: Pulling fs layer
51ca34013f86: Pulling fs layer
13ad8e9f7492: Pulling fs layer
8d4b3ea85338: Pulling fs layer
2907c3729756: Pulling fs layer
71260b7e8aae: Pulling fs layer
2907c3729756: Waiting
71260b7e8aae: Waiting
8d4b3ea85338: Waiting
ec400c1e0947: Verifying Checksum
ec400c1e0947: Download complete
ec400c1e0947: Pull complete
8d4b3ea85338: Verifying Checksum
8d4b3ea85338: Download complete
2907c3729756: Verifying Checksum
2907c3729756: Download complete
71260b7e8aae: Verifying Checksum
71260b7e8aae: Download complete
13ad8e9f7492: Verifying Checksum
13ad8e9f7492: Download complete
51ca34013f86: Verifying Checksum
51ca34013f86: Download complete
51ca34013f86: Pull complete
13ad8e9f7492: Pull complete
8d4b3ea85338: Pull complete
2907c3729756: Pull complete
71260b7e8aae: Pull complete
Digest: sha256:b63a8bba5e0b42114310b1bd8d28be45e4392e819c70d822a0fe6a3b1452c5fd
Status: Downloaded newer image for anonymous/vllm-baseline:human-99abb8b650c66664cdc84d815b7f306f33bd9881
","tive tokens: 5, Number of accepted tokens: 8, Number of draft tokens: 15, Number of emitted tokens: 4.
INFO 01-16 18:24:09 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.08 verification_time_ms=0.19

Profiling iterations:  60%|    | 18/30 [00:39<00:26,  2.19s/it]
Profiling iterations:  63%|   | 19/30 [00:41<00:24,  2.19s/it]INFO 01-16 18:24:14 [metrics.py:481] Avg prompt throughput: 1757.0 tokens/s, Avg generation throughput: 553.3 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:24:14 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.533, System efficiency: 0.222, Number of speculative tokens: 5, Number of accepted tokens: 8, Number of draft tokens: 15, Number of emitted tokens: 4.
INFO 01-16 18:24:14 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.32 scoring_time_ms=11.09 verification_time_ms=0.19

Profiling iterations:  67%|   | 20/30 [00:43<00:21,  2.19s/it]
Profiling iterations:  70%|   | 21/30 [00:45<00:19,  2.19s/it]INFO 01-16 18:24:19 [metrics.py:481] Avg prompt throughput: 1755.8 tokens/s, Avg generation throughput: 553.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 01-16 18:24:19 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.640, System efficiency: 0.367, Number of speculative tokens: 5, Number of accepted tokens: 16, Number of draft tokens: 25, Number of emitted tokens: 11.
INFO 01-16 18:24:19 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.32 scoring_time_ms=11.13 verification_time_ms=0.19

Profiling iterations:  73%|  | 22/30 [00:48<00:17,  2.19s/it]
Profiling iterations:  77%|  | 23/30 [00:50<00:15,  2.19s/it]INFO 01-16 18:24:24 [metrics.py:481] Avg prompt throughput: 1756.6 tokens/s, Avg generation throughput: 552.5 tokens/s, Running: 7 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:24:24 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.640, System efficiency: 0.367, Number of speculative tokens: 5, Number of accepted tokens: 16, Number of draft tokens: 25, Number of emitted tokens: 11.

Profiling iterations:  80%|  | 24/30 [00:52<00:13,  2.19s/it]INFO 01-16 18:24:24 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.28 scoring_time_ms=11.90 verification_time_ms=0.24

Profiling iterations:  83%| | 25/30 [00:54<00:10,  2.19s/it]
Profiling iterations:  87%| | 26/30 [00:56<00:08,  2.19s/it]INFO 01-16 18:24:29 [metrics.py:481] Avg prompt throughput: 2637.6 tokens/s, Avg generation throughput: 535.1 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-16 18:24:29 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.600, System efficiency: 0.389, Number of speculative tokens: 5, Number of accepted tokens: 18, Number of draft tokens: 30, Number of emitted tokens: 14.
INFO 01-16 18:24:29 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.30 scoring_time_ms=11.25 verification_time_ms=0.21

Profiling iterations:  90%| | 27/30 [00:59<00:06,  2.19s/it]
Profiling iterations:  93%|| 28/30 [01:01<00:04,  2.19s/it]INFO 01-16 18:24:34 [metrics.py:481] Avg prompt throughput: 1756.3 tokens/s, Avg generation throughput: 554.0 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 01-16 18:24:34 [metrics.py:503] Speculative metrics: Draft acceptance rate: 0.600, System efficiency: 0.389, Number of speculative tokens: 5, Number of accepted tokens: 18, Number of draft tokens: 30, Number of emitted tokens: 14.
INFO 01-16 18:24:34 [spec_decode_worker.py:1115] SpecDecodeWorker stage times: average_time_per_proposal_tok_ms=0.31 scoring_time_ms=11.15 verification_time_ms=0.20

Profiling iterations:  97%|| 29/30 [01:03<00:02,  2.19s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.19s/it]
Profiling iterations: 100%|| 30/30 [01:05<00:00,  2.19s/it]
Avg latency: 2.1865847636334363 seconds
10% percentile latency: 2.1813670035011454 seconds
25% percentile latency: 2.1856023475002075 seconds
50% percentile latency: 2.1877113260015904 seconds
75% percentile latency: 2.189251810749738 seconds
90% percentile latency: 2.19012676820239 seconds
99% percentile latency: 2.1929258488898995 seconds
[rank0]:[W116 18:24:38.555969443 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

BENCHMARK_COMPLETE
",,claude_code_rerun_fixable
