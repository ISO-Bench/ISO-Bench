[
  {
    "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "commit_short": null,
    "commit_subject": null,
    "repo": null,
    "perf_command": null,
    "files_changed": null,
    "pr_url": null,
    "models": null,
    "parent_commit": null,
    "gpu_config": null,
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": null,
    "model": null,
    "has_agent_patch": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "agent_ttft_mean": 656.64,
    "agent_ttft_median": 719.77,
    "agent_ttft_p99": 1087.64,
    "agent_tpot_mean": 30.98,
    "agent_tpot_median": 25.54,
    "agent_tpot_p99": 210.1,
    "agent_itl_mean": 24.46,
    "agent_itl_median": 17.03,
    "agent_itl_p99": 397.91,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null,
    "data_source": null,
    "_meta": {
      "commit_short": "99abb8b6",
      "action": "INSERT",
      "error": null
    }
  },
  {
    "commit_hash": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "commit_short": "22d33bac",
    "commit_subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "b0e96aaebbfbe8e70478e4192a5a13864ffdefa6",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": 596.34,
    "baseline_ttft_median": 728.56,
    "baseline_ttft_p99": 876.87,
    "baseline_tpot_mean": 22.94,
    "baseline_tpot_median": 14.95,
    "baseline_tpot_p99": 149.16,
    "baseline_itl_mean": 14.22,
    "baseline_itl_median": 12.86,
    "baseline_itl_p99": 16.83,
    "baseline_latency_avg": NaN,
    "baseline_throughput": 2046.93,
    "human_ttft_mean": 786.79,
    "human_ttft_median": 849.31,
    "human_ttft_p99": 1265.03,
    "human_tpot_mean": 19.96,
    "human_tpot_median": 19.56,
    "human_tpot_p99": 23.2,
    "human_itl_mean": 19.96,
    "human_itl_median": 16.94,
    "human_itl_p99": 114.13,
    "human_latency_avg": NaN,
    "human_throughput": 3946.1,
    "agent_ttft_mean": 651.12,
    "agent_ttft_median": 701.84,
    "agent_ttft_p99": 1081.64,
    "agent_tpot_mean": 30.51,
    "agent_tpot_median": 25.76,
    "agent_tpot_p99": 216.86,
    "agent_itl_mean": 24.52,
    "agent_itl_median": 17.12,
    "agent_itl_p99": 410.71,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": "INFO 01-02 17:09:08 [__init__.py:256] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null,
    "data_source": "merged",
    "_meta": {
      "commit_short": "22d33bac",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "commit_short": "9badee53",
    "commit_subject": "Fix performance when `--generation-config` is not ",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "beebf4742af80296d3c3a657c66d512615c550c1",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": 2894.68,
    "baseline_ttft_median": 2744.14,
    "baseline_ttft_p99": 5268.14,
    "baseline_tpot_mean": 18.21,
    "baseline_tpot_median": 19.07,
    "baseline_tpot_p99": 19.69,
    "baseline_itl_mean": 18.19,
    "baseline_itl_median": 14.86,
    "baseline_itl_p99": 31.15,
    "baseline_latency_avg": NaN,
    "baseline_throughput": 10588.27,
    "human_ttft_mean": 168.63,
    "human_ttft_median": 163.24,
    "human_ttft_p99": 219.03,
    "human_tpot_mean": 7.83,
    "human_tpot_median": 7.95,
    "human_tpot_p99": 9.19,
    "human_itl_mean": 7.83,
    "human_itl_median": 6.79,
    "human_itl_p99": 51.59,
    "human_latency_avg": NaN,
    "human_throughput": 3424.18,
    "agent_ttft_mean": 174.68,
    "agent_ttft_median": 177.73,
    "agent_ttft_p99": 236.2,
    "agent_tpot_mean": 9.85,
    "agent_tpot_median": 8.09,
    "agent_tpot_p99": 56.03,
    "agent_itl_mean": 7.98,
    "agent_itl_median": 6.42,
    "agent_itl_p99": 79.35,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": 94.17448560808103,
    "human_improvement_tpot_mean": 57.001647446457994,
    "human_improvement_itl_mean": 56.95437053326003,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": -11.494417879408074,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": "INFO 01-02 16:41:24 [__init__.py:253] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', max_concurrency=None, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1315, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 876, in main\n    input_requests = sample_sharegpt_requests(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 102, in sample_sharegpt_requests\n    with open(dataset_path, encoding='utf-8') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null,
    "data_source": "merged",
    "_meta": {
      "commit_short": "9badee53",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "e206b5433109d298e53451015465b2bf8f03ef0a",
    "commit_short": "e206b543",
    "commit_subject": "[v0][Core] Use xgrammar shared context to avoid co",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --guided-decoding-backend xgrammar",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "1d35662e6dc199431bfe4004cc84d66fd9b297b1",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": 576.76,
    "baseline_ttft_median": 569.57,
    "baseline_ttft_p99": 993.18,
    "baseline_tpot_mean": 22.04,
    "baseline_tpot_median": 22.19,
    "baseline_tpot_p99": 27.97,
    "baseline_itl_mean": 22.03,
    "baseline_itl_median": 17.04,
    "baseline_itl_p99": 84.57,
    "baseline_latency_avg": NaN,
    "baseline_throughput": 3116.22,
    "human_ttft_mean": 574.18,
    "human_ttft_median": 533.96,
    "human_ttft_p99": 920.66,
    "human_tpot_mean": 22.76,
    "human_tpot_median": 23.42,
    "human_tpot_p99": 30.25,
    "human_itl_mean": 22.66,
    "human_itl_median": 16.98,
    "human_itl_p99": 273.01,
    "human_latency_avg": NaN,
    "human_throughput": 3105.08,
    "agent_ttft_mean": 669.91,
    "agent_ttft_median": 705.32,
    "agent_ttft_p99": 1099.28,
    "agent_tpot_mean": 30.88,
    "agent_tpot_median": 23.74,
    "agent_tpot_p99": 225.1,
    "agent_itl_mean": 24.56,
    "agent_itl_median": 17.15,
    "agent_itl_p99": 288.98,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": 0.44732644427492213,
    "human_improvement_tpot_mean": -3.2667876588021887,
    "human_improvement_itl_mean": -2.8597367226509256,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": 1.452079763302976,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": "INFO 01-02 17:04:36 [__init__.py:207] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT]\n                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf}]\n                            [--dataset-path DATASET_PATH]\n                            [--max-concurrency MAX_CONCURRENCY] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--logprobs LOGPROBS]\n                            [--request-rate REQUEST_RATE]\n                            [--burstiness BURSTINESS] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm] [--profile]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME] [--ignore-eos]\n                            [--percentile-metrics PERCENTILE_METRICS]\n                            [--metric-percentiles METRIC_PERCENTILES]\n                            [--goodput GOODPUT [GOODPUT ...]]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--random-prefix-len RANDOM_PREFIX_LEN]\n                            [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]\n                            [--hf-output-len HF_OUTPUT_LEN]\n                            [--tokenizer-mode {auto,slow,mistral,custom}]\n                            [--served-model-name SERVED_MODEL_NAME]\n                            [--lora-modules LORA_MODULES [LORA_MODULES ...]]\nbenchmark_serving.py: error: unrecognized arguments: --guided-decoding-backend xgrammar\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null,
    "data_source": "merged",
    "_meta": {
      "commit_short": "e206b543",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "commit_short": "89a84b0b",
    "commit_subject": "[Core] Use array to speedup padding (#6779)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py",
      "vllm/model_executor/sampling_metadata.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/6779",
    "models": [
      "N/A"
    ],
    "parent_commit": "084a01fd3544557990f8af8af6fd3c1185bae848",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "Qwen/Qwen1.5-0.5B",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 3558.51,
    "agent_ttft_mean": 356.05,
    "agent_ttft_median": 349.72,
    "agent_ttft_p99": 407.54,
    "agent_tpot_mean": 23.73,
    "agent_tpot_median": 23.36,
    "agent_tpot_p99": 38.75,
    "agent_itl_mean": 28.48,
    "agent_itl_median": 19.34,
    "agent_itl_p99": 348.87,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 89a84b0bb7b30706a02836234a94493ea8f780bf\nMessage: [Core] Use array to speedup padding (#6779)\n\nThis script measures the actual performance impact of using arrays instead of lists\nfor token storage in vLLM's sampling metadata preparation.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom array import array\nfrom typing import Dict, Any, Tuple, Optional, List\nimport random\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.model_executor.sampling_metadata\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"SamplingTensors.from_lists\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Realistic vLLM workload parameters\n    batch_size = 32  # Number of sequences\n    vocab_size = 32000  # Llama vocab size\n    max_prompt_len = 2048\n    max_output_len = 512\n    \n    # Generate token lists that would be used in sampling\n    prompt_tokens = []\n    output_tokens = []\n    \n    for _ in range(batch_size):\n        # Generate varying length prompts and outputs\n        prompt_len = random.randint(128, max_prompt_len)\n        output_len = random.randint(1, max_output_len)\n        \n        # Use arrays as per the optimization\n        prompt_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(prompt_len)])\n        output_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(output_len)])\n        \n        prompt_tokens.append(prompt_seq)\n        output_tokens.append(output_seq)\n    \n    # Other sampling parameters\n    temperatures = [0.7] * batch_size\n    top_ps = [0.9] * batch_size\n    top_ks = [40] * batch_size\n    min_ps = [0.0] * batch_size\n    presence_penalties = [0.0] * batch_size\n    frequency_penalties = [0.0] * batch_size\n    repetition_penalties = [1.0] * batch_size\n    sampling_seeds = [random.randint(0, 2**31-1) for _ in range(batch_size)]\n    sample_indices = list(range(batch_size))\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"temperatures\": temperatures,\n        \"top_ps\": top_ps,\n        \"top_ks\": top_ks,\n        \"min_ps\": min_ps,\n        \"presence_penalties\": presence_penalties,\n        \"frequency_penalties\": frequency_penalties,\n        \"repetition_penalties\": repetition_penalties,\n        \"sampling_seeds\": sampling_seeds,\n        \"sample_indices\": sample_indices,\n        \"prompt_tokens\": prompt_tokens,\n        \"output_tokens\": output_tokens,\n        \"vocab_size\": vocab_size,\n        \"extra_seeds_to_generate\": 0,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call SamplingTensors.from_lists with the prepared data\n    result = target(\n        temperatures=data[\"temperatures\"],\n        top_ps=data[\"top_ps\"],\n        top_ks=data[\"top_ks\"],\n        min_ps=data[\"min_ps\"],\n        presence_penalties=data[\"presence_penalties\"],\n        frequency_penalties=data[\"frequency_penalties\"],\n        repetition_penalties=data[\"repetition_penalties\"],\n        sampling_seeds=data[\"sampling_seeds\"],\n        sample_indices=data[\"sample_indices\"],\n        prompt_tokens=data[\"prompt_tokens\"],\n        output_tokens=data[\"output_tokens\"],\n        vocab_size=data[\"vocab_size\"],\n        extra_seeds_to_generate=data[\"extra_seeds_to_generate\"],\n        device=data[\"device\"],\n        dtype=data[\"dtype\"]\n    )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store the tensor attributes of SamplingTensors\n    tensors_dict = {\n        \"temperatures\": result.temperatures.cpu(),\n        \"top_ps\": result.top_ps.cpu(),\n        \"top_ks\": result.top_ks.cpu(),\n        \"min_ps\": result.min_ps.cpu(),\n        \"presence_penalties\": result.presence_penalties.cpu(),\n        \"frequency_penalties\": result.frequency_penalties.cpu(),\n        \"repetition_penalties\": result.repetition_penalties.cpu(),\n        \"prompt_tokens\": result.prompt_tokens.cpu(),\n        \"output_tokens\": result.output_tokens.cpu(),\n        \"sampling_seeds\": result.sampling_seeds.cpu(),\n        \"sample_indices\": result.sample_indices.cpu(),\n    }\n    torch.save({\"type\": \"sampling_tensors\", \"data\": tensors_dict}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Check each tensor attribute\n    attrs_to_check = [\n        \"temperatures\", \"top_ps\", \"top_ks\", \"min_ps\",\n        \"presence_penalties\", \"frequency_penalties\", \"repetition_penalties\",\n        \"prompt_tokens\", \"output_tokens\", \"sampling_seeds\", \"sample_indices\"\n    ]\n    \n    for attr in attrs_to_check:\n        current_tensor = getattr(current_result, attr).cpu()\n        ref_tensor = reference_result[attr]\n        \n        assert current_tensor.shape == ref_tensor.shape, f\"{attr} shape mismatch\"\n        assert current_tensor.dtype == ref_tensor.dtype, f\"{attr} dtype mismatch\"\n        \n        if current_tensor.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_tensor,\n            ref_tensor,\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This optimization primarily affects CPU operations (array vs list)\n    # so we time on CPU\n    warmup = 5\n    iters = 20\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"89a84b0bb7b30706a02836234a94493ea8f780bf\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization affects CPU operations\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "89a84b0b",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "19d98e0c",
    "commit_short": "19d98e0c",
    "commit_subject": null,
    "repo": "vllm-project/vllm",
    "perf_command": null,
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "2b04c209ee98174f29f1fc98f0dc3222d652a7bd",
    "gpu_config": null,
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-10",
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2358.41,
    "agent_ttft_mean": 1099.58,
    "agent_ttft_median": 1141.58,
    "agent_ttft_p99": 1339.15,
    "agent_tpot_mean": 35.95,
    "agent_tpot_median": 35.18,
    "agent_tpot_p99": 42.74,
    "agent_itl_mean": 35.89,
    "agent_itl_median": 31.97,
    "agent_itl_p99": 126.39,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null,
    "data_source": "separate",
    "_meta": {
      "commit_short": "19d98e0c",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
    "commit_short": "6e36f4fa",
    "commit_subject": "improve chunked prefill performance",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/basic_correctness/test_chunked_prefill.py",
      "vllm/core/scheduler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7874",
    "models": [
      "N/A"
    ],
    "parent_commit": "dd2a6a82e3f41b4673b1dbb24b2e99230ea96981",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2413.58,
    "agent_ttft_mean": 1011.46,
    "agent_ttft_median": 1119.57,
    "agent_ttft_p99": 1398.78,
    "agent_tpot_mean": 30.52,
    "agent_tpot_median": 27.71,
    "agent_tpot_p99": 71.21,
    "agent_itl_mean": 33.53,
    "agent_itl_median": 22.23,
    "agent_itl_p99": 189.79,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6e36f4fa6ce64619b9ea94c88a157f5783a63a65\nMessage: improve chunked prefill performance\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\nfrom dataclasses import dataclass, field\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, the target is the Scheduler class\n        module_path = \"vllm.core.scheduler\"\n        symbol_name = \"Scheduler\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Mock Classes for Testing\n# =======================\n@dataclass\nclass SequenceData:\n    \"\"\"Mock sequence data\"\"\"\n    prompt_token_ids: List[int] = field(default_factory=list)\n    output_token_ids: List[int] = field(default_factory=list)\n    \n    def get_len(self):\n        return len(self.prompt_token_ids) + len(self.output_token_ids)\n    \n    def get_num_computed_tokens(self):\n        return 0\n\nclass Sequence:\n    \"\"\"Mock sequence\"\"\"\n    def __init__(self, seq_id, prompt_tokens):\n        self.seq_id = seq_id\n        self.data = SequenceData(prompt_token_ids=prompt_tokens)\n        self.status = \"WAITING\"\n    \n    def get_num_new_tokens(self):\n        return len(self.data.prompt_token_ids)\n    \n    def is_finished(self):\n        return False\n\nclass SequenceGroup:\n    \"\"\"Mock sequence group\"\"\"\n    def __init__(self, request_id, seqs, is_prefill=True):\n        self.request_id = request_id\n        self.seqs = seqs\n        self._is_prefill = is_prefill\n        self.lora_int_id = 0\n        self.sampling_params = None\n        self.pooling_params = None\n        self.lora_request = None\n        self.prompt_adapter_request = None\n        self.multi_modal_data = None\n        self.state = None\n        self.metrics = None\n    \n    def is_prefill(self):\n        return self._is_prefill\n    \n    def get_seqs(self, status=None):\n        if status:\n            return [s for s in self.seqs if s.status == status]\n        return self.seqs\n    \n    def get_max_num_running_seqs(self):\n        return len(self.seqs)\n    \n    def is_encoder_decoder(self):\n        return False\n    \n    def get_encoder_seq(self):\n        return None\n    \n    def is_finished(self):\n        return all(s.is_finished() for s in self.seqs)\n    \n    def init_multi_step(self, num_scheduler_steps):\n        pass\n    \n    def maybe_set_first_scheduled_time(self, now):\n        pass\n\n@dataclass\nclass ScheduledSequenceGroup:\n    seq_group: SequenceGroup\n    token_chunk_size: int\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Create a mix of prefill and decode requests to test chunked prefill scheduling\n    num_prefill_requests = 8\n    num_decode_requests = 16\n    prefill_seq_len = 512  # Tokens per prefill request\n    \n    # Create prefill sequence groups\n    prefill_groups = []\n    for i in range(num_prefill_requests):\n        seq = Sequence(f\"prefill_{i}\", list(range(prefill_seq_len)))\n        seq.status = \"WAITING\"\n        group = SequenceGroup(f\"prefill_req_{i}\", [seq], is_prefill=True)\n        prefill_groups.append(group)\n    \n    # Create decode sequence groups (already running)\n    decode_groups = []\n    for i in range(num_decode_requests):\n        seq = Sequence(f\"decode_{i}\", [0])  # Single token for decode\n        seq.status = \"RUNNING\"\n        group = SequenceGroup(f\"decode_req_{i}\", [seq], is_prefill=False)\n        decode_groups.append(group)\n    \n    # Create swapped sequence groups\n    swapped_groups = []\n    for i in range(4):\n        seq = Sequence(f\"swapped_{i}\", [0])\n        seq.status = \"SWAPPED\"\n        group = SequenceGroup(f\"swapped_req_{i}\", [seq], is_prefill=False)\n        swapped_groups.append(group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32,\n        \"hw_info\": hw_info,\n        \"prefill_groups\": prefill_groups,\n        \"decode_groups\": decode_groups,\n        \"swapped_groups\": swapped_groups,\n        \"max_num_batched_tokens\": 2048,\n        \"max_num_seqs\": 256,\n        \"enable_chunking\": True,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Import necessary vLLM components\n    try:\n        from vllm.core.scheduler import Scheduler, SchedulingBudget\n        from vllm.core.scheduler import SchedulerPrefillOutputs, SchedulerSwappedInOutputs\n        from vllm.config import SchedulerConfig, CacheConfig\n    except ImportError as e:\n        # Fallback: simulate the scheduling behavior\n        return simulate_chunked_prefill_scheduling(data)\n    \n    # Create scheduler config\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=data[\"max_num_batched_tokens\"],\n        max_num_seqs=data[\"max_num_seqs\"],\n        max_model_len=2048,\n        chunked_prefill_enabled=data[\"enable_chunking\"],\n    )\n    \n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        cache_dtype=\"auto\",\n    )\n    \n    # Create scheduler instance\n    scheduler = Scheduler(\n        scheduler_config=scheduler_config,\n        cache_config=cache_config,\n        lora_config=None,\n    )\n    \n    # Add sequence groups to scheduler queues\n    for group in data[\"prefill_groups\"]:\n        scheduler.waiting.append(group)\n    \n    for group in data[\"decode_groups\"]:\n        scheduler.running.append(group)\n    \n    for group in data[\"swapped_groups\"]:\n        scheduler.swapped.append(group)\n    \n    # Execute the chunked prefill scheduling\n    with torch.no_grad():\n        result = scheduler._schedule_chunked_prefill()\n    \n    # Extract scheduling order for comparison\n    scheduled_order = []\n    for seq_group in result.scheduled_seq_groups:\n        scheduled_order.append({\n            \"request_id\": seq_group.seq_group.request_id,\n            \"is_prefill\": seq_group.seq_group.is_prefill(),\n            \"token_chunk_size\": seq_group.token_chunk_size,\n        })\n    \n    return {\n        \"scheduled_order\": scheduled_order,\n        \"num_prefill_groups\": result.num_prefill_groups,\n        \"num_batched_tokens\": result.num_batched_tokens,\n        \"preempted\": result.preempted,\n    }\n\ndef simulate_chunked_prefill_scheduling(data: Dict[str, Any]) -> Any:\n    \"\"\"Simulate the scheduling behavior when vLLM is not available.\"\"\"\n    \n    # Simulate the optimized scheduling order:\n    # 1. Decode requests first (from running)\n    # 2. Swapped-in decode requests\n    # 3. Swapped-in prefill requests  \n    # 4. Running prefill requests (chunked)\n    # 5. New prefill requests\n    \n    scheduled_order = []\n    \n    # Schedule decode requests first (optimization)\n    for group in data[\"decode_groups\"]:\n        scheduled_order.append({\n            \"request_id\": group.request_id,\n            \"is_prefill\": False,\n            \"token_chunk_size\": 1,\n        })\n    \n    # Schedule swapped requests\n    for group in data[\"swapped_groups\"]:\n        scheduled_order.append({\n            \"request_id\": group.request_id,\n            \"is_prefill\": group.is_prefill(),\n            \"token_chunk_size\": 1 if not group.is_prefill() else 512,\n        })\n    \n    # Schedule new prefill requests (chunked)\n    token_budget = data[\"max_num_batched_tokens\"] - len(data[\"decode_groups\"])\n    for group in data[\"prefill_groups\"]:\n        if token_budget > 0:\n            chunk_size = min(512, token_budget)\n            scheduled_order.append({\n                \"request_id\": group.request_id,\n                \"is_prefill\": True,\n                \"token_chunk_size\": chunk_size,\n            })\n            token_budget -= chunk_size\n    \n    return {\n        \"scheduled_order\": scheduled_order,\n        \"num_prefill_groups\": len([s for s in scheduled_order if s[\"is_prefill\"]]),\n        \"num_batched_tokens\": sum(s[\"token_chunk_size\"] for s in scheduled_order),\n        \"preempted\": 0,\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    \n    # Check that both results have the same structure\n    assert set(current_result.keys()) == set(reference_result.keys()), \\\n        f\"Result keys mismatch: {current_result.keys()} vs {reference_result.keys()}\"\n    \n    # Check scheduling order maintains decode-first priority\n    current_order = current_result[\"scheduled_order\"]\n    reference_order = reference_result[\"scheduled_order\"]\n    \n    # Verify decode requests are scheduled before prefills\n    def get_first_prefill_index(order):\n        for i, item in enumerate(order):\n            if item[\"is_prefill\"]:\n                return i\n        return len(order)\n    \n    current_first_prefill = get_first_prefill_index(current_order)\n    reference_first_prefill = get_first_prefill_index(reference_order)\n    \n    # The optimization should schedule decodes first\n    assert current_first_prefill > 0, \"No decode requests scheduled before prefills\"\n    \n    # Check numerical values\n    assert abs(current_result[\"num_batched_tokens\"] - reference_result[\"num_batched_tokens\"]) <= 512, \\\n        f\"Token count mismatch: {current_result['num_batched_tokens']} vs {reference_result['num_batched_tokens']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6e36f4fa6ce64619b9ea94c88a157f5783a63a65\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "6e36f4fa",
      "action": "UPDATE",
      "error": null
    }
  },
  {
    "commit_hash": "7c01f706418d593b3cf23d2ec9110dca7151c539",
    "commit_short": "7c01f706",
    "commit_subject": "[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/5974",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2229.44,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 7c01f706418d593b3cf23d2ec9110dca7151c539\nMessage: [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimization is in SequenceStatus.is_finished\n        module_path = \"vllm.sequence\"\n        symbol_name = \"SequenceStatus\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # The optimization is for SequenceStatus.is_finished which checks if a status is finished\n    # We need to create a workload that tests this method with various status values\n    \n    SequenceStatus, _ = resolve_target()\n    \n    # Create all possible status values to test\n    all_statuses = [\n        SequenceStatus.WAITING,\n        SequenceStatus.RUNNING,\n        SequenceStatus.SWAPPED,\n        SequenceStatus.FINISHED_STOPPED,\n        SequenceStatus.FINISHED_LENGTH_CAPPED,\n        SequenceStatus.FINISHED_ABORTED,\n        SequenceStatus.FINISHED_IGNORED,\n    ]\n    \n    # Create a large test set with repeated status checks to measure performance\n    # Simulate realistic usage patterns with more finished statuses (common in batch processing)\n    test_statuses = []\n    # 30% waiting/running/swapped, 70% finished (realistic for batch inference)\n    for _ in range(10000):\n        if np.random.random() < 0.3:\n            test_statuses.append(np.random.choice(all_statuses[:3]))\n        else:\n            test_statuses.append(np.random.choice(all_statuses[3:]))\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": None,  # Not applicable for this optimization\n        \"hw_info\": hw_info,\n        \"SequenceStatus\": SequenceStatus,\n        \"test_statuses\": test_statuses,\n        \"all_statuses\": all_statuses,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    SequenceStatus = data[\"SequenceStatus\"]\n    test_statuses = data[\"test_statuses\"]\n    \n    # The optimization is in the is_finished static method\n    # We'll call it many times to measure the performance improvement\n    results = []\n    for status in test_statuses:\n        result = SequenceStatus.is_finished(status)\n        results.append(result)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store as JSON since results are boolean values\n    import pickle\n    with open(filepath, 'wb') as f:\n        pickle.dump(result, f)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    import pickle\n    with open(filepath, 'rb') as f:\n        return pickle.load(f)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, list), f\"Expected list, got {type(current_result)}\"\n    assert isinstance(reference_result, list), f\"Expected list, got {type(reference_result)}\"\n    assert len(current_result) == len(reference_result), f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr == ref, f\"Mismatch at index {i}: {curr} vs {ref}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-only optimization (enum comparison)\n    warmup = 5\n    iters = 100\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"7c01f706418d593b3cf23d2ec9110dca7151c539\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pkl\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU-only optimization\n        \"dtype\": \"None\",  # Not applicable\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "7c01f706",
      "action": "UPDATE_TO_FAILURE",
      "error": "Server crashed after applying patch"
    }
  },
  {
    "commit_hash": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
    "commit_short": "fc7b8d1e",
    "commit_subject": "[Performance] e2e overheads reduction: Small followup diff (#7364)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/core/block_manager_v1.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7364",
    "models": [
      "N/A"
    ],
    "parent_commit": "67abdbb42fdbb59c274130368981c0d0ac3539e3",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2214.0,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: fc7b8d1eefcbe837a56b7c080509417fe5167e6c\nMessage: [Performance] e2e overheads reduction: Small followup diff (#7364)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use SequenceGroup.get_finished_seqs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.sequence\"\n        symbol_name = \"SequenceGroup\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm.core.scheduler import SequenceData\n    from vllm import SamplingParams\n    from vllm.beam_search import LoRARequest\n    \n    # Create sampling params\n    sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)\n    \n    # Number of test scenarios\n    num_single_seq_groups = 500  # Single sequence groups (fast path)\n    num_multi_seq_groups = 100   # Multi-sequence groups (beam search)\n    \n    single_seq_groups = []\n    multi_seq_groups = []\n    \n    # Create single sequence groups (common case)\n    for i in range(num_single_seq_groups):\n        seq_id = f\"single_{i}\"\n        seq_data = SequenceData([1, 2, 3, 4, 5])  # Mock prompt tokens\n        seq = Sequence(\n            seq_id=seq_id,\n            inputs={\"prompt_token_ids\": [1, 2, 3, 4, 5]},\n            block_size=16\n        )\n        # Mark some as finished\n        if i % 3 == 0:\n            seq.status = SequenceStatus.FINISHED_STOPPED\n        \n        seq_group = SequenceGroup(\n            request_id=f\"req_single_{i}\",\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time()\n        )\n        single_seq_groups.append(seq_group)\n    \n    # Create multi-sequence groups (beam search case)\n    for i in range(num_multi_seq_groups):\n        seqs = []\n        beam_width = 4\n        for j in range(beam_width):\n            seq_id = f\"multi_{i}_{j}\"\n            seq_data = SequenceData([1, 2, 3, 4, 5])\n            seq = Sequence(\n                seq_id=seq_id,\n                inputs={\"prompt_token_ids\": [1, 2, 3, 4, 5]},\n                block_size=16\n            )\n            # Mark some beams as finished\n            if j < 2 and i % 2 == 0:\n                seq.status = SequenceStatus.FINISHED_STOPPED\n            seqs.append(seq)\n        \n        seq_group = SequenceGroup(\n            request_id=f\"req_multi_{i}\",\n            seqs=seqs,\n            sampling_params=sampling_params,\n            arrival_time=time.time()\n        )\n        multi_seq_groups.append(seq_group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float32,  # CPU optimization\n        \"hw_info\": hw_info,\n        \"single_seq_groups\": single_seq_groups,\n        \"multi_seq_groups\": multi_seq_groups,\n        \"all_seq_groups\": single_seq_groups + multi_seq_groups\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Test get_finished_seqs() which was optimized\n    results = {\n        \"single_finished\": [],\n        \"multi_finished\": []\n    }\n    \n    # Test single sequence groups (optimized fast path)\n    for seq_group in data[\"single_seq_groups\"]:\n        finished = seq_group.get_finished_seqs()\n        results[\"single_finished\"].append(len(finished))\n    \n    # Test multi-sequence groups\n    for seq_group in data[\"multi_seq_groups\"]:\n        finished = seq_group.get_finished_seqs()\n        results[\"multi_finished\"].append(len(finished))\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        for key in current_result:\n            if isinstance(current_result[key], list):\n                assert len(current_result[key]) == len(reference_result[key]), f\"Length mismatch for {key}\"\n                assert current_result[key] == reference_result[key], f\"Value mismatch for {key}\"\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n    else:\n        assert current_result == reference_result, f\"Value mismatch\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations with high precision.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU optimization - use CPU timing\n    warmup = 5\n    iters = 100  # More iterations for CPU timing stability\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"fc7b8d1eefcbe837a56b7c080509417fe5167e6c\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU optimization\n        \"dtype\": \"torch.float32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "fc7b8d1e",
      "action": "UPDATE_TO_FAILURE",
      "error": "No metrics in agent output"
    }
  },
  {
    "commit_hash": "3a243095e5e7b655b63ab08fbd5936cb40850415",
    "commit_short": "3a243095",
    "commit_subject": "Optimize `_get_ranks` in Sampler (#3623)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3623",
    "models": [
      "N/A"
    ],
    "parent_commit": "64172a976c8d975b3aec946f1675716d2532d94f",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2518.78,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 3a243095e5e7b655b63ab08fbd5936cb40850415\nMessage: Optimize `_get_ranks` in Sampler (#3623)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimized function is _get_ranks\n        module_path = \"vllm.model_executor.layers.sampler\"\n        symbol_name = \"_get_ranks\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float32  # logprobs are float32 as per code\n    \n    # Realistic workload sizes for _get_ranks function\n    # This function processes logprobs during sampling\n    batch_size = 64  # Number of sequences being processed\n    vocab_size = 32000  # Typical vocab size for LLMs like Llama\n    \n    # Create logprobs tensor (2D: [batch_size, vocab_size])\n    # Use realistic distribution - log of softmax outputs\n    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n    logprobs = torch.log_softmax(logits, dim=-1)\n    \n    # Create indices tensor - chosen token indices for each sequence\n    # These would be the sampled tokens\n    indices = torch.randint(0, vocab_size, (batch_size,), device=device, dtype=torch.long)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"logprobs\": logprobs,\n        \"indices\": indices,\n        \"batch_size\": batch_size,\n        \"vocab_size\": vocab_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the optimized _get_ranks function\n    with torch.no_grad():\n        result = target(data[\"logprobs\"], data[\"indices\"])\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Ranks should be exact integers\n        rtol, atol = 0, 0\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"3a243095e5e7b655b63ab08fbd5936cb40850415\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "3a243095",
      "action": "UPDATE_TO_FAILURE",
      "error": "No metrics in agent output"
    }
  },
  {
    "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
    "commit_short": "e3580537",
    "commit_subject": "[Performance] Enable chunked prefill and prefix caching together (#7753)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048",
    "files_changed": [
      "tests/basic_correctness/test_chunked_prefill.py",
      "tests/core/test_block_manager.py",
      "tests/core/test_chunked_prefill_scheduler.py",
      "vllm/core/block_manager_v1.py",
      "vllm/core/block_manager_v2.py",
      "vllm/core/embedding_model_block_manager.py",
      "vllm/core/interfaces.py",
      "vllm/core/scheduler.py",
      "vllm/worker/model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7753",
    "models": [
      "N/A"
    ],
    "parent_commit": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "agent_name": "claude-code",
    "agent_model": "sonnet-4.5",
    "benchmark_date": "2026-01-14",
    "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
    "has_agent_patch": true,
    "patch_path": null,
    "baseline_ttft_mean": NaN,
    "baseline_ttft_median": NaN,
    "baseline_ttft_p99": NaN,
    "baseline_tpot_mean": NaN,
    "baseline_tpot_median": NaN,
    "baseline_tpot_p99": NaN,
    "baseline_itl_mean": NaN,
    "baseline_itl_median": NaN,
    "baseline_itl_p99": NaN,
    "baseline_latency_avg": NaN,
    "baseline_throughput": NaN,
    "human_ttft_mean": NaN,
    "human_ttft_median": NaN,
    "human_ttft_p99": NaN,
    "human_tpot_mean": NaN,
    "human_tpot_median": NaN,
    "human_tpot_p99": NaN,
    "human_itl_mean": NaN,
    "human_itl_median": NaN,
    "human_itl_p99": NaN,
    "human_latency_avg": NaN,
    "human_throughput": 2496.89,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "human_improvement_ttft_mean": NaN,
    "human_improvement_tpot_mean": NaN,
    "human_improvement_itl_mean": NaN,
    "agent_improvement_ttft_mean": NaN,
    "agent_improvement_tpot_mean": NaN,
    "agent_improvement_itl_mean": NaN,
    "agent_vs_human_ttft_mean": NaN,
    "agent_vs_human_tpot_mean": NaN,
    "agent_vs_human_itl_mean": NaN,
    "human_improvement_ttft_median": NaN,
    "human_improvement_ttft_p99": NaN,
    "agent_improvement_ttft_median": NaN,
    "agent_improvement_ttft_p99": NaN,
    "agent_vs_human_ttft_median": NaN,
    "agent_vs_human_ttft_p99": NaN,
    "human_improvement_latency_avg": NaN,
    "human_improvement_throughput": NaN,
    "agent_improvement_latency_avg": NaN,
    "agent_improvement_throughput": NaN,
    "agent_vs_human_latency_avg": NaN,
    "agent_vs_human_throughput": NaN,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: e3580537a41a46b0f3cd750b86b633c1857a8c90\nMessage: [Performance] Enable chunked prefill and prefix caching together (#7753)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target mark_blocks_as_computed\n    if not (module_path and symbol_name):\n        # Based on the diff, the key change is in mark_blocks_as_computed method\n        module_path = \"vllm.core.block_manager_v1\"\n        symbol_name = \"BlockSpaceManagerV1.mark_blocks_as_computed\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create a block manager setup that exercises chunked prefill + prefix caching\n    block_size = 16\n    num_gpu_blocks = 256\n    num_cpu_blocks = 0\n    \n    # Import the block manager class\n    from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm import SamplingParams\n    \n    # Create block manager with prefix caching enabled\n    block_manager = BlockSpaceManagerV1(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=num_cpu_blocks,\n        watermark=0.01,\n        enable_caching=True  # Enable prefix caching\n    )\n    \n    # Create a sequence group with a long prompt to test chunked prefill\n    prompt_length = 512  # Long enough to require multiple chunks\n    token_chunk_size = 64  # Chunk size for chunked prefill\n    \n    # Create sequence\n    seq = Sequence(\n        seq_id=0,\n        inputs={\"prompt_token_ids\": list(range(prompt_length))},\n        block_size=block_size\n    )\n    \n    # Create sequence group\n    seq_group = SequenceGroup(\n        request_id=\"test_request\",\n        seqs=[seq],\n        arrival_time=time.time(),\n        sampling_params=SamplingParams()\n    )\n    \n    # Allocate blocks for the sequence\n    block_manager.allocate(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"block_manager\": block_manager,\n        \"seq_group\": seq_group,\n        \"token_chunk_size\": token_chunk_size,\n        \"block_size\": block_size,\n        \"prompt_length\": prompt_length\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    block_manager = data[\"block_manager\"]\n    seq_group = data[\"seq_group\"]\n    token_chunk_size = data[\"token_chunk_size\"]\n    prompt_length = data[\"prompt_length\"]\n    \n    # Simulate chunked prefill by marking blocks as computed in chunks\n    results = []\n    num_chunks = (prompt_length + token_chunk_size - 1) // token_chunk_size\n    \n    for chunk_idx in range(num_chunks):\n        # Update the number of computed tokens for the sequence\n        for seq in seq_group.get_seqs():\n            current_computed = min((chunk_idx + 1) * token_chunk_size, prompt_length)\n            seq.data.update_num_computed_tokens(current_computed)\n        \n        # Call the optimized function with token_chunk_size\n        with torch.no_grad():\n            block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n            \n            # Get computed blocks for verification\n            computed_blocks = []\n            for seq in seq_group.get_seqs():\n                blocks = block_manager.get_all_computed_blocks(seq)\n                computed_blocks.append(len(blocks))\n        \n        results.append({\n            \"chunk_idx\": chunk_idx,\n            \"computed_blocks\": computed_blocks[0] if computed_blocks else 0\n        })\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"list\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert type(current_result) == type(reference_result), f\"Type mismatch\"\n    assert len(current_result) == len(reference_result), f\"Length mismatch\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr[\"chunk_idx\"] == ref[\"chunk_idx\"], f\"Chunk index mismatch at {i}\"\n        assert curr[\"computed_blocks\"] == ref[\"computed_blocks\"], f\"Computed blocks mismatch at {i}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"e3580537a41a46b0f3cd750b86b633c1857a8c90\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "data_source": "merged",
    "_meta": {
      "commit_short": "e3580537",
      "action": "UPDATE_TO_FAILURE",
      "error": "No metrics in agent output"
    }
  }
]