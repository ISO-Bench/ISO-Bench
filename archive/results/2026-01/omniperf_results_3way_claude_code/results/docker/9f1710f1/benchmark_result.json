{
  "commit": "9f1710f1",
  "timestamp": "2026-01-12T13:06:28.244855",
  "results": {
    "agent": {
      "commit_hash": "9f1710f1",
      "status": "success",
      "benchmark_type": "serving",
      "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
      "duration_s": 173.55830478668213,
      "version": "agent",
      "error": null,
      "metrics": {
        "ttft_mean": 385.73,
        "ttft_median": 344.25,
        "ttft_p99": 565.73,
        "tpot_mean": 39.53,
        "tpot_median": 38.8,
        "tpot_p99": 50.8,
        "itl_mean": 39.53,
        "itl_median": 30.25,
        "itl_p99": 253.38,
        "request_throughput": 0.93,
        "output_throughput": 59.6
      },
      "raw_output": "ph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:21<00:00,  1.62it/s]\nINFO 01-12 05:05:42 [model_runner.py:1570] Graph capturing finished in 22 secs, took 0.67 GiB\nINFO 01-12 05:05:42 [llm_engine.py:441] init engine (profile, create kv cache, warmup model) took 30.19 seconds\nINFO 01-12 05:05:43 [api_server.py:958] Starting vLLM API server on http://0.0.0.0:8000\nINFO 01-12 05:05:43 [launcher.py:26] Available routes are:\nINFO 01-12 05:05:43 [launcher.py:34] Route: /openapi.json, Methods: HEAD, GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /docs, Methods: HEAD, GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /redoc, Methods: HEAD, GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /health, Methods: GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /ping, Methods: POST, GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /tokenize, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /detokenize, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/models, Methods: GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /version, Methods: GET\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/chat/completions, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/completions, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/embeddings, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /pooling, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /score, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/score, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /rerank, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v1/rerank, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /v2/rerank, Methods: POST\nINFO 01-12 05:05:43 [launcher.py:34] Route: /invocations, Methods: POST\nINFO:     Started server process [193]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     127.0.0.1:55112 - \"GET /v1/models HTTP/1.1\" 200 OK\nSERVER_READY after 73s\nINFO:     127.0.0.1:55122 - \"GET /v1/models HTTP/1.1\" 200 OK\n=== Running serving benchmark ===\nINFO 01-12 05:05:48 [__init__.py:256] Automatically detected platform cuda.\nINFO:     127.0.0.1:40994 - \"POST /v1/completions HTTP/1.1\" 200 OK\nloc(\"/opt/vllm_baseline/vllm/attention/ops/triton_decode_attention.py\":311:16): error: operation scheduled before its operands\nINFO 01-12 05:05:52 [metrics.py:481] Avg prompt throughput: 958.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.7%, CPU KV cache usage: 0.0%.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='deepseek-ai/DeepSeek-V2-Lite-Chat', tokenizer=None, use_beam_search=False, num_prompts=20, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=8192, random_output_len=64, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n\n  0%|          | 0/20 [00:00<?, ?it/s]INFO:     127.0.0.1:40998 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:41006 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:41010 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:41012 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n  5%|\u258c         | 1/20 [00:02<00:52,  2.77s/it]INFO 01-12 05:05:57 [metrics.py:481] Avg prompt throughput: 6809.7 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\n\n 10%|\u2588         | 2/20 [00:03<00:25,  1.44s/it]INFO:     127.0.0.1:43984 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:43988 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 15%|\u2588\u258c        | 3/20 [00:05<00:29,  1.72s/it]\n 20%|\u2588\u2588        | 4/20 [00:05<00:17,  1.10s/it]\n 25%|\u2588\u2588\u258c       | 5/20 [00:06<00:16,  1.08s/it]\n 30%|\u2588\u2588\u2588       | 6/20 [00:06<00:10,  1.30it/s]INFO:     127.0.0.1:43992 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:44004 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 01-12 05:06:02 [metrics.py:481] Avg prompt throughput: 6673.1 tokens/s, Avg generation throughput: 52.1 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO:     127.0.0.1:44020 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 35%|\u2588\u2588\u2588\u258c      | 7/20 [00:10<00:21,  1.63s/it]INFO:     127.0.0.1:44032 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:44040 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 45%|\u2588\u2588\u2588\u2588\u258c     | 9/20 [00:12<00:15,  1.38s/it]INFO:     127.0.0.1:44050 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 50%|\u2588\u2588\u2588\u2588\u2588     | 10/20 [00:12<00:11,  1.20s/it]INFO 01-12 05:06:07 [metrics.py:481] Avg prompt throughput: 6811.6 tokens/s, Avg generation throughput: 54.9 tokens/s, Running: 2 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.\nINFO:     127.0.0.1:42456 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 11/20 [00:14<00:11,  1.32s/it]INFO:     127.0.0.1:42466 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 12/20 [00:15<00:09,  1.18s/it]INFO:     127.0.0.1:42476 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:42484 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 13/20 [00:16<00:08,  1.28s/it]\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 14/20 [00:17<00:07,  1.17s/it]INFO:     127.0.0.1:42498 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:42512 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO 01-12 05:06:12 [metrics.py:481] Avg prompt throughput: 6677.4 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 1 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\nINFO:     127.0.0.1:42518 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:42528 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 15/20 [00:19<00:06,  1.29s/it]\n 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 16/20 [00:19<00:04,  1.05s/it]\n 85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 17/20 [00:21<00:03,  1.11s/it]\n 95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 19/20 [00:21<00:00,  1.51it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:21<00:00,  1.07s/it]\n============ Serving Benchmark Result ============\nSuccessful requests:                     20        \nBenchmark duration (s):                  21.48     \nTotal input tokens:                      163840    \nTotal generated tokens:                  1280      \nRequest throughput (req/s):              0.93      \nOutput token throughput (tok/s):         59.60     \nTotal Token throughput (tok/s):          7688.63   \n---------------Time to First Token----------------\nMean TTFT (ms):                          385.73    \nMedian TTFT (ms):                        344.25    \nP99 TTFT (ms):                           565.73    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          39.53     \nMedian TPOT (ms):                        38.80     \nP99 TPOT (ms):                           50.80     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           39.53     \nMedian ITL (ms):                         30.25     \nP99 ITL (ms):                            253.38    \n==================================================\n=== BENCHMARK_COMPLETE ===\n"
    },
    "baseline": {
      "commit_hash": "9f1710f1",
      "status": "success",
      "benchmark_type": "serving",
      "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
      "duration_s": 153.97,
      "version": "baseline",
      "error": null,
      "metrics": {
        "ttft_mean": 382.82,
        "ttft_median": 346.02,
        "ttft_p99": 556.23,
        "tpot_mean": 35.78,
        "tpot_median": 36.92,
        "tpot_p99": 44.71,
        "itl_mean": 35.78,
        "itl_median": 28.95,
        "itl_p99": 251.82,
        "request_throughput": 0.94,
        "output_throughput": 60.29
      }
    },
    "human": {
      "commit_hash": "9f1710f1",
      "status": "success",
      "benchmark_type": "serving",
      "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
      "duration_s": 142.53,
      "version": "human",
      "error": null,
      "metrics": {
        "ttft_mean": 387.43,
        "ttft_median": 346.71,
        "ttft_p99": 576.76,
        "tpot_mean": 36.41,
        "tpot_median": 37.15,
        "tpot_p99": 44.66,
        "itl_mean": 36.41,
        "itl_median": 29.09,
        "itl_p99": 253.09,
        "request_throughput": 0.94,
        "output_throughput": 60.21
      }
    }
  }
}