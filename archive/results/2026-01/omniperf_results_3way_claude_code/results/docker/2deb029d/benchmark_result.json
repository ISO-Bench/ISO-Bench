{
  "commit": "2deb029d",
  "timestamp": "2026-01-12T14:22:25.610911",
  "results": {
    "baseline": {
      "commit_hash": "2deb029d",
      "status": "success",
      "benchmark_type": "prefix_caching",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "duration_s": 286.9866797924042,
      "version": "baseline",
      "error": null,
      "metrics": {
        "warmup_time_s": 5.331376314163208,
        "generate_time_s": 3.5913496017456055,
        "input_throughput": 18290.6,
        "output_throughput": 5671.5
      },
      "raw_output": "=== Running prefix_caching benchmark ===\nINFO 01-12 06:20:50 llm_engine.py:194] Initializing an LLM engine (v0.5.5) with config: model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=True, num_scheduler_steps=1, enable_prefix_caching=True)\nINFO 01-12 06:20:52 model_runner.py:879] Starting to load model neuralmagic/Meta-Llama-3-8B-Instruct-FP8...\nWARNING 01-12 06:20:52 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-12 06:20:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.06s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.40s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.34s/it]\n\nINFO 01-12 06:20:58 model_runner.py:890] Loading model weights took 8.4596 GB\nINFO 01-12 06:20:58 gpu_executor.py:121] # GPU blocks: 31150, # CPU blocks: 2048\nTesting filtered datasets\n------warm up------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<08:41,  5.26s/it, est. speed input: 122.51 toks/s, output: 37.99 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 18.99it/s, est. speed input: 12247.01 toks/s, output: 3797.52 toks/s]\ncost time 5.331376314163208\n------start generating------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:03<05:48,  3.52s/it, est. speed input: 182.99 toks/s, output: 56.74 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 28.36it/s, est. speed input: 18290.60 toks/s, output: 5671.50 toks/s]\ncost time 3.5913496017456055\n=== BENCHMARK_COMPLETE ===\n"
    },
    "human": {
      "commit_hash": "2deb029d",
      "status": "success",
      "benchmark_type": "prefix_caching",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "duration_s": 35.330939292907715,
      "version": "human",
      "error": null,
      "metrics": {
        "warmup_time_s": 3.7692360877990723,
        "generate_time_s": 3.579690456390381,
        "input_throughput": 18336.93,
        "output_throughput": 5685.86
      },
      "raw_output": "=== Running prefix_caching benchmark ===\nINFO 01-12 06:21:30 llm_engine.py:194] Initializing an LLM engine (v0.5.5) with config: model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=True, num_scheduler_steps=1, enable_prefix_caching=True)\nINFO 01-12 06:21:32 model_runner.py:879] Starting to load model neuralmagic/Meta-Llama-3-8B-Instruct-FP8...\nWARNING 01-12 06:21:32 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-12 06:21:32 weight_utils.py:236] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.92it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.53it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.57it/s]\n\nINFO 01-12 06:21:34 model_runner.py:890] Loading model weights took 8.4596 GB\nINFO 01-12 06:21:35 gpu_executor.py:121] # GPU blocks: 31150, # CPU blocks: 2048\nTesting filtered datasets\n------warm up------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:03<06:06,  3.70s/it, est. speed input: 174.10 toks/s, output: 53.98 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 26.98it/s, est. speed input: 17401.91 toks/s, output: 5395.93 toks/s]\ncost time 3.7692360877990723\n------start generating------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:03<05:48,  3.52s/it, est. speed input: 183.45 toks/s, output: 56.88 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 28.43it/s, est. speed input: 18336.93 toks/s, output: 5685.86 toks/s]\ncost time 3.579690456390381\n=== BENCHMARK_COMPLETE ===\n"
    },
    "agent": {
      "commit_hash": "2deb029d",
      "status": "success",
      "benchmark_type": "prefix_caching",
      "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "duration_s": 36.72913336753845,
      "version": "agent",
      "error": null,
      "metrics": {
        "warmup_time_s": 5.233770132064819,
        "generate_time_s": 3.566107988357544,
        "input_throughput": 18418.29,
        "output_throughput": 5711.09
      },
      "raw_output": "=== AGENT BENCHMARK: Applying patch to installed vLLM ===\nvLLM installed at: /opt/vllm_baseline/vllm\nApplying agent patch...\nHmm...  Looks like a unified diff to me...\nThe text leading up to this was:\n--------------------------\n|diff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\n|index 432a6651a..f658615a0 100644\n|--- a/vllm/core/block/prefix_caching_block.py\n|+++ b/vllm/core/block/prefix_caching_block.py\n--------------------------\npatching file vllm/core/block/prefix_caching_block.py\nUsing Plan A...\nHunk #1 succeeded at 152.\nHunk #2 succeeded at 242.\nHunk #3 succeeded at 414.\nHunk #4 succeeded at 498.\nHunk #5 succeeded at 513.\nHunk #6 succeeded at 578.\nHunk #7 succeeded at 589.\nHunk #8 succeeded at 800.\nHunk #9 succeeded at 847.\nHmm...  Ignoring the trailing garbage.\ndone\nPatch applied successfully!\nvLLM version: 0.5.5\nRemoved vllm_bench/vllm to ensure patched installed vLLM is used\n=== Running prefix_caching benchmark ===\nINFO 01-12 06:22:06 llm_engine.py:194] Initializing an LLM engine (v0.5.5) with config: model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=True, num_scheduler_steps=1, enable_prefix_caching=True)\nINFO 01-12 06:22:08 model_runner.py:879] Starting to load model neuralmagic/Meta-Llama-3-8B-Instruct-FP8...\nWARNING 01-12 06:22:08 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-12 06:22:08 weight_utils.py:236] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.18it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.71it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.77it/s]\n\nINFO 01-12 06:22:10 model_runner.py:890] Loading model weights took 8.4596 GB\nINFO 01-12 06:22:10 gpu_executor.py:121] # GPU blocks: 31150, # CPU blocks: 2048\nTesting filtered datasets\n------warm up------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<08:31,  5.17s/it, est. speed input: 124.84 toks/s, output: 38.71 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 19.35it/s, est. speed input: 12479.72 toks/s, output: 3869.67 toks/s]\ncost time 5.233770132064819\n------start generating------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:03<05:46,  3.50s/it, est. speed input: 184.26 toks/s, output: 57.14 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 28.56it/s, est. speed input: 18418.29 toks/s, output: 5711.09 toks/s]\ncost time 3.566107988357544\n=== BENCHMARK_COMPLETE ===\n"
    }
  }
}