{
  "instance": {
    "commit_hash": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "commit_subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100"
  },
  "result": {
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "Baseline benchmark produced no metrics",
    "server_logs": null,
    "duration_s": 3363.249935865402,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 17:05:28 [__init__.py:256] Automatically detected platform cuda.\n0.8.2.dev3+gb0e96aae",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 17:09:08 [__init__.py:256] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "commit": "22d33bac",
    "full_commit": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "parent_commit": "b0e96aaebbfbe8e70478e4192a5a13864ffdefa6",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path",
    "has_agent_patch": true
  }
}