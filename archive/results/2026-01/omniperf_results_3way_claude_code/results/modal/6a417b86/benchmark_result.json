{
  "instance": {
    "commit_hash": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "commit_subject": "fix neuron performance issue (#13589)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 1762.0,
      "ttft_median": 1179.08,
      "ttft_p99": 7150.52,
      "tpot_mean": 67.02,
      "tpot_median": 71.42,
      "tpot_p99": 99.17,
      "itl_mean": 67.14,
      "itl_median": 26.64,
      "itl_p99": 94.28
    },
    "human_metrics": {
      "ttft_mean": 1160.36,
      "ttft_median": 1116.72,
      "ttft_p99": 1941.04,
      "tpot_mean": 30.05,
      "tpot_median": 29.64,
      "tpot_p99": 64.24,
      "itl_mean": 29.19,
      "itl_median": 26.19,
      "itl_p99": 82.18
    },
    "agent_metrics": {
      "ttft_mean": 1097.58,
      "ttft_median": 1073.82,
      "ttft_p99": 1850.36,
      "tpot_mean": 27.85,
      "tpot_median": 26.94,
      "tpot_p99": 61.37,
      "itl_mean": 26.58,
      "itl_median": 23.54,
      "itl_p99": 93.52
    },
    "human_improvement": {
      "ttft_mean": 34.14528944381385,
      "ttft_median": 5.288869287919386,
      "ttft_p99": 72.85456162628732,
      "tpot_mean": 55.162638018501944,
      "itl_mean": 56.52368185880251
    },
    "agent_improvement": {
      "ttft_mean": 37.70828603859251,
      "ttft_median": 8.927299250262918,
      "ttft_p99": 74.12272114475591,
      "tpot_mean": 58.44524022679797,
      "itl_mean": 60.41108132260947
    },
    "agent_vs_human": {
      "ttft_mean": 5.410389879003066,
      "ttft_median": 3.8416075650118287,
      "ttft_p99": 4.671722375633684,
      "tpot_mean": 7.321131447587352,
      "itl_mean": 8.94141829393629
    },
    "error": null,
    "server_logs": null,
    "duration_s": 3244.8352863788605,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "0.7.4.dev2+gd3ea5011",
    "baseline_install_method": "wheel",
    "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  10.39     \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              9.62      \nOutput token throughput (tok/s):         1166.16   \nTotal Token throughput (tok/s):          6094.14   \n---------------Time to First Token----------------\nMean TTFT (ms):                          1762.00   \nMedian TTFT (ms):                        1179.08   \nP99 TTFT (ms):                           7150.52   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          67.02     \nMedian TPOT (ms):                        71.42     \nP99 TPOT (ms):                           99.17     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           67.14     \nMedian ITL (ms):                         26.64     \nP99 ITL (ms):                            94.28     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:30,  1.10it/s]\n  2%|\u258f         | 2/100 [00:01<01:04,  1.51it/s]\n  3%|\u258e         | 3/100 [00:01<00:40,  2.39it/s]\n  4%|\u258d         | 4/100 [00:07<04:01,  2.51s/it]\n  5%|\u258c         | 5/100 [00:08<03:18,  2.09s/it]\n  6%|\u258c         | 6/100 [00:08<02:19,  1.49s/it]\n  7%|\u258b         | 7/100 [00:09<01:59,  1.29s/it]\n  9%|\u2589         | 9/100 [00:09<01:03,  1.43it/s]\n 24%|\u2588\u2588\u258d       | 24/100 [00:10<00:09,  8.35it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:10<00:03, 19.02it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 63/100 [00:10<00:01, 34.54it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 88/100 [00:10<00:00, 56.44it/s]\n100%|\u2588\u2588\u2588",
    "human_version": "0.7.4.dev3+g6a417b86",
    "human_install_method": "wheel",
    "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  5.09      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              19.64     \nOutput token throughput (tok/s):         2379.82   \nTotal Token throughput (tok/s):          12436.51  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1160.36   \nMedian TTFT (ms):                        1116.72   \nP99 TTFT (ms):                           1941.04   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30.05     \nMedian TPOT (ms):                        29.64     \nP99 TPOT (ms):                           64.24     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.19     \nMedian ITL (ms):                         26.19     \nP99 ITL (ms):                            82.18     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:32,  1.07it/s]\n  2%|\u258f         | 2/100 [00:01<01:01,  1.60it/s]\n  4%|\u258d         | 4/100 [00:02<00:42,  2.26it/s]\n  5%|\u258c         | 5/100 [00:03<01:07,  1.41it/s]\n  6%|\u258c         | 6/100 [00:03<00:55,  1.70it/s]\n  7%|\u258b         | 7/100 [00:04<01:01,  1.52it/s]\n  8%|\u258a         | 8/100 [00:04<00:45,  2.03it/s]\n 21%|\u2588\u2588        | 21/100 [00:04<00:06, 12.21it/s]\n 39%|\u2588\u2588\u2588\u2589      | 39/100 [00:04<00:02, 28.76it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 60/100 [00:04<00:00, 50.45it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 79/100 [00:05<00:00, 70.52it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 19.64it/s]\n",
    "agent_install_method": "python_overlay",
    "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.67      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.40     \nOutput token throughput (tok/s):         2593.37   \nTotal Token throughput (tok/s):          13552.49  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1097.58   \nMedian TTFT (ms):                        1073.82   \nP99 TTFT (ms):                           1850.36   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.85     \nMedian TPOT (ms):                        26.94     \nP99 TPOT (ms):                           61.37     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.58     \nMedian ITL (ms):                         23.54     \nP99 ITL (ms):                            93.52     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:27,  1.14it/s]\n  2%|\u258f         | 2/100 [00:01<00:59,  1.65it/s]\n  3%|\u258e         | 3/100 [00:01<00:36,  2.64it/s]\n  4%|\u258d         | 4/100 [00:01<00:41,  2.32it/s]\n  5%|\u258c         | 5/100 [00:03<01:07,  1.40it/s]\n  6%|\u258c         | 6/100 [00:03<00:52,  1.79it/s]\n  7%|\u258b         | 7/100 [00:04<00:56,  1.63it/s]\n  9%|\u2589         | 9/100 [00:04<00:31,  2.89it/s]\n 28%|\u2588\u2588\u258a       | 28/100 [00:04<00:03, 19.49it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 50/100 [00:04<00:01, 41.55it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 69/100 [00:04<00:00, 61.92it/s]\n 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 97/100 [00:04<00:00, 96.07it/s]\n100%|\u2588\u2588\u2588",
    "commit": "6a417b86",
    "full_commit": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "parent_commit": "d3ea50113c08bdd3c5cfda42ec6ecbc72328d7d1",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "subject": "fix neuron performance issue (#13589)",
    "has_agent_patch": true
  }
}