{
  "instance": {
    "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100"
  },
  "result": {
    "status": "version_bug",
    "gpu_config": "H100:1",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "vLLM 0.6.3.post2.dev398+g4a18fd14 has known port binding bug (issue #8791) - serving benchmarks not supported",
    "server_logs": null,
    "duration_s": 35.60082292556763,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
    "baseline_install_method": "wheel",
    "commit": "b2e0ad3b",
    "full_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "subject": "[Perf] Reduce peak memory usage of llama (#10339)",
    "has_agent_patch": true
  }
}