{
  "instance": {
    "commit_hash": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "commit_subject": "[Core] Speed up decode by remove synchronizing ope",
    "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 25.71,
      "ttft_median": 24.33,
      "ttft_p99": 73.69,
      "tpot_mean": 4.76,
      "tpot_median": 4.66,
      "tpot_p99": 5.69,
      "itl_mean": 4.8,
      "itl_median": 4.59,
      "itl_p99": 11.13
    },
    "human_metrics": {
      "ttft_mean": 22.59,
      "ttft_median": 21.72,
      "ttft_p99": 55.97,
      "tpot_mean": 4.2,
      "tpot_median": 4.18,
      "tpot_p99": 4.55,
      "itl_mean": 4.2,
      "itl_median": 4.15,
      "itl_p99": 5.06
    },
    "agent_metrics": {
      "ttft_mean": 22.93,
      "ttft_median": 21.7,
      "ttft_p99": 53.88,
      "tpot_mean": 4.3,
      "tpot_median": 4.3,
      "tpot_p99": 5.02,
      "itl_mean": 4.31,
      "itl_median": 4.21,
      "itl_p99": 6.06
    },
    "human_improvement": {
      "ttft_mean": 12.135355892648779,
      "ttft_median": 10.727496917385942,
      "ttft_p99": 24.046682046410638,
      "tpot_mean": 11.764705882352935,
      "itl_mean": 12.499999999999993
    },
    "agent_improvement": {
      "ttft_mean": 10.812913263321668,
      "ttft_median": 10.809699958898475,
      "ttft_p99": 26.882887773103537,
      "tpot_mean": 9.663865546218489,
      "itl_mean": 10.20833333333334
    },
    "agent_vs_human": {
      "ttft_mean": -1.505090748118636,
      "ttft_median": 0.09208103130754869,
      "ttft_p99": 3.7341432910487695,
      "tpot_mean": -2.3809523809523725,
      "itl_mean": -2.6190476190476053
    },
    "error": null,
    "server_logs": null,
    "duration_s": 5890.607915878296,
    "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 17:21:32 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev117+gf728ab8e3",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 17:25:16 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae0e3d04180>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.15    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.10    \nTotal Token throughput (tok/s):          620.48    \n---------------Time to First Token----------------\nMean TTFT (ms):                          25.71     \nMedian TTFT (ms):                        24.33     \nP99 TTFT (ms):                           73.69     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.76      \nMedian TPOT (ms):                        4.66      \nP99 TPOT (ms):                           5.69      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.80      \nMedian ITL (ms):                         4.59      \nP99 ITL (ms):                            11.13     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:59,  1.66it/s]\n  2%|\u258f         | 2/100 [00:01<01:12,  1.35it/s]\n  3%|\u258e         | 3/100 [00:01<00:52,  1.85it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.60it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.84it/s]\n  7%|\u258b         | 7/100 [00:06<01:45,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.09it/s]\n  9%|\u2589         | 9/100 [00:07<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:19,  1.12it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:03,  1.38it/s]",
    "human_version": "INFO 01-02 17:27:33 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev118+g299ebb62b",
    "human_install_method": "wheel",
    "human_raw": "INFO 01-02 17:30:42 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b3b10f10220>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.09    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.17    \nTotal Token throughput (tok/s):          620.84    \n---------------Time to First Token----------------\nMean TTFT (ms):                          22.59     \nMedian TTFT (ms):                        21.72     \nP99 TTFT (ms):                           55.97     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.20      \nMedian TPOT (ms):                        4.18      \nP99 TPOT (ms):                           4.55      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.20      \nMedian ITL (ms):                         4.15      \nP99 ITL (ms):                            5.06      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:52,  1.89it/s]\n  2%|\u258f         | 2/100 [00:01<01:10,  1.39it/s]\n  3%|\u258e         | 3/100 [00:01<00:51,  1.87it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.61it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.86it/s]\n  7%|\u258b         | 7/100 [00:06<01:44,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.10it/s]\n  9%|\u2589         | 9/100 [00:07<01:28,  1.03it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.37it/s]\n 11%|\u2588         | 11/100 [00:08<01:18,  1.13it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:04,  1.37it/s]",
    "agent_install_method": "python_overlay",
    "agent_raw": "INFO 01-02 17:35:21 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b1dffa140e0>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.06    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.20    \nTotal Token throughput (tok/s):          621.00    \n---------------Time to First Token----------------\nMean TTFT (ms):                          22.93     \nMedian TTFT (ms):                        21.70     \nP99 TTFT (ms):                           53.88     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.30      \nMedian TPOT (ms):                        4.30      \nP99 TPOT (ms):                           5.02      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.31      \nMedian ITL (ms):                         4.21      \nP99 ITL (ms):                            6.06      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:57,  1.71it/s]\n  2%|\u258f         | 2/100 [00:01<01:12,  1.36it/s]\n  3%|\u258e         | 3/100 [00:01<00:51,  1.87it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.60it/s]\n  6%|\u258c         | 6/100 [00:03<00:51,  1.84it/s]\n  7%|\u258b         | 7/100 [00:06<01:45,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.09it/s]\n  9%|\u2589         | 9/100 [00:07<01:28,  1.03it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.38it/s]\n 11%|\u2588         | 11/100 [00:08<01:19,  1.13it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:03,  1.38it/s]",
    "commit": "299ebb62",
    "full_commit": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "parent_commit": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9",
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "subject": "[Core] Speed up decode by remove synchronizing ope",
    "has_agent_patch": true
  }
}