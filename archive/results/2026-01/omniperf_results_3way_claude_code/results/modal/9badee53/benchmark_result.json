{
  "instance": {
    "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "commit_subject": "Fix performance when `--generation-config` is not ",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json"
  },
  "result": {
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "Baseline benchmark produced no metrics",
    "server_logs": null,
    "duration_s": 1670.1454713344574,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 16:39:34 [__init__.py:253] Automatically detected platform cuda.\n0.7.4.dev208+gbeebf474",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 16:41:24 [__init__.py:253] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', max_concurrency=None, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1315, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 876, in main\n    input_requests = sample_sharegpt_requests(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 102, in sample_sharegpt_requests\n    with open(dataset_path, encoding='utf-8') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
    "commit": "9badee53",
    "full_commit": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "parent_commit": "beebf4742af80296d3c3a657c66d512615c550c1",
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "subject": "Fix performance when `--generation-config` is not ",
    "has_agent_patch": true
  }
}