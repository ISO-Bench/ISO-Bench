{
  "instance": {
    "commit_hash": "dae68969774e41b93b01cd31171ca033a92b574a",
    "commit_subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)",
    "perf_command": "VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHMLA python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8"
  },
  "result": {
    "status": "baseline_failed",
    "gpu_config": "H100:8",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "BASELINE server failed to start",
    "duration_s": 6726.257014274597,
    "perf_command": "VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHMLA python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8",
    "benchmark_mode": "serving",
    "baseline_version": "INFO 01-02 17:05:27 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev281+gc34eeec5",
    "commit": "dae68969",
    "full_commit": "dae68969774e41b93b01cd31171ca033a92b574a",
    "parent_commit": "c34eeec58d3a94437c5311e256f8ba21d1912a39",
    "model": "deepseek-ai/DeepSeek-R1",
    "subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)",
    "has_agent_patch": true
  }
}