{
  "instance": {
    "commit_hash": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 59.81,
      "ttft_median": 57.77,
      "ttft_p99": 90.17,
      "tpot_mean": 10.38,
      "tpot_median": 10.18,
      "tpot_p99": 12.34,
      "itl_mean": 10.38,
      "itl_median": 9.9,
      "itl_p99": 22.12
    },
    "human_metrics": {
      "ttft_mean": 58.71,
      "ttft_median": 57.56,
      "ttft_p99": 85.33,
      "tpot_mean": 10.25,
      "tpot_median": 10.16,
      "tpot_p99": 11.51,
      "itl_mean": 10.25,
      "itl_median": 9.89,
      "itl_p99": 24.46
    },
    "agent_metrics": {
      "ttft_mean": 58.19,
      "ttft_median": 56.7,
      "ttft_p99": 85.91,
      "tpot_mean": 9.96,
      "tpot_median": 9.89,
      "tpot_p99": 11.2,
      "itl_mean": 9.96,
      "itl_median": 9.6,
      "itl_p99": 20.83
    },
    "human_improvement": {
      "ttft_mean": 1.8391573315499103,
      "ttft_median": 0.3635104725636158,
      "ttft_p99": 5.367638904291897,
      "tpot_mean": 1.2524084778420113,
      "itl_mean": 1.2524084778420113
    },
    "agent_improvement": {
      "ttft_mean": 2.708577161009872,
      "ttft_median": 1.8521724078241306,
      "ttft_p99": 4.7244094488189035,
      "tpot_mean": 4.046242774566473,
      "itl_mean": 4.046242774566473
    },
    "agent_vs_human": {
      "ttft_mean": 0.8857094191790208,
      "ttft_median": 1.4940931202223755,
      "ttft_p99": -0.6797140513301281,
      "tpot_mean": 2.8292682926829187,
      "itl_mean": 2.8292682926829187
    },
    "error": null,
    "server_logs": null,
    "duration_s": 4199.745561122894,
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 17:09:07 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev364+g63d635d1",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 17:12:59 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.87    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12590     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         121.20    \nTotal Token throughput (tok/s):          1107.01   \n---------------Time to First Token----------------\nMean TTFT (ms):                          59.81     \nMedian TTFT (ms):                        57.77     \nP99 TTFT (ms):                           90.17     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          10.38     \nMedian TPOT (ms):                        10.18     \nP99 TPOT (ms):                           12.34     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.38     \nMedian ITL (ms):                         9.90      \nP99 ITL (ms):                            22.12     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:15,  1.37s/it]\n  2%|\u258f         | 2/100 [00:02<01:49,  1.12s/it]\n  3%|\u258e         | 3/100 [00:02<01:11,  1.36it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:53,  1.77it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:08<01:06,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:02,  1.42it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.44it/s]\n 13%|\u2588\u258e        | 13/1",
    "human_version": "INFO 01-02 17:15:07 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev365+g70b808fe",
    "human_install_method": "wheel",
    "human_raw": "INFO 01-02 17:17:13 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.83    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12590     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         121.26    \nTotal Token throughput (tok/s):          1107.53   \n---------------Time to First Token----------------\nMean TTFT (ms):                          58.71     \nMedian TTFT (ms):                        57.56     \nP99 TTFT (ms):                           85.33     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          10.25     \nMedian TPOT (ms):                        10.16     \nP99 TPOT (ms):                           11.51     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.25     \nMedian ITL (ms):                         9.89      \nP99 ITL (ms):                            24.46     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:16,  1.38s/it]\n  2%|\u258f         | 2/100 [00:02<01:49,  1.11s/it]\n  3%|\u258e         | 3/100 [00:02<01:11,  1.35it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:52,  1.77it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:08<01:06,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:02,  1.43it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.44it/s]\n 13%|\u2588\u258e        | 13/1",
    "agent_install_method": "python_overlay",
    "agent_raw": "INFO 01-02 17:21:24 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.79    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12537     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         120.80    \nTotal Token throughput (tok/s):          1107.45   \n---------------Time to First Token----------------\nMean TTFT (ms):                          58.19     \nMedian TTFT (ms):                        56.70     \nP99 TTFT (ms):                           85.91     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          9.96      \nMedian TPOT (ms):                        9.89      \nP99 TPOT (ms):                           11.20     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           9.96      \nMedian ITL (ms):                         9.60      \nP99 ITL (ms):                            20.83     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:11,  1.33s/it]\n  2%|\u258f         | 2/100 [00:02<01:46,  1.09s/it]\n  3%|\u258e         | 3/100 [00:02<01:10,  1.38it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:52,  1.78it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 11%|\u2588         | 11/100 [00:09<01:03,  1.39it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.43it/s]\n 13%|\u2588\u258e        | 13/100 [00:10<00:53,  1.61it/s]\n 14%|\u2588\u258d        | 14/1",
    "commit": "70b808fe",
    "full_commit": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "parent_commit": "63d635d17962377df089cdc9d4a2684f0b007208",
    "model": "Qwen/Qwen2-VL-7B",
    "subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync",
    "has_agent_patch": true
  }
}