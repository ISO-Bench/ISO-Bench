{
  "instance": {
    "commit_hash": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "commit_subject": "[Kernel][ROCM] Upstream prefix prefill speed up fo",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 2435.9,
      "ttft_median": 2491.88,
      "ttft_p99": 4335.24,
      "tpot_mean": 40.71,
      "tpot_median": 37.21,
      "tpot_p99": 201.86,
      "itl_mean": 36.79,
      "itl_median": 23.42,
      "itl_p99": 208.4
    },
    "human_metrics": {
      "ttft_mean": 2520.72,
      "ttft_median": 2487.94,
      "ttft_p99": 4477.38,
      "tpot_mean": 41.47,
      "tpot_median": 37.75,
      "tpot_p99": 205.33,
      "itl_mean": 37.55,
      "itl_median": 24.23,
      "itl_p99": 210.08
    },
    "agent_metrics": {
      "ttft_mean": 2454.66,
      "ttft_median": 2379.54,
      "ttft_p99": 4394.01,
      "tpot_mean": 40.8,
      "tpot_median": 37.32,
      "tpot_p99": 201.45,
      "itl_mean": 36.85,
      "itl_median": 23.54,
      "itl_p99": 210.19
    },
    "human_improvement": {
      "ttft_mean": -3.4820805451783614,
      "ttft_median": 0.1581135528195601,
      "ttft_p99": -3.2787112132200367,
      "tpot_mean": -1.8668631785801966,
      "itl_mean": -2.0657787442239686
    },
    "agent_improvement": {
      "ttft_mean": -0.7701465577404558,
      "ttft_median": 4.5082427725251675,
      "ttft_p99": -1.3556342901431164,
      "tpot_mean": -0.22107590272659375,
      "itl_mean": -0.1630877955966357
    },
    "agent_vs_human": {
      "ttft_mean": 2.620679805769778,
      "ttft_median": 4.357018256067272,
      "ttft_p99": 1.8620264529702613,
      "tpot_mean": 1.6156257535567924,
      "itl_mean": 1.8641810918774853
    },
    "error": null,
    "server_logs": null,
    "duration_s": 5920.347839593887,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 17:27:01 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev156+gf67e9e9f2",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 17:30:28 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b8bc3d6f740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.26      \nTotal input tokens:                      153600    \nTotal generated tokens:                  36968     \nRequest throughput (req/s):              41.34     \nOutput token throughput (tok/s):         5093.69   \nTotal Token throughput (tok/s):          26257.67  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2435.90   \nMedian TTFT (ms):                        2491.88   \nP99 TTFT (ms):                           4335.24   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          40.71     \nMedian TPOT (ms):                        37.21     \nP99 TPOT (ms):                           201.86    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.79     \nMedian ITL (ms):                         23.42     \nP99 ITL (ms):                            208.40    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<06:37,  1.33s/it]\n  1%|          | 3/300 [00:01<02:26,  2.03it/s]\n  1%|\u258f         | 4/300 [00:02<02:16,  2.17it/s]\n  2%|\u258f         | 6/300 [00:03<02:22,  2.07it/s]\n  2%|\u258f         | 7/300 [00:03<02:32,  1.92it/s]\n  3%|\u258e         | 8/300 [00:05<04:21,  1.11it/s]\n  3%|\u258e         | 9/300 [00:05<03:19,  1.46it/s]\n  4%|\u258e         | 11/300 [00:05<01:59,  2.41it/s]\n  4%|\u258d         | 13/300 [00:06<01:30,  3.17it/s]\n  5%|\u258c         | 16/300 [00:06<00:55,  5.13it/s]\n  7%|\u258b         | 20/300 [00:06<00:33,  8.30it/s]\n  8%|\u258a         ",
    "human_version": "INFO 01-02 17:31:14 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev157+gbc7c4d206",
    "human_install_method": "wheel",
    "human_raw": "INFO 01-02 17:34:30 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b29c415f740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.43      \nTotal input tokens:                      153600    \nTotal generated tokens:                  37000     \nRequest throughput (req/s):              40.35     \nOutput token throughput (tok/s):         4976.58   \nTotal Token throughput (tok/s):          25636.13  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2520.72   \nMedian TTFT (ms):                        2487.94   \nP99 TTFT (ms):                           4477.38   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          41.47     \nMedian TPOT (ms):                        37.75     \nP99 TPOT (ms):                           205.33    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           37.55     \nMedian ITL (ms):                         24.23     \nP99 ITL (ms):                            210.08    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<07:33,  1.52s/it]\n  1%|          | 3/300 [00:01<02:19,  2.13it/s]\n  1%|\u258f         | 4/300 [00:02<02:11,  2.24it/s]\n  2%|\u258f         | 5/300 [00:02<02:08,  2.30it/s]\n  2%|\u258f         | 6/300 [00:03<02:24,  2.03it/s]\n  2%|\u258f         | 7/300 [00:03<02:55,  1.67it/s]\n  3%|\u258e         | 8/300 [00:05<04:46,  1.02it/s]\n  3%|\u258e         | 9/300 [00:06<03:36,  1.34it/s]\n  4%|\u258e         | 11/300 [00:06<02:02,  2.36it/s]\n  4%|\u258d         | 12/300 [00:06<01:54,  2.51it/s]\n  5%|\u258c         | 15/300 [00:06<01:01,  4.66it/s]\n  7%|\u258b         |",
    "agent_install_method": "python_overlay",
    "agent_raw": "INFO 01-02 17:37:27 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b7a9feef740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.29      \nTotal input tokens:                      153600    \nTotal generated tokens:                  37000     \nRequest throughput (req/s):              41.17     \nOutput token throughput (tok/s):         5077.53   \nTotal Token throughput (tok/s):          26156.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2454.66   \nMedian TTFT (ms):                        2379.54   \nP99 TTFT (ms):                           4394.01   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          40.80     \nMedian TPOT (ms):                        37.32     \nP99 TPOT (ms):                           201.45    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.85     \nMedian ITL (ms):                         23.54     \nP99 ITL (ms):                            210.19    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<07:03,  1.42s/it]\n  1%|          | 3/300 [00:01<02:10,  2.27it/s]\n  1%|\u258f         | 4/300 [00:02<02:06,  2.34it/s]\n  2%|\u258f         | 5/300 [00:02<01:43,  2.84it/s]\n  2%|\u258f         | 6/300 [00:03<02:27,  1.99it/s]\n  2%|\u258f         | 7/300 [00:03<02:57,  1.65it/s]\n  3%|\u258e         | 8/300 [00:05<04:49,  1.01it/s]\n  3%|\u258e         | 9/300 [00:05<03:36,  1.34it/s]\n  3%|\u258e         | 10/300 [00:06<02:39,  1.82it/s]\n  4%|\u258d         | 12/300 [00:06<01:45,  2.72it/s]\n  5%|\u258c         | 15/300 [00:06<00:58,  4.87it/s]\n  6%|\u258c         |",
    "commit": "bc7c4d20",
    "full_commit": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "parent_commit": "f67e9e9f221e9791733b827585d6eb6dbc23133c",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "subject": "[Kernel][ROCM] Upstream prefix prefill speed up fo",
    "has_agent_patch": true
  }
}