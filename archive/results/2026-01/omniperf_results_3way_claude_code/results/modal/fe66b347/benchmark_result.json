{
  "instance": {
    "commit_hash": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing ",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 6225.57,
      "ttft_median": 4998.48,
      "ttft_p99": 17311.88,
      "tpot_mean": 82.23,
      "tpot_median": 87.68,
      "tpot_p99": 117.5,
      "itl_mean": 82.23,
      "itl_median": 68.35,
      "itl_p99": 101.29
    },
    "human_metrics": {
      "ttft_mean": 5722.95,
      "ttft_median": 4649.01,
      "ttft_p99": 15374.29,
      "tpot_mean": 71.34,
      "tpot_median": 75.78,
      "tpot_p99": 102.94,
      "itl_mean": 71.34,
      "itl_median": 61.28,
      "itl_p99": 110.49
    },
    "agent_metrics": {
      "ttft_mean": 5874.58,
      "ttft_median": 4777.7,
      "ttft_p99": 15940.41,
      "tpot_mean": 74.58,
      "tpot_median": 79.08,
      "tpot_p99": 106.43,
      "itl_mean": 74.58,
      "itl_median": 63.35,
      "itl_p99": 112.78
    },
    "human_improvement": {
      "ttft_mean": 8.073477609279148,
      "ttft_median": 6.991525423728801,
      "ttft_p99": 11.192256415825433,
      "tpot_mean": 13.24334184604159,
      "itl_mean": 13.24334184604159
    },
    "agent_improvement": {
      "ttft_mean": 5.637877334926759,
      "ttft_median": 4.416942750596176,
      "ttft_p99": 7.92213208501908,
      "tpot_mean": 9.30317402407881,
      "itl_mean": 9.30317402407881
    },
    "agent_vs_human": {
      "ttft_mean": -2.6495076839741762,
      "ttft_median": -2.7681162225936187,
      "ttft_p99": -3.6822513429888404,
      "tpot_mean": -4.541631623212776,
      "itl_mean": -4.541631623212776
    },
    "error": null,
    "server_logs": null,
    "duration_s": 2762.1037936210632,
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "INFO 01-02 15:28:20 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev450+g270a5da4",
    "baseline_install_method": "wheel",
    "baseline_raw": "INFO 01-02 15:32:47 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2af84c127c40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  19.83     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              15.13     \nOutput token throughput (tok/s):         1936.46   \nTotal Token throughput (tok/s):          9682.32   \n---------------Time to First Token----------------\nMean TTFT (ms):                          6225.57   \nMedian TTFT (ms):                        4998.48   \nP99 TTFT (ms):                           17311.88  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          82.23     \nMedian TPOT (ms):                        87.68     \nP99 TPOT (ms):                           117.50    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           82.23     \nMedian ITL (ms):                         68.35     \nP99 ITL (ms):                            101.29    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:16<1:20:49, 16.22s/it]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 257/300 [00:19<00:02, 17.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:19<00:00, 15.13it/s]\n",
    "human_version": "INFO 01-02 15:34:23 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev451+gfe66b347",
    "human_install_method": "wheel",
    "human_raw": "INFO 01-02 15:36:29 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b317384fc40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  17.86     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              16.80     \nOutput token throughput (tok/s):         2150.62   \nTotal Token throughput (tok/s):          10753.09  \n---------------Time to First Token----------------\nMean TTFT (ms):                          5722.95   \nMedian TTFT (ms):                        4649.01   \nP99 TTFT (ms):                           15374.29  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          71.34     \nMedian TPOT (ms):                        75.78     \nP99 TPOT (ms):                           102.94    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           71.34     \nMedian ITL (ms):                         61.28     \nP99 ITL (ms):                            110.49    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:11:00, 14.25s/it]\n 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 201/300 [00:14<00:04, 19.92it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:17<00:00, 16.80it/s]\n",
    "agent_install_method": "python_overlay",
    "agent_raw": "INFO 01-02 15:40:15 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b4892867c40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  18.49     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              16.22     \nOutput token throughput (tok/s):         2076.40   \nTotal Token throughput (tok/s):          10382.02  \n---------------Time to First Token----------------\nMean TTFT (ms):                          5874.58   \nMedian TTFT (ms):                        4777.70   \nP99 TTFT (ms):                           15940.41  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          74.58     \nMedian TPOT (ms):                        79.08     \nP99 TPOT (ms):                           106.43    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           74.58     \nMedian ITL (ms):                         63.35     \nP99 ITL (ms):                            112.78    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:13:46, 14.80s/it]\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 209/300 [00:14<00:04, 19.94it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:18<00:00, 16.22it/s]\n",
    "commit": "fe66b347",
    "full_commit": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "parent_commit": "270a5da495d24e947a71e2fa0c56635f4fad2dc3",
    "model": "ibm-ai-platform/Bamba-9B",
    "subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing ",
    "has_agent_patch": true
  }
}