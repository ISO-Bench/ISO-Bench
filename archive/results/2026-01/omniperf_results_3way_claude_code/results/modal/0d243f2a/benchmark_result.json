{
  "instance": {
    "commit_hash": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "commit_subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1"
  },
  "result": {
    "status": "error",
    "gpu_config": "H100:1",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "BASELINE server failed to start. Logs:               ^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/models/mixtral.py\", line 87, in __init__\n[rank0]:     self.experts = FusedMoE(num_experts=num_experts,\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 309, in __init__\n[rank0]:     self.quant_method.create_weights(layer=self, **moe_quant_params)\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 83, in create_weights\n[rank0]:     w2_weight = torch.nn.Parameter(torch.empty(\n[rank0]:                                    ^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 812.56 MiB is free. Process 1 has 78.38 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 13.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W102 16:43:43.194580942 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
    "server_logs": null,
    "duration_s": 2192.627078294754,
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "0.7.3.dev240+g88f6ba32",
    "baseline_install_method": "wheel",
    "commit": "0d243f2a",
    "full_commit": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "parent_commit": "88f6ba3281f727d5641d362476ae68562b666081",
    "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS",
    "has_agent_patch": true
  }
}