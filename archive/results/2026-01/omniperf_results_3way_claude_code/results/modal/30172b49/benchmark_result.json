{
  "instance": {
    "commit_hash": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "commit_subject": "[V1] Optimize handling of sampling metadata and re",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100"
  },
  "result": {
    "status": "success",
    "gpu_config": "H100:1",
    "baseline_metrics": {
      "ttft_mean": 1115.66,
      "ttft_median": 1124.73,
      "ttft_p99": 1854.79,
      "tpot_mean": 26.96,
      "tpot_median": 26.41,
      "tpot_p99": 60.69,
      "itl_mean": 25.94,
      "itl_median": 23.11,
      "itl_p99": 82.89
    },
    "human_metrics": {
      "ttft_mean": 1103.5,
      "ttft_median": 1110.34,
      "ttft_p99": 1850.6,
      "tpot_mean": 27.01,
      "tpot_median": 26.55,
      "tpot_p99": 58.98,
      "itl_mean": 26.05,
      "itl_median": 23.14,
      "itl_p99": 87.94
    },
    "agent_metrics": {
      "ttft_mean": 1074.88,
      "ttft_median": 1037.52,
      "ttft_p99": 1812.42,
      "tpot_mean": 27.06,
      "tpot_median": 26.49,
      "tpot_p99": 52.54,
      "itl_mean": 26.08,
      "itl_median": 23.27,
      "itl_p99": 92.46
    },
    "human_improvement": {
      "ttft_mean": 1.089937794668634,
      "ttft_median": 1.2794181714722734,
      "ttft_p99": 0.22590158454596235,
      "tpot_mean": -0.18545994065282162,
      "itl_mean": -0.42405551272166314
    },
    "agent_improvement": {
      "ttft_mean": 3.6552354660021846,
      "ttft_median": 7.753860926622393,
      "ttft_p99": 2.2843556413394452,
      "tpot_mean": -0.3709198813056301,
      "itl_mean": -0.539707016191199
    },
    "agent_vs_human": {
      "ttft_mean": 2.593565926597181,
      "ttft_median": 6.558351495938177,
      "ttft_p99": 2.063114665513879,
      "tpot_mean": -0.18511662347277733,
      "itl_mean": -0.11516314779269705
    },
    "error": null,
    "server_logs": null,
    "duration_s": 1080.960917711258,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "baseline_version": "dev",
    "baseline_install_method": "wheel",
    "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.61      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.70     \nOutput token throughput (tok/s):         2629.24   \nTotal Token throughput (tok/s):          13739.93  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1115.66   \nMedian TTFT (ms):                        1124.73   \nP99 TTFT (ms):                           1854.79   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          26.96     \nMedian TPOT (ms):                        26.41     \nP99 TPOT (ms):                           60.69     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           25.94     \nMedian ITL (ms):                         23.11     \nP99 ITL (ms):                            82.89     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:25,  1.16it/s]\n  2%|\u258f         | 2/100 [00:01<00:58,  1.68it/s]\n  4%|\u258d         | 4/100 [00:01<00:40,  2.36it/s]\n  5%|\u258c         | 5/100 [00:03<01:02,  1.53it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.86it/s]\n  7%|\u258b         | 7/100 [00:04<00:53,  1.73it/s]\n  9%|\u2589         | 9/100 [00:04<00:30,  2.97it/s]\n 26%|\u2588\u2588\u258c       | 26/100 [00:04<00:04, 17.65it/s]",
    "human_version": "dev",
    "human_install_method": "wheel",
    "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.61      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.71     \nOutput token throughput (tok/s):         2630.94   \nTotal Token throughput (tok/s):          13748.82  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1103.50   \nMedian TTFT (ms):                        1110.34   \nP99 TTFT (ms):                           1850.60   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.01     \nMedian TPOT (ms):                        26.55     \nP99 TPOT (ms):                           58.98     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.05     \nMedian ITL (ms):                         23.14     \nP99 ITL (ms):                            87.94     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:23,  1.18it/s]\n  2%|\u258f         | 2/100 [00:01<00:57,  1.70it/s]\n  3%|\u258e         | 3/100 [00:01<00:35,  2.74it/s]\n  4%|\u258d         | 4/100 [00:01<00:42,  2.27it/s]\n  5%|\u258c         | 5/100 [00:03<01:06,  1.43it/s]\n  6%|\u258c         | 6/100 [00:03<00:52,  1.80it/s]\n  7%|\u258b         | 7/100 [00:04<00:55,  1.67it/s]\n  9%|\u2589         | 9/100 [00:04<00:30,  2.95it/s]\n",
    "agent_install_method": "python_overlay",
    "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.59      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.81     \nOutput token throughput (tok/s):         2641.91   \nTotal Token throughput (tok/s):          13806.16  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1074.88   \nMedian TTFT (ms):                        1037.52   \nP99 TTFT (ms):                           1812.42   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.06     \nMedian TPOT (ms):                        26.49     \nP99 TPOT (ms):                           52.54     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.08     \nMedian ITL (ms):                         23.27     \nP99 ITL (ms):                            92.46     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:24,  1.17it/s]\n  2%|\u258f         | 2/100 [00:01<00:57,  1.69it/s]\n  4%|\u258d         | 4/100 [00:01<00:38,  2.46it/s]\n  5%|\u258c         | 5/100 [00:03<01:01,  1.54it/s]\n  6%|\u258c         | 6/100 [00:03<00:49,  1.90it/s]\n  7%|\u258b         | 7/100 [00:04<00:54,  1.70it/s]\n  9%|\u2589         | 9/100 [00:04<00:31,  2.93it/s]\n 28%|\u2588\u2588\u258a       | 28/100 [00:04<00:03, 19.20it/s]",
    "commit": "30172b49",
    "full_commit": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "parent_commit": "a4d577b37944cbfa1bc62e4869667d1e2739d62a",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "subject": "[V1] Optimize handling of sampling metadata and re",
    "has_agent_patch": true
  }
}