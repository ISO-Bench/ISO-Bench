{
  "instance": {
    "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "commit_subject": "[Core] Use array to speedup padding (#6779)",
    "repo": "vllm",
    "perf_command": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 89a84b0bb7b30706a02836234a94493ea8f780bf\nMessage: [Core] Use array to speedup padding (#6779)\n\nThis script measures the actual performance impact of using arrays instead of lists\nfor token storage in vLLM's sampling metadata preparation.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom array import array\nfrom typing import Dict, Any, Tuple, Optional, List\nimport random\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.model_executor.sampling_metadata\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"SamplingTensors.from_lists\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Realistic vLLM workload parameters\n    batch_size = 32  # Number of sequences\n    vocab_size = 32000  # Llama vocab size\n    max_prompt_len = 2048\n    max_output_len = 512\n    \n    # Generate token lists that would be used in sampling\n    prompt_tokens = []\n    output_tokens = []\n    \n    for _ in range(batch_size):\n        # Generate varying length prompts and outputs\n        prompt_len = random.randint(128, max_prompt_len)\n        output_len = random.randint(1, max_output_len)\n        \n        # Use arrays as per the optimization\n        prompt_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(prompt_len)])\n        output_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(output_len)])\n        \n        prompt_tokens.append(prompt_seq)\n        output_tokens.append(output_seq)\n    \n    # Other sampling parameters\n    temperatures = [0.7] * batch_size\n    top_ps = [0.9] * batch_size\n    top_ks = [40] * batch_size\n    min_ps = [0.0] * batch_size\n    presence_penalties = [0.0] * batch_size\n    frequency_penalties = [0.0] * batch_size\n    repetition_penalties = [1.0] * batch_size\n    sampling_seeds = [random.randint(0, 2**31-1) for _ in range(batch_size)]\n    sample_indices = list(range(batch_size))\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"temperatures\": temperatures,\n        \"top_ps\": top_ps,\n        \"top_ks\": top_ks,\n        \"min_ps\": min_ps,\n        \"presence_penalties\": presence_penalties,\n        \"frequency_penalties\": frequency_penalties,\n        \"repetition_penalties\": repetition_penalties,\n        \"sampling_seeds\": sampling_seeds,\n        \"sample_indices\": sample_indices,\n        \"prompt_tokens\": prompt_tokens,\n        \"output_tokens\": output_tokens,\n        \"vocab_size\": vocab_size,\n        \"extra_seeds_to_generate\": 0,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call SamplingTensors.from_lists with the prepared data\n    result = target(\n        temperatures=data[\"temperatures\"],\n        top_ps=data[\"top_ps\"],\n        top_ks=data[\"top_ks\"],\n        min_ps=data[\"min_ps\"],\n        presence_penalties=data[\"presence_penalties\"],\n        frequency_penalties=data[\"frequency_penalties\"],\n        repetition_penalties=data[\"repetition_penalties\"],\n        sampling_seeds=data[\"sampling_seeds\"],\n        sample_indices=data[\"sample_indices\"],\n        prompt_tokens=data[\"prompt_tokens\"],\n        output_tokens=data[\"output_tokens\"],\n        vocab_size=data[\"vocab_size\"],\n        extra_seeds_to_generate=data[\"extra_seeds_to_generate\"],\n        device=data[\"device\"],\n        dtype=data[\"dtype\"]\n    )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store the tensor attributes of SamplingTensors\n    tensors_dict = {\n        \"temperatures\": result.temperatures.cpu(),\n        \"top_ps\": result.top_ps.cpu(),\n        \"top_ks\": result.top_ks.cpu(),\n        \"min_ps\": result.min_ps.cpu(),\n        \"presence_penalties\": result.presence_penalties.cpu(),\n        \"frequency_penalties\": result.frequency_penalties.cpu(),\n        \"repetition_penalties\": result.repetition_penalties.cpu(),\n        \"prompt_tokens\": result.prompt_tokens.cpu(),\n        \"output_tokens\": result.output_tokens.cpu(),\n        \"sampling_seeds\": result.sampling_seeds.cpu(),\n        \"sample_indices\": result.sample_indices.cpu(),\n    }\n    torch.save({\"type\": \"sampling_tensors\", \"data\": tensors_dict}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Check each tensor attribute\n    attrs_to_check = [\n        \"temperatures\", \"top_ps\", \"top_ks\", \"min_ps\",\n        \"presence_penalties\", \"frequency_penalties\", \"repetition_penalties\",\n        \"prompt_tokens\", \"output_tokens\", \"sampling_seeds\", \"sample_indices\"\n    ]\n    \n    for attr in attrs_to_check:\n        current_tensor = getattr(current_result, attr).cpu()\n        ref_tensor = reference_result[attr]\n        \n        assert current_tensor.shape == ref_tensor.shape, f\"{attr} shape mismatch\"\n        assert current_tensor.dtype == ref_tensor.dtype, f\"{attr} dtype mismatch\"\n        \n        if current_tensor.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_tensor,\n            ref_tensor,\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This optimization primarily affects CPU operations (array vs list)\n    # so we time on CPU\n    warmup = 5\n    iters = 20\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"89a84b0bb7b30706a02836234a94493ea8f780bf\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization affects CPU operations\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py",
      "vllm/model_executor/sampling_metadata.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/6779",
    "models": [
      "N/A"
    ]
  },
  "result": {
    "status": "error",
    "gpu_config": "H100:1",
    "baseline_metrics": {},
    "human_metrics": {},
    "agent_metrics": null,
    "human_improvement": {},
    "agent_improvement": null,
    "agent_vs_human": null,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 084a01fd3544",
    "server_logs": null,
    "duration_s": 2147.820925951004,
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "commit": "89a84b0b",
    "full_commit": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "parent_commit": "084a01fd3544557990f8af8af6fd3c1185bae848",
    "model": "Qwen/Qwen1.5-0.5B",
    "subject": "[Core] Use array to speedup padding (#6779)",
    "has_agent_patch": true
  }
}