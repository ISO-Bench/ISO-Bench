{
  "human_commit": "8bc68e19",
  "human_commit_full": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
  "parent_commit": "0fca3cdcf265cd375bca684d951702b6b7adf65a",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "Server crashed during startup",
  "duration_s": 120.314133644104,
  "metrics": {},
  "raw_output": "Applying aimv2 compatibility fix (pre-import)...\nChecking vLLM compatibility...\nvLLM import failed - applying compatibility fixes...\ntransformers.models.mllama OK\nFixing transformers compatibility (LogitsWarper missing, mllama not needed)...\nvLLM api_server still failing - trying lm-format-enforcer upgrade...\nRequirement already satisfied: lm-format-enforcer in /usr/local/lib/python3.10/dist-packages (0.10.1)\nCollecting lm-format-enforcer\n  Downloading lm_format_enforcer-0.11.3-py3-none-any.whl (45 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 45.4/45.4 KB 4.2 MB/s eta 0:00:00\nRequirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (2.11.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (25.0)\nRequirement already satisfied: interegular>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (0.3.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from lm-format-enforcer) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (4.15.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->lm-format-enforcer) (0.7.0)\nInstalling collected packages: lm-format-enforcer\n  Attempting uninstall: lm-format-enforcer\n    Found existing installation: lm-format-enforcer 0.10.1\n    Uninstalling lm-format-enforcer-0.10.1:\n      Successfully uninstalled lm-format-enforcer-0.10.1\nSuccessfully installed lm-format-enforcer-0.11.3\nDetecting vLLM version...\nFound vLLM at: /usr/local/lib/python3.10/dist-packages/vllm\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/rotary_embedding.py\n  Patching: /usr/local/lib/python3.10/dist-packages/vllm/config.py\nSearching for Python with vLLM...\nFound vLLM at: /usr/bin/python3\nvLLM version: 0.4.2 at /usr/local/lib/python3.10/dist-packages/vllm\nvLLM 0.4.2 OK\nFixing outlines.fsm compatibility (fsm.guide missing)...\nFound existing installation: outlines 0.0.34\nUninstalling outlines-0.0.34:\n  Successfully uninstalled outlines-0.0.34\nCollecting outlines==0.0.34\n  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.5/76.5 KB 5.5 MB/s eta 0:00:00\nInstalling collected packages: outlines\nSuccessfully installed outlines-0.0.34\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWarning: outlines.fsm fix failed - trying alternative approach...\nPatching vLLM to skip guided_decoding import...\nDownloading benchmark scripts...\nBenchmark scripts downloaded\ntotal 8\ndrwxr-xr-x 2 root root 4096 Jan 19 17:23 .\ndrwxr-xr-x 3 root root 4096 Jan 19 17:23 ..\nCreating sonnet dataset...\n=== Starting vLLM server for HUMAN benchmark ===\nINFO 01-19 17:23:30 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct)\nINFO 01-19 17:23:31 selector.py:37] Using FlashAttention-2 backend.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 184, in <module>\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 370, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 328, in __init__\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 446, in _init_engine\n[rank0]:     return engine_class(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 163, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 41, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 23, in _init_executor\n[rank0]:     self._init_non_spec_worker()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 69, in _init_non_spec_worker\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 120, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 163, in load_model\n[rank0]:     self.model = get_model(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 225, in load_model\n[rank0]:     model = _initialize_model(model_config, self.load_config,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 88, in _initialize_model\n[rank0]:     return model_class(config=model_config.hf_config,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 346, in __init__\n[rank0]:     self.model = LlamaModel(config,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 274, in __init__\n[rank0]:     self.layers = nn.ModuleList([\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 275, in <listcomp>\n[rank0]:     LlamaDecoderLayer(config, cache_config, quant_config)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 199, in __init__\n[rank0]:     self.self_attn = LlamaAttention(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 146, in __init__\n[rank0]:     self.rotary_emb = get_rope(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 524, in get_rope\n[rank0]:     raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n[rank0]: ValueError: Unknown RoPE scaling type llama3\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-bench:8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd' locally\n8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd: Pulling from anonymous/vllm-bench\n0ed7121bfad6: Pulling fs layer\nad38c075dd0f: Pulling fs layer\n0ed7121bfad6: Download complete\nad38c075dd0f: Download complete\nad38c075dd0f: Pull complete\n0ed7121bfad6: Pull complete\nDigest: sha256:7d825fd3e21ed36a8e7d960b9bab5e6410c04391376c1ab1abd421f78fdcf81a\nStatus: Downloaded newer image for anonymous/vllm-bench:8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd\n",
  "timestamp": "2026-01-19 17:23:33"
}