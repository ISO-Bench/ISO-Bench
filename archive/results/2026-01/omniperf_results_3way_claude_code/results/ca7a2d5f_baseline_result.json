{
  "human_commit": "ca7a2d5f",
  "human_commit_full": "ca7a2d5f28eac9621474563cdda0e08596222755",
  "parent_commit": "333681408feabb97193880303b23f6571ba39045",
  "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 105.34350228309631,
  "metrics": {
    "output_token_throughput_tok_s": 3821.83,
    "ttft_mean_ms": 2586.0,
    "tpot_mean_ms": 22.54,
    "itl_mean_ms": 22.4
  },
  "raw_output": "ark_serving.py  --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --num-prompts 300 --seed 0 --dataset-name sonnet --sonnet-input-len 256 --sonnet-output-len 64 --dataset-path /opt/vllm_bench/benchmarks/sonnet.txt --base-url http://localhost:8000\n/opt/vllm_bench/benchmarks/benchmark_serving.py:18: SyntaxWarning: invalid escape sequence '\\ '\n  --request-rate <request_rate> \\ # By default <request_rate> is inf\nINFO 01-18 15:34:45 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url='http://localhost:8000', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sonnet', dataset_path='/opt/vllm_bench/benchmarks/sonnet.txt', model='deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, sharegpt_output_len=None, sonnet_input_len=256, sonnet_output_len=64, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:03<19:17,  3.87s/it]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 257/300 [00:05<00:00, 66.47it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:05<00:00, 59.72it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.02      \nTotal input tokens:                      89100     \nTotal generated tokens:                  19200     \nRequest throughput (req/s):              59.72     \nOutput token throughput (tok/s):         3821.83   \nTotal Token throughput (tok/s):          21557.52  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2586.00   \nMedian TTFT (ms):                        2332.29   \nP99 TTFT (ms):                           4167.80   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          22.54     \nMedian TPOT (ms):                        23.78     \nP99 TPOT (ms):                           24.60     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           22.40     \nMedian ITL (ms):                         23.37     \nP99 ITL (ms):                            32.01     \n==================================================\nStopping vLLM server...\nBENCHMARK_DONE\n/opt/vllm_bench/benchmarks/benchmark_serving.py:18: SyntaxWarning: invalid escape sequence '\\ '\n  --request-rate <request_rate> \\ # By default <request_rate> is inf\nINFO 01-18 15:34:45 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url='http://localhost:8000', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sonnet', dataset_path='/opt/vllm_bench/benchmarks/sonnet.txt', model='deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, sharegpt_output_len=None, sonnet_input_len=256, sonnet_output_len=64, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:03<19:17,  3.87s/it]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 257/300 [00:05<00:00, 66.47it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:05<00:00, 59.72it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.02      \nTotal input tokens:                      89100     \nTotal generated tokens:                  19200     \nRequest throughput (req/s):              59.72     \nOutput token throughput (tok/s):         3821.83   \nTotal Token throughput (tok/s):          21557.52  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2586.00   \nMedian TTFT (ms):                        2332.29   \nP99 TTFT (ms):                           4167.80   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          22.54     \nMedian TPOT (ms):                        23.78     \nP99 TPOT (ms):                           24.60     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           22.40     \nMedian ITL (ms):                         23.37     \nP99 ITL (ms):                            32.01     \n==================================================\n",
  "timestamp": "2026-01-18 15:34:58"
}