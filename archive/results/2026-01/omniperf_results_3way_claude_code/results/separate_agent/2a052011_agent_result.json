{
  "human_commit": "2a052011",
  "human_commit_full": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
  "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
  "model": "nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 34.37234377861023,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nApplying compatibility fixes preemptively...\nFixing transformers compatibility (LogitsWarper missing)...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file tests/kernels/test_moe.py\nchecking file vllm/model_executor/models/mixtral.py\npatching file tests/kernels/test_moe.py\npatching file vllm/model_executor/models/mixtral.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/config.py\n  Patching: /opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\nSearching for Python with vLLM...\nFound vLLM at: /usr/bin/python3\nvLLM 0.4.1 OK\nFixing outlines.fsm compatibility (fsm.guide missing)...\nFound existing installation: outlines 0.0.34\nUninstalling outlines-0.0.34:\n  Successfully uninstalled outlines-0.0.34\nCollecting outlines==0.0.34\n  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.5/76.5 KB 4.1 MB/s eta 0:00:00\nInstalling collected packages: outlines\nSuccessfully installed outlines-0.0.34\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWarning: outlines.fsm fix failed - trying alternative approach...\nPatching vLLM to skip guided_decoding import...\nCloning vLLM repo for benchmark scripts...\nCloning into 'vllm_bench'...\n=== Starting vLLM server for AGENT benchmark ===\nWARNING 01-09 21:51:33 config.py:196] fp8 quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 01-09 21:51:33 llm_engine.py:100] Initializing an LLM engine (v0.4.1) with config: model='nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8', speculative_config=None, tokenizer='nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\nINFO 01-09 21:51:34 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-09 21:51:34 selector.py:27] Using FlashAttention-2 backend.\nWARNING 01-09 21:51:35 fp8.py:29] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nWARNING 01-09 21:51:35 utils.py:465] For Mixtral FP8 quantization, we currently do not quantize the attention layers until their FP8 performance is improved.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 168, in <module>\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 366, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 324, in __init__\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 442, in _init_engine\n[rank0]:     return engine_class(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 159, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 41, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 23, in _init_executor\n[rank0]:     self._init_non_spec_worker()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 69, in _init_non_spec_worker\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 118, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 164, in load_model\n[rank0]:     self.model = get_model(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 222, in load_model\n[rank0]:     model = _initialize_model(model_config, self.load_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 88, in _initialize_model\n[rank0]:     return model_class(config=model_config.hf_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/mixtral.py\", line 411, in __init__\n[rank0]:     self.model = MixtralModel(config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/mixtral.py\", line 355, in __init__\n[rank0]:     self.layers = nn.ModuleList([\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/mixtral.py\", line 356, in <listcomp>\n[rank0]:     MixtralDecoderLayer(config, quant_config=quant_config)\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/mixtral.py\", line 295, in __init__\n[rank0]:     self.block_sparse_moe = MixtralMoE(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/mixtral.py\", line 101, in __init__\n[rank0]:     torch.empty(self.num_total_experts,\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 78, in __torch_function__\n[rank0]:     return func(*args, **kwargs)\n[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU \nSERVER_CRASHED\n",
  "timestamp": "2026-01-09 21:51:37"
}