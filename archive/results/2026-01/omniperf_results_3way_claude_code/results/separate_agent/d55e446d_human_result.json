{
  "human_commit": "d55e446d",
  "human_commit_full": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
  "parent_commit": "ec82c3e388b962a30a02fa376c222cef787b3c14",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "status": "error",
  "error": "No metrics in output",
  "duration_s": 198.48287510871887,
  "metrics": {},
  "raw_output": "Checking vLLM compatibility...\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-10 01:00:12 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-10 01:00:12 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-10 01:00:12 [__init__.py:246] Automatically detected platform cuda.\nvLLM import failed - applying compatibility fixes...\ntransformers.models.mllama OK\nSkipping transformers downgrade - vLLM uses mllama which needs transformers>=4.45\nDetecting vLLM version...\nFound vLLM at: WARNING 01-10 01:00:23 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-10 01:00:23 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-10 01:00:23 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-10 01:00:23 [__init__.py:246] Automatically detected platform cuda.\nFixing rope_scaling compatibility for Llama-3.1 models...\nSearching for Python with vLLM...\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-10 01:00:27 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-10 01:00:27 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-10 01:00:27 [__init__.py:246] Automatically detected platform cuda.\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-10 01:00:31 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-10 01:00:31 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-10 01:00:31 [__init__.py:246] Automatically detected platform cuda.\nERROR: Could not find Python with vLLM installed\nChecking available Pythons...\n/usr/bin/python3\nUnable to find image 'anonymous/vllm-bench:d55e446d1320d0f5f22bc3584f81f18d7924f166' locally\nd55e446d1320d0f5f22bc3584f81f18d7924f166: Pulling from anonymous/vllm-bench\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\na04181a7ff83: Already exists\nb89dfbaf9e2c: Already exists\ndc1d202b3ed0: Already exists\nad0178e8a361: Already exists\ne4a092a24edb: Already exists\ncd9db290845e: Already exists\ndffec7fbf4dd: Pulling fs layer\na8be18ab8904: Pulling fs layer\n0b9c9172e472: Pulling fs layer\nec85484421a8: Pulling fs layer\nc05e22123773: Pulling fs layer\n7fcbac520dff: Pulling fs layer\n3e3534bc8c88: Pulling fs layer\n6091d971ea6c: Pulling fs layer\ne6972b3eaa65: Pulling fs layer\nc05e22123773: Waiting\n3e3534bc8c88: Waiting\n7fcbac520dff: Waiting\ne6972b3eaa65: Waiting\n6091d971ea6c: Waiting\nec85484421a8: Waiting\n0b9c9172e472: Verifying Checksum\n0b9c9172e472: Download complete\nec85484421a8: Verifying Checksum\nec85484421a8: Download complete\nc05e22123773: Verifying Checksum\nc05e22123773: Download complete\n7fcbac520dff: Verifying Checksum\n7fcbac520dff: Download complete\n3e3534bc8c88: Download complete\n6091d971ea6c: Verifying Checksum\n6091d971ea6c: Download complete\ne6972b3eaa65: Verifying Checksum\ne6972b3eaa65: Download complete\na8be18ab8904: Verifying Checksum\na8be18ab8904: Download complete\ndffec7fbf4dd: Verifying Checksum\ndffec7fbf4dd: Download complete\ndffec7fbf4dd: Pull complete\na8be18ab8904: Pull complete\n0b9c9172e472: Pull complete\nec85484421a8: Pull complete\nc05e22123773: Pull complete\n7fcbac520dff: Pull complete\n3e3534bc8c88: Pull complete\n6091d971ea6c: Pull complete\ne6972b3eaa65: Pull complete\nDigest: sha256:be8a6cb5ac04558de59667ba956a1ab0c8f13bd710540af15e9eb8a9f6783de0\nStatus: Downloaded newer image for anonymous/vllm-bench:d55e446d1320d0f5f22bc3584f81f18d7924f166\n",
  "timestamp": "2026-01-10 09:00:32"
}