{
  "human_commit": "ad8d696a",
  "human_commit_full": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
  "parent_commit": "3d925165f2b18379640a63fbb42de95440d63b64",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 169.21642589569092,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nFixing transformers compatibility (LogitsWarper missing)...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file vllm/core/scheduler.py\npatching file vllm/core/scheduler.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/config.py\n  Patching: /opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\nSearching for Python with vLLM...\nFound vLLM at: /usr/bin/python3\nvLLM 0.4.1 OK\nFixing outlines.fsm compatibility (fsm.guide missing)...\nFound existing installation: outlines 0.0.34\nUninstalling outlines-0.0.34:\n  Successfully uninstalled outlines-0.0.34\nCollecting outlines==0.0.34\n  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.5/76.5 KB 5.3 MB/s eta 0:00:00\nInstalling collected packages: outlines\nSuccessfully installed outlines-0.0.34\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWarning: outlines.fsm fix failed - patching vLLM files directly...\nPatching all guided_decoding modules...\nGuided decoding modules patched\nCloning vLLM repo for benchmark scripts...\nCloning into 'vllm_bench'...\n=== Starting vLLM server for AGENT benchmark ===\nINFO 01-10 07:41:06 api_server.py:149] vLLM API server version 0.4.1\nINFO 01-10 07:41:06 api_server.py:150] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, model_loader_extra_config=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\nINFO 01-10 07:41:06 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\nINFO 01-10 07:41:07 utils.py:580] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-10 07:41:07 selector.py:28] Using FlashAttention backend.\nINFO 01-10 07:41:08 weight_utils.py:193] Using model weights format ['*.safetensors']\nINFO 01-10 07:41:11 model_runner.py:173] Loading model weights took 14.9595 GB\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\n    engine = AsyncLLMEngine.from_engine_args(\n  File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 361, in from_engine_args\n    engine = cls(\n  File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 319, in __init__\n    self.engine = self._init_engine(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 437, in _init_engine\n    return engine_class(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 160, in __init__\n    self._initialize_kv_caches()\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 236, in _initialize_kv_caches\n    self.model_executor.determine_num_available_blocks())\n  File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 110, in determine_num_available_blocks\n    return self.driver_worker.determine_num_available_blocks()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 138, in determine_num_available_blocks\n    self.model_runner.profile_run()\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 927, in profile_run\n    self.execute_model(seqs, kv_caches)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n    return func(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 848, in execute_model\n    hidden_states = model_executable(**execute_model_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 360, in forward\n    hidden_states = self.model(input_ids, positions, kv_caches,\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 286, in forward\n    hidden_states, residual = layer(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 224, in forward\n    hidden_states = self.input_layernorm(hidden_states)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/vllm_baseline/vllm/model_executor/layers/layernorm.py\", line 60, in forward\n    ops.rms_norm(\n  File \"/opt/vllm_baseline/vllm/_custom_ops.py\", line 106, in rms_norm\n    vllm_ops.rms_norm(out, input, weight, epsilon)\nNameError: name 'vllm_ops' is not defined\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-baseline:baseline-3d925165f2b1' locally\nbaseline-3d925165f2b1: Pulling from anonymous/vllm-baseline\naece8493d397: Already exists\n45f7ea5367fe: Already exists\n3d97a47c3c73: Already exists\n12cd4d19752f: Already exists\nda5a484f9d74: Already exists\n650665e2f893: Already exists\n50fb5d30dfa2: Already exists\n40490f1ea89f: Already exists\nbd31198d990e: Already exists\n461f333b6303: Already exists\n50f7a049eb5f: Already exists\n1ebedcec87ac: Pulling fs layer\n91841371e71b: Pulling fs layer\n5b3515203c6e: Pulling fs layer\n9c97122c82ff: Pulling fs layer\neb4f3b5ca231: Pulling fs layer\nbd15c12b8129: Pulling fs layer\n72eb2bdf588a: Pulling fs layer\nf397220ad710: Pulling fs layer\n299dabd77cbb: Pulling fs layer\n9c97122c82ff: Waiting\neb4f3b5ca231: Waiting\nf397220ad710: Waiting\nbd15c12b8129: Waiting\n72eb2bdf588a: Waiting\n299dabd77cbb: Waiting\n5b3515203c6e: Verifying Checksum\n5b3515203c6e: Download complete\n9c97122c82ff: Verifying Checksum\n9c97122c82ff: Download complete\neb4f3b5ca231: Download complete\nbd15c12b8129: Download complete\n72eb2bdf588a: Verifying Checksum\n72eb2bdf588a: Download complete\nf397220ad710: Verifying Checksum\nf397220ad710: Download complete\n299dabd77cbb: Verifying Checksum\n299dabd77cbb: Download complete\n1ebedcec87ac: Verifying Checksum\n1ebedcec87ac: Download complete\n1ebedcec87ac: Pull complete\n91841371e71b: Verifying Checksum\n91841371e71b: Download complete\n91841371e71b: Pull complete\n5b3515203c6e: Pull complete\n9c97122c82ff: Pull complete\neb4f3b5ca231: Pull complete\nbd15c12b8129: Pull complete\n72eb2bdf588a: Pull complete\nf397220ad710: Pull complete\n299dabd77cbb: Pull complete\nDigest: sha256:d25ddf6af5550509012db3453a880fdca2c8a93a9bbb29db957362b8fbf85743\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-3d925165f2b1\n",
  "timestamp": "2026-01-10 07:41:12"
}