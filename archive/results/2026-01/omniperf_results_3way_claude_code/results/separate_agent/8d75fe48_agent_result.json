{
  "human_commit": "8d75fe48",
  "human_commit_full": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
  "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
  "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 36.958558082580566,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nApplying compatibility fixes preemptively...\nFixing transformers compatibility (LogitsWarper missing)...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file vllm/_custom_ops.py\nchecking file vllm/model_executor/layers/quantization/fp8.py\npatching file vllm/_custom_ops.py\npatching file vllm/model_executor/layers/quantization/fp8.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/config.py\n  Patching: /opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\nSearching for Python with vLLM...\nFound vLLM at: /usr/bin/python3\nvLLM 0.4.3 OK\nFixing outlines.fsm compatibility (fsm.guide missing)...\nFound existing installation: outlines 1.2.3\nUninstalling outlines-1.2.3:\n  Successfully uninstalled outlines-1.2.3\nCollecting outlines==0.0.34\n  Downloading outlines-0.0.34-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.5/76.5 KB 5.4 MB/s eta 0:00:00\nInstalling collected packages: outlines\nSuccessfully installed outlines-0.0.34\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nWarning: outlines.fsm fix failed - patching vLLM files directly...\nPatching all guided_decoding modules...\nGuided decoding modules patched\nCloning vLLM repo for benchmark scripts...\nCloning into 'vllm_bench'...\n=== Starting vLLM server for AGENT benchmark ===\nINFO 01-10 05:53:18 api_server.py:177] vLLM API server version 0.4.3\nINFO 01-10 05:53:18 api_server.py:178] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, rope_scaling=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\nWARNING 01-10 05:53:18 config.py:218] fp8 quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 01-10 05:53:18 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=neuralmagic/Meta-Llama-3-8B-Instruct-FP8)\nWARNING 01-10 05:53:19 fp8.py:30] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-10 05:53:20 weight_utils.py:221] Using model weights format ['*.safetensors']\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 186, in <module>\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 395, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 349, in __init__\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 470, in _init_engine\n[rank0]:     return engine_class(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 222, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 41, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 24, in _init_executor\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 121, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 146, in load_model\n[rank0]:     self.model = get_model(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 249, in load_model\n[rank0]:     model.load_weights(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 435, in load_weights\n[rank0]:     param = params_dict[name]\n[rank0]: KeyError: 'model.layers.18.mlp.down_proj.input_scale'\nSERVER_CRASHED\n",
  "timestamp": "2026-01-10 05:53:23"
}