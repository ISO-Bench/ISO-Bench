{
  "human_commit": "3092375e",
  "human_commit_full": "3092375e274e9e003961e600e10a6192d33ceaa0",
  "parent_commit": "3cd91dc9555e6f10e55f23d37782c65b0366f7cf",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 90.2216203212738,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nvLLM uses mllama - checking transformers version...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file vllm/envs.py\nchecking file vllm/v1/serial_utils.py\npatching file vllm/envs.py\npatching file vllm/v1/serial_utils.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/transformers_utils/config.py\nSearching for Python with vLLM...\nINFO 01-10 00:00:10 [__init__.py:239] Automatically detected platform cuda.\nINFO 01-10 00:00:13 [__init__.py:239] Automatically detected platform cuda.\nWARNING: Could not find Python with vLLM, trying default python3\nINFO 01-10 00:00:17 [__init__.py:239] Automatically detected platform cuda.\nvLLM import failed\noutlines.fsm OK\nCloning vLLM repo for benchmark scripts...\nCloning into 'vllm_bench'...\n=== Starting vLLM server for AGENT benchmark ===\nINFO 01-10 00:00:31 [__init__.py:239] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 19, in <module>\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\n  File \"/opt/vllm_baseline/vllm/config.py\", line 30, in <module>\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n  File \"/opt/vllm_baseline/vllm/model_executor/__init__.py\", line 3, in <module>\n    from vllm.model_executor.parameter import (BasevLLMParameter,\n  File \"/opt/vllm_baseline/vllm/model_executor/parameter.py\", line 9, in <module>\n    from vllm.distributed import get_tensor_model_parallel_rank\n  File \"/opt/vllm_baseline/vllm/distributed/__init__.py\", line 3, in <module>\n    from .communication_op import *\n  File \"/opt/vllm_baseline/vllm/distributed/communication_op.py\", line 8, in <module>\n    from .parallel_state import get_tp_group\n  File \"/opt/vllm_baseline/vllm/distributed/parallel_state.py\", line 122, in <module>\n    from vllm.platforms import current_platform\n  File \"/opt/vllm_baseline/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/utils.py\", line 2059, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nImportError: /opt/vllm_baseline/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-baseline:baseline-3cd91dc9555e' locally\nbaseline-3cd91dc9555e: Pulling from anonymous/vllm-baseline\n3c645031de29: Already exists\n0d6448aff889: Already exists\n0a7674e3e8fe: Already exists\nb71b637b97c5: Already exists\n56dc85502937: Already exists\nec6d5f6c9ed9: Already exists\n47b8539d532f: Already exists\nfd9cc1ad8dee: Already exists\n83525caeeb35: Already exists\n8e79813a7b9d: Already exists\n312a542960e3: Already exists\n949691b47390: Already exists\n7cbb09265719: Already exists\ncb7c80b8c4f1: Already exists\nf4d72f8f1249: Already exists\n5b7bacc7057e: Already exists\ne64afe835865: Already exists\n12910821d266: Already exists\nc2e156ac312f: Already exists\n9ba5614620d8: Already exists\n0e0b9afae2de: Already exists\n6e36bbc5664e: Already exists\nf54b6e948b66: Already exists\nca26e1d286eb: Already exists\n9bebb419ad34: Pulling fs layer\n634f274e8a37: Pulling fs layer\nf76281b6dfd1: Pulling fs layer\nff9004fbcea8: Pulling fs layer\n7aa226acb8e6: Pulling fs layer\n0c298516babe: Pulling fs layer\ne43966a39056: Pulling fs layer\nd555276d72e9: Pulling fs layer\naab4066e2724: Pulling fs layer\n7aa226acb8e6: Waiting\n0c298516babe: Waiting\ne43966a39056: Waiting\nd555276d72e9: Waiting\naab4066e2724: Waiting\nff9004fbcea8: Waiting\nf76281b6dfd1: Verifying Checksum\nf76281b6dfd1: Download complete\nff9004fbcea8: Verifying Checksum\nff9004fbcea8: Download complete\n7aa226acb8e6: Verifying Checksum\n7aa226acb8e6: Download complete\n0c298516babe: Verifying Checksum\n0c298516babe: Download complete\n9bebb419ad34: Verifying Checksum\n9bebb419ad34: Download complete\nd555276d72e9: Verifying Checksum\nd555276d72e9: Download complete\naab4066e2724: Verifying Checksum\naab4066e2724: Download complete\n9bebb419ad34: Pull complete\n634f274e8a37: Verifying Checksum\n634f274e8a37: Download complete\n634f274e8a37: Pull complete\nf76281b6dfd1: Pull complete\nff9004fbcea8: Pull complete\n7aa226acb8e6: Pull complete\n0c298516babe: Pull complete\ne43966a39056: Verifying Checksum\ne43966a39056: Download complete\ne43966a39056: Pull complete\nd555276d72e9: Pull complete\naab4066e2724: Pull complete\nDigest: sha256:2ee8ca546ee09e1274ad35cbe01e72c886b1f740d92dcd4d2512e53c52fb35e0\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-3cd91dc9555e\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 19, in <module>\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\n  File \"/opt/vllm_baseline/vllm/config.py\", line 30, in <module>\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n  File \"/opt/vllm_baseline/vllm/model_executor/__init__.py\", line 3, in <module>\n    from vllm.model_executor.parameter import (BasevLLMParameter,\n  File \"/opt/vllm_baseline/vllm/model_executor/parameter.py\", line 9, in <module>\n    from vllm.distributed import get_tensor_model_parallel_rank\n  File \"/opt/vllm_baseline/vllm/distributed/__init__.py\", line 3, in <module>\n    from .communication_op import *\n  File \"/opt/vllm_baseline/vllm/distributed/communication_op.py\", line 8, in <module>\n    from .parallel_state import get_tp_group\n  File \"/opt/vllm_baseline/vllm/distributed/parallel_state.py\", line 122, in <module>\n    from vllm.platforms import current_platform\n  File \"/opt/vllm_baseline/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/utils.py\", line 2059, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nImportError: /opt/vllm_baseline/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled\n",
  "timestamp": "2026-01-10 08:00:33"
}