{
  "human_commit": "6ce01f30",
  "human_commit_full": "6ce01f30667bbae33f112152e07a3b66b841078f",
  "parent_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5",
  "model": "meta-llama/Meta-Llama-3-8B",
  "status": "error",
  "error": "Server crashed during startup",
  "duration_s": 38.138975858688354,
  "metrics": {},
  "raw_output": "Applying aimv2 compatibility fix (pre-import)...\nChecking vLLM compatibility...\nvLLM imports OK - no compatibility fixes needed\nDetecting vLLM version...\nFound vLLM at: /usr/local/lib/python3.10/dist-packages/vllm\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /usr/local/lib/python3.10/dist-packages/vllm/config.py\nSearching for Python with vLLM...\nFound vLLM at: /usr/bin/python3\nvLLM version: 0.5.3.post1 at /usr/local/lib/python3.10/dist-packages/vllm\nvLLM 0.5.3.post1 OK\noutlines.fsm OK\nDownloading benchmark scripts...\nBenchmark scripts downloaded\ntotal 92\ndrwxr-xr-x 2 root root  4096 Jan 18 12:08 .\ndrwxr-xr-x 3 root root  4096 Jan 18 12:08 ..\n-rw-r--r-- 1 root root 15648 Jan 18 12:08 backend_request_func.py\n-rw-r--r-- 1 root root 12070 Jan 18 12:08 benchmark_latency.py\n-rw-r--r-- 1 root root 29475 Jan 18 12:08 benchmark_serving.py\n-rw-r--r-- 1 root root 22177 Jan 18 12:08 benchmark_throughput.py\nCreating sonnet dataset...\n=== Starting vLLM server for HUMAN benchmark ===\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 23, in <module>\n    from vllm.entrypoints.openai.cli_args import make_arg_parser\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/cli_args.py\", line 12, in <module>\n    from vllm.entrypoints.openai.serving_engine import (LoRAModulePath,\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_engine.py\", line 28, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\nSERVER_CRASHED\n",
  "timestamp": "2026-01-18 12:09:00"
}