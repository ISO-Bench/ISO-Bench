{
  "human_commit": "e7523c2e",
  "human_commit_full": "e7523c2e031bc96740723ab63833d1cf94229ab4",
  "parent_commit": "a869baca73eb90ae7bd18402915dc4bfc36cf06b",
  "model": "google/gemma-3-12b-it",
  "status": "error",
  "error": "No serving metrics in output",
  "duration_s": 321.28080582618713,
  "metrics": {},
  "raw_output": "n run_method\nERROR 01-18 07:13:33 [engine.py:454]     return func(*args, **kwargs)\nERROR 01-18 07:13:33 [engine.py:454]            ^^^^^^^^^^^^^^^^^^^^^\nERROR 01-18 07:13:33 [engine.py:454]   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 315, in initialize_cache\nERROR 01-18 07:13:33 [engine.py:454]     raise_if_cache_size_invalid(\nERROR 01-18 07:13:33 [engine.py:454]   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 569, in raise_if_cache_size_invalid\nERROR 01-18 07:13:33 [engine.py:454]     raise ValueError(\nERROR 01-18 07:13:33 [engine.py:454] ValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (50352). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 456, in run_mp_engine\n    raise e from None\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 442, in run_mp_engine\n    engine = MQLLMEngine.from_vllm_config(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 129, in from_vllm_config\n    return cls(\n           ^^^^\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 83, in __init__\n    self.engine = LLMEngine(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 268, in __init__\n    self._initialize_kv_caches()\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 426, in _initialize_kv_caches\n    self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n  File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 123, in initialize_cache\n    self.collective_rpc(\"initialize_cache\",\n  File \"/opt/vllm_baseline/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/utils.py\", line 2605, in run_method\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 315, in initialize_cache\n    raise_if_cache_size_invalid(\n  File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 569, in raise_if_cache_size_invalid\n    raise ValueError(\nValueError: The model's max seq len (131072) is larger than the maximum number of tokens that can be stored in KV cache (50352). Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.\n[rank0]:[W118 07:13:33.622071306 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1376, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1324, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 153, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 280, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\nSERVER_START_FAILED\n",
  "timestamp": "2026-01-18 15:17:36"
}