{
  "human_commit": "b2e0ad3b",
  "human_commit_full": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
  "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 58.02767729759216,
  "metrics": {
    "output_token_throughput_tok_s": 2539.67,
    "ttft_mean_ms": 948.83,
    "tpot_mean_ms": 19.57,
    "itl_mean_ms": 19.55
  },
  "raw_output": ": invalid escape sequence '\\ '\n  --request-rate <request_rate> \\ # By default <request_rate> is inf\nNamespace(backend='vllm', base_url='http://localhost:8000', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sonnet', dataset_path='/opt/vllm_bench/benchmarks/sonnet.txt', model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sharegpt_output_len=None, sonnet_input_len=256, sonnet_output_len=64, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<03:04,  1.86s/it]\n 18%|\u2588\u258a        | 18/100 [00:01<00:06, 12.48it/s]\n 33%|\u2588\u2588\u2588\u258e      | 33/100 [00:02<00:02, 24.91it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 49/100 [00:02<00:01, 39.90it/s]\n 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 65/100 [00:02<00:00, 56.47it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 82/100 [00:02<00:00, 73.98it/s]\n 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 99/100 [00:02<00:00, 91.90it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 39.68it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.52      \nTotal input tokens:                      23100     \nTotal generated tokens:                  6400      \nRequest throughput (req/s):              39.68     \nOutput token throughput (tok/s):         2539.67   \nTotal Token throughput (tok/s):          11706.28  \n---------------Time to First Token----------------\nMean TTFT (ms):                          948.83    \nMedian TTFT (ms):                        919.83    \nP99 TTFT (ms):                           1755.88   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.57     \nMedian TPOT (ms):                        20.13     \nP99 TPOT (ms):                           23.92     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           19.55     \nMedian ITL (ms):                         15.19     \nP99 ITL (ms):                            215.42    \n==================================================\nStopping vLLM server...\nBENCHMARK_DONE\n/opt/vllm_bench/benchmarks/benchmark_serving.py:18: SyntaxWarning: invalid escape sequence '\\ '\n  --request-rate <request_rate> \\ # By default <request_rate> is inf\nNamespace(backend='vllm', base_url='http://localhost:8000', host='localhost', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='sonnet', dataset_path='/opt/vllm_bench/benchmarks/sonnet.txt', model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, sharegpt_output_len=None, sonnet_input_len=256, sonnet_output_len=64, sonnet_prefix_len=200, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, request_rate=inf, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, percentile_metrics='ttft,tpot,itl', metric_percentiles='99')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<03:04,  1.86s/it]\n 18%|\u2588\u258a        | 18/100 [00:01<00:06, 12.48it/s]\n 33%|\u2588\u2588\u2588\u258e      | 33/100 [00:02<00:02, 24.91it/s]\n 49%|\u2588\u2588\u2588\u2588\u2589     | 49/100 [00:02<00:01, 39.90it/s]\n 65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 65/100 [00:02<00:00, 56.47it/s]\n 82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 82/100 [00:02<00:00, 73.98it/s]\n 99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 99/100 [00:02<00:00, 91.90it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 39.68it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.52      \nTotal input tokens:                      23100     \nTotal generated tokens:                  6400      \nRequest throughput (req/s):              39.68     \nOutput token throughput (tok/s):         2539.67   \nTotal Token throughput (tok/s):          11706.28  \n---------------Time to First Token----------------\nMean TTFT (ms):                          948.83    \nMedian TTFT (ms):                        919.83    \nP99 TTFT (ms):                           1755.88   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.57     \nMedian TPOT (ms):                        20.13     \nP99 TPOT (ms):                           23.92     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           19.55     \nMedian ITL (ms):                         15.19     \nP99 ITL (ms):                            215.42    \n==================================================\n",
  "timestamp": "2026-01-18 14:19:32"
}