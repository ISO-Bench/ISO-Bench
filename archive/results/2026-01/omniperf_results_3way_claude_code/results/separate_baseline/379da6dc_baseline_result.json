{
  "human_commit": "379da6dc",
  "human_commit_full": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
  "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
  "status": "error",
  "benchmark_type": "serving",
  "model": "meta-llama/Meta-Llama-3-70B",
  "duration_s": 4.983915567398071,
  "error": "Server crashed during startup",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-08 09:37:49",
  "raw_output": "=== Using cached baseline image (skipping build) ===\n=== Starting vLLM server for BASELINE benchmark ===\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 26, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/serving_chat.py\", line 20, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/opt/vllm_baseline/vllm/model_executor/guided_decoding/__init__.py\", line 5, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/opt/vllm_baseline/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\nBASELINE_SERVER_CRASHED\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
}