{
  "human_commit": "d7740ea4",
  "human_commit_full": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
  "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87",
  "status": "success",
  "benchmark_type": "throughput",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "duration_s": 537.5572843551636,
  "error": null,
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": 14.83,
  "throughput_tok_s": 7592.42,
  "timestamp": "2026-01-10 05:11:57",
  "raw_output": "/python3.10/dist-packages (from transformers>=4.40.0->vllm==0.4.2+cu124) (4.67.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->vllm==0.4.2+cu124) (0.6.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm==0.4.2+cu124) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm==0.4.2+cu124) (0.10.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm==0.4.2+cu124) (1.9.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->vllm==0.4.2+cu124) (4.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->vllm==0.4.2+cu124) (1.3.1)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (0.16.0)\nRequirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (0.6.4)\nRequirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (15.0.1)\nRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (1.1.1)\nRequirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.4.2+cu124) (1.1.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->vllm==0.4.2+cu124) (1.3.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->vllm==0.4.2+cu124) (1.0.9)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.19.1->vllm==0.4.2+cu124) (1.1.8)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->outlines==0.0.34->vllm==0.4.2+cu124) (3.0.2)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm==0.4.2+cu124) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm==0.4.2+cu124) (2025.4.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->outlines==0.0.34->vllm==0.4.2+cu124) (0.27.0)\nRequirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines==0.0.34->vllm==0.4.2+cu124) (0.44.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->vllm==0.4.2+cu124) (1.3.0)\nBuilding wheels for collected packages: vllm\n  Building editable for vllm (pyproject.toml): started\n  Building editable for vllm (pyproject.toml): still running...\n  Building editable for vllm (pyproject.toml): still running...\n  Building editable for vllm (pyproject.toml): still running...\n  Building editable for vllm (pyproject.toml): finished with status 'done'\n  Created wheel for vllm: filename=vllm-0.4.2+cu124-0.editable-cp310-cp310-linux_x86_64.whl size=10578 sha256=cf07056254c0c5355e03f49b7fe8a1ccc243ab1139565a8fd9fd448af4bab306\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mc5af2bb/wheels/ec/af/dd/1cca977b918400b9562e1f829b75dc39a632828ce8612be7a0\nSuccessfully built vllm\nInstalling collected packages: vllm\nSuccessfully installed vllm-0.4.2+cu124\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nInstalling pyairports...\nCollecting pyairports\n  Downloading pyairports-0.0.1.tar.gz (3.1 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nBuilding wheels for collected packages: pyairports\n  Building wheel for pyairports (pyproject.toml): started\n  Building wheel for pyairports (pyproject.toml): finished with status 'done'\n  Created wheel for pyairports: filename=pyairports-0.0.1-py3-none-any.whl size=3483 sha256=df330bcc6f0a8f4915780ea42414197ad425475a130493969483512fc04d8053\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6x8_vjua/wheels/11/d6/10/dd881deddb65aca156e8a31a523f590067bdc8fc85c5c3a316\nSuccessfully built pyairports\nInstalling collected packages: pyairports\nSuccessfully installed pyairports-0.0.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n=== Verifying baseline vLLM installation ===\nvLLM version: 0.4.2\n=== Running throughput benchmark ===\nNamespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None)\nINFO 01-10 05:10:33 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\nINFO 01-10 05:10:34 utils.py:638] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-10 05:10:35 selector.py:27] Using FlashAttention-2 backend.\nINFO 01-10 05:10:36 weight_utils.py:199] Using model weights format ['*.safetensors']\nINFO 01-10 05:10:39 model_runner.py:175] Loading model weights took 14.9595 GB\nINFO 01-10 05:10:39 gpu_executor.py:118] # GPU blocks: 27700, # CPU blocks: 2048\nINFO 01-10 05:10:42 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-10 05:10:42 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-10 05:10:45 model_runner.py:1017] Graph capturing finished in 4 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s]\nProcessed prompts:   0%|          | 1/1000 [00:17<4:47:22, 17.26s/it]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:34<01:23,  8.86it/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588     | 512/1000 [00:50<00:55,  8.86it/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [00:51<00:42, 11.58it/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:07<00:17, 13.40it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:07<00:00, 14.92it/s]\nThroughput: 14.83 requests/s, 7592.42 tokens/s\ndebconf: delaying package configuration, since apt-utils is not installed\nFrom https://github.com/vllm-project/vllm\n * branch            cc466a32903d53d0ceca459b766d74ad668c8f87 -> FETCH_HEAD\nNote: switching to 'cc466a32903d53d0ceca459b766d74ad668c8f87'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at cc466a3 [Core][Distributed] support cpu&device in broadcast tensor dict (#4660)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nUsing Python 3.10.12 environment at: /usr\nUninstalled 1 package in 65ms\n - vllm==0.4.2+cu124 (from file:///vllm-workspace/dist/vllm-0.4.2%2Bcu124-cp310-cp310-linux_x86_64.whl)\nUsing Python 3.10.12 environment at: /usr\nResolved 5 packages in 64ms\nDownloading setuptools (1.1MiB)\n Downloaded setuptools\nPrepared 2 packages in 81ms\nInstalled 2 packages in 18ms\n + setuptools==80.9.0\n + wheel==0.45.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
}