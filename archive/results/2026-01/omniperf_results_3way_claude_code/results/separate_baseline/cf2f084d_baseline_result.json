{
  "human_commit": "cf2f084d",
  "human_commit_full": "cf2f084d56a1293cb08da2393984cdc7685ac019",
  "parent_commit": "f721096d48a7e3b98dffcb9b400bf58989cef64d",
  "status": "error",
  "benchmark_type": "serving",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "duration_s": 5.973726987838745,
  "error": "Server crashed during startup",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-08 20:33:02",
  "raw_output": "=== Using cached baseline image (skipping build) ===\n=== Starting vLLM server for BASELINE benchmark ===\nINFO 01-08 20:33:00 api_server.py:148] vLLM API server version 0.3.3\nINFO 01-08 20:33:00 api_server.py:149] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=4096, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, seed=0, swap_space=4, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', scheduler_delay_factor=0.0, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n    hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1117, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n    response = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 426, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-6960147d-7f78a7e45810bc98646b0924;7848151c-08a8-4eef-9a91-9830e35c0316)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/workspace/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\n  File \"/workspace/vllm/engine/async_llm_engine.py\", line 326, in from_engine_args\n    engine_configs = engine_args.create_engine_configs()\n  File \"/workspace/vllm/engine/arg_utils.py\", line 330, in create_engine_configs\n    model_config = ModelConfig(\n  File \"/workspace/vllm/config.py\", line 118, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n  File \"/workspace/vllm/transformers_utils/config.py\", line 21, in get_config\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1250, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 649, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 708, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 543, in cached_files\n    raise OSError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-6960147d-7f78a7e45810bc98646b0924;7848151c-08a8-4eef-9a91-9830e35c0316)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nBASELINE_SERVER_CRASHED\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
}