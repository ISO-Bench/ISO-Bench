{
  "commit": "fc7b8d1e",
  "model": "meta-llama/Meta-Llama-3-8B-Instruct",
  "benchmark_type": "baseline",
  "image": "anonymous/vllm-baseline:baseline-67abdbb42fdb",
  "status": "error",
  "error": "No throughput metrics in output",
  "logs": "ters MAX_PROMPT_ADAPTERS]\n                     [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n                     [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu}]\n                     [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n                     [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n                     [--speculative-model SPECULATIVE_MODEL]\n                     [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]\n                     [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]\n                     [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]\n                     [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]\n                     [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]\n                     [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]\n                     [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]\n                     [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]\n                     [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]\n                     [--disable-logprobs-during-spec-decoding DISABLE_LOGPROBS_DURING_SPEC_DECODING]\n                     [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n                     [--ignore-patterns IGNORE_PATTERNS]\n                     [--preemption-mode PREEMPTION_MODE]\n                     [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n                     [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n                     [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n                     [--engine-use-ray] [--disable-log-requests]\n                     [--max-log-len MAX_LOG_LEN]\napi_server.py: error: unrecognized arguments: python /workspace/vllm/benchmarks/benchmark_serving.py --backend vllm --num-prompts 100 --endpoint /v1/completions\n"
}