{
  "human_commit": "30172b49",
  "human_commit_full": "30172b4947c52890b808c6da3a6c7580f55cbb74",
  "parent_commit": "a4d577b37944cbfa1bc62e4869667d1e2739d62a",
  "status": "error",
  "benchmark_type": "serving",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "duration_s": 10.10665512084961,
  "error": "Server crashed during startup",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-09 04:20:01",
  "raw_output": ":59 engine.py:389]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 118, in from_engine_args\nERROR 01-08 20:19:59 engine.py:389]     engine_config = engine_args.create_engine_config(usage_context)\nERROR 01-08 20:19:59 engine.py:389]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 1119, in create_engine_config\nERROR 01-08 20:19:59 engine.py:389]     model_config = self.create_model_config()\nERROR 01-08 20:19:59 engine.py:389]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 1039, in create_model_config\nERROR 01-08 20:19:59 engine.py:389]     return ModelConfig(\nERROR 01-08 20:19:59 engine.py:389]            ^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/opt/vllm_baseline/vllm/config.py\", line 304, in __init__\nERROR 01-08 20:19:59 engine.py:389]     hf_config = get_config(self.model, trust_remote_code, revision,\nERROR 01-08 20:19:59 engine.py:389]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 260, in get_config\nERROR 01-08 20:19:59 engine.py:389]     config_dict, _ = PretrainedConfig.get_config_dict(\nERROR 01-08 20:19:59 engine.py:389]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\nERROR 01-08 20:19:59 engine.py:389]     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\nERROR 01-08 20:19:59 engine.py:389]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\nERROR 01-08 20:19:59 engine.py:389]     resolved_config_file = cached_file(\nERROR 01-08 20:19:59 engine.py:389]                            ^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\nERROR 01-08 20:19:59 engine.py:389]     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\nERROR 01-08 20:19:59 engine.py:389]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nERROR 01-08 20:19:59 engine.py:389]   File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 542, in cached_files\nERROR 01-08 20:19:59 engine.py:389]     raise OSError(\nERROR 01-08 20:19:59 engine.py:389] OSError: You are trying to access a gated repo.\nERROR 01-08 20:19:59 engine.py:389] Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\nERROR 01-08 20:19:59 engine.py:389] 401 Client Error. (Request ID: Root=1-696081ef-2403c6c0749e13a45d2e4856;92c6b633-48ac-41f7-a10e-a20352e36110)\nERROR 01-08 20:19:59 engine.py:389] \nERROR 01-08 20:19:59 engine.py:389] Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nERROR 01-08 20:19:59 engine.py:389] Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 478, in cached_files\n    hf_hub_download(\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1117, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 426, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-696081ef-2403c6c0749e13a45d2e4856;92c6b633-48ac-41f7-a10e-a20352e36110)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 391, in run_mp_engine\n    raise e\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 380, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/multiprocessing/engine.py\", line 118, in from_engine_args\n    engine_config = engine_args.create_engine_config(usage_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 1119, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 1039, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 304, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 260, in get_config\n    config_dict, _ = PretrainedConfig.get_config_dict(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n                           ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\", line 542, in cached_files\n    raise OSError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n401 Client Error. (Request ID: Root=1-696081ef-2403c6c0749e13a45d2e4856;92c6b633-48ac-41f7-a10e-a20352e36110)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\nBASELINE_SERVER_CRASHED\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3.12 -m pip install --upgrade pip\n"
}