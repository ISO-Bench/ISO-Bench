{
  "human_commit": "b6d10354",
  "human_commit_full": "b6d103542c654fb63013a1e45a586d654ae36a2a",
  "parent_commit": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447",
  "status": "error",
  "benchmark_type": "latency",
  "model": "meta-llama/Llama-2-70b-hf",
  "duration_s": 497.6003065109253,
  "error": "No latency metrics in output",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-08 19:00:05",
  "raw_output": "6.2->starlette<0.48.0,>=0.40.0->fastapi->vllm==0.4.0+cu124) (1.3.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->vllm==0.4.0+cu124) (1.3.0)\nBuilding wheels for collected packages: vllm\n  Building editable for vllm (pyproject.toml): started\n  Building editable for vllm (pyproject.toml): still running...\n  Building editable for vllm (pyproject.toml): still running...\n  Building editable for vllm (pyproject.toml): finished with status 'done'\n  Created wheel for vllm: filename=vllm-0.4.0+cu124-0.editable-cp310-cp310-linux_x86_64.whl size=10545 sha256=315abe7d33ecfca3e9265dd1115da82169a9a595a0d810d91a4d79202f6599b6\n  Stored in directory: /tmp/pip-ephem-wheel-cache-gt_kfgbd/wheels/ec/af/dd/1cca977b918400b9562e1f829b75dc39a632828ce8612be7a0\nSuccessfully built vllm\nInstalling collected packages: vllm\nSuccessfully installed vllm-0.4.0+cu124\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n=== Verifying baseline vLLM installation ===\nvLLM version: 0.4.0\n=== Running latency benchmark ===\nNamespace(model='meta-llama/Llama-2-70b-hf', tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters=3, trust_remote_code=False, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', profile=False, profile_result_dir=None, device='cuda', block_size=16, enable_chunked_prefill=False, ray_workers_use_nsight=False, download_dir=None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-70b-hf/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 479, in cached_files\n    hf_hub_download(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1010, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1117, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1658, in _raise_on_head_call_error\n    raise head_call_error\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1546, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1463, in get_hf_file_metadata\n    r = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 286, in _request_wrapper\n    response = _request_wrapper(\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 310, in _request_wrapper\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 426, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-695ffeb2-14214e716410ff9f6eee2389;323039f6-cf29-4e5c-a2a3-79b0f2a7d922)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-70b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 171, in <module>\n    main(args)\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 19, in main\n    llm = LLM(model=args.model,\n  File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 112, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 173, in from_engine_args\n    engine_configs = engine_args.create_engine_configs()\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 382, in create_engine_configs\n    model_config = ModelConfig(\n  File \"/opt/vllm_baseline/vllm/config.py\", line 120, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 22, in get_config\n    config = AutoConfig.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\", line 1250, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 649, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 708, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 321, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 543, in cached_files\n    raise OSError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-70b-hf.\n401 Client Error. (Request ID: Root=1-695ffeb2-14214e716410ff9f6eee2389;323039f6-cf29-4e5c-a2a3-79b0f2a7d922)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-70b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-70b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\nUnable to find image 'anonymous/vllm-baseline:b6d103542c654fb63013a1e45a586d654ae36a2a' locally\nb6d103542c654fb63013a1e45a586d654ae36a2a: Pulling from anonymous/vllm-baseline\naece8493d397: Already exists\n45f7ea5367fe: Already exists\n3d97a47c3c73: Already exists\n12cd4d19752f: Already exists\nda5a484f9d74: Already exists\n5e5846364eee: Already exists\nfd355de1d1f2: Already exists\n3480bb79c638: Already exists\ne7016935dd60: Already exists\n9800f3929d8c: Already exists\nc834972e083f: Already exists\n16bbe6e547b8: Pulling fs layer\n120df886176f: Pulling fs layer\n6a4bca37de10: Pulling fs layer\ne4fe04bff929: Pulling fs layer\n55d16a35a68e: Pulling fs layer\n0090a862d194: Pulling fs layer\n0b1c952c4445: Pulling fs layer\n55d16a35a68e: Waiting\ne4fe04bff929: Waiting\n0b1c952c4445: Waiting\n0090a862d194: Waiting\n16bbe6e547b8: Verifying Checksum\n16bbe6e547b8: Download complete\n16bbe6e547b8: Pull complete\ne4fe04bff929: Verifying Checksum\ne4fe04bff929: Download complete\n6a4bca37de10: Verifying Checksum\n6a4bca37de10: Download complete\n55d16a35a68e: Verifying Checksum\n55d16a35a68e: Download complete\n0090a862d194: Verifying Checksum\n0090a862d194: Download complete\n0b1c952c4445: Verifying Checksum\n0b1c952c4445: Download complete\n120df886176f: Verifying Checksum\n120df886176f: Download complete\n120df886176f: Pull complete\n6a4bca37de10: Pull complete\ne4fe04bff929: Pull complete\n55d16a35a68e: Pull complete\n0090a862d194: Pull complete\n0b1c952c4445: Pull complete\nDigest: sha256:314eb4d039e3b863e19fc674bba6029264e473ed4199c6ef7895050856edb12a\nStatus: Downloaded newer image for anonymous/vllm-baseline:b6d103542c654fb63013a1e45a586d654ae36a2a\ndebconf: delaying package configuration, since apt-utils is not installed\nFrom https://github.com/vllm-project/vllm\n * branch            51c31bc10ca7c48b580cd58fcd741ba4d6db4447 -> FETCH_HEAD\nNote: switching to '51c31bc10ca7c48b580cd58fcd741ba4d6db4447'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 51c31bc CMake build elf without PTX (#3739)\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\nUsing Python 3.10.12 environment at: /usr\nwarning: Skipping vllm as it is not installed\nwarning: No packages to uninstall\nUsing Python 3.10.12 environment at: /usr\nResolved 5 packages in 66ms\nDownloading setuptools (1.1MiB)\n Downloaded setuptools\nPrepared 2 packages in 77ms\nInstalled 2 packages in 18ms\n + setuptools==80.9.0\n + wheel==0.45.1\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
}