{
  "human_commit": "ccf02fcb",
  "human_commit_full": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
  "parent_commit": "acaea3bb07883c80b71643ebee1cd08d555797bc",
  "status": "error",
  "benchmark_type": "throughput",
  "model": "ibm-ai-platform/Bamba-9B",
  "duration_s": 1664.0738596916199,
  "error": "No throughput metrics in output",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-08 20:24:33",
  "raw_output": "                  [--tensor-parallel-size TENSOR_PARALLEL_SIZE]\n                               [--enable-expert-parallel]\n                               [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]\n                               [--ray-workers-use-nsight]\n                               [--block-size {8,16,32,64,128}]\n                               [--enable-prefix-caching | --no-enable-prefix-caching]\n                               [--disable-sliding-window]\n                               [--use-v2-block-manager]\n                               [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]\n                               [--seed SEED] [--swap-space SWAP_SPACE]\n                               [--cpu-offload-gb CPU_OFFLOAD_GB]\n                               [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]\n                               [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]\n                               [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]\n                               [--max-num-partial-prefills MAX_NUM_PARTIAL_PREFILLS]\n                               [--max-long-partial-prefills MAX_LONG_PARTIAL_PREFILLS]\n                               [--long-prefill-token-threshold LONG_PREFILL_TOKEN_THRESHOLD]\n                               [--max-num-seqs MAX_NUM_SEQS]\n                               [--max-logprobs MAX_LOGPROBS]\n                               [--disable-log-stats]\n                               [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n                               [--rope-scaling ROPE_SCALING]\n                               [--rope-theta ROPE_THETA]\n                               [--hf-overrides HF_OVERRIDES] [--enforce-eager]\n                               [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]\n                               [--disable-custom-all-reduce]\n                               [--tokenizer-pool-size TOKENIZER_POOL_SIZE]\n                               [--tokenizer-pool-type TOKENIZER_POOL_TYPE]\n                               [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]\n                               [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]\n                               [--mm-processor-kwargs MM_PROCESSOR_KWARGS]\n                               [--disable-mm-preprocessor-cache]\n                               [--enable-lora] [--enable-lora-bias]\n                               [--max-loras MAX_LORAS]\n                               [--max-lora-rank MAX_LORA_RANK]\n                               [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]\n                               [--lora-dtype {auto,float16,bfloat16}]\n                               [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]\n                               [--max-cpu-loras MAX_CPU_LORAS]\n                               [--fully-sharded-loras]\n                               [--enable-prompt-adapter]\n                               [--max-prompt-adapters MAX_PROMPT_ADAPTERS]\n                               [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]\n                               [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu,hpu}]\n                               [--num-scheduler-steps NUM_SCHEDULER_STEPS]\n                               [--use-tqdm-on-load | --no-use-tqdm-on-load]\n                               [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]\n                               [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]\n                               [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]\n                               [--speculative-model SPECULATIVE_MODEL]\n                               [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,ptpc_fp8,fbgemm_fp8,modelopt,nvfp4,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,quark,moe_wna16,None}]\n                               [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]\n                               [--speculative-disable-mqa-scorer]\n                               [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]\n                               [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]\n                               [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]\n                               [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]\n                               [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]\n                               [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]\n                               [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]\n                               [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]\n                               [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]\n                               [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]\n                               [--ignore-patterns IGNORE_PATTERNS]\n                               [--preemption-mode PREEMPTION_MODE]\n                               [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]\n                               [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]\n                               [--show-hidden-metrics-for-version SHOW_HIDDEN_METRICS_FOR_VERSION]\n                               [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]\n                               [--collect-detailed-traces COLLECT_DETAILED_TRACES]\n                               [--disable-async-output-proc]\n                               [--scheduling-policy {fcfs,priority}]\n                               [--scheduler-cls SCHEDULER_CLS]\n                               [--override-neuron-config OVERRIDE_NEURON_CONFIG]\n                               [--override-pooler-config OVERRIDE_POOLER_CONFIG]\n                               [--compilation-config COMPILATION_CONFIG]\n                               [--kv-transfer-config KV_TRANSFER_CONFIG]\n                               [--worker-cls WORKER_CLS]\n                               [--worker-extension-cls WORKER_EXTENSION_CLS]\n                               [--generation-config GENERATION_CONFIG]\n                               [--override-generation-config OVERRIDE_GENERATION_CONFIG]\n                               [--enable-sleep-mode] [--calculate-kv-scales]\n                               [--additional-config ADDITIONAL_CONFIG]\n                               [--enable-reasoning]\n                               [--reasoning-parser {deepseek_r1}]\n                               [--disable-log-requests]\nbenchmark_throughput.py: error: unrecognized arguments: python benchmarks/benchmark_serving.py\nUnable to find image 'anonymous/vllm-bench:ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c' locally\nccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c: Pulling from anonymous/vllm-bench\n3c645031de29: Already exists\n0d6448aff889: Already exists\n0a7674e3e8fe: Already exists\nb71b637b97c5: Already exists\n56dc85502937: Already exists\nec6d5f6c9ed9: Already exists\n47b8539d532f: Already exists\nfd9cc1ad8dee: Already exists\n83525caeeb35: Already exists\n8e79813a7b9d: Already exists\n312a542960e3: Already exists\n479ef8d53ea8: Already exists\n1a983e62f512: Already exists\nf632d9eacc9c: Already exists\n94039385962c: Already exists\n472a1e963f29: Already exists\n4f4fb700ef54: Already exists\n8d1974882f1e: Pulling fs layer\nbaa6bcaae4db: Pulling fs layer\nb3b7ba85e45c: Pulling fs layer\n6aedec50d5e6: Pulling fs layer\nbb47a1b0f9f6: Pulling fs layer\n9c7d1b825620: Pulling fs layer\n6aedec50d5e6: Waiting\nbb47a1b0f9f6: Waiting\n9c7d1b825620: Waiting\nb3b7ba85e45c: Verifying Checksum\nb3b7ba85e45c: Download complete\n6aedec50d5e6: Download complete\nbb47a1b0f9f6: Verifying Checksum\nbb47a1b0f9f6: Download complete\n9c7d1b825620: Verifying Checksum\n9c7d1b825620: Download complete\nbaa6bcaae4db: Verifying Checksum\nbaa6bcaae4db: Download complete\n8d1974882f1e: Verifying Checksum\n8d1974882f1e: Download complete\n8d1974882f1e: Pull complete\nbaa6bcaae4db: Pull complete\nb3b7ba85e45c: Pull complete\n6aedec50d5e6: Pull complete\nbb47a1b0f9f6: Pull complete\n9c7d1b825620: Pull complete\nDigest: sha256:8c6cb56746f93e26656eb6f4800353f489ca225c0e4212c915524055e6ee4e7b\nStatus: Downloaded newer image for anonymous/vllm-bench:ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c\ndebconf: delaying package configuration, since apt-utils is not installed\nFrom https://github.com/vllm-project/vllm\n * branch            acaea3bb07883c80b71643ebee1cd08d555797bc -> FETCH_HEAD\nNote: switching to 'acaea3bb07883c80b71643ebee1cd08d555797bc'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at acaea3b [Bugfix][V1] Fix flashinfer sampling (#14815)\nUsing Python 3.10.12 environment at: /usr\nwarning: Skipping vllm as it is not installed\nwarning: No packages to uninstall\nUsing Python 3.10.12 environment at: /usr\nResolved 5 packages in 47ms\nDownloading setuptools (1.1MiB)\nDownloading cmake (27.6MiB)\n Downloaded setuptools\n Downloaded cmake\nPrepared 5 packages in 761ms\nInstalled 5 packages in 109ms\n + cmake==4.2.1\n + ninja==1.13.0\n + packaging==25.0\n + setuptools==80.9.0\n + wheel==0.45.1\n"
}