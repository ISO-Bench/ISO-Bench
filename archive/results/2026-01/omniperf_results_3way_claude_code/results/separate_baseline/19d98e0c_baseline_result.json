{
  "human_commit": "19d98e0c",
  "human_commit_full": "19d98e0c7db96713f0e2201649159431177a56e2",
  "parent_commit": "2b04c209ee98174f29f1fc98f0dc3222d652a7bd",
  "status": "error",
  "benchmark_type": "serving",
  "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
  "duration_s": 154.07087635993958,
  "error": "No metrics in output",
  "ttft_mean": null,
  "ttft_median": null,
  "ttft_p99": null,
  "tpot_mean": null,
  "tpot_median": null,
  "tpot_p99": null,
  "itl_mean": null,
  "itl_median": null,
  "itl_p99": null,
  "throughput_req_s": null,
  "throughput_tok_s": null,
  "timestamp": "2026-01-09 03:03:45",
  "raw_output": "\nINFO 01-08 19:02:48 [__init__.py:207] Automatically detected platform cuda.\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT]\n                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf}]\n                            [--dataset-path DATASET_PATH]\n                            [--max-concurrency MAX_CONCURRENCY] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--logprobs LOGPROBS]\n                            [--request-rate REQUEST_RATE]\n                            [--burstiness BURSTINESS] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm] [--profile]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME] [--ignore-eos]\n                            [--percentile-metrics PERCENTILE_METRICS]\n                            [--metric-percentiles METRIC_PERCENTILES]\n                            [--goodput GOODPUT [GOODPUT ...]]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--random-prefix-len RANDOM_PREFIX_LEN]\n                            [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]\n                            [--hf-output-len HF_OUTPUT_LEN]\n                            [--tokenizer-mode {auto,slow,mistral,custom}]\n                            [--served-model-name SERVED_MODEL_NAME]\n                            [--lora-modules LORA_MODULES [LORA_MODULES ...]]\nbenchmark_serving.py: error: the following arguments are required: --model\n"
}