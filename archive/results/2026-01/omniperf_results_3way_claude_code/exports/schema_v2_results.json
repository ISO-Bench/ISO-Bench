[
  {
    "commit_hash": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
    "commit_short": "015069b0",
    "commit_subject": "[Misc] Optimize the Qwen3_ReasoningParser extract_",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --request-rate 1",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "fbefc8a78d22b20eac042c586805c7dcbfc66b1c",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.361701965332031e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen3-7B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "commit_short": "0d243f2a",
    "commit_subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "88f6ba3281f727d5641d362476ae68562b666081",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2192.627078294754,
    "error": "BASELINE server failed to start. Logs:               ^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/models/mixtral.py\", line 87, in __init__\n[rank0]:     self.experts = FusedMoE(num_experts=num_experts,\n[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 309, in __init__\n[rank0]:     self.quant_method.create_weights(layer=self, **moe_quant_params)\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 83, in create_weights\n[rank0]:     w2_weight = torch.nn.Parameter(torch.empty(\n[rank0]:                                    ^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.11/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 896.00 MiB. GPU 0 has a total capacity of 79.18 GiB of which 812.56 MiB is free. Process 1 has 78.38 GiB memory in use. Of the allocated memory 77.77 GiB is allocated by PyTorch, and 13.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W102 16:43:43.194580942 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.7.3.dev240+g88f6ba32",
    "human_version": null,
    "agent_version": null,
    "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
    "commit_short": "0ec82edd",
    "commit_subject": "[perf] Speed up align sum kernels (#21079)",
    "repo": "vllm",
    "perf_command": "vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100",
    "files_changed": [
      "benchmarks/kernels/benchmark_moe_align_block_size.py",
      "csrc/moe/moe_align_sum_kernels.cu",
      "vllm/model_executor/layers/fused_moe/moe_align_block_size.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21079",
    "models": [
      "Qwen/Qwen3-30B-A3B",
      "Qwen/Qwen3-30B-A3B-FP8",
      "ibm-granite/granite-4.0-tiny-preview"
    ],
    "parent_commit": "005ae9be6c22dfa2c2c5580b50b41e67faee4a87",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "c_cuda",
    "duration_s": 3963.4564945697784,
    "error": "BASELINE server failed to start. Logs: apturing CUDA graph shapes:  52%|\u2588\u2588\u2588\u2588\u2588\u258f    | 35/67 [00:42<00:37,  1.17s/it]\nCapturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258e    | 36/67 [00:43<00:38,  1.23s/it]\nCapturing CUDA graph shapes:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 37/67 [00:44<00:36,  1.21s/it]\nCapturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 38/67 [00:45<00:34,  1.18s/it]\nCapturing CUDA graph shapes:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 39/67 [00:47<00:33,  1.20s/it]\nCapturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2589    | 40/67 [00:48<00:31,  1.17s/it]\nCapturing CUDA graph shapes:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 41/67 [00:49<00:30,  1.16s/it]\nCapturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 42/67 [00:50<00:29,  1.16s/it]\nCapturing CUDA graph shapes:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 43/67 [00:51<00:27,  1.15s/it]\nCapturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 44/67 [00:52<00:26,  1.14s/it]\nCapturing CUDA graph shapes:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 45/67 [00:53<00:24,  1.13s/it]\nCapturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 46/67 [00:55<00:23,  1.13s/it]\nCapturing CUDA graph shapes:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 47/67 [00:56<00:22,  1.14s/it]\nCapturing CUDA graph shapes:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 48/67 [00:57<00:21,  1.14s/it]\nCapturing CUDA graph shapes:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 49/67 [00:59<00:23,  1.32s/it]\nCapturing CUDA graph shapes:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 50/67 [01:00<00:23,  1.36s/it]\nCapturing CUDA graph shapes:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 51/67 [01:01<00:21,  1.35s/it]\nCapturing CUDA graph shapes:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 52/67 [01:03<00:20,  1.34s/it]\nCapturing CUDA graph shapes:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 53/67 [01:04<00:17,  1.27s/it]\nCapturing CUDA graph shapes:  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 54/67 [01:05<00:15,  1.23s/it]\nCapturing CUDA graph shapes:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 55/67 [01:06<00:14,  1.21s/it]\nCapturing CUDA graph shapes:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 56/67 [01:07<00:13,  1.20s/it]\nCapturing CUDA graph shapes:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 57/67 [01:08<00:11,  1.18s/it]\nCapturing CUDA graph shapes:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 58/67 [01:10<00:10,  1.16s/it]\nCapturing CUDA graph shapes:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 59/67 [01:11<00:09,  1.17s/it]\nCapturing CUDA graph shapes:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 60/67 [01:12<00:08,  1.17s/it]",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.0rc2.dev19+g005ae9be6",
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen3-30B-A3B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 0ec82edda59aaf5cf3b07aadf4ecce1aa1131add\nMessage: [perf] Speed up align sum kernels (#21079)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, targeting the main Python function\n        module_path = \"vllm.model_executor.layers.fused_moe.moe_align_block_size\"\n        symbol_name = \"moe_align_block_size\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # MoE alignment workload parameters\n    # Based on typical MoE configurations\n    num_tokens = 4096\n    num_experts = 64\n    topk = 2  # Top-k experts per token\n    block_size = 128\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # Generate realistic topk_ids tensor\n    # Each token selects topk experts from num_experts\n    topk_ids = torch.randint(\n        0, num_experts, \n        (num_tokens * topk,), \n        dtype=torch.int32, \n        device=device\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": torch.int32,\n        \"hw_info\": hw_info,\n        \"topk_ids\": topk_ids,\n        \"num_experts\": num_experts,\n        \"block_size\": block_size,\n        \"topk\": topk,\n        \"num_tokens\": num_tokens\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the moe_align_block_size function\n    with torch.no_grad():\n        sorted_ids, expert_ids, num_tokens_post_pad = target(\n            data[\"topk_ids\"],\n            data[\"num_experts\"],\n            data[\"block_size\"],\n            data[\"topk\"]\n        )\n    \n    return {\n        \"sorted_ids\": sorted_ids,\n        \"expert_ids\": expert_ids,\n        \"num_tokens_post_pad\": num_tokens_post_pad\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\n        \"type\": \"dict\",\n        \"data\": {\n            \"sorted_ids\": result[\"sorted_ids\"].cpu(),\n            \"expert_ids\": result[\"expert_ids\"].cpu(),\n            \"num_tokens_post_pad\": result[\"num_tokens_post_pad\"].cpu()\n        }\n    }, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Check sorted_ids tensor\n    current_sorted = current_result[\"sorted_ids\"]\n    ref_sorted = reference_result[\"sorted_ids\"]\n    assert current_sorted.shape == ref_sorted.shape, f\"sorted_ids shape mismatch: {current_sorted.shape} vs {ref_sorted.shape}\"\n    assert current_sorted.dtype == ref_sorted.dtype, f\"sorted_ids dtype mismatch: {current_sorted.dtype} vs {ref_sorted.dtype}\"\n    \n    # For integer tensors, require exact match\n    torch.testing.assert_close(\n        current_sorted.cpu(),\n        ref_sorted.cpu(),\n        rtol=0, atol=0\n    )\n    \n    # Check expert_ids tensor\n    current_expert = current_result[\"expert_ids\"]\n    ref_expert = reference_result[\"expert_ids\"]\n    assert current_expert.shape == ref_expert.shape, f\"expert_ids shape mismatch: {current_expert.shape} vs {ref_expert.shape}\"\n    assert current_expert.dtype == ref_expert.dtype, f\"expert_ids dtype mismatch: {current_expert.dtype} vs {ref_expert.dtype}\"\n    \n    torch.testing.assert_close(\n        current_expert.cpu(),\n        ref_expert.cpu(),\n        rtol=0, atol=0\n    )\n    \n    # Check num_tokens_post_pad\n    current_num = current_result[\"num_tokens_post_pad\"]\n    ref_num = reference_result[\"num_tokens_post_pad\"]\n    assert current_num.shape == ref_num.shape, f\"num_tokens_post_pad shape mismatch: {current_num.shape} vs {ref_num.shape}\"\n    assert current_num.dtype == ref_num.dtype, f\"num_tokens_post_pad dtype mismatch: {current_num.dtype} vs {ref_num.dtype}\"\n    \n    torch.testing.assert_close(\n        current_num.cpu(),\n        ref_num.cpu(),\n        rtol=0, atol=0\n    )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        # This optimization requires CUDA\n        error_data = {\n            \"target_resolved\": True,\n            \"error\": \"moe_align_block_size requires CUDA device\",\n            \"error_code\": 2,\n            \"error_name\": \"CAPABILITY_UNSUPPORTED\"\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"0ec82edda59aaf5cf3b07aadf4ecce1aa1131add\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.int32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
    "commit_short": "21d93c14",
    "commit_subject": "Optimize Mixtral with expert parallelism (#2090)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8",
    "files_changed": [
      "Dockerfile",
      "README.md",
      "docs/source/models/supported_models.rst",
      "vllm/config.py",
      "vllm/model_executor/models/__init__.py",
      "vllm/model_executor/models/mixtral.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/2090",
    "models": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1"
    ],
    "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed",
    "status": "error",
    "gpu_config": "H100:8",
    "benchmark_mode": "standalone",
    "patch_type": null,
    "duration_s": 67.88227987289429,
    "error": "Baseline wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local\n  \u00d7 Failed to download `vllm @\n  \u2502 https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u251c\u2500\u25b6 Failed to fetch:\n  \u2502   `https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u2570\u2500\u25b6 HTTP status client error (404 Not Found) for url\n      (https://vllm-wheels.s3.us-west-2.amazonaws.com/f1c8520146031a650404a6ab120ee11e91c10bed/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "mistralai/Mixtral-8x7B-v0.1",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 21d93c140d0a97af5f0c59e660cf04bd417fd424\nMessage: Optimize Mixtral with expert parallelism (#2090)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target the new MixtralMoE\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.models.mixtral\"\n        symbol_name = \"MixtralMoE\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Mixtral-8x7B configuration\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Mixtral-8x7B parameters\n    batch_size = 4\n    seq_len = 512  # Reduced for memory constraints\n    hidden_size = 4096\n    intermediate_size = 14336\n    num_experts = 8\n    top_k = 2\n    \n    # Create input tensors\n    hidden_states = torch.randn(batch_size, seq_len, hidden_size, \n                                device=device, dtype=dtype)\n    \n    # Create a mock config object\n    class MockConfig:\n        def __init__(self):\n            self.hidden_size = hidden_size\n            self.intermediate_size = intermediate_size\n            self.num_local_experts = num_experts\n            self.num_experts_per_tok = top_k\n            self.rms_norm_eps = 1e-5\n    \n    config = MockConfig()\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_states\": hidden_states,\n        \"config\": config,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"hidden_size\": hidden_size,\n        \"intermediate_size\": intermediate_size,\n        \"num_experts\": num_experts,\n        \"top_k\": top_k\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Initialize the MoE module\n    moe = target(data[\"config\"], linear_method=None)\n    moe = moe.to(data[\"device\"])\n    moe.eval()\n    \n    with torch.no_grad():\n        # Forward pass through MoE\n        output = moe(data[\"hidden_states\"])\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    if isinstance(data, dict) and \"data\" in data:\n        result = data[\"data\"]\n        if isinstance(result, torch.Tensor) and torch.cuda.is_available():\n            result = result.cuda()\n        return result\n    return data\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, \\\n            f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, \\\n            f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-2, 1e-3  # Relaxed for MoE due to routing differences\n        else:\n            rtol, atol = 1e-4, 1e-5\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Create experiment function\n    def run_experiment():\n        return experiment(data)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(run_experiment, warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(run_experiment, warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"21d93c140d0a97af5f0c59e660cf04bd417fd424\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "commit_short": "22d33bac",
    "commit_subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "b0e96aaebbfbe8e70478e4192a5a13864ffdefa6",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3363.249935865402,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:05:28 [__init__.py:256] Automatically detected platform cuda.\n0.8.2.dev3+gb0e96aae",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:09:08 [__init__.py:256] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b",
    "commit_short": "22dd9c27",
    "commit_subject": "[Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel (#20308)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/attention/ops/triton_unified_attention.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/20308",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "parent_commit": "a6d795d593046abd490b16349bcd9b40feedd334",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 5317.946018695831,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.9.2rc2.dev58+ga6d795d59",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:27:45 [__init__.py:253] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_latency.py\", line 17, in <module>\n    from vllm import LLM, SamplingParams\n  File \"<frozen importlib._bootstrap>\", line 1229, in _handle_fromlist\n  File \"/usr/local/lib/python3.11/site-packages/vllm/__init__.py\", line 64, in __getattr__\n    module = import_module(module_name, __package__)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 20, in <module>\n    from vllm.config import (CompilationConfig, ModelDType, TokenizerMode,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 37, in <module>\n    from vllm.transformers_utils.config import (\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 33, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py\", line 26, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py\", line 76, in <module>\n    AutoConfig.register(\"aimv2\", AIMv2Config)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1081, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\nValueError: 'aimv2' is already used by a Transformers config, pick another name.\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 22dd9c2730dc1124b9d0ac15fff223d0b8d9020b\nMessage: [Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel (#20308)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - targeting unified attention\n    if not module_path:\n        # Try the torch.ops.vllm path first (custom op)\n        try:\n            target = torch.ops.vllm.unified_attention\n            return target, \"torch.ops.vllm.unified_attention\"\n        except (AttributeError, RuntimeError):\n            pass\n        \n        # Fallback to Python module\n        module_path = \"vllm.attention.ops.triton_unified_attention\"\n        symbol_name = \"triton_unified_attention_2d\"\n    \n    # Import with error handling\n    try:\n        if module_path.startswith(\"torch.ops\"):\n            # Handle torch ops specially\n            parts = module_path.split(\".\")\n            target = torch.ops\n            for part in parts[2:]:  # Skip \"torch.ops\"\n                target = getattr(target, part)\n            if symbol_name:\n                target = getattr(target, symbol_name)\n            fq_name = f\"{module_path}.{symbol_name}\" if symbol_name else module_path\n        else:\n            # Standard module import\n            module = importlib.import_module(module_path)\n            target = module\n            if symbol_name:\n                for attr in symbol_name.split(\".\"):\n                    target = getattr(target, attr)\n            fq_name = f\"{module_path}.{symbol_name}\" if symbol_name else module_path\n        \n        return target, fq_name\n        \n    except (ImportError, AttributeError, RuntimeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Prefill attention workload - matches the optimization target\n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Standard prefill configuration for 7B model\n    batch_size = 4\n    seq_len = 2048  # Long sequence to trigger the optimization benefit\n    num_heads = 32\n    head_dim = 128\n    num_kv_heads = 32  # Standard MHA, not GQA\n    \n    # Adjust for hardware constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = max(1, batch_size // 2)\n        seq_len = min(1024, seq_len)\n    \n    # Create attention inputs with proper layout for unified attention\n    # Shape: [batch, seq_len, num_heads, head_dim]\n    query = torch.randn(batch_size, seq_len, num_heads, head_dim, \n                        device=device, dtype=dtype)\n    key = torch.randn(batch_size, seq_len, num_kv_heads, head_dim,\n                     device=device, dtype=dtype)\n    value = torch.randn(batch_size, seq_len, num_kv_heads, head_dim,\n                       device=device, dtype=dtype)\n    \n    # Attention scale\n    scale = 1.0 / math.sqrt(head_dim)\n    \n    # Context lengths for prefill (all tokens are in context)\n    context_lens = torch.full((batch_size,), seq_len, device=device, dtype=torch.int32)\n    \n    # For unified attention, we need additional parameters\n    query_lens = context_lens.clone()  # In prefill, query_len = context_len\n    max_query_len = seq_len\n    max_context_len = seq_len\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"query\": query,\n        \"key\": key,\n        \"value\": value,\n        \"scale\": scale,\n        \"context_lens\": context_lens,\n        \"query_lens\": query_lens,\n        \"max_query_len\": max_query_len,\n        \"max_context_len\": max_context_len,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_heads\": num_heads,\n        \"head_dim\": head_dim,\n        \"num_kv_heads\": num_kv_heads,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Try to call the unified attention operator\n    with torch.no_grad():\n        if \"torch.ops\" in fq_name:\n            # Using torch.ops.vllm.unified_attention custom op\n            try:\n                result = target(\n                    data[\"query\"],\n                    data[\"key\"],\n                    data[\"value\"],\n                    data[\"context_lens\"],\n                    data[\"query_lens\"],\n                    data[\"max_query_len\"],\n                    data[\"max_context_len\"],\n                    data[\"scale\"]\n                )\n            except (RuntimeError, TypeError) as e:\n                # Fallback: Try simplified signature\n                result = target(\n                    data[\"query\"],\n                    data[\"key\"],\n                    data[\"value\"],\n                    data[\"scale\"]\n                )\n        else:\n            # Direct Python function call (triton kernel wrapper)\n            # The triton kernel has a different interface\n            # Reshape tensors for 2D attention kernel\n            batch_seq = data[\"batch_size\"] * data[\"seq_len\"]\n            q = data[\"query\"].reshape(batch_seq, data[\"num_heads\"], data[\"head_dim\"])\n            k = data[\"key\"].reshape(batch_seq, data[\"num_kv_heads\"], data[\"head_dim\"])\n            v = data[\"value\"].reshape(batch_seq, data[\"num_kv_heads\"], data[\"head_dim\"])\n            \n            # Create output tensor\n            output = torch.empty_like(q)\n            \n            # Call triton kernel with appropriate parameters\n            result = target(\n                output,\n                q,\n                k,\n                v,\n                data[\"scale\"],\n                data[\"context_lens\"],\n                data[\"seq_len\"],\n                data[\"seq_len\"]  # max_seq_len\n            )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        # Move to CPU for comparison\n        current_cpu = current_result.cpu()\n        reference_cpu = reference_result.cpu()\n        \n        # Handle NaN and Inf values\n        if torch.isnan(current_cpu).any() or torch.isnan(reference_cpu).any():\n            assert torch.isnan(current_cpu).equal(torch.isnan(reference_cpu)), \"NaN mismatch\"\n            mask = ~torch.isnan(current_cpu)\n            torch.testing.assert_close(\n                current_cpu[mask],\n                reference_cpu[mask],\n                rtol=rtol, atol=atol\n            )\n        else:\n            torch.testing.assert_close(\n                current_cpu,\n                reference_cpu,\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            end = time.perf_counter()\n            times.append((end - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95)] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"22dd9c2730dc1124b9d0ac15fff223d0b8d9020b\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "commit_short": "25ebed2f",
    "commit_subject": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/v1/worker/gpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11214",
    "models": [
      "N/A"
    ],
    "parent_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310",
    "status": "version_bug",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 27.686750173568726,
    "error": "vLLM 0.6.4.post2.dev375+gd263bd9d has known port binding bug (issue #8791) - serving benchmarks not supported",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev375+gd263bd9d",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 25ebed2f8ca6d747d63f2be9ede023c561851ac8\nMessage: [V1][Minor] Cache np arange to reduce input preparation overhead (#11214)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimization is in GPUModelRunner._prepare_inputs\n        module_path = \"vllm.v1.worker.gpu_model_runner\"\n        symbol_name = \"GPUModelRunner\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # The optimization caches np.arange to avoid repeated allocations\n    # We need to simulate the _prepare_inputs method's workload\n    \n    device = torch.device(\"cpu\")  # This optimization is CPU-side\n    dtype = torch.float32\n    \n    # Realistic vLLM batch parameters\n    max_num_reqs = 256  # Maximum number of requests\n    max_model_len = 4096  # Maximum model length\n    max_num_tokens = 8192  # Maximum number of batched tokens\n    block_size = 16  # KV cache block size\n    \n    # Simulate a typical batch with varying sequence lengths\n    num_reqs = 32  # Active requests in batch\n    \n    # Generate realistic scheduled tokens per request (mix of prefill and decode)\n    np.random.seed(42)\n    num_scheduled_tokens = []\n    for i in range(num_reqs):\n        if i < 4:  # Some prefill requests\n            tokens = np.random.randint(128, 512)\n        else:  # Mostly decode requests\n            tokens = 1\n        num_scheduled_tokens.append(tokens)\n    num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n    \n    total_num_scheduled_tokens = num_scheduled_tokens.sum()\n    \n    # Pre-allocate arrays like in the actual implementation\n    positions_np = np.zeros(max_num_tokens, dtype=np.int64)\n    num_computed_tokens_cpu = np.random.randint(0, 1024, size=max_num_reqs, dtype=np.int32)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"max_num_reqs\": max_num_reqs,\n        \"max_model_len\": max_model_len,\n        \"max_num_tokens\": max_num_tokens,\n        \"num_reqs\": num_reqs,\n        \"num_scheduled_tokens\": num_scheduled_tokens,\n        \"total_num_scheduled_tokens\": int(total_num_scheduled_tokens),\n        \"positions_np\": positions_np,\n        \"num_computed_tokens_cpu\": num_computed_tokens_cpu,\n        \"block_size\": block_size,\n        # Cache for optimized version\n        \"arange_np\": np.arange(max(max_num_reqs, max_model_len), dtype=np.int32)\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Extract parameters\n    num_reqs = data[\"num_reqs\"]\n    num_scheduled_tokens = data[\"num_scheduled_tokens\"]\n    total_num_scheduled_tokens = data[\"total_num_scheduled_tokens\"]\n    positions_np = data[\"positions_np\"][:total_num_scheduled_tokens]\n    num_computed_tokens_cpu = data[\"num_computed_tokens_cpu\"]\n    \n    # Check if we have the cached arange (optimized version)\n    if \"arange_np\" in data and os.getenv(\"IMPL_TAG\", \"child\") == \"child\":\n        # Optimized version with cached arange\n        arange_np = data[\"arange_np\"]\n        \n        # Get request indices\n        req_indices = np.repeat(arange_np[:num_reqs], num_scheduled_tokens)\n        \n        # Get batched arange using cached array\n        arange = np.concatenate([arange_np[:n] for n in num_scheduled_tokens])\n    else:\n        # Original version - create arange every time\n        # Get request indices\n        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n        \n        # Get batched arange - original implementation\n        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n    \n    # Get positions (common to both versions)\n    np.add(num_computed_tokens_cpu[req_indices], arange, out=positions_np)\n    \n    # Return the computed arrays for equivalence checking\n    result = {\n        \"req_indices\": req_indices.copy(),\n        \"arange\": arange.copy(),\n        \"positions\": positions_np.copy()\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert numpy arrays to torch tensors for consistent storage\n    torch_result = {}\n    for key, value in result.items():\n        if isinstance(value, np.ndarray):\n            torch_result[key] = torch.from_numpy(value)\n        else:\n            torch_result[key] = value\n    torch.save({\"type\": \"dict\", \"data\": torch_result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    # Convert back to numpy for comparison\n    result = {}\n    for key, value in data[\"data\"].items():\n        if isinstance(value, torch.Tensor):\n            result[key] = value.numpy()\n        else:\n            result[key] = value\n    return result\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, dict) and isinstance(reference_result, dict)\n    assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n    \n    for key in current_result:\n        current = current_result[key]\n        reference = reference_result[key]\n        \n        if isinstance(current, np.ndarray):\n            assert current.shape == reference.shape, f\"{key}: shape mismatch\"\n            assert current.dtype == reference.dtype, f\"{key}: dtype mismatch\"\n            np.testing.assert_array_equal(current, reference, err_msg=f\"{key} arrays not equal\")\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU optimization\n    warmup = 5\n    iters = 100  # More iterations for CPU timing\n    \n    # Time the operation\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"25ebed2f8ca6d747d63f2be9ede023c561851ac8\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",\n        \"dtype\": \"int32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": \"exact\",\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "296f927f2493908984707354e3cc5d7b2e41650b",
    "commit_short": "296f927f",
    "commit_subject": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fix",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "0032903a5bb7c7c655f52f4efdfcc221947e9ca8",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.910064697265625e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "ibm-ai-platform/Bamba-9B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "commit_short": "299ebb62",
    "commit_subject": "[Core] Speed up decode by remove synchronizing ope",
    "repo": "vllm-project/vllm",
    "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 5890.607915878296,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:21:32 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev117+gf728ab8e3",
    "human_version": "INFO 01-02 17:27:33 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev118+g299ebb62b",
    "agent_version": null,
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 25.71,
    "baseline_ttft_median": 24.33,
    "baseline_ttft_p99": 73.69,
    "baseline_tpot_mean": 4.76,
    "baseline_tpot_median": 4.66,
    "baseline_tpot_p99": 5.69,
    "baseline_itl_mean": 4.8,
    "baseline_itl_median": 4.59,
    "baseline_itl_p99": 11.13,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 22.59,
    "human_ttft_median": 21.72,
    "human_ttft_p99": 55.97,
    "human_tpot_mean": 4.2,
    "human_tpot_median": 4.18,
    "human_tpot_p99": 4.55,
    "human_itl_mean": 4.2,
    "human_itl_median": 4.15,
    "human_itl_p99": 5.06,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 22.93,
    "agent_ttft_median": 21.7,
    "agent_ttft_p99": 53.88,
    "agent_tpot_mean": 4.3,
    "agent_tpot_median": 4.3,
    "agent_tpot_p99": 5.02,
    "agent_itl_mean": 4.31,
    "agent_itl_median": 4.21,
    "agent_itl_p99": 6.06,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 12.135355892648779,
    "human_improvement_tpot_mean": 11.764705882352935,
    "human_improvement_itl_mean": 12.499999999999993,
    "agent_improvement_ttft_mean": 10.812913263321668,
    "agent_improvement_tpot_mean": 9.663865546218489,
    "agent_improvement_itl_mean": 10.20833333333334,
    "agent_vs_human_ttft_mean": -1.505090748118636,
    "agent_vs_human_tpot_mean": -2.3809523809523725,
    "agent_vs_human_itl_mean": -2.6190476190476053,
    "human_improvement_ttft_median": 10.727496917385942,
    "human_improvement_ttft_p99": 24.046682046410638,
    "agent_improvement_ttft_median": 10.809699958898475,
    "agent_improvement_ttft_p99": 26.882887773103537,
    "agent_vs_human_ttft_median": 0.09208103130754869,
    "agent_vs_human_ttft_p99": 3.7341432910487695,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:25:16 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae0e3d04180>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.15    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.10    \nTotal Token throughput (tok/s):          620.48    \n---------------Time to First Token----------------\nMean TTFT (ms):                          25.71     \nMedian TTFT (ms):                        24.33     \nP99 TTFT (ms):                           73.69     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.76      \nMedian TPOT (ms):                        4.66      \nP99 TPOT (ms):                           5.69      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.80      \nMedian ITL (ms):                         4.59      \nP99 ITL (ms):                            11.13     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:59,  1.66it/s]\n  2%|\u258f         | 2/100 [00:01<01:12,  1.35it/s]\n  3%|\u258e         | 3/100 [00:01<00:52,  1.85it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.60it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.84it/s]\n  7%|\u258b         | 7/100 [00:06<01:45,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.09it/s]\n  9%|\u2589         | 9/100 [00:07<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:19,  1.12it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:03,  1.38it/s]",
    "human_raw": "INFO 01-02 17:30:42 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b3b10f10220>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.09    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.17    \nTotal Token throughput (tok/s):          620.84    \n---------------Time to First Token----------------\nMean TTFT (ms):                          22.59     \nMedian TTFT (ms):                        21.72     \nP99 TTFT (ms):                           55.97     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.20      \nMedian TPOT (ms):                        4.18      \nP99 TPOT (ms):                           4.55      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.20      \nMedian ITL (ms):                         4.15      \nP99 ITL (ms):                            5.06      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:52,  1.89it/s]\n  2%|\u258f         | 2/100 [00:01<01:10,  1.39it/s]\n  3%|\u258e         | 3/100 [00:01<00:51,  1.87it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.61it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.86it/s]\n  7%|\u258b         | 7/100 [00:06<01:44,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.10it/s]\n  9%|\u2589         | 9/100 [00:07<01:28,  1.03it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.37it/s]\n 11%|\u2588         | 11/100 [00:08<01:18,  1.13it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:04,  1.37it/s]",
    "agent_raw": "INFO 01-02 17:35:21 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b1dffa140e0>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='Qwen/Qwen2.5-1.5B-Instruct', tokenizer='Qwen/Qwen2.5-1.5B-Instruct', best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.06    \nTotal input tokens:                      51200     \nTotal generated tokens:                  12800     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         124.20    \nTotal Token throughput (tok/s):          621.00    \n---------------Time to First Token----------------\nMean TTFT (ms):                          22.93     \nMedian TTFT (ms):                        21.70     \nP99 TTFT (ms):                           53.88     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          4.30      \nMedian TPOT (ms):                        4.30      \nP99 TPOT (ms):                           5.02      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           4.31      \nMedian ITL (ms):                         4.21      \nP99 ITL (ms):                            6.06      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:57,  1.71it/s]\n  2%|\u258f         | 2/100 [00:01<01:12,  1.36it/s]\n  3%|\u258e         | 3/100 [00:01<00:51,  1.87it/s]\n  4%|\u258d         | 4/100 [00:02<00:59,  1.60it/s]\n  6%|\u258c         | 6/100 [00:03<00:51,  1.84it/s]\n  7%|\u258b         | 7/100 [00:06<01:45,  1.13s/it]\n  8%|\u258a         | 8/100 [00:06<01:24,  1.09it/s]\n  9%|\u2589         | 9/100 [00:07<01:28,  1.03it/s]\n 10%|\u2588         | 10/100 [00:07<01:05,  1.38it/s]\n 11%|\u2588         | 11/100 [00:08<01:19,  1.13it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:03,  1.38it/s]",
    "test_script": null
  },
  {
    "commit_hash": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
    "commit_short": "2a052011",
    "commit_subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --quantization fp8",
    "files_changed": [
      "tests/kernels/test_moe.py",
      "vllm/model_executor/models/mixtral.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4527",
    "models": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1"
    ],
    "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 1059.8280100822449,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 36fb68f94792",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3\nMessage: [Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the main optimization is in MixtralMoE\n        module_path = \"vllm.model_executor.models.mixtral\"\n        symbol_name = \"MixtralMoE\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Mixtral-8x7B configuration\n    num_experts = 8\n    top_k = 2\n    hidden_size = 4096\n    intermediate_size = 14336  # Per expert\n    \n    # Adjust for memory constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = 2\n        seq_len = 512\n    else:\n        batch_size = 4\n        seq_len = 1024\n    \n    # Create input hidden states\n    num_tokens = batch_size * seq_len\n    hidden_states = torch.randn(num_tokens, hidden_size, device=device, dtype=dtype)\n    \n    # Try to import Fp8Config for FP8 quantization\n    try:\n        from vllm.model_executor.layers.quantization.fp8 import Fp8Config\n        # Create FP8 config for testing\n        quant_config = Fp8Config(\n            is_checkpoint_fp8_serialized=False,\n            activation_scheme=\"dynamic\"\n        )\n    except ImportError:\n        quant_config = None\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_states\": hidden_states,\n        \"num_experts\": num_experts,\n        \"top_k\": top_k,\n        \"hidden_size\": hidden_size,\n        \"intermediate_size\": intermediate_size,\n        \"quant_config\": quant_config,\n        \"tp_size\": 1,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_tokens\": num_tokens\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    MixtralMoE, fq_name = resolve_target()\n    \n    # Create MoE layer instance\n    moe_layer = MixtralMoE(\n        num_experts=data[\"num_experts\"],\n        top_k=data[\"top_k\"],\n        hidden_size=data[\"hidden_size\"],\n        intermediate_size=data[\"intermediate_size\"],\n        params_dtype=data[\"dtype\"],\n        tp_size=data[\"tp_size\"],\n        quant_config=data[\"quant_config\"]\n    ).to(data[\"device\"])\n    \n    # Initialize weights with realistic values\n    with torch.no_grad():\n        # Gate weights\n        torch.nn.init.xavier_uniform_(moe_layer.gate.weight)\n        \n        # Expert weights - using new naming from the commit\n        if hasattr(moe_layer, 'w13_weight'):\n            # New implementation\n            torch.nn.init.xavier_uniform_(moe_layer.w13_weight)\n            torch.nn.init.xavier_uniform_(moe_layer.w2_weight)\n        else:\n            # Old implementation fallback\n            if hasattr(moe_layer, 'ws'):\n                torch.nn.init.xavier_uniform_(moe_layer.ws)\n                torch.nn.init.xavier_uniform_(moe_layer.w2s)\n        \n        # Process weights for FP8 if applicable\n        if hasattr(moe_layer, 'process_weights_after_loading'):\n            moe_layer.process_weights_after_loading()\n    \n    # Run forward pass\n    moe_layer.eval()\n    with torch.no_grad():\n        result = moe_layer.forward(data[\"hidden_states\"])\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        elif \"float8\" in str(current_result.dtype):\n            rtol, atol = 5e-2, 1e-2  # More tolerance for FP8\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "2deb029d115dadd012ce5ea70487a207cb025493",
    "commit_short": "2deb029d",
    "commit_subject": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)",
    "repo": "vllm",
    "perf_command": "python3 benchmarks/benchmark_prefix_caching.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --output-len 200 --enable-prefix-caching [--use-v2-block-manager]",
    "files_changed": [
      "tests/core/block/test_prefix_caching_block.py",
      "vllm/core/block/prefix_caching_block.py",
      "vllm/core/block_manager_v2.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7822",
    "models": [
      "N/A"
    ],
    "parent_commit": null,
    "status": "baseline_failed",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 118.38352465629578,
    "error": null,
    "error_message": "Baseline benchmark produced no metrics",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "unknown",
    "agent_error": null,
    "patch_path": "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0011/model_patch.diff",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 2deb029d115dadd012ce5ea70487a207cb025493\nMessage: [Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, the main optimization is in PrefixCachingBlockAllocator\n        module_path = \"vllm.core.block.prefix_caching_block\"\n        symbol_name = \"PrefixCachingBlockAllocator\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # The optimization is about marking blocks as computed in batch\n    # We need to simulate a prefix caching scenario with multiple sequences\n    # sharing common prefixes\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Workload parameters for prefix caching\n    block_size = 16\n    num_blocks = 256  # Total blocks available\n    num_sequences = 8  # Number of sequences sharing prefixes\n    common_prefix_blocks = 4  # Number of blocks in common prefix\n    unique_blocks_per_seq = 2  # Additional unique blocks per sequence\n    \n    # Token IDs for common prefix and unique suffixes\n    common_token_ids = list(range(block_size * common_prefix_blocks))\n    unique_token_ids_per_seq = []\n    for i in range(num_sequences):\n        start_id = block_size * common_prefix_blocks + i * block_size * unique_blocks_per_seq\n        unique_ids = list(range(start_id, start_id + block_size * unique_blocks_per_seq))\n        unique_token_ids_per_seq.append(unique_ids)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"block_size\": block_size,\n        \"num_blocks\": num_blocks,\n        \"num_sequences\": num_sequences,\n        \"common_token_ids\": common_token_ids,\n        \"unique_token_ids_per_seq\": unique_token_ids_per_seq,\n        \"common_prefix_blocks\": common_prefix_blocks,\n        \"unique_blocks_per_seq\": unique_blocks_per_seq,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create a PrefixCachingBlockAllocator instance\n    allocator = target(\n        num_blocks=data[\"num_blocks\"],\n        block_size=data[\"block_size\"]\n    )\n    \n    # Simulate allocating blocks for multiple sequences with common prefixes\n    # This triggers the optimization path\n    allocated_blocks = []\n    block_ids_to_mark = []\n    \n    for seq_idx in range(data[\"num_sequences\"]):\n        # Allocate blocks for common prefix (should reuse after first sequence)\n        prev_block = None\n        seq_blocks = []\n        \n        # Common prefix blocks\n        for block_idx in range(data[\"common_prefix_blocks\"]):\n            start_idx = block_idx * data[\"block_size\"]\n            end_idx = start_idx + data[\"block_size\"]\n            token_ids = data[\"common_token_ids\"][start_idx:end_idx]\n            \n            block = allocator.allocate_immutable_block(\n                prev_block=prev_block,\n                token_ids=token_ids\n            )\n            seq_blocks.append(block)\n            prev_block = block\n            \n            # Track block IDs that would be marked as computed\n            if block.block_id is not None:\n                block_ids_to_mark.append(block.block_id)\n        \n        # Unique suffix blocks\n        for block_idx in range(data[\"unique_blocks_per_seq\"]):\n            start_idx = block_idx * data[\"block_size\"]\n            end_idx = start_idx + data[\"block_size\"]\n            token_ids = data[\"unique_token_ids_per_seq\"][seq_idx][start_idx:end_idx]\n            \n            block = allocator.allocate_immutable_block(\n                prev_block=prev_block,\n                token_ids=token_ids\n            )\n            seq_blocks.append(block)\n            prev_block = block\n            \n            if block.block_id is not None:\n                block_ids_to_mark.append(block.block_id)\n        \n        allocated_blocks.append(seq_blocks)\n    \n    # The key operation: mark blocks as computed\n    # This is where the optimization happens - batching the marking\n    allocator.mark_blocks_as_computed([])\n    \n    # Get computed block IDs for verification\n    computed_block_ids = []\n    for seq_blocks in allocated_blocks:\n        seq_block_ids = [b.block_id for b in seq_blocks if b.block_id is not None]\n        computed_ids = allocator.get_computed_block_ids(\n            [], seq_block_ids, skip_last_block_id=False\n        )\n        computed_block_ids.append(computed_ids)\n    \n    # Return result for equivalence checking\n    result = {\n        \"num_allocated_sequences\": len(allocated_blocks),\n        \"total_blocks_allocated\": sum(len(blocks) for blocks in allocated_blocks),\n        \"computed_block_ids\": computed_block_ids,\n        \"cache_hit_rate\": allocator.get_prefix_cache_hit_rate(),\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, dict) and isinstance(reference_result, dict)\n    \n    # Check that the same number of sequences were allocated\n    assert current_result[\"num_allocated_sequences\"] == reference_result[\"num_allocated_sequences\"]\n    assert current_result[\"total_blocks_allocated\"] == reference_result[\"total_blocks_allocated\"]\n    \n    # Check computed block IDs match\n    current_computed = current_result[\"computed_block_ids\"]\n    reference_computed = reference_result[\"computed_block_ids\"]\n    \n    assert len(current_computed) == len(reference_computed)\n    for curr_seq, ref_seq in zip(current_computed, reference_computed):\n        assert len(curr_seq) == len(ref_seq), f\"Computed blocks mismatch: {len(curr_seq)} vs {len(ref_seq)}\"\n        # Block IDs might differ but count should match\n        assert len(curr_seq) == len(ref_seq)\n    \n    # Cache hit rate should be similar (allowing for small differences)\n    rtol, atol = 1e-3, 1e-4\n    assert abs(current_result[\"cache_hit_rate\"] - reference_result[\"cache_hit_rate\"]) < atol\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # For this optimization, we're testing CPU-side block management\n    # so we use CPU timing\n    warmup = 5\n    iters = 20\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"2deb029d115dadd012ce5ea70487a207cb025493\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Block management is CPU-side\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "commit_short": "2f192835",
    "commit_subject": "[Core] latency optimization (#3890)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/core/block_manager_v1.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3890",
    "models": [
      "N/A"
    ],
    "parent_commit": "95baec828f3ee046074dace1d88202a920b7dc15",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 874.4110426902771,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 95baec828f3e",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 2f1928354903ae0c6edfe76cc90081eb513ead2c\nMessage: [Core] latency optimization (#3890)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target is _is_last_block_full\n    if not (module_path and symbol_name):\n        module_path = \"vllm.core.block_manager_v1\"\n        symbol_name = \"BlockSpaceManagerV1._is_last_block_full\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            if attr == \"BlockSpaceManagerV1\":\n                target = getattr(target, attr)\n            elif attr == \"_is_last_block_full\":\n                # Get the class first, then the method\n                target = target._is_last_block_full\n                break\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Mock Sequence Data\n# =======================\nclass MockSequenceData:\n    \"\"\"Mock SequenceData to test the optimization.\"\"\"\n    def __init__(self, token_ids: List[int]):\n        self._token_ids = token_ids\n    \n    def get_token_ids(self) -> List[int]:\n        \"\"\"Original slow method that creates a list.\"\"\"\n        return self._token_ids.copy()\n    \n    def get_len(self) -> int:\n        \"\"\"Optimized fast method that returns length directly.\"\"\"\n        return len(self._token_ids)\n\nclass MockSequence:\n    \"\"\"Mock Sequence object for testing.\"\"\"\n    def __init__(self, token_ids: List[int], block_size: int = 16):\n        self.data = MockSequenceData(token_ids)\n        self.block_size = block_size\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Create sequences with varying token lengths to test block boundary checks\n    # This optimization affects checking if the last block is full\n    block_size = 16\n    \n    # Generate test sequences of different lengths\n    test_sequences = []\n    sequence_lengths = [\n        15,   # Not full block\n        16,   # Exactly one full block\n        31,   # One full + partial\n        32,   # Exactly two full blocks\n        64,   # Multiple full blocks\n        127,  # Many blocks + partial\n        128,  # Many full blocks\n        256,  # Large sequence\n        512,  # Very large sequence\n        1024, # Extra large sequence\n        2048, # Huge sequence (typical prompt)\n    ]\n    \n    for seq_len in sequence_lengths:\n        # Create realistic token IDs (vocabulary size ~32000 for typical LLMs)\n        token_ids = [int(x) for x in np.random.randint(0, 32000, seq_len)]\n        test_sequences.append(MockSequence(token_ids, block_size))\n    \n    # Also create many small sequences for throughput testing\n    for _ in range(1000):\n        seq_len = np.random.randint(1, 256)\n        token_ids = [int(x) for x in np.random.randint(0, 32000, seq_len)]\n        test_sequences.append(MockSequence(token_ids, block_size))\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float32,  # Not relevant for this optimization\n        \"hw_info\": hw_info,\n        \"test_sequences\": test_sequences,\n        \"block_size\": block_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Import the actual BlockSpaceManagerV1 class\n    from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n    \n    # Create a minimal BlockSpaceManagerV1 instance\n    manager = BlockSpaceManagerV1(\n        block_size=data[\"block_size\"],\n        num_gpu_blocks=1024,\n        num_cpu_blocks=256,\n        watermark=0.01,\n        sliding_window=None,\n        enable_caching=False\n    )\n    \n    results = []\n    \n    # Test the _is_last_block_full method on all sequences\n    for seq in data[\"test_sequences\"]:\n        # The optimized method now calls get_len() instead of get_token_ids()\n        is_full = manager._is_last_block_full(seq)\n        results.append(is_full)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"list\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, list), f\"Expected list, got {type(current_result)}\"\n    assert isinstance(reference_result, list), f\"Expected list, got {type(reference_result)}\"\n    assert len(current_result) == len(reference_result), f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr == ref, f\"Mismatch at index {i}: {curr} vs {ref}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This optimization is CPU-bound (method call optimization)\n    warmup = 5\n    iters = 100\n    \n    # Time the experiment\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"2f1928354903ae0c6edfe76cc90081eb513ead2c\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU-bound optimization\n        \"dtype\": \"none\",  # Not relevant for this optimization\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "commit_short": "30172b49",
    "commit_subject": "[V1] Optimize handling of sampling metadata and re",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "a4d577b37944cbfa1bc62e4869667d1e2739d62a",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 1080.960917711258,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "dev",
    "human_version": "dev",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 1115.66,
    "baseline_ttft_median": 1124.73,
    "baseline_ttft_p99": 1854.79,
    "baseline_tpot_mean": 26.96,
    "baseline_tpot_median": 26.41,
    "baseline_tpot_p99": 60.69,
    "baseline_itl_mean": 25.94,
    "baseline_itl_median": 23.11,
    "baseline_itl_p99": 82.89,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 1103.5,
    "human_ttft_median": 1110.34,
    "human_ttft_p99": 1850.6,
    "human_tpot_mean": 27.01,
    "human_tpot_median": 26.55,
    "human_tpot_p99": 58.98,
    "human_itl_mean": 26.05,
    "human_itl_median": 23.14,
    "human_itl_p99": 87.94,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 1074.88,
    "agent_ttft_median": 1037.52,
    "agent_ttft_p99": 1812.42,
    "agent_tpot_mean": 27.06,
    "agent_tpot_median": 26.49,
    "agent_tpot_p99": 52.54,
    "agent_itl_mean": 26.08,
    "agent_itl_median": 23.27,
    "agent_itl_p99": 92.46,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 1.089937794668634,
    "human_improvement_tpot_mean": -0.18545994065282162,
    "human_improvement_itl_mean": -0.42405551272166314,
    "agent_improvement_ttft_mean": 3.6552354660021846,
    "agent_improvement_tpot_mean": -0.3709198813056301,
    "agent_improvement_itl_mean": -0.539707016191199,
    "agent_vs_human_ttft_mean": 2.593565926597181,
    "agent_vs_human_tpot_mean": -0.18511662347277733,
    "agent_vs_human_itl_mean": -0.11516314779269705,
    "human_improvement_ttft_median": 1.2794181714722734,
    "human_improvement_ttft_p99": 0.22590158454596235,
    "agent_improvement_ttft_median": 7.753860926622393,
    "agent_improvement_ttft_p99": 2.2843556413394452,
    "agent_vs_human_ttft_median": 6.558351495938177,
    "agent_vs_human_ttft_p99": 2.063114665513879,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.61      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.70     \nOutput token throughput (tok/s):         2629.24   \nTotal Token throughput (tok/s):          13739.93  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1115.66   \nMedian TTFT (ms):                        1124.73   \nP99 TTFT (ms):                           1854.79   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          26.96     \nMedian TPOT (ms):                        26.41     \nP99 TPOT (ms):                           60.69     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           25.94     \nMedian ITL (ms):                         23.11     \nP99 ITL (ms):                            82.89     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:25,  1.16it/s]\n  2%|\u258f         | 2/100 [00:01<00:58,  1.68it/s]\n  4%|\u258d         | 4/100 [00:01<00:40,  2.36it/s]\n  5%|\u258c         | 5/100 [00:03<01:02,  1.53it/s]\n  6%|\u258c         | 6/100 [00:03<00:50,  1.86it/s]\n  7%|\u258b         | 7/100 [00:04<00:53,  1.73it/s]\n  9%|\u2589         | 9/100 [00:04<00:30,  2.97it/s]\n 26%|\u2588\u2588\u258c       | 26/100 [00:04<00:04, 17.65it/s]",
    "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.61      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.71     \nOutput token throughput (tok/s):         2630.94   \nTotal Token throughput (tok/s):          13748.82  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1103.50   \nMedian TTFT (ms):                        1110.34   \nP99 TTFT (ms):                           1850.60   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.01     \nMedian TPOT (ms):                        26.55     \nP99 TPOT (ms):                           58.98     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.05     \nMedian ITL (ms):                         23.14     \nP99 ITL (ms):                            87.94     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:23,  1.18it/s]\n  2%|\u258f         | 2/100 [00:01<00:57,  1.70it/s]\n  3%|\u258e         | 3/100 [00:01<00:35,  2.74it/s]\n  4%|\u258d         | 4/100 [00:01<00:42,  2.27it/s]\n  5%|\u258c         | 5/100 [00:03<01:06,  1.43it/s]\n  6%|\u258c         | 6/100 [00:03<00:52,  1.80it/s]\n  7%|\u258b         | 7/100 [00:04<00:55,  1.67it/s]\n  9%|\u2589         | 9/100 [00:04<00:30,  2.95it/s]\n",
    "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.59      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.81     \nOutput token throughput (tok/s):         2641.91   \nTotal Token throughput (tok/s):          13806.16  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1074.88   \nMedian TTFT (ms):                        1037.52   \nP99 TTFT (ms):                           1812.42   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.06     \nMedian TPOT (ms):                        26.49     \nP99 TPOT (ms):                           52.54     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.08     \nMedian ITL (ms):                         23.27     \nP99 ITL (ms):                            92.46     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:\nNo module named 'vllm._version'\n  from .version import __version__, __version_tuple__  # isort:skip\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:24,  1.17it/s]\n  2%|\u258f         | 2/100 [00:01<00:57,  1.69it/s]\n  4%|\u258d         | 4/100 [00:01<00:38,  2.46it/s]\n  5%|\u258c         | 5/100 [00:03<01:01,  1.54it/s]\n  6%|\u258c         | 6/100 [00:03<00:49,  1.90it/s]\n  7%|\u258b         | 7/100 [00:04<00:54,  1.70it/s]\n  9%|\u2589         | 9/100 [00:04<00:31,  2.93it/s]\n 28%|\u2588\u2588\u258a       | 28/100 [00:04<00:03, 19.20it/s]",
    "test_script": null
  },
  {
    "commit_hash": "3092375e274e9e003961e600e10a6192d33ceaa0",
    "commit_short": "3092375e",
    "commit_subject": "[V1][Performance] Implement custom serializaton fo",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "3cd91dc9555e6f10e55f23d37782c65b0366f7cf",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2721.2717039585114,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 16:40:01 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev42+g3cd91dc95",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:44:16 [__init__.py:239] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "310aca88c984983189a57f1b72e3b1dde89fb92f",
    "commit_short": "310aca88",
    "commit_subject": "[perf]fix current stream (#11870)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
    "files_changed": [
      "vllm/distributed/device_communicators/pynccl.py",
      "vllm/distributed/parallel_state.py",
      "vllm/utils.py",
      "vllm/worker/multi_step_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11870",
    "models": [
      "N/A"
    ],
    "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834",
    "status": "success",
    "gpu_config": "H100:4",
    "benchmark_mode": "standalone",
    "patch_type": null,
    "duration_s": 878.7205080986023,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.6.post2.dev145+ga732900e",
    "human_version": "0.6.6.post2.dev146+g310aca88",
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-70B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 4261.011293366673,
    "baseline_throughput": 51.1,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 4311.028618633319,
    "human_throughput": 102.1,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 4245.959685633333,
    "agent_throughput": 102.3,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -1.1738369561355046,
    "human_improvement_throughput": 99.80430528375732,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": 0.3532402684961676,
    "agent_improvement_throughput": 100.19569471624266,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": 1.5093598014808367,
    "agent_vs_human_throughput": 0.19588638589618299,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None)\nINFO 01-01 06:31:23 __init__.py:179] Automatically detected platform cuda.\nINFO 01-01 06:31:33 config.py:516] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-",
    "human_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None)\nINFO 01-01 06:36:18 __init__.py:179] Automatically detected platform cuda.\nINFO 01-01 06:36:27 config.py:516] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-",
    "agent_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None)\nINFO 01-01 06:40:47 __init__.py:179] Automatically detected platform cuda.\nINFO 01-01 06:40:58 config.py:516] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 310aca88c984983189a57f1b72e3b1dde89fb92f\nMessage: [perf]fix current stream\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for stream access optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    if hw_info[\"device\"] != \"cuda\":\n        print(json.dumps({\n            \"target_resolved\": False,\n            \"error\": \"CUDA device required for stream optimization test\"\n        }))\n        sys.exit(1)\n    \n    device = torch.device(\"cuda\")\n    dtype = torch.float16\n    \n    # Create a workload that simulates multiple NCCL operations\n    # that would benefit from cached stream access\n    batch_size = 64\n    hidden_size = 4096\n    num_layers = 32\n    \n    # Simulate tensor sizes used in distributed communication\n    tensors = []\n    for _ in range(num_layers):\n        # Typical tensor sizes in model parallel scenarios\n        tensor = torch.randn(batch_size, hidden_size, device=device, dtype=dtype)\n        tensors.append(tensor)\n    \n    # Create multiple streams to simulate multi-stream scenarios\n    streams = [torch.cuda.Stream() for _ in range(4)]\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"tensors\": tensors,\n        \"streams\": streams,\n        \"num_iterations\": 1000,  # Number of stream accesses to test\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized stream access pattern.\"\"\"\n    \n    # Try to import the optimized current_stream function\n    try:\n        from vllm.distributed.device_communicators.pynccl import current_stream\n        use_optimized = True\n    except (ImportError, AttributeError):\n        # Fallback to standard torch.cuda.current_stream\n        current_stream = torch.cuda.current_stream\n        use_optimized = False\n    \n    tensors = data[\"tensors\"]\n    streams = data[\"streams\"]\n    num_iterations = data[\"num_iterations\"]\n    \n    # Result tracking\n    stream_accesses = []\n    \n    # Simulate a pattern similar to PyNcclCommunicator operations\n    # where current_stream is accessed frequently\n    for i in range(num_iterations):\n        # Pattern 1: Simple stream access (like in all_reduce)\n        stream = current_stream()\n        stream_accesses.append(stream)\n        \n        # Pattern 2: Stream switching and access (like in multi-step runner)\n        stream_idx = i % len(streams)\n        with torch.cuda.stream(streams[stream_idx]):\n            current = current_stream()\n            stream_accesses.append(current)\n            \n            # Simulate some work to make it realistic\n            if i % 10 == 0 and tensors:\n                tensor_idx = i % len(tensors)\n                # Small operation to keep GPU busy\n                _ = tensors[tensor_idx] * 0.999\n    \n    # Verify all accesses worked\n    assert len(stream_accesses) == num_iterations * 2\n    \n    result = {\n        \"num_stream_accesses\": len(stream_accesses),\n        \"use_optimized\": use_optimized,\n        \"unique_streams\": len(set(id(s) for s in stream_accesses)),\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For stream optimization, we check that the same number of operations completed\n    assert current_result[\"num_stream_accesses\"] == reference_result[\"num_stream_accesses\"], \\\n        f\"Stream access count mismatch: {current_result['num_stream_accesses']} vs {reference_result['num_stream_accesses']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if optimization is available\n    try:\n        from vllm.distributed.device_communicators.pynccl import current_stream\n        opt_path_hit = True\n    except (ImportError, AttributeError):\n        opt_path_hit = False\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        # Should not reach here due to setup check\n        warmup = 3\n        iters = 10\n        times = []\n        for _ in range(warmup):\n            _ = experiment(data)\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"310aca88c984983189a57f1b72e3b1dde89fb92f\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float16\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": \"behavioral\",\n        \"opt_path_hit\": opt_path_hit\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "3127e975fb9417d10513e25b80820870f594c627",
    "commit_short": "3127e975",
    "commit_subject": "[CI/Build] Make pre-commit faster (#12212)",
    "repo": "vllm",
    "perf_command": null,
    "files_changed": [
      ".github/workflows/pre-commit.yml",
      ".pre-commit-config.yaml"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12212",
    "models": [
      "N/A"
    ],
    "parent_commit": null,
    "status": "no_perf_command",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.790855407714844e-05,
    "error": null,
    "error_message": "No perf_command in dataset",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "unknown",
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 3127e975fb9417d10513e25b80820870f594c627\nMessage: [CI/Build] Make pre-commit faster (#12212)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # This commit modifies CI/build configuration files only\n    # No Python modules or functions are changed\n    error_data = {\n        \"target_resolved\": False,\n        \"error\": \"No optimizable code paths in commit - CI/build configuration only\",\n        \"error_code\": 3,\n        \"error_name\": \"OPT_PATH_NOT_TRIGGERED\",\n        \"opt_path_hit\": False,\n        \"commit_type\": \"ci_config\"\n    }\n    print(json.dumps(error_data))\n    sys.exit(3)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Since this is a CI configuration change, return minimal data\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"test_tensor\": torch.randn(1, 1, device=device, dtype=dtype)\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    # No optimization to execute - this is a CI configuration change\n    return data[\"test_tensor\"]\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Check for CI configuration commit\n    resolve_target()  # This will exit with error code 3\n    \n    # Unreachable code below (for completeness)\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"3127e975fb9417d10513e25b80820870f594c627\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": False\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "commit_short": "3476ed08",
    "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "benchmarks/benchmark_latency.py",
      "tests/conftest.py",
      "tests/core/block/test_block_table.py",
      "tests/core/block/test_cpu_gpu_block_allocator.py",
      "tests/core/block/test_naive_block.py",
      "tests/core/block/test_prefix_caching_block.py",
      "tests/spec_decode/test_batch_expansion.py",
      "vllm/core/block/block_table.py",
      "vllm/core/block/common.py",
      "vllm/core/block/cpu_gpu_block_allocator.py",
      "vllm/core/block/interfaces.py",
      "vllm/core/block/naive_block.py",
      "vllm/core/block/prefix_caching_block.py",
      "vllm/core/block_manager_v2.py",
      "vllm/engine/llm_engine.py",
      "vllm/entrypoints/openai/serving_completion.py",
      "vllm/model_executor/sampling_metadata.py",
      "vllm/outputs.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/5602",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2412.491879463196,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 54600709b6d4",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 3476ed0809ec91a3457da0cb90543133a4f4b519\nMessage: [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use CpuGpuBlockAllocator\n    if not (module_path and symbol_name):\n        module_path = \"vllm.core.block.cpu_gpu_block_allocator\"\n        symbol_name = \"CpuGpuBlockAllocator\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Block manager configuration\n    block_size = 16\n    num_gpu_blocks = 1024\n    num_cpu_blocks = 512\n    \n    # Sequence configuration for testing block allocation\n    batch_size = 8\n    seq_len = 512\n    num_sequences = 4\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"block_size\": block_size,\n        \"num_gpu_blocks\": num_gpu_blocks,\n        \"num_cpu_blocks\": num_cpu_blocks,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_sequences\": num_sequences,\n        \"allocator_type\": \"prefix_caching\",  # Test the optimized prefix caching allocator\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create the block allocator\n    allocator = target.create(\n        allocator_type=data[\"allocator_type\"],\n        num_gpu_blocks=data[\"num_gpu_blocks\"],\n        num_cpu_blocks=data[\"num_cpu_blocks\"],\n        block_size=data[\"block_size\"],\n    )\n    \n    # Simulate block allocation patterns\n    results = {\n        \"allocated_blocks\": [],\n        \"num_free_gpu\": [],\n        \"num_free_cpu\": []\n    }\n    \n    # Test allocation of mutable and immutable blocks\n    allocated_blocks = []\n    \n    # Allocate some mutable blocks\n    for i in range(data[\"num_sequences\"]):\n        try:\n            block = allocator.allocate_mutable_block(\n                prev_block=None,\n                device=data[\"device\"]\n            )\n            allocated_blocks.append(block)\n            results[\"allocated_blocks\"].append(block.block_id if hasattr(block, 'block_id') else i)\n        except AttributeError:\n            # Fallback for old API\n            block = allocator.allocate_mutable(\n                prev_block=None,\n                device=data[\"device\"]\n            )\n            allocated_blocks.append(block)\n            results[\"allocated_blocks\"].append(block.block_id if hasattr(block, 'block_id') else i)\n    \n    # Check free blocks\n    from vllm.config import Device\n    results[\"num_free_gpu\"].append(allocator.get_num_free_blocks(Device.GPU))\n    results[\"num_free_cpu\"].append(allocator.get_num_free_blocks(Device.CPU))\n    \n    # Test immutable block allocation with token IDs\n    token_ids = list(range(data[\"block_size\"]))\n    for i in range(data[\"num_sequences\"] // 2):\n        try:\n            block = allocator.allocate_immutable_block(\n                prev_block=None,\n                token_ids=token_ids,\n                device=data[\"device\"]\n            )\n            allocated_blocks.append(block)\n        except AttributeError:\n            # Fallback for old API\n            block = allocator.allocate_immutable(\n                prev_block=None,\n                token_ids=token_ids,\n                device=data[\"device\"]\n            )\n            allocated_blocks.append(block)\n    \n    results[\"num_free_gpu\"].append(allocator.get_num_free_blocks(Device.GPU))\n    \n    # Free some blocks\n    for block in allocated_blocks[:len(allocated_blocks)//2]:\n        allocator.free(block)\n    \n    results[\"num_free_gpu\"].append(allocator.get_num_free_blocks(Device.GPU))\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check that the same keys exist\n        assert set(current_result.keys()) == set(reference_result.keys()), \\\n            f\"Keys mismatch: {current_result.keys()} vs {reference_result.keys()}\"\n        \n        # For block allocation, we mainly care about the number of free blocks\n        # being consistent after operations\n        if \"num_free_gpu\" in current_result:\n            # Check final state is equivalent\n            assert current_result[\"num_free_gpu\"][-1] == reference_result[\"num_free_gpu\"][-1], \\\n                f\"Final GPU free blocks mismatch: {current_result['num_free_gpu'][-1]} vs {reference_result['num_free_gpu'][-1]}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            end = time.perf_counter()\n            times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"3476ed0809ec91a3457da0cb90543133a4f4b519\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "35fad35a485eac9195c510731ba4a9d297dfd963",
    "commit_short": "35fad35a",
    "commit_subject": "[V1][Sampler] Faster top-k only implementation (#1",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "733e7c9e95f5b066ac420b00701eef7ea164a79e",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2504.3982920646667,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 16:37:42 [__init__.py:239] Automatically detected platform cuda.\n0.8.3.dev36+g733e7c9e",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:42:31 [__init__.py:239] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
    "commit_short": "379da6dc",
    "commit_subject": "[Kernel] [FP8] Improve FP8 linear layer performance (#4691)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-70B --input-len 1000 --output-len 50 --tensor-parallel-size 4 --quantization fp8",
    "files_changed": [
      "vllm/_custom_ops.py",
      "vllm/model_executor/layers/quantization/fp8.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4691",
    "models": [
      "meta-llama/Meta-Llama-3-70B"
    ],
    "parent_commit": "ebce310b7433e050086f52ca48571807df467f50",
    "status": "error",
    "gpu_config": "H100:4",
    "benchmark_mode": "serving",
    "patch_type": null,
    "duration_s": 1175.21702170372,
    "error": "Baseline wheel install failed: Failed to install wheel: Using Python 3.11.5 environment at: /usr/local\n  \u00d7 Failed to download `vllm @\n  \u2502 https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u251c\u2500\u25b6 Failed to fetch:\n  \u2502   `https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl`\n  \u2570\u2500\u25b6 HTTP status client error (404 Not Found) for url\n      (https://vllm-wheels.s3.us-west-2.amazonaws.com/ebce310b7433e050086f52ca48571807df467f50/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl)\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-70B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 379da6dcb5f5d062d0452b2fc23291e5113dcf04\nMessage: [Kernel] [FP8] Improve FP8 linear layer performance (#4691)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_fp8\"] = major >= 9  # Hopper+\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_fp8\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target scaled_fp8_quant\n    if not (module_path and symbol_name):\n        # Based on the diff, the main optimization is in scaled_fp8_quant\n        module_path = \"vllm._custom_ops\"\n        symbol_name = \"scaled_fp8_quant\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the FP8 quantization optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # Check FP8 support\n    if not hw_info.get(\"supports_fp8\", False) and hw_info[\"device\"] == \"cuda\":\n        # Fall back to FP16 for testing on older GPUs\n        dtype = torch.float16\n    else:\n        dtype = torch.float16  # Input dtype, will be quantized to FP8\n    \n    # Llama 70B-like dimensions for linear layers\n    # Testing various sizes that would appear in the model\n    configs = [\n        # [M, K] dimensions for input tensors to linear layers\n        {\"batch_seq\": 256, \"hidden\": 8192},   # Typical decode batch\n        {\"batch_seq\": 2048, \"hidden\": 8192},  # Prefill scenario\n        {\"batch_seq\": 64, \"hidden\": 8192},    # Small batch\n        {\"batch_seq\": 16, \"hidden\": 8192},    # Edge case (< 17)\n        {\"batch_seq\": 17, \"hidden\": 8192},    # Boundary case\n        {\"batch_seq\": 512, \"hidden\": 8192},   # Medium batch\n    ]\n    \n    # Use the config that triggers the optimization best\n    # The PR mentions padding to 17 for better performance\n    config = configs[1]  # Use prefill scenario as main test\n    \n    # Create input tensor\n    M = config[\"batch_seq\"]\n    K = config[\"hidden\"]\n    \n    input_tensor = torch.randn(M, K, device=device, dtype=dtype)\n    \n    # Scale for static quantization (optional)\n    scale = None  # Use dynamic quantization by default\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"input\": input_tensor,\n        \"scale\": scale,\n        \"batch_dim_padding\": 17,  # The key optimization parameter\n        \"M\": M,\n        \"K\": K,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized FP8 quantization operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Extract parameters\n    input_tensor = data[\"input\"]\n    scale = data[\"scale\"]\n    batch_dim_padding = data[\"batch_dim_padding\"]\n    \n    # Check if we're testing the new version with batch_dim_padding\n    import inspect\n    sig = inspect.signature(target)\n    has_padding_param = \"batch_dim_padding\" in sig.parameters\n    \n    with torch.no_grad():\n        if has_padding_param:\n            # New optimized version\n            output, out_scale = target(input_tensor, scale, batch_dim_padding)\n        else:\n            # Old version without padding\n            output, out_scale = target(input_tensor, scale)\n    \n    return (output, out_scale)\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    output, scale = result\n    torch.save({\n        \"type\": \"fp8_quant_result\",\n        \"output\": output.cpu(),\n        \"scale\": scale.cpu(),\n    }, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return (data[\"output\"], data[\"scale\"])\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    current_output, current_scale = current_result\n    ref_output, ref_scale = reference_result\n    \n    # Check scale equivalence\n    assert current_scale.shape == ref_scale.shape, f\"Scale shape mismatch\"\n    torch.testing.assert_close(\n        current_scale.cpu(),\n        ref_scale.cpu(),\n        rtol=1e-5, atol=1e-7\n    )\n    \n    # Check output shape - may differ due to padding\n    # Only check the non-padded portion\n    min_batch = min(current_output.shape[0], ref_output.shape[0])\n    \n    # Check dtype\n    assert current_output.dtype == ref_output.dtype, f\"Dtype mismatch\"\n    \n    # For FP8, we need more relaxed tolerances\n    if current_output.dtype == torch.float8_e4m3fn:\n        # FP8 comparison - check the non-padded portion\n        current_slice = current_output[:min_batch].cpu().float()\n        ref_slice = ref_output[:min_batch].cpu().float()\n        \n        torch.testing.assert_close(\n            current_slice,\n            ref_slice,\n            rtol=5e-2, atol=1e-2\n        )\n    else:\n        # Fallback for other dtypes\n        torch.testing.assert_close(\n            current_output[:min_batch].cpu(),\n            ref_output[:min_batch].cpu(),\n            rtol=1e-3, atol=1e-4\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if we can run FP8 tests\n    if hw_info[\"device\"] == \"cpu\":\n        # CPU doesn't support FP8\n        print(json.dumps({\n            \"error_code\": 2,\n            \"error_name\": \"CAPABILITY_UNSUPPORTED\",\n            \"error_message\": \"FP8 operations require CUDA device\",\n            \"target_resolved\": True,\n            \"opt_path_hit\": False\n        }))\n        sys.exit(2)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"379da6dcb5f5d062d0452b2fc23291e5113dcf04\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float8_e4m3fn\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "3a243095e5e7b655b63ab08fbd5936cb40850415",
    "commit_short": "3a243095",
    "commit_subject": "Optimize `_get_ranks` in Sampler (#3623)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3623",
    "models": [
      "N/A"
    ],
    "parent_commit": "64172a976c8d975b3aec946f1675716d2532d94f",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 845.2380247116089,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 64172a976c8d",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 3a243095e5e7b655b63ab08fbd5936cb40850415\nMessage: Optimize `_get_ranks` in Sampler (#3623)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimized function is _get_ranks\n        module_path = \"vllm.model_executor.layers.sampler\"\n        symbol_name = \"_get_ranks\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float32  # logprobs are float32 as per code\n    \n    # Realistic workload sizes for _get_ranks function\n    # This function processes logprobs during sampling\n    batch_size = 64  # Number of sequences being processed\n    vocab_size = 32000  # Typical vocab size for LLMs like Llama\n    \n    # Create logprobs tensor (2D: [batch_size, vocab_size])\n    # Use realistic distribution - log of softmax outputs\n    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n    logprobs = torch.log_softmax(logits, dim=-1)\n    \n    # Create indices tensor - chosen token indices for each sequence\n    # These would be the sampled tokens\n    indices = torch.randint(0, vocab_size, (batch_size,), device=device, dtype=torch.long)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"logprobs\": logprobs,\n        \"indices\": indices,\n        \"batch_size\": batch_size,\n        \"vocab_size\": vocab_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the optimized _get_ranks function\n    with torch.no_grad():\n        result = target(data[\"logprobs\"], data[\"indices\"])\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Ranks should be exact integers\n        rtol, atol = 0, 0\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"3a243095e5e7b655b63ab08fbd5936cb40850415\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "3b61cb450d899dc423feb264c297d4d18d701678",
    "commit_short": "3b61cb45",
    "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "files_changed": [
      "csrc/cache_kernels.cu",
      "vllm/v1/attention/backends/flash_attn.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/10989",
    "models": [
      "N/A"
    ],
    "parent_commit": "edc4fa31888b4a41060acb7b16250540f051ad59",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "c_cuda",
    "duration_s": 1478.5293717384338,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev277+gedc4fa31",
    "human_version": "0.6.4.post2.dev278+g3b61cb45",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": "Incremental build failed: Git reset failed: fatal: Unable to create '/__modal/volumes/vo-1SXSnTcT2e5vHaQnKwvF7G/build_edc4fa31888b4a41060acb7b16250540f051ad59/.git/index.lock': File exists.\n\nAnother git process seems to be running in this repository, e.g.\nan editor opened by 'git commit'. Please make sure all processes\nare terminated then try again. If it still fails, a git process\nmay have crashed in this repository earlier:\nremove the file manually to continue.\n",
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1691.4719065666693,
    "baseline_throughput": 9819.5,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1706.3253376333307,
    "human_throughput": 9822.4,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -0.8781364330673833,
    "human_improvement_throughput": 0.029533071948669852,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-02 11:39:19 config.py:405] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-02 11:39:19 arg_utils.py:1068] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you en",
    "human_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-02 11:42:24 config.py:405] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-02 11:42:24 arg_utils.py:1068] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you en",
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 3b61cb450d899dc423feb264c297d4d18d701678\nMessage: [V1] Further reduce CPU overheads in flash-attn (#10989)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the target is FlashAttentionImpl\n        module_path = \"vllm.v1.attention.backends.flash_attn\"\n        symbol_name = \"FlashAttentionImpl\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Flash attention workload for decode phase (which this optimization targets)\n    batch_size = 32  # Multiple requests\n    num_heads = 32\n    head_size = 128\n    num_kv_heads = 32  # No GQA for simplicity\n    block_size = 16\n    num_blocks = 128\n    max_seq_len = 1024\n    query_len = 1  # Decode phase - single token generation\n    \n    # Create attention implementation\n    FlashAttentionImpl, _ = resolve_target()\n    attn_impl = FlashAttentionImpl(\n        num_heads=num_heads,\n        head_size=head_size,\n        scale=1.0 / math.sqrt(head_size),\n        num_kv_heads=num_kv_heads,\n        alibi_slopes=None,\n        sliding_window=None,\n        kv_cache_dtype=\"auto\",\n        blocksparse_params=None,\n        logits_soft_cap=None,\n    )\n    \n    # Create inputs\n    num_actual_tokens = batch_size * query_len\n    num_padded_tokens = ((num_actual_tokens + 7) // 8) * 8  # Pad to multiple of 8\n    \n    # Query, key, value tensors (padded)\n    query = torch.randn(num_padded_tokens, num_heads, head_size, \n                        device=device, dtype=dtype)\n    key = torch.randn(num_padded_tokens, num_kv_heads, head_size,\n                     device=device, dtype=dtype)\n    value = torch.randn(num_padded_tokens, num_kv_heads, head_size,\n                       device=device, dtype=dtype)\n    \n    # KV cache\n    kv_cache = torch.zeros(2, num_blocks, block_size, num_kv_heads, head_size,\n                          device=device, dtype=dtype)\n    \n    # Metadata\n    query_start_loc = torch.arange(0, batch_size + 1, dtype=torch.int32, device=device)\n    seq_lens = torch.randint(64, max_seq_len - 1, (batch_size,), dtype=torch.int32, device=device)\n    seq_start_loc = torch.zeros(batch_size + 1, dtype=torch.int32, device=device)\n    seq_start_loc[1:] = torch.cumsum(seq_lens, dim=0)\n    \n    # Block table\n    max_blocks_per_seq = (max_seq_len + block_size - 1) // block_size\n    block_table = torch.zeros(batch_size, max_blocks_per_seq, dtype=torch.int32, device=device)\n    for i in range(batch_size):\n        num_blocks_needed = (seq_lens[i].item() + block_size - 1) // block_size\n        block_table[i, :num_blocks_needed] = torch.arange(i * max_blocks_per_seq, \n                                                         i * max_blocks_per_seq + num_blocks_needed)\n    \n    # Slot mapping (not padded - this is the key difference)\n    slot_mapping = torch.zeros(num_actual_tokens, dtype=torch.int64, device=device)\n    for i in range(batch_size):\n        seq_len = seq_lens[i].item()\n        block_idx = seq_len // block_size\n        block_offset = seq_len % block_size\n        slot_idx = block_table[i, block_idx].item() * block_size + block_offset\n        slot_mapping[i] = slot_idx\n    \n    # Create metadata object\n    from vllm.attention.backends.dual_chunk_flash_attn import FlashAttentionMetadata\n    attn_metadata = FlashAttentionMetadata(\n        num_actual_tokens=num_actual_tokens,\n        max_query_len=query_len,\n        query_start_loc=query_start_loc,\n        max_seq_len=max_seq_len,\n        seq_start_loc=seq_start_loc,\n        block_table=block_table,\n        slot_mapping=slot_mapping,\n    )\n    \n    # Preallocate output\n    output = torch.empty(num_padded_tokens, num_heads * head_size,\n                         device=device, dtype=dtype)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"attn_impl\": attn_impl,\n        \"query\": query,\n        \"key\": key,\n        \"value\": value,\n        \"kv_cache\": kv_cache,\n        \"attn_metadata\": attn_metadata,\n        \"output\": output,\n        \"num_actual_tokens\": num_actual_tokens,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    with torch.no_grad():\n        result = data[\"attn_impl\"].forward(\n            query=data[\"query\"],\n            key=data[\"key\"],\n            value=data[\"value\"],\n            kv_cache=data[\"kv_cache\"],\n            attn_metadata=data[\"attn_metadata\"],\n            output=data[\"output\"],\n        )\n    \n    # Return only the actual tokens (not padded)\n    return result[:data[\"num_actual_tokens\"]].clone()\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    if isinstance(data, dict) and \"data\" in data:\n        result = data[\"data\"]\n        if isinstance(result, torch.Tensor) and torch.cuda.is_available():\n            result = result.cuda()\n        return result\n    return data\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation  \n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            end = time.perf_counter()\n            times.append((end - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"3b61cb450d899dc423feb264c297d4d18d701678\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "4c822298981a8f7521492075ff72659985fc4c3f",
    "commit_short": "4c822298",
    "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Nu",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model meta-llama/Llama-3.2-1B-Instruct --num-speculative-tokens 5",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "c8d70e2437feecdb3762ce17298df33439ae1bd1",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 2378.0147426128387,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "dev",
    "human_version": "dev",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1076.7869629999989,
    "baseline_throughput": 102.1,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1078.497049699996,
    "human_throughput": 153.2,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 1077.2987806333333,
    "agent_throughput": 153.5,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -0.1588138377189094,
    "human_improvement_throughput": 50.04897159647405,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": -0.04753193072736259,
    "agent_improvement_throughput": 50.34280117531832,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": 0.1111054561527156,
    "agent_vs_human_throughput": 0.19582245430810144,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model='meta-llama/Llama-3.2-1B-Instruct', speculative_model_quantization=None, num_speculative_tokens=5, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None)\nINFO 01-02 15:42:03 __init__.py:197] ",
    "human_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model='meta-llama/Llama-3.2-1B-Instruct', speculative_model_quantization=None, num_speculative_tokens=5, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None)\nINFO 01-02 15:44:36 __init__.py:197] ",
    "agent_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model='meta-llama/Llama-3.2-1B-Instruct', speculative_model_quantization=None, num_speculative_tokens=5, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None)\nINFO 01-02 15:47:07 __init__.py:197] ",
    "test_script": null
  },
  {
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "commit_short": "4fb56914",
    "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "files_changed": [
      "csrc/layernorm_kernels.cu",
      "csrc/layernorm_quant_kernels.cu",
      "csrc/quantization/fp8/common.cu",
      "tests/kernels/core/test_layernorm.py",
      "vllm/model_executor/layers/linear.py",
      "vllm/model_executor/layers/quantization/fp8.py",
      "vllm/model_executor/models/deepseek_v2.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21116",
    "models": [
      "deepseek-ai/DeepSeek-V3-0324"
    ],
    "parent_commit": null,
    "status": "error",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3837.0142991542816,
    "error": null,
    "error_message": "Baseline server failed to start",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "serving",
    "agent_error": null,
    "patch_path": "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0023/model_patch.diff",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 4fb56914c5f27ef062e10d44a0f79c6ceab382f9\nMessage: [perf] Add fused MLA QKV + strided layernorm (#21116)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - targeting RMSNorm\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.layernorm\"\n        symbol_name = \"RMSNorm\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Setup for strided RMSNorm kernel test\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Model sizes for 7B scale testing\n    batch_size = 4\n    seq_len = 2048\n    hidden_size = 4096\n    \n    # Create strided input by slicing a larger tensor\n    # This mimics the fused QKV scenario where we slice projections\n    last_dim = 2 * hidden_size  # Create extra space for striding\n    \n    # Create base tensor with extra width\n    x_base = torch.randn(batch_size * seq_len, last_dim, device=device, dtype=dtype)\n    # Scale down to prevent overflow\n    x_base *= (1.0 / math.sqrt(2 * hidden_size))\n    \n    # Create strided view by slicing\n    x_strided = x_base[..., :hidden_size]\n    \n    # Verify it's actually strided\n    assert not x_strided.is_contiguous()\n    assert x_strided.stride(-1) == 1  # Last dim stride is 1\n    assert x_strided.stride(-2) == last_dim  # Second-to-last stride is last_dim\n    \n    # Create RMSNorm layer\n    RMSNorm, _ = resolve_target()\n    layer = RMSNorm(hidden_size).to(device=device, dtype=dtype)\n    layer.weight.data.normal_(mean=1.0, std=0.1)\n    \n    # Also prepare residual for fused add variant\n    residual = torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype)\n    residual *= (1.0 / math.sqrt(2 * hidden_size))\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"x_base\": x_base,\n        \"x_strided\": x_strided,\n        \"layer\": layer,\n        \"residual\": residual,\n        \"hidden_size\": hidden_size,\n        \"batch_seq\": batch_size * seq_len,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Test the strided RMSNorm kernel\n    x_strided = data[\"x_strided\"]\n    layer = data[\"layer\"]\n    residual = data[\"residual\"]\n    \n    # Clone inputs to avoid in-place modification\n    x_work = x_strided.clone()\n    residual_work = residual.clone() if residual is not None else None\n    \n    with torch.no_grad():\n        # Call the RMSNorm forward which internally uses the optimized kernel\n        # The optimization allows this to work efficiently with strided input\n        output = layer(x_work, residual_work)\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Ensure we're testing the strided path\n    assert not data[\"x_strided\"].is_contiguous(), \"Input must be strided to test optimization\"\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"4fb56914c5f27ef062e10d44a0f79c6ceab382f9\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "526de822d501c792b051c864ba873a836d78d5bf",
    "commit_short": "526de822",
    "commit_subject": "[Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --dtype bfloat16 --enable-chunked-prefill False --load-format dummy --batch-size BS --num-iters-warmup 2 --num-iters 5 --input-len INPUT_LEN --output-len OUTPUT_LEN --model MODEL",
    "files_changed": [
      "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11698",
    "models": [
      "Qwen/Qwen2-7B-Instruct",
      "microsoft/Phi-3-medium-128k-instruct",
      "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "mistralai/Mistral-7B-Instruct-v0.3"
    ],
    "parent_commit": null,
    "status": "error",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3620.286018848419,
    "error": null,
    "error_message": "Baseline server failed to start",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "latency",
    "agent_error": null,
    "patch_path": "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0024/model_patch.diff",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 526de822d501c792b051c864ba873a836d78d5bf\nMessage: [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_int8\"] = True\n        hw_info[\"supports_fp16\"] = True\n        hw_info[\"supports_bf16\"] = major >= 8\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_int8\"] = True\n        hw_info[\"supports_fp16\"] = False\n        hw_info[\"supports_bf16\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.quantization.compressed_tensors.triton_scaled_mm\"\n        symbol_name = \"triton_scaled_mm\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # INT8 quantized model workload - testing the heuristic for different M sizes\n    # The commit message mentions \"int8 models\" and \"2.8x speedup\"\n    # Test multiple M sizes to trigger different tile shape branches\n    \n    # Configuration for int8 quantized GEMM\n    configs = [\n        # Small M (next_power_of_2 <= 32)\n        {\"M\": 16, \"K\": 4096, \"N\": 4096},\n        # Medium M (next_power_of_2 <= 64)\n        {\"M\": 48, \"K\": 4096, \"N\": 4096},\n        # Large M (next_power_of_2 <= 128)\n        {\"M\": 96, \"K\": 4096, \"N\": 11008},\n        # Very large M (next_power_of_2 > 128)\n        {\"M\": 256, \"K\": 4096, \"N\": 11008},\n    ]\n    \n    # Use medium config as default for consistent testing\n    config = configs[2]  # M=96 case\n    \n    M, K, N = config[\"M\"], config[\"K\"], config[\"N\"]\n    \n    # INT8 inputs for quantized models\n    input_tensor = torch.randint(-128, 127, (M, K), dtype=torch.int8, device=device)\n    weight_tensor = torch.randint(-128, 127, (K, N), dtype=torch.int8, device=device)\n    \n    # Scaling factors for dequantization\n    scale_a = torch.randn(M, 1, dtype=torch.float32, device=device).abs() * 0.1\n    scale_b = torch.randn(N, 1, dtype=torch.float32, device=device).abs() * 0.1\n    \n    # Optional bias\n    bias = torch.randn(N, dtype=torch.float32, device=device)\n    \n    # Output dtype\n    out_dtype = torch.float32\n    \n    data = {\n        \"device\": device,\n        \"dtype\": torch.int8,\n        \"hw_info\": hw_info,\n        \"input\": input_tensor,\n        \"weight\": weight_tensor,\n        \"scale_a\": scale_a,\n        \"scale_b\": scale_b,\n        \"out_dtype\": out_dtype,\n        \"bias\": bias,\n        \"M\": M,\n        \"K\": K,\n        \"N\": N,\n        \"use_heuristic\": True  # Key parameter for the optimization\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Get the use_heuristic flag from environment or default to True for child commit\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    use_heuristic = (impl_tag == \"child\")\n    \n    with torch.no_grad():\n        # Call triton_scaled_mm with the heuristic flag\n        result = target(\n            input=data[\"input\"],\n            weight=data[\"weight\"],\n            scale_a=data[\"scale_a\"],\n            scale_b=data[\"scale_b\"],\n            out_dtype=data[\"out_dtype\"],\n            bias=data[\"bias\"],\n            block_size_m=32,  # Default values that will be overridden by heuristic\n            block_size_n=32,\n            block_size_k=32,\n            use_heuristic=use_heuristic\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # For float32 output from int8 quantized ops\n        rtol, atol = 1e-5, 1e-6\n        \n        # Move to CPU for comparison\n        current_cpu = current_result.cpu()\n        reference_cpu = reference_result.cpu()\n        \n        # Check for special values\n        if torch.isnan(current_cpu).any() or torch.isnan(reference_cpu).any():\n            assert torch.isnan(current_cpu).equal(torch.isnan(reference_cpu)), \"NaN mismatch\"\n            mask = ~torch.isnan(current_cpu)\n            torch.testing.assert_close(\n                current_cpu[mask],\n                reference_cpu[mask],\n                rtol=rtol, atol=atol\n            )\n        else:\n            torch.testing.assert_close(\n                current_cpu,\n                reference_cpu,\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            end = time.perf_counter()\n            times.append((end - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"526de822d501c792b051c864ba873a836d78d5bf\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.int8\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "commit_short": "58eee5f2",
    "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/transformers_utils/tokenizer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/20000",
    "models": [
      "N/A"
    ],
    "parent_commit": "067c34a1559400e956311f067ddd185f54207a2b",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 564.3076100349426,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.1.dev310+g067c34a15",
    "human_version": "0.10.1.dev311+g58eee5f2e",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 838.15,
    "baseline_ttft_median": 869.87,
    "baseline_ttft_p99": 1340.78,
    "baseline_tpot_mean": 19.56,
    "baseline_tpot_median": 17.09,
    "baseline_tpot_p99": 125.21,
    "baseline_itl_mean": 16.71,
    "baseline_itl_median": 12.75,
    "baseline_itl_p99": 196.23,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 811.07,
    "human_ttft_median": 848.89,
    "human_ttft_p99": 1333.52,
    "human_tpot_mean": 20.41,
    "human_tpot_median": 16.38,
    "human_tpot_p99": 191.76,
    "human_itl_mean": 16.47,
    "human_itl_median": 12.52,
    "human_itl_p99": 199.49,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 835.48,
    "agent_ttft_median": 869.46,
    "agent_ttft_p99": 1339.83,
    "agent_tpot_mean": 18.64,
    "agent_tpot_median": 16.93,
    "agent_tpot_p99": 35.83,
    "agent_itl_mean": 16.69,
    "agent_itl_median": 12.86,
    "agent_itl_p99": 194.1,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 3.230925252043182,
    "human_improvement_tpot_mean": -4.345603271983648,
    "human_improvement_itl_mean": 1.4362657091562057,
    "agent_improvement_ttft_mean": 0.3185587305374884,
    "agent_improvement_tpot_mean": 4.703476482617578,
    "agent_improvement_itl_mean": 0.11968880909634692,
    "agent_vs_human_ttft_mean": -3.0096045963973475,
    "agent_vs_human_tpot_mean": 8.672219500244976,
    "agent_vs_human_itl_mean": -1.3357619914997112,
    "human_improvement_ttft_median": 2.4118546449469482,
    "human_improvement_ttft_p99": 0.5414758573367735,
    "agent_improvement_ttft_median": 0.04713347971535611,
    "agent_improvement_ttft_p99": 0.07085427885261157,
    "agent_vs_human_ttft_median": -2.4231643675859122,
    "agent_vs_human_ttft_p99": -0.47318375427439746,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 02:50:14 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b9d3fa28f40>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:50:22 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.01      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12340     \nRequest throughput (req/s):              33.18     \nOutput token throughput (tok/s):         4094.73   \nTotal Token throughput (tok/s):          21051.01  \n---------------Time to First Token----------------\nMean TTFT (ms):                          838.15    \nMedian TTFT (ms):                        869.87    \nP99 TTFT (ms):                           1340.78   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.56     \nMedian TPOT (ms):                        17.09     \nP99 TPOT (ms):                           125.21    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.71     \nMedian ITL (ms):                         12.75     \nP99 ITL (ms):                            196.23    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:14,  1.3",
    "human_raw": "INFO 01-02 02:53:13 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b28c4838f40>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:53:20 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.96      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12301     \nRequest throughput (req/s):              33.81     \nOutput token throughput (tok/s):         4159.56   \nTotal Token throughput (tok/s):          21438.92  \n---------------Time to First Token----------------\nMean TTFT (ms):                          811.07    \nMedian TTFT (ms):                        848.89    \nP99 TTFT (ms):                           1333.52   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.41     \nMedian TPOT (ms):                        16.38     \nP99 TPOT (ms):                           191.76    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.47     \nMedian ITL (ms):                         12.52     \nP99 ITL (ms):                            199.49    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:13,  1.3",
    "agent_raw": "INFO 01-02 02:55:33 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2aed2ff04fe0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 02:55:39 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.01      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12340     \nRequest throughput (req/s):              33.24     \nOutput token throughput (tok/s):         4102.14   \nTotal Token throughput (tok/s):          21089.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          835.48    \nMedian TTFT (ms):                        869.46    \nP99 TTFT (ms):                           1339.83   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          18.64     \nMedian TPOT (ms):                        16.93     \nP99 TPOT (ms):                           35.83     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.69     \nMedian ITL (ms):                         12.86     \nP99 ITL (ms):                            194.10    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:13,  1.3",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc\nMessage: [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the target is decode_tokens\n        module_path = \"vllm.transformers_utils.tokenizer\"\n        symbol_name = \"decode_tokens\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # This is a tokenizer optimization, CPU-based\n    device = \"cpu\"  # Tokenization is CPU-based\n    dtype = torch.float32\n    \n    # Create realistic token ID sequences\n    # Typical scenarios: batch decoding of generated sequences\n    batch_sizes = [1, 8, 32, 128]  # Various batch sizes\n    seq_lengths = [32, 128, 512, 2048]  # Various sequence lengths\n    vocab_size = 32000  # Typical vocab size for models like Llama\n    \n    # Use medium batch/seq for balanced workload\n    batch_size = 32\n    seq_len = 512\n    \n    # Generate random token IDs (simulating model output)\n    np.random.seed(42)\n    token_ids_batch = []\n    for _ in range(batch_size):\n        # Generate realistic token sequences with some special tokens\n        tokens = np.random.randint(0, vocab_size, seq_len).tolist()\n        # Add some special tokens (typical IDs: 0=pad, 1=bos, 2=eos)\n        if np.random.random() < 0.1:\n            tokens[0] = 1  # BOS token\n        if np.random.random() < 0.1:\n            tokens[-1] = 2  # EOS token\n        # Add some padding tokens\n        num_pad = np.random.randint(0, min(10, seq_len // 4))\n        if num_pad > 0:\n            tokens[-num_pad:] = [0] * num_pad\n        token_ids_batch.append(tokens)\n    \n    # Try to get a real tokenizer for testing\n    tokenizer = None\n    try:\n        from transformers import AutoTokenizer\n        # Use a common model's tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(\n            \"gpt2\",  # Use GPT-2 as it's small and commonly available\n            use_fast=True,  # Use fast tokenizer for better performance\n            trust_remote_code=False\n        )\n    except Exception:\n        # If transformers is not available, create a mock tokenizer\n        class MockTokenizer:\n            def __init__(self):\n                self.vocab_size = vocab_size\n                # Simulate the _decode optimization\n                self._has_fast_decode = os.getenv(\"IMPL_TAG\", \"child\") == \"child\"\n            \n            def decode(self, token_ids, skip_special_tokens=True):\n                \"\"\"Simulate decode with list-to-list conversion overhead.\"\"\"\n                # Simulate the overhead of list conversion\n                if isinstance(token_ids, list):\n                    # This simulates the slow path\n                    token_ids = list(token_ids)  # Unnecessary copy\n                # Simulate decoding (just return a placeholder string)\n                return f\"decoded_seq_len_{len(token_ids)}\"\n            \n            def _decode(self, token_ids, skip_special_tokens=True):\n                \"\"\"Simulate faster internal decode without conversion.\"\"\"\n                # This is the fast path - no unnecessary conversion\n                # Simulate decoding (just return a placeholder string)\n                return f\"decoded_seq_len_{len(token_ids)}\"\n        \n        tokenizer = MockTokenizer()\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"tokenizer\": tokenizer,\n        \"token_ids_batch\": token_ids_batch,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"skip_special_tokens\": True,  # Common use case\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    decode_tokens, fq_name = resolve_target()\n    \n    tokenizer = data[\"tokenizer\"]\n    token_ids_batch = data[\"token_ids_batch\"]\n    skip_special_tokens = data[\"skip_special_tokens\"]\n    \n    # Decode all sequences in the batch\n    results = []\n    for token_ids in token_ids_batch:\n        # Call the target function\n        decoded = decode_tokens(\n            tokenizer,\n            token_ids,\n            skip_special_tokens=skip_special_tokens\n        )\n        results.append(decoded)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # For string results, save as JSON\n    if isinstance(result, list) and all(isinstance(x, str) for x in result):\n        import json\n        with open(filepath, 'w') as f:\n            json.dump({\"type\": \"string_list\", \"data\": result}, f)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    if filepath.endswith('.json'):\n        import json\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n        return data.get(\"data\", data)\n    else:\n        data = torch.load(filepath)\n        return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, list) and isinstance(reference_result, list):\n        assert len(current_result) == len(reference_result), f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n        for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n            if isinstance(curr, str) and isinstance(ref, str):\n                assert curr == ref, f\"String mismatch at index {i}: '{curr}' vs '{ref}'\"\n            else:\n                assert curr == ref, f\"Value mismatch at index {i}: {curr} vs {ref}\"\n    else:\n        assert current_result == reference_result, f\"Result mismatch: {current_result} vs {reference_result}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_operation(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU operation (tokenization)\n    warmup = 5\n    iters = 20  # More iterations for CPU timing\n    \n    # Adjust iterations based on workload size\n    total_tokens = data[\"batch_size\"] * data[\"seq_len\"]\n    if total_tokens > 10000:\n        iters = 10  # Reduce for large workloads\n    elif total_tokens < 1000:\n        iters = 50  # Increase for small workloads\n    \n    result, timing_stats = time_cpu_operation(\n        lambda: experiment(data), \n        warmup=warmup, \n        iterations=iters\n    )\n    \n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.json\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Tokenization is CPU-based\n        \"dtype\": \"str\",  # Working with strings\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),  # String comparison is exact\n        \"opt_path_hit\": True,\n        \"batch_size\": data[\"batch_size\"],\n        \"seq_len\": data[\"seq_len\"]\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "61b8cea3b42feab021d506e9143551de18f9165c",
    "commit_short": "61b8cea3",
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "tests/v1/attention/test_attention_backends.py",
      "tests/v1/attention/utils.py",
      "vllm/v1/attention/backends/flashinfer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21137",
    "models": [
      "meta-llama/Meta-Llama-3-8B-Instruct",
      "meta-llama/Llama-3.2-3B-Instruct"
    ],
    "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 5413.949769735336,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.0rc3.dev11+g526078a96",
    "human_version": "0.10.0rc3.dev12+g61b8cea3b",
    "agent_version": null,
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": 74.94,
    "baseline_request_throughput": 74.94,
    "baseline_output_throughput": 9592.53,
    "baseline_total_throughput": 28776.09,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": 75.02,
    "human_request_throughput": 75.02,
    "human_output_throughput": 9602.76,
    "human_total_throughput": 28806.77,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": 75.04,
    "agent_request_throughput": 75.04,
    "agent_output_throughput": 9604.7,
    "agent_total_throughput": 28812.61,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": 0.10675206832132145,
    "human_improvement_request_throughput": 0.10675206832132145,
    "human_improvement_output_throughput": 0.106645483516857,
    "human_improvement_total_throughput": 0.1066162915114607,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": 0.13344008540166605,
    "agent_improvement_request_throughput": 0.13344008540166605,
    "agent_improvement_output_throughput": 0.12686955370481062,
    "agent_improvement_total_throughput": 0.12691091805731924,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": 0.026659557451359946,
    "agent_vs_human_request_throughput": 0.026659557451359946,
    "agent_vs_human_output_throughput": 0.02020252510737027,
    "agent_vs_human_total_throughput": 0.02027301221206038,
    "baseline_raw": "INFO 01-02 17:22:40 [__init__.py:235] Automatically detected platform cuda.\nWhen dataset path is not set, it will default to random dataset\nINFO 01-02 17:22:46 [datasets.py:355] Sampling input_len from [255, 255] and output_len from [128, 128]\nINFO 01-02 17:22:56 [config.py:1605] Using max model len 131072\nINFO 01-02 17:22:56 [config.py:2416] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 01-02 17:23:03 [__init__.py:235] Automatically detected platform cuda.\nINFO 01-02 17:23:08 [core.py:574] Waiting for init message from front-end.\nINFO 01-02 17:23:08 [core.py:72] Initializing a V1 LLM engine (v0.10.0rc3.dev11+g526078a96) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nINFO 01-02 17:23:10 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 01-02 17:23:10 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 01-02 17:23:10 [gpu_model_runner.py:1843] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\nINFO 01-02 17:23:11 [gpu_model_runner.py:1875] Loading model from scratch...\nINFO 01-02 17:23:11 [cuda.py:290] Using Flash Attention",
    "human_raw": "INFO 01-02 17:25:44 [__init__.py:235] Automatically detected platform cuda.\nWhen dataset path is not set, it will default to random dataset\nINFO 01-02 17:25:50 [datasets.py:355] Sampling input_len from [255, 255] and output_len from [128, 128]\nINFO 01-02 17:25:59 [config.py:1605] Using max model len 131072\nINFO 01-02 17:25:59 [config.py:2416] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 01-02 17:26:06 [__init__.py:235] Automatically detected platform cuda.\nINFO 01-02 17:26:10 [core.py:574] Waiting for init message from front-end.\nINFO 01-02 17:26:10 [core.py:72] Initializing a V1 LLM engine (v0.10.0rc3.dev12+g61b8cea3b) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nINFO 01-02 17:26:12 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 01-02 17:26:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 01-02 17:26:12 [gpu_model_runner.py:1843] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\nINFO 01-02 17:26:12 [gpu_model_runner.py:1875] Loading model from scratch...\nINFO 01-02 17:26:13 [cuda.py:290] Using Flash Attention",
    "agent_raw": "INFO 01-02 17:28:10 [__init__.py:235] Automatically detected platform cuda.\nWhen dataset path is not set, it will default to random dataset\nINFO 01-02 17:28:16 [datasets.py:355] Sampling input_len from [255, 255] and output_len from [128, 128]\nINFO 01-02 17:28:26 [config.py:1605] Using max model len 131072\nINFO 01-02 17:28:26 [config.py:2416] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 01-02 17:28:33 [__init__.py:235] Automatically detected platform cuda.\nINFO 01-02 17:28:37 [core.py:574] Waiting for init message from front-end.\nINFO 01-02 17:28:37 [core.py:72] Initializing a V1 LLM engine (v0.10.0rc3.dev11+g526078a96) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\nINFO 01-02 17:28:39 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nWARNING 01-02 17:28:39 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\nINFO 01-02 17:28:39 [gpu_model_runner.py:1843] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\nINFO 01-02 17:28:39 [gpu_model_runner.py:1875] Loading model from scratch...\nINFO 01-02 17:28:39 [cuda.py:290] Using Flash Attention",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 61b8cea3b42feab021d506e9143551de18f9165c\nMessage: [Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.v1.attention.backends.flashinfer\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"FlashInferMetadataBuilder\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Import required classes for setup\n    try:\n        from vllm.config import VllmConfig, ModelConfig, CacheConfig\n        from vllm.attention.layers.chunked_local_attention import CommonAttentionMetadata\n        from vllm.v1.kv_cache_interface import FullAttentionSpec\n        \n        # Create realistic configuration\n        model_config = ModelConfig(\n            model=\"meta-llama/Llama-2-7b-hf\",\n            tokenizer=\"meta-llama/Llama-2-7b-hf\",\n            tokenizer_mode=\"auto\",\n            trust_remote_code=False,\n            max_model_len=2048,\n            dtype=\"float16\" if hw_info[\"device\"] == \"cuda\" else \"float32\",\n            seed=42,\n        )\n        \n        cache_config = CacheConfig(\n            block_size=16,\n            gpu_memory_utilization=0.9,\n            cache_dtype=\"auto\",\n        )\n        \n        # Create VllmConfig\n        vllm_config = VllmConfig(\n            model_config=model_config,\n            cache_config=cache_config,\n        )\n        \n        # Create KV cache spec\n        kv_cache_spec = FullAttentionSpec(\n            num_layers=32,\n            num_kv_heads=32,\n            head_size=128,\n            dtype=dtype,\n            block_size=16,\n            device=device,\n        )\n        \n        # Create realistic batch metadata\n        batch_size = 32  # Mix of prefill and decode\n        num_prefills = 8\n        num_decodes = batch_size - num_prefills\n        \n        # Sequence lengths\n        prefill_lens = [512, 1024, 768, 256, 2048, 384, 640, 896]\n        decode_lens = [128] * num_decodes\n        seq_lens_cpu = torch.tensor(decode_lens + prefill_lens, dtype=torch.int32)\n        seq_lens = seq_lens_cpu.to(device)\n        \n        # Context lengths (how much has been processed)\n        decode_context = decode_lens  # All decoded\n        prefill_context = [0] * num_prefills  # Just starting\n        context_lens_cpu = torch.tensor(decode_context + prefill_context, dtype=torch.int32)\n        context_lens = context_lens_cpu.to(device)\n        \n        # Query tokens\n        num_decode_tokens = num_decodes * 1  # 1 token per decode\n        num_prefill_tokens = sum(prefill_lens)\n        num_actual_tokens = num_decode_tokens + num_prefill_tokens\n        \n        # Query start locations\n        query_start_loc_cpu = torch.zeros(batch_size + 1, dtype=torch.int32)\n        query_start_loc_cpu[1:num_decodes+1] = torch.arange(1, num_decodes+1)\n        cumsum_prefill = torch.cumsum(torch.tensor(prefill_lens), dim=0)\n        query_start_loc_cpu[num_decodes+1:] = num_decode_tokens + cumsum_prefill\n        query_start_loc = query_start_loc_cpu.to(device)\n        \n        # Computed tokens\n        num_computed_tokens_cpu = torch.cat([\n            torch.ones(num_decodes, dtype=torch.int32),\n            torch.tensor(prefill_lens, dtype=torch.int32)\n        ])\n        num_computed_tokens = num_computed_tokens_cpu.to(device)\n        \n        # Block table\n        max_blocks = (max(seq_lens_cpu.tolist()) + 15) // 16\n        max_block_idx = 10000\n        block_table_tensor = torch.randint(0, max_block_idx, \n                                          (batch_size, max_blocks),\n                                          dtype=torch.int32, device=device)\n        \n        # Create CommonAttentionMetadata\n        common_attn_metadata = CommonAttentionMetadata(\n            seq_lens=seq_lens,\n            seq_lens_cpu=seq_lens_cpu,\n            context_lens=context_lens,\n            context_lens_cpu=context_lens_cpu,\n            block_table_tensor=block_table_tensor,\n            num_computed_tokens=num_computed_tokens,\n            num_computed_tokens_cpu=num_computed_tokens_cpu,\n            query_start_loc=query_start_loc,\n            query_start_loc_cpu=query_start_loc_cpu,\n            num_actual_tokens=num_actual_tokens,\n        )\n        \n    except ImportError as e:\n        # Fallback to mock data if imports fail\n        common_attn_metadata = None\n        vllm_config = None\n        kv_cache_spec = None\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"common_attn_metadata\": common_attn_metadata,\n        \"vllm_config\": vllm_config,\n        \"kv_cache_spec\": kv_cache_spec,\n        \"num_prefills\": num_prefills,\n        \"num_decodes\": num_decodes,\n        \"common_prefix_len\": 0,  # No shared prefix for this test\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create a FlashInferMetadataBuilder instance\n    builder = target(\n        vllm_config=data[\"vllm_config\"],\n        kv_cache_spec=data[\"kv_cache_spec\"],\n        device=data[\"device\"],\n    )\n    \n    # Call the build method - this is what we're optimizing\n    with torch.no_grad():\n        result = builder.build(\n            common_attn_metadata=data[\"common_attn_metadata\"],\n            num_prefills=data[\"num_prefills\"],\n            num_decodes=data[\"num_decodes\"],\n            common_prefix_len=data[\"common_prefix_len\"],\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Extract key fields from FlashInferMetadata for comparison\n    if hasattr(result, '__dict__'):\n        # Convert to dict for storage\n        result_dict = {}\n        for key, value in result.__dict__.items():\n            if isinstance(value, torch.Tensor):\n                result_dict[key] = value.cpu()\n            elif value is not None and not callable(value):\n                result_dict[key] = value\n        torch.save({\"type\": \"flashinfer_metadata\", \"data\": result_dict}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For FlashInferMetadata, compare key fields\n    if isinstance(reference_result, dict):\n        current_dict = {}\n        for key, value in current_result.__dict__.items():\n            if isinstance(value, torch.Tensor):\n                current_dict[key] = value.cpu()\n            elif value is not None and not callable(value):\n                current_dict[key] = value\n        \n        for key in reference_result:\n            if key in current_dict:\n                ref_val = reference_result[key]\n                cur_val = current_dict[key]\n                \n                if isinstance(ref_val, torch.Tensor):\n                    assert cur_val.shape == ref_val.shape, f\"Shape mismatch for {key}\"\n                    assert cur_val.dtype == ref_val.dtype, f\"Dtype mismatch for {key}\"\n                    \n                    # Determine tolerances\n                    if cur_val.dtype in (torch.float16, torch.bfloat16):\n                        rtol, atol = 1e-3, 1e-4\n                    elif cur_val.dtype in (torch.int32, torch.int64):\n                        rtol, atol = 0, 0\n                    else:\n                        rtol, atol = 1e-5, 1e-7\n                    \n                    if rtol == 0:\n                        assert torch.equal(cur_val, ref_val), f\"Mismatch for {key}\"\n                    else:\n                        torch.testing.assert_close(cur_val, ref_val, rtol=rtol, atol=atol)\n                elif isinstance(ref_val, (int, float, bool, str)):\n                    assert cur_val == ref_val, f\"Value mismatch for {key}: {cur_val} vs {ref_val}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"61b8cea3b42feab021d506e9143551de18f9165c\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
    "commit_short": "660470e5",
    "commit_subject": "[Core] Optimize evictor-v2 performance (#7193)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
    "files_changed": [
      "vllm/core/evictor_v2.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7193",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2860.7253444194794,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 8d59dbb00044",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 660470e5a36b8e52083615ad7c85e9b4fd4c72ce\nMessage: [Core] Optimize evictor-v2 performance (#7193)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import OrderedDict\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimization is in LRUEvictor\n        module_path = \"vllm.core.evictor_v2\"\n        symbol_name = \"LRUEvictor\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # LRU Evictor workload - simulate cache management scenario\n    # The optimization improves evict() by early-breaking and update() by using move_to_end\n    \n    # Create test data for LRU cache eviction\n    num_blocks = 10000  # Large number of blocks to stress the evictor\n    num_operations = 50000  # Mix of adds, updates, and evictions\n    \n    # Generate block metadata\n    blocks = []\n    for i in range(num_blocks):\n        block_id = i\n        content_hash = hash(f\"content_{i}\") & 0x7FFFFFFF\n        num_hashed_tokens = np.random.randint(1, 128)\n        last_accessed = float(i) / 1000.0  # Monotonic timestamps initially\n        blocks.append({\n            \"block_id\": block_id,\n            \"content_hash\": content_hash,\n            \"num_hashed_tokens\": num_hashed_tokens,\n            \"last_accessed\": last_accessed\n        })\n    \n    # Generate operation sequence (realistic cache access pattern)\n    operations = []\n    current_time = float(num_blocks) / 1000.0\n    \n    # Initial population\n    for block in blocks[:1000]:\n        operations.append({\n            \"type\": \"add\",\n            \"block_id\": block[\"block_id\"],\n            \"content_hash\": block[\"content_hash\"],\n            \"num_hashed_tokens\": block[\"num_hashed_tokens\"],\n            \"last_accessed\": block[\"last_accessed\"]\n        })\n    \n    # Mix of operations\n    np.random.seed(42)\n    for _ in range(num_operations):\n        op_type = np.random.choice([\"update\", \"evict\", \"add\", \"remove\"], p=[0.6, 0.2, 0.15, 0.05])\n        \n        if op_type == \"update\":\n            # Update a random existing block\n            block_id = np.random.randint(0, min(1000, len(blocks)))\n            current_time += 0.001\n            operations.append({\n                \"type\": \"update\",\n                \"block_id\": block_id,\n                \"last_accessed\": current_time\n            })\n        elif op_type == \"evict\":\n            operations.append({\"type\": \"evict\"})\n        elif op_type == \"add\":\n            # Add a new block if we have any left\n            if len(operations) < len(blocks):\n                idx = len([op for op in operations if op[\"type\"] == \"add\"])\n                if idx < len(blocks):\n                    block = blocks[idx]\n                    current_time += 0.001\n                    operations.append({\n                        \"type\": \"add\",\n                        \"block_id\": block[\"block_id\"],\n                        \"content_hash\": block[\"content_hash\"],\n                        \"num_hashed_tokens\": block[\"num_hashed_tokens\"],\n                        \"last_accessed\": current_time\n                    })\n        elif op_type == \"remove\":\n            # Remove a random block\n            block_id = np.random.randint(0, min(1000, len(blocks)))\n            operations.append({\n                \"type\": \"remove\",\n                \"block_id\": block_id\n            })\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float32,\n        \"hw_info\": hw_info,\n        \"blocks\": blocks,\n        \"operations\": operations\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    LRUEvictor, fq_name = resolve_target()\n    \n    # Create evictor instance\n    evictor = LRUEvictor()\n    \n    # Track results for equivalence checking\n    results = {\n        \"evicted_blocks\": [],\n        \"final_state\": {},\n        \"operation_count\": 0\n    }\n    \n    # Execute operations\n    for op in data[\"operations\"]:\n        try:\n            if op[\"type\"] == \"add\":\n                if op[\"block_id\"] not in evictor.free_table:\n                    evictor.add(\n                        op[\"block_id\"],\n                        op[\"content_hash\"],\n                        op[\"num_hashed_tokens\"],\n                        op[\"last_accessed\"]\n                    )\n            elif op[\"type\"] == \"update\":\n                if op[\"block_id\"] in evictor.free_table:\n                    evictor.update(op[\"block_id\"], op[\"last_accessed\"])\n            elif op[\"type\"] == \"evict\":\n                if len(evictor.free_table) > 0:\n                    evicted_id, evicted_hash = evictor.evict()\n                    results[\"evicted_blocks\"].append({\n                        \"block_id\": evicted_id,\n                        \"content_hash\": evicted_hash\n                    })\n            elif op[\"type\"] == \"remove\":\n                if op[\"block_id\"] in evictor.free_table:\n                    evictor.remove(op[\"block_id\"])\n            \n            results[\"operation_count\"] += 1\n            \n        except (ValueError, KeyError):\n            # Handle expected errors gracefully\n            pass\n    \n    # Capture final state\n    results[\"final_state\"] = {\n        \"num_blocks\": evictor.num_blocks,\n        \"block_ids\": list(evictor.free_table.keys())[:100]  # Sample for verification\n    }\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check evicted blocks match\n        assert len(current_result[\"evicted_blocks\"]) == len(reference_result[\"evicted_blocks\"]), \\\n            f\"Evicted block count mismatch: {len(current_result['evicted_blocks'])} vs {len(reference_result['evicted_blocks'])}\"\n        \n        for i, (curr, ref) in enumerate(zip(current_result[\"evicted_blocks\"], reference_result[\"evicted_blocks\"])):\n            assert curr[\"block_id\"] == ref[\"block_id\"], \\\n                f\"Evicted block ID mismatch at index {i}: {curr['block_id']} vs {ref['block_id']}\"\n            assert curr[\"content_hash\"] == ref[\"content_hash\"], \\\n                f\"Evicted block hash mismatch at index {i}: {curr['content_hash']} vs {ref['content_hash']}\"\n        \n        # Check final state\n        assert current_result[\"final_state\"][\"num_blocks\"] == reference_result[\"final_state\"][\"num_blocks\"], \\\n            f\"Final block count mismatch: {current_result['final_state']['num_blocks']} vs {reference_result['final_state']['num_blocks']}\"\n        \n        assert current_result[\"final_state\"][\"block_ids\"] == reference_result[\"final_state\"][\"block_ids\"], \\\n            \"Final block IDs mismatch\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-only operation (no GPU kernels involved)\n    warmup = 3\n    iters = 10\n    \n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"660470e5a36b8e52083615ad7c85e9b4fd4c72ce\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Evictor is CPU-only\n        \"dtype\": \"none\",  # No tensor operations\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
    "commit_short": "67da5720",
    "commit_subject": "[PERF] Speed up Qwen2.5-VL model by speed up rotar",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-7B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "5c04bb8b863bfdef8122b193631479315cc764f5",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.314018249511719e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "commit_short": "6a417b86",
    "commit_subject": "fix neuron performance issue (#13589)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "d3ea50113c08bdd3c5cfda42ec6ecbc72328d7d1",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3244.8352863788605,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.7.4.dev2+gd3ea5011",
    "human_version": "0.7.4.dev3+g6a417b86",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 1762.0,
    "baseline_ttft_median": 1179.08,
    "baseline_ttft_p99": 7150.52,
    "baseline_tpot_mean": 67.02,
    "baseline_tpot_median": 71.42,
    "baseline_tpot_p99": 99.17,
    "baseline_itl_mean": 67.14,
    "baseline_itl_median": 26.64,
    "baseline_itl_p99": 94.28,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 1160.36,
    "human_ttft_median": 1116.72,
    "human_ttft_p99": 1941.04,
    "human_tpot_mean": 30.05,
    "human_tpot_median": 29.64,
    "human_tpot_p99": 64.24,
    "human_itl_mean": 29.19,
    "human_itl_median": 26.19,
    "human_itl_p99": 82.18,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 1097.58,
    "agent_ttft_median": 1073.82,
    "agent_ttft_p99": 1850.36,
    "agent_tpot_mean": 27.85,
    "agent_tpot_median": 26.94,
    "agent_tpot_p99": 61.37,
    "agent_itl_mean": 26.58,
    "agent_itl_median": 23.54,
    "agent_itl_p99": 93.52,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 34.14528944381385,
    "human_improvement_tpot_mean": 55.162638018501944,
    "human_improvement_itl_mean": 56.52368185880251,
    "agent_improvement_ttft_mean": 37.70828603859251,
    "agent_improvement_tpot_mean": 58.44524022679797,
    "agent_improvement_itl_mean": 60.41108132260947,
    "agent_vs_human_ttft_mean": 5.410389879003066,
    "agent_vs_human_tpot_mean": 7.321131447587352,
    "agent_vs_human_itl_mean": 8.94141829393629,
    "human_improvement_ttft_median": 5.288869287919386,
    "human_improvement_ttft_p99": 72.85456162628732,
    "agent_improvement_ttft_median": 8.927299250262918,
    "agent_improvement_ttft_p99": 74.12272114475591,
    "agent_vs_human_ttft_median": 3.8416075650118287,
    "agent_vs_human_ttft_p99": 4.671722375633684,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  10.39     \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              9.62      \nOutput token throughput (tok/s):         1166.16   \nTotal Token throughput (tok/s):          6094.14   \n---------------Time to First Token----------------\nMean TTFT (ms):                          1762.00   \nMedian TTFT (ms):                        1179.08   \nP99 TTFT (ms):                           7150.52   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          67.02     \nMedian TPOT (ms):                        71.42     \nP99 TPOT (ms):                           99.17     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           67.14     \nMedian ITL (ms):                         26.64     \nP99 ITL (ms):                            94.28     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:30,  1.10it/s]\n  2%|\u258f         | 2/100 [00:01<01:04,  1.51it/s]\n  3%|\u258e         | 3/100 [00:01<00:40,  2.39it/s]\n  4%|\u258d         | 4/100 [00:07<04:01,  2.51s/it]\n  5%|\u258c         | 5/100 [00:08<03:18,  2.09s/it]\n  6%|\u258c         | 6/100 [00:08<02:19,  1.49s/it]\n  7%|\u258b         | 7/100 [00:09<01:59,  1.29s/it]\n  9%|\u2589         | 9/100 [00:09<01:03,  1.43it/s]\n 24%|\u2588\u2588\u258d       | 24/100 [00:10<00:09,  8.35it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:10<00:03, 19.02it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 63/100 [00:10<00:01, 34.54it/s]\n 88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 88/100 [00:10<00:00, 56.44it/s]\n100%|\u2588\u2588\u2588",
    "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  5.09      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              19.64     \nOutput token throughput (tok/s):         2379.82   \nTotal Token throughput (tok/s):          12436.51  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1160.36   \nMedian TTFT (ms):                        1116.72   \nP99 TTFT (ms):                           1941.04   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30.05     \nMedian TPOT (ms):                        29.64     \nP99 TPOT (ms):                           64.24     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.19     \nMedian ITL (ms):                         26.19     \nP99 ITL (ms):                            82.18     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:32,  1.07it/s]\n  2%|\u258f         | 2/100 [00:01<01:01,  1.60it/s]\n  4%|\u258d         | 4/100 [00:02<00:42,  2.26it/s]\n  5%|\u258c         | 5/100 [00:03<01:07,  1.41it/s]\n  6%|\u258c         | 6/100 [00:03<00:55,  1.70it/s]\n  7%|\u258b         | 7/100 [00:04<01:01,  1.52it/s]\n  8%|\u258a         | 8/100 [00:04<00:45,  2.03it/s]\n 21%|\u2588\u2588        | 21/100 [00:04<00:06, 12.21it/s]\n 39%|\u2588\u2588\u2588\u2589      | 39/100 [00:04<00:02, 28.76it/s]\n 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 60/100 [00:04<00:00, 50.45it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 79/100 [00:05<00:00, 70.52it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 19.64it/s]\n",
    "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  4.67      \nTotal input tokens:                      51200     \nTotal generated tokens:                  12116     \nRequest throughput (req/s):              21.40     \nOutput token throughput (tok/s):         2593.37   \nTotal Token throughput (tok/s):          13552.49  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1097.58   \nMedian TTFT (ms):                        1073.82   \nP99 TTFT (ms):                           1850.36   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          27.85     \nMedian TPOT (ms):                        26.94     \nP99 TPOT (ms):                           61.37     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.58     \nMedian ITL (ms):                         23.54     \nP99 ITL (ms):                            93.52     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:27,  1.14it/s]\n  2%|\u258f         | 2/100 [00:01<00:59,  1.65it/s]\n  3%|\u258e         | 3/100 [00:01<00:36,  2.64it/s]\n  4%|\u258d         | 4/100 [00:01<00:41,  2.32it/s]\n  5%|\u258c         | 5/100 [00:03<01:07,  1.40it/s]\n  6%|\u258c         | 6/100 [00:03<00:52,  1.79it/s]\n  7%|\u258b         | 7/100 [00:04<00:56,  1.63it/s]\n  9%|\u2589         | 9/100 [00:04<00:31,  2.89it/s]\n 28%|\u2588\u2588\u258a       | 28/100 [00:04<00:03, 19.49it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 50/100 [00:04<00:01, 41.55it/s]\n 69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 69/100 [00:04<00:00, 61.92it/s]\n 97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 97/100 [00:04<00:00, 96.07it/s]\n100%|\u2588\u2588\u2588",
    "test_script": null
  },
  {
    "commit_hash": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "commit_short": "6ce01f30",
    "commit_subject": "[Performance] Optimize `get_seqs` (#7051)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/core/block_manager_v1.py",
      "vllm/sequence.py",
      "vllm/transformers_utils/detokenizer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7051",
    "models": [
      "N/A"
    ],
    "parent_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2418.7523851394653,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 6a11fdfbb8d6",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6ce01f30667bbae33f112152e07a3b66b841078f\nMessage: [Performance] Optimize `get_seqs` (#7051)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target is SequenceGroup.get_seqs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.sequence\"\n        symbol_name = \"SequenceGroup\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import necessary classes\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm import SamplingParams\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create a realistic workload with multiple sequences\n    # This simulates a typical scenario with beam search or best_of sampling\n    num_sequences = 8  # Common beam width or best_of value\n    seq_len = 512  # Moderate sequence length\n    block_size = 16  # Standard block size\n    \n    # Create sequences with varying states to test different code paths\n    sequences = []\n    for i in range(num_sequences):\n        inputs = {\n            \"prompt\": f\"Test prompt {i}\",\n            \"prompt_token_ids\": list(range(100)),  # 100 prompt tokens\n            \"multi_modal_data\": {}\n        }\n        seq = Sequence(\n            seq_id=i,\n            inputs=inputs,\n            block_size=block_size,\n            eos_token_id=2\n        )\n        \n        # Set different statuses to test filtering\n        if i < 3:\n            seq.status = SequenceStatus.RUNNING\n        elif i < 5:\n            seq.status = SequenceStatus.WAITING\n        elif i < 7:\n            seq.status = SequenceStatus.SWAPPED\n        else:\n            seq.status = SequenceStatus.FINISHED_STOPPED\n        \n        # Add some output tokens to make sequences more realistic\n        for j in range(i * 10):  # Variable output lengths\n            seq.data._output_token_ids.append(100 + j)\n        \n        sequences.append(seq)\n    \n    # Create sampling parameters\n    sampling_params = SamplingParams(\n        temperature=0.7,\n        top_p=0.9,\n        max_tokens=100,\n        best_of=num_sequences\n    )\n    \n    # Create sequence group\n    seq_group = SequenceGroup(\n        request_id=\"test_request\",\n        seqs=sequences,\n        arrival_time=time.time(),\n        sampling_params=sampling_params\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"seq_group\": seq_group,\n        \"sequences\": sequences,\n        \"num_iterations\": 10000  # Number of get_seqs calls to make\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    seq_group = data[\"seq_group\"]\n    num_iterations = data[\"num_iterations\"]\n    \n    # Test the optimized get_seqs method with different patterns\n    results = {\n        \"all_seqs\": [],\n        \"running_seqs\": [],\n        \"waiting_seqs\": [],\n        \"swapped_seqs\": [],\n        \"finished_seqs\": []\n    }\n    \n    # Simulate realistic access patterns\n    for _ in range(num_iterations):\n        # Most common: get all sequences\n        all_seqs = seq_group.get_seqs()\n        results[\"all_seqs\"].append(len(all_seqs))\n        \n        # Filter by status (tests the optimization for filtered access)\n        running = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        results[\"running_seqs\"].append(len(running))\n        \n        waiting = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        results[\"waiting_seqs\"].append(len(waiting))\n        \n        # Less common statuses\n        if _ % 10 == 0:\n            swapped = seq_group.get_seqs(status=SequenceStatus.SWAPPED)\n            results[\"swapped_seqs\"].append(len(swapped))\n            \n            finished = seq_group.get_seqs(status=SequenceStatus.FINISHED_STOPPED)\n            results[\"finished_seqs\"].append(len(finished))\n    \n    # Also test related optimized methods\n    for _ in range(num_iterations // 10):\n        # Test property access that uses get_seqs internally\n        _ = seq_group.prompt\n        _ = seq_group.prompt_token_ids\n        _ = seq_group.multi_modal_data\n        \n        # Test methods that iterate over sequences\n        _ = seq_group.get_unfinished_seqs()\n        _ = seq_group.get_finished_seqs()\n        _ = seq_group.is_finished()\n        _ = seq_group.is_prefill()\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        \n        for key in current_result:\n            curr_val = current_result[key]\n            ref_val = reference_result[key]\n            \n            if isinstance(curr_val, list) and isinstance(ref_val, list):\n                # For list results, check they have same values\n                assert curr_val == ref_val, f\"Mismatch in {key}: {curr_val} vs {ref_val}\"\n            else:\n                assert curr_val == ref_val, f\"Mismatch in {key}\"\n    else:\n        assert current_result == reference_result, \"Results don't match\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-bound optimization (Python object manipulation)\n    warmup = 3\n    iters = 10\n    \n    # CPU timing\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6ce01f30667bbae33f112152e07a3b66b841078f\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU-bound optimization\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "commit_short": "6d0734c5",
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/envs.py",
      "vllm/model_executor/layers/fused_moe/config.py",
      "vllm/model_executor/layers/fused_moe/fused_moe.py",
      "vllm/model_executor/layers/quantization/fp8.py",
      "vllm/model_executor/layers/quantization/modelopt.py",
      "vllm/utils/flashinfer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/20645",
    "models": [
      "mistralai/Mistral-7B-Instruct-v0.3",
      "deepseek-ai/DeepSeek-R1"
    ],
    "parent_commit": "7d94577138e3d4c7bcfd781337ee1e5a2befa685",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 612.2444472312927,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.9.2rc2.dev356+g7d9457713",
    "human_version": "0.9.2rc2.dev357+g6d0734c56",
    "agent_version": null,
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 2194.88,
    "baseline_ttft_median": 2134.71,
    "baseline_ttft_p99": 3955.59,
    "baseline_tpot_mean": 83.26,
    "baseline_tpot_median": 34.96,
    "baseline_tpot_p99": 202.51,
    "baseline_itl_mean": 29.83,
    "baseline_itl_median": 15.53,
    "baseline_itl_p99": 201.71,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 2166.98,
    "human_ttft_median": 2284.63,
    "human_ttft_p99": 3906.37,
    "human_tpot_mean": 84.91,
    "human_tpot_median": 34.93,
    "human_tpot_p99": 199.6,
    "human_itl_mean": 29.78,
    "human_itl_median": 15.57,
    "human_itl_p99": 198.1,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 2194.78,
    "agent_ttft_median": 2303.24,
    "agent_ttft_p99": 3891.2,
    "agent_tpot_mean": 84.08,
    "agent_tpot_median": 35.13,
    "agent_tpot_p99": 203.75,
    "agent_itl_mean": 30.34,
    "agent_itl_median": 15.81,
    "agent_itl_p99": 199.64,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 1.2711401078874511,
    "human_improvement_tpot_mean": -1.9817439346624925,
    "human_improvement_itl_mean": 0.16761649346294724,
    "agent_improvement_ttft_mean": 0.004556057734359466,
    "agent_improvement_tpot_mean": -0.9848666826807508,
    "agent_improvement_itl_mean": -1.7096882333221641,
    "agent_vs_human_ttft_mean": -1.2828913972440994,
    "agent_vs_human_tpot_mean": 0.9775055941585188,
    "agent_vs_human_itl_mean": -1.8804566823371345,
    "human_improvement_ttft_median": -7.022967990968332,
    "human_improvement_ttft_p99": 1.2443150073693243,
    "agent_improvement_ttft_median": -7.894749169676431,
    "agent_improvement_ttft_p99": 1.627822903789329,
    "agent_vs_human_ttft_median": -0.8145739135002023,
    "agent_vs_human_ttft_p99": 0.38834007019304556,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 12-30 05:34:28 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b7e35086ac0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:34:35 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.96      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21507     \nRequest throughput (req/s):              50.36     \nOutput token throughput (tok/s):         3610.21   \nTotal Token throughput (tok/s):          29333.04  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2194.88   \nMedian TTFT (ms):                        2134.71   \nP99 TTFT (ms):                           3955.59   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          83.26     \nMedian TPOT (ms):                        34.96     \nP99 TPOT (ms):                           202.51    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.83     \nMedian ITL (ms):                         15.53     \nP99 ITL (ms):                            201.71    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
    "human_raw": "INFO 12-30 05:37:46 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b6271b863e0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:37:52 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  5.93      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21411     \nRequest throughput (req/s):              50.55     \nOutput token throughput (tok/s):         3607.82   \nTotal Token throughput (tok/s):          29428.88  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2166.98   \nMedian TTFT (ms):                        2284.63   \nP99 TTFT (ms):                           3906.37   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          84.91     \nMedian TPOT (ms):                        34.93     \nP99 TPOT (ms):                           199.60    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.78     \nMedian ITL (ms):                         15.57     \nP99 ITL (ms):                            198.10    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
    "agent_raw": "INFO 12-30 05:40:06 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b4cac46e340>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', max_concurrency=None, model='mistralai/Mistral-7B-Instruct-v0.3', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:40:12 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  6.00      \nTotal input tokens:                      153238    \nTotal generated tokens:                  21631     \nRequest throughput (req/s):              50.01     \nOutput token throughput (tok/s):         3605.53   \nTotal Token throughput (tok/s):          29147.81  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2194.78   \nMedian TTFT (ms):                        2303.24   \nP99 TTFT (ms):                           3891.20   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          84.08     \nMedian TPOT (ms):                        35.13     \nP99 TPOT (ms):                           203.75    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           30.34     \nMedian ITL (ms):                         15.81     \nP99 ITL (ms):                            199.64    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n/usr/local/lib/python3.11/site-packages/vllm/benchmarks/serve.py:948: FutureWarni",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6d0734c562e759fdb7076d762222b3881e62ab1f\nMessage: [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_fp8\"] = major >= 9  # Hopper+\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_fp8\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on commit analysis, target the new FP8 MoE function\n        module_path = \"vllm.model_executor.layers.fused_moe.fused_moe\"\n        symbol_name = \"flashinfer_fused_moe_blockscale_fp8\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # FP8 MoE workload configuration\n    device = torch.device(hw_info[\"device\"])\n    \n    # Use FP8 if supported, otherwise fall back to FP16\n    if hw_info.get(\"supports_fp8\", False):\n        dtype = torch.float8_e4m3fn\n        weight_dtype = torch.float8_e4m3fn\n    else:\n        dtype = torch.float16\n        weight_dtype = torch.float16\n    \n    # MoE configuration (based on typical Mixtral/DeepSeek models)\n    batch_size = 4\n    seq_len = 512  # Reduced for stable timing\n    hidden_size = 4096\n    intermediate_size = 14336\n    num_experts = 8\n    top_k = 2\n    num_expert_group = 2\n    topk_group = 2\n    local_num_experts = num_experts  # Single GPU case\n    expert_offset = 0\n    block_shape = [128, 128]  # Standard block size for FP8\n    \n    # Input hidden states\n    x = torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=torch.float16)\n    \n    # Router logits and bias\n    routing_logits = torch.randn(batch_size * seq_len, num_experts, device=device, dtype=torch.float32)\n    routing_bias = torch.randn(num_experts, device=device, dtype=torch.float32)\n    \n    # Expert weights (gate and up projections combined as w13)\n    # Shape: [num_experts, 2 * intermediate_size, hidden_size] for FP8\n    w13_weight = torch.randn(\n        num_experts, 2 * intermediate_size, hidden_size,\n        device=device, dtype=weight_dtype\n    )\n    \n    # Down projection weights\n    w2_weight = torch.randn(\n        num_experts, hidden_size, intermediate_size,\n        device=device, dtype=weight_dtype\n    )\n    \n    # FP8 scale factors (blockwise quantization)\n    num_blocks_w13 = (2 * intermediate_size * hidden_size) // (block_shape[0] * block_shape[1])\n    num_blocks_w2 = (hidden_size * intermediate_size) // (block_shape[0] * block_shape[1])\n    \n    w13_weight_scale_inv = torch.ones(\n        num_experts, math.ceil(num_blocks_w13 ** 0.5), math.ceil(num_blocks_w13 ** 0.5),\n        device=device, dtype=torch.float32\n    )\n    \n    w2_weight_scale_inv = torch.ones(\n        num_experts, math.ceil(num_blocks_w2 ** 0.5), math.ceil(num_blocks_w2 ** 0.5),\n        device=device, dtype=torch.float32\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"routing_logits\": routing_logits,\n        \"routing_bias\": routing_bias,\n        \"x\": x,\n        \"w13_weight\": w13_weight,\n        \"w13_weight_scale_inv\": w13_weight_scale_inv,\n        \"w2_weight\": w2_weight,\n        \"w2_weight_scale_inv\": w2_weight_scale_inv,\n        \"global_num_experts\": num_experts,\n        \"top_k\": top_k,\n        \"num_expert_group\": num_expert_group,\n        \"topk_group\": topk_group,\n        \"intermediate_size\": intermediate_size,\n        \"expert_offset\": expert_offset,\n        \"local_num_experts\": local_num_experts,\n        \"block_shape\": block_shape,\n        \"routed_scaling\": 1.0\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the FlashInfer FP8 MoE kernel\n    with torch.no_grad():\n        result = target(\n            routing_logits=data[\"routing_logits\"],\n            routing_bias=data[\"routing_bias\"],\n            x=data[\"x\"],\n            w13_weight=data[\"w13_weight\"],\n            w13_weight_scale_inv=data[\"w13_weight_scale_inv\"],\n            w2_weight=data[\"w2_weight\"],\n            w2_weight_scale_inv=data[\"w2_weight_scale_inv\"],\n            global_num_experts=data[\"global_num_experts\"],\n            top_k=data[\"top_k\"],\n            num_expert_group=data[\"num_expert_group\"],\n            topk_group=data[\"topk_group\"],\n            intermediate_size=data[\"intermediate_size\"],\n            expert_offset=data[\"expert_offset\"],\n            local_num_experts=data[\"local_num_experts\"],\n            block_shape=data[\"block_shape\"],\n            routed_scaling=data[\"routed_scaling\"]\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    if isinstance(data, dict) and \"data\" in data:\n        result = data[\"data\"]\n        if isinstance(result, torch.Tensor) and result.device.type == \"cpu\":\n            result = result.cuda()\n        return result\n    return data\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # FP8 operations have higher tolerance\n        if \"float8\" in str(current_result.dtype):\n            rtol, atol = 5e-2, 1e-2\n        elif current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if FP8 is supported\n    if not hw_info.get(\"supports_fp8\", False):\n        error_data = {\n            \"error_code\": 2,\n            \"error_name\": \"CAPABILITY_UNSUPPORTED\",\n            \"error_message\": \"FP8 not supported on this hardware\",\n            \"target_resolved\": True,\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6d0734c562e759fdb7076d762222b3881e62ab1f\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float8_e4m3fn\" if hw_info.get(\"supports_fp8\", False) else \"torch.float16\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
    "commit_short": "6d646d08",
    "commit_subject": "[Core] Optimize Async + Multi-step (#8050)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
    "files_changed": [
      "tests/multi_step/test_correctness_async_llm.py",
      "vllm/engine/async_llm_engine.py",
      "vllm/engine/llm_engine.py",
      "vllm/engine/output_processor/multi_step.py",
      "vllm/sequence.py",
      "vllm/worker/model_runner.py",
      "vllm/worker/multi_step_model_runner.py",
      "vllm/worker/multi_step_worker.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/8050",
    "models": [
      "meta-llama/Llama-3-8B-Instruct"
    ],
    "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3837.9603555202484,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 95a178f86120",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6d646d08a2e0e73e83e313a5ae470c1f9e4f200e\nMessage: [Core] Optimize Async + Multi-step (#8050)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target _process_model_outputs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.engine.llm_engine\"\n        symbol_name = \"LLMEngine._process_model_outputs\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # This optimization targets async + multi-step execution\n    # We need to simulate the SchedulerContext and output processing\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Simulate multi-step decode workload\n    batch_size = 32  # Multiple concurrent requests\n    seq_len = 128     # Moderate sequence length for decode\n    hidden_size = 4096  # Typical LLM hidden size\n    vocab_size = 32000  # Typical vocab size\n    num_steps = 8       # Multi-step lookahead\n    \n    # Create mock data structures\n    from vllm.engine.llm_engine import SchedulerContext\n    from vllm.core.scheduler import SequenceGroupMetadata, SequenceData\n    from vllm.core.scheduler import SchedulerOutputs\n    from vllm.engine.async_llm_engine import SamplerOutput\n    import msgspec\n    \n    # Create mock sequence group metadata\n    seq_group_metadata_list = []\n    for i in range(batch_size):\n        seq_data = {\n            i: SequenceData(\n                _prompt_token_ids=np.array([1] * 64, dtype=np.int32),\n                _output_token_ids=np.array([2] * seq_len, dtype=np.int32)\n            )\n        }\n        \n        metadata = SequenceGroupMetadata(\n            request_id=f\"req_{i}\",\n            is_prompt=False,  # Decode phase\n            seq_data=seq_data,\n            sampling_params=None,\n            block_tables={i: list(range(16))},\n        )\n        seq_group_metadata_list.append(metadata)\n    \n    # Create mock scheduler outputs\n    scheduler_outputs = SchedulerOutputs(\n        scheduled_seq_groups=[],\n        num_batched_tokens=batch_size,\n        blocks_to_swap_in=[],\n        blocks_to_swap_out=[],\n        blocks_to_copy=[],\n        ignored_seq_groups=[],\n        num_lookahead_slots=num_steps - 1,\n        running_queue_size=batch_size,\n        preempted=0,\n        num_prefill_groups=0\n    )\n    \n    # Create mock sampler outputs for multi-step\n    sampler_outputs = []\n    for step in range(num_steps):\n        # Simulate logits and sampled tokens\n        logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n        sampled_token_ids = torch.randint(0, vocab_size, (batch_size,), device=device)\n        \n        sampler_output = SamplerOutput(\n            outputs=[],  # Will be populated during processing\n            sampled_token_ids=sampled_token_ids,\n            sampled_token_probs=None,\n            logprobs=None,\n            hidden_states=None,\n            prefill_hidden_states=None\n        )\n        sampler_outputs.append(sampler_output)\n    \n    # Create scheduler context\n    ctx = SchedulerContext()\n    ctx.seq_group_metadata_list = seq_group_metadata_list\n    ctx.scheduler_outputs = scheduler_outputs\n    \n    # Add outputs to queue (simulating async processing)\n    for i, output in enumerate(sampler_outputs):\n        is_async = True  # Async processing\n        is_last_step = (i == len(sampler_outputs) - 1)\n        ctx.output_queue.append(\n            ([output], seq_group_metadata_list, scheduler_outputs, is_async, is_last_step)\n        )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"ctx\": ctx,\n        \"num_steps\": num_steps,\n        \"batch_size\": batch_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    # Import the actual LLMEngine class\n    from vllm.engine.llm_engine import LLMEngine\n    from vllm.config import (ModelConfig, CacheConfig, ParallelConfig, \n                            SchedulerConfig, DeviceConfig, LoadConfig)\n    \n    # Create minimal engine configuration\n    model_config = ModelConfig(\n        model=\"facebook/opt-125m\",  # Small model for testing\n        tokenizer=\"facebook/opt-125m\",\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        dtype=data[\"dtype\"],\n        seed=42,\n    )\n    \n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        cache_dtype=\"auto\",\n    )\n    \n    parallel_config = ParallelConfig(\n        pipeline_parallel_size=1,\n        tensor_parallel_size=1,\n    )\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=2048,\n        max_num_seqs=128,\n        max_model_len=2048,\n        use_v2_block_manager=True,\n        num_scheduler_steps=data[\"num_steps\"],  # Multi-step\n    )\n    \n    device_config = DeviceConfig(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    load_config = LoadConfig()\n    \n    # Create mock engine with minimal setup\n    # We're testing _process_model_outputs specifically\n    class MockLLMEngine:\n        def __init__(self):\n            self.scheduler_contexts = [data[\"ctx\"]]\n            self.scheduler = [None]  # Mock scheduler\n            self.model_config = model_config\n            self.scheduler_config = scheduler_config\n            self.output_processor = None\n            self.process_request_outputs_callback = None\n            \n        def _process_model_outputs(self, ctx):\n            # This is the optimized method we're testing\n            # The optimization refactored this to take ctx directly\n            if len(ctx.output_queue) == 0:\n                return None\n            \n            # Process outputs from queue\n            results = []\n            while ctx.output_queue:\n                (outputs, seq_group_metadata_list, scheduler_outputs, \n                 is_async, is_last_step) = ctx.output_queue.popleft()\n                \n                # Simulate processing\n                for output in outputs:\n                    results.append(output)\n                    \n                if self.process_request_outputs_callback:\n                    self.process_request_outputs_callback(results)\n            \n            return results\n    \n    engine = MockLLMEngine()\n    \n    # Execute the optimized method\n    with torch.no_grad():\n        result = engine._process_model_outputs(data[\"ctx\"])\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store the number of processed outputs as a simple metric\n    if result is not None:\n        torch.save({\"type\": \"output_count\", \"data\": len(result)}, filepath)\n    else:\n        torch.save({\"type\": \"output_count\", \"data\": 0}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", 0)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For this optimization, we check that the same number of outputs are processed\n    current_count = len(current_result) if current_result else 0\n    ref_count = reference_result\n    \n    assert current_count == ref_count, f\"Output count mismatch: {current_count} vs {ref_count}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6d646d08a2e0e73e83e313a5ae470c1f9e4f200e\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "commit_short": "6dd94dbe",
    "commit_subject": "[perf] fix perf regression from #12253 (#12380)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
    "files_changed": [
      "vllm/worker/model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12380",
    "models": [
      "meta-llama/Meta-Llama-3-8B"
    ],
    "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 588.0005221366882,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 10:15:37 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797",
    "human_version": "INFO 01-02 10:17:55 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev365+g6dd94dbe",
    "agent_version": "INFO 01-02 10:20:45 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev364+g0e74d797c.d20220101",
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "wheel",
    "benchmark_type": null,
    "agent_error": "Agent benchmark produced no metrics",
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1349.4546385000026,
    "baseline_throughput": 204.7,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1022.5203391999951,
    "human_throughput": 255.9,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": 24.227142578383667,
    "human_improvement_throughput": 25.012212994626292,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 10:16:16 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-02 10:16:26 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
    "human_raw": "INFO 01-02 10:18:35 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-02 10:18:45 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
    "agent_raw": "INFO 01-02 10:21:25 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nINFO 01-02 10:21:34 config.py:520] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting t",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6dd94dbe94c1820a1e224cba65efcf0befa97995\nMessage: [perf] fix perf regression from #12253 (#12380)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the target is ModelInputForGPUBuilder\n        module_path = \"vllm.worker.model_runner\"\n        symbol_name = \"ModelInputForGPUBuilder\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    try:\n        from vllm.worker.model_runner import GPUModelRunnerBase\n        from vllm.config import VllmConfig, ModelConfig, CacheConfig, DeviceConfig, ParallelConfig, SchedulerConfig\n        from vllm.core.scheduler import SequenceGroupMetadata, SequenceData\n        from vllm import SamplingParams\n        import weakref\n    except ImportError as e:\n        print(json.dumps({\"error\": f\"Failed to import vLLM components: {e}\", \"target_resolved\": False}))\n        sys.exit(1)\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create minimal config for model runner\n    model_config = ModelConfig(\n        model=\"facebook/opt-125m\",  # Small model for testing\n        tokenizer=\"facebook/opt-125m\",\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        dtype=dtype,\n        seed=42,\n        max_model_len=2048,\n    )\n    \n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        cache_dtype=\"auto\",\n    )\n    \n    device_config = DeviceConfig(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    parallel_config = ParallelConfig(\n        pipeline_parallel_size=1,\n        tensor_parallel_size=1,\n    )\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=2048,\n        max_num_seqs=256,\n        max_model_len=2048,\n    )\n    \n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        device_config=device_config,\n        parallel_config=parallel_config,\n        scheduler_config=scheduler_config,\n    )\n    \n    # Create a mock runner with minimal setup\n    class MockRunner(GPUModelRunnerBase):\n        def __init__(self, vllm_config):\n            self.model_config = vllm_config.model_config\n            self.cache_config = vllm_config.cache_config\n            self.device_config = vllm_config.device_config\n            self.parallel_config = vllm_config.parallel_config\n            self.scheduler_config = vllm_config.scheduler_config\n            self.vllm_config = vllm_config\n            self.device = device\n            self.pin_memory = False\n            self.block_size = 16\n            self.sliding_window = None\n            self.lora_config = None\n            self.prompt_adapter_config = None\n            self.multi_modal_input_mapper = None\n            self.attn_backend = None\n            self.inter_data_cache = {}\n            self._model_input_cls = None\n    \n    runner = MockRunner(vllm_config)\n    \n    # Create sequence group metadata for testing\n    # Mix of prefill and decode requests to test the decode_only flag behavior\n    seq_groups = []\n    \n    # Create decode requests (should keep decode_only=True)\n    for i in range(5):\n        seq_data = SequenceData([1, 2, 3, 4, 5])\n        seq_data._num_computed_tokens = 4  # Simulate decode phase\n        seq_group = SequenceGroupMetadata(\n            request_id=f\"decode_{i}\",\n            is_prompt=False,\n            seq_data={i: seq_data},\n            sampling_params=SamplingParams(),\n            block_tables={i: [0, 1]},\n        )\n        seq_groups.append(seq_group)\n    \n    # Create prefill request (should set decode_only=False)\n    seq_data_prefill = SequenceData([1, 2, 3, 4, 5, 6, 7, 8])\n    seq_data_prefill._num_computed_tokens = 0  # Simulate prefill phase\n    seq_group_prefill = SequenceGroupMetadata(\n        request_id=\"prefill_0\",\n        is_prompt=True,\n        seq_data={100: seq_data_prefill},\n        sampling_params=SamplingParams(),\n        block_tables=None,\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"runner\": runner,\n        \"seq_groups_decode_only\": seq_groups,\n        \"seq_groups_mixed\": seq_groups + [seq_group_prefill],\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create builder instance with the mock runner\n    import weakref\n    builder = target(weakref.proxy(data[\"runner\"]))\n    \n    # Test both scenarios: decode-only and mixed batches\n    results = {}\n    \n    # Scenario 1: Decode-only batch\n    builder.prepare()\n    for seq_group in data[\"seq_groups_decode_only\"]:\n        builder.add_seq_group(seq_group)\n    results[\"decode_only_flag\"] = builder.decode_only\n    \n    # Scenario 2: Mixed batch (decode + prefill)\n    builder.prepare()\n    for seq_group in data[\"seq_groups_mixed\"]:\n        builder.add_seq_group(seq_group)\n    results[\"mixed_flag\"] = builder.decode_only\n    \n    # Measure the overhead of repeated prepare calls\n    prepare_times = []\n    for _ in range(100):\n        start = time.perf_counter()\n        builder.prepare()\n        end = time.perf_counter()\n        prepare_times.append(end - start)\n    \n    results[\"avg_prepare_time_ms\"] = np.mean(prepare_times) * 1000\n    results[\"prepare_overhead_ms\"] = np.std(prepare_times) * 1000\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Check that decode_only flag behavior is consistent\n    assert current_result[\"decode_only_flag\"] == reference_result[\"decode_only_flag\"], \\\n        f\"decode_only_flag mismatch: {current_result['decode_only_flag']} vs {reference_result['decode_only_flag']}\"\n    \n    assert current_result[\"mixed_flag\"] == reference_result[\"mixed_flag\"], \\\n        f\"mixed_flag mismatch: {current_result['mixed_flag']} vs {reference_result['mixed_flag']}\"\n    \n    # The timing may vary, but the flag behavior should be identical\n    # Decode-only batch should have decode_only=True\n    assert current_result[\"decode_only_flag\"] == True, \"Decode-only batch should have decode_only=True\"\n    # Mixed batch should have decode_only=False\n    assert current_result[\"mixed_flag\"] == False, \"Mixed batch should have decode_only=False\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - this is primarily a CPU operation\n    warmup = 3\n    iters = 100  # More iterations since this is a fast operation\n    \n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6dd94dbe94c1820a1e224cba65efcf0befa97995\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
    "commit_short": "6e36f4fa",
    "commit_subject": "improve chunked prefill performance",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/basic_correctness/test_chunked_prefill.py",
      "vllm/core/scheduler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7874",
    "models": [
      "N/A"
    ],
    "parent_commit": "dd2a6a82e3f41b4673b1dbb24b2e99230ea96981",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3063.732679128647,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for dd2a6a82e3f4",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 6e36f4fa6ce64619b9ea94c88a157f5783a63a65\nMessage: improve chunked prefill performance\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\nfrom dataclasses import dataclass, field\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, the target is the Scheduler class\n        module_path = \"vllm.core.scheduler\"\n        symbol_name = \"Scheduler\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Mock Classes for Testing\n# =======================\n@dataclass\nclass SequenceData:\n    \"\"\"Mock sequence data\"\"\"\n    prompt_token_ids: List[int] = field(default_factory=list)\n    output_token_ids: List[int] = field(default_factory=list)\n    \n    def get_len(self):\n        return len(self.prompt_token_ids) + len(self.output_token_ids)\n    \n    def get_num_computed_tokens(self):\n        return 0\n\nclass Sequence:\n    \"\"\"Mock sequence\"\"\"\n    def __init__(self, seq_id, prompt_tokens):\n        self.seq_id = seq_id\n        self.data = SequenceData(prompt_token_ids=prompt_tokens)\n        self.status = \"WAITING\"\n    \n    def get_num_new_tokens(self):\n        return len(self.data.prompt_token_ids)\n    \n    def is_finished(self):\n        return False\n\nclass SequenceGroup:\n    \"\"\"Mock sequence group\"\"\"\n    def __init__(self, request_id, seqs, is_prefill=True):\n        self.request_id = request_id\n        self.seqs = seqs\n        self._is_prefill = is_prefill\n        self.lora_int_id = 0\n        self.sampling_params = None\n        self.pooling_params = None\n        self.lora_request = None\n        self.prompt_adapter_request = None\n        self.multi_modal_data = None\n        self.state = None\n        self.metrics = None\n    \n    def is_prefill(self):\n        return self._is_prefill\n    \n    def get_seqs(self, status=None):\n        if status:\n            return [s for s in self.seqs if s.status == status]\n        return self.seqs\n    \n    def get_max_num_running_seqs(self):\n        return len(self.seqs)\n    \n    def is_encoder_decoder(self):\n        return False\n    \n    def get_encoder_seq(self):\n        return None\n    \n    def is_finished(self):\n        return all(s.is_finished() for s in self.seqs)\n    \n    def init_multi_step(self, num_scheduler_steps):\n        pass\n    \n    def maybe_set_first_scheduled_time(self, now):\n        pass\n\n@dataclass\nclass ScheduledSequenceGroup:\n    seq_group: SequenceGroup\n    token_chunk_size: int\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Create a mix of prefill and decode requests to test chunked prefill scheduling\n    num_prefill_requests = 8\n    num_decode_requests = 16\n    prefill_seq_len = 512  # Tokens per prefill request\n    \n    # Create prefill sequence groups\n    prefill_groups = []\n    for i in range(num_prefill_requests):\n        seq = Sequence(f\"prefill_{i}\", list(range(prefill_seq_len)))\n        seq.status = \"WAITING\"\n        group = SequenceGroup(f\"prefill_req_{i}\", [seq], is_prefill=True)\n        prefill_groups.append(group)\n    \n    # Create decode sequence groups (already running)\n    decode_groups = []\n    for i in range(num_decode_requests):\n        seq = Sequence(f\"decode_{i}\", [0])  # Single token for decode\n        seq.status = \"RUNNING\"\n        group = SequenceGroup(f\"decode_req_{i}\", [seq], is_prefill=False)\n        decode_groups.append(group)\n    \n    # Create swapped sequence groups\n    swapped_groups = []\n    for i in range(4):\n        seq = Sequence(f\"swapped_{i}\", [0])\n        seq.status = \"SWAPPED\"\n        group = SequenceGroup(f\"swapped_req_{i}\", [seq], is_prefill=False)\n        swapped_groups.append(group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32,\n        \"hw_info\": hw_info,\n        \"prefill_groups\": prefill_groups,\n        \"decode_groups\": decode_groups,\n        \"swapped_groups\": swapped_groups,\n        \"max_num_batched_tokens\": 2048,\n        \"max_num_seqs\": 256,\n        \"enable_chunking\": True,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Import necessary vLLM components\n    try:\n        from vllm.core.scheduler import Scheduler, SchedulingBudget\n        from vllm.core.scheduler import SchedulerPrefillOutputs, SchedulerSwappedInOutputs\n        from vllm.config import SchedulerConfig, CacheConfig\n    except ImportError as e:\n        # Fallback: simulate the scheduling behavior\n        return simulate_chunked_prefill_scheduling(data)\n    \n    # Create scheduler config\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=data[\"max_num_batched_tokens\"],\n        max_num_seqs=data[\"max_num_seqs\"],\n        max_model_len=2048,\n        chunked_prefill_enabled=data[\"enable_chunking\"],\n    )\n    \n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        cache_dtype=\"auto\",\n    )\n    \n    # Create scheduler instance\n    scheduler = Scheduler(\n        scheduler_config=scheduler_config,\n        cache_config=cache_config,\n        lora_config=None,\n    )\n    \n    # Add sequence groups to scheduler queues\n    for group in data[\"prefill_groups\"]:\n        scheduler.waiting.append(group)\n    \n    for group in data[\"decode_groups\"]:\n        scheduler.running.append(group)\n    \n    for group in data[\"swapped_groups\"]:\n        scheduler.swapped.append(group)\n    \n    # Execute the chunked prefill scheduling\n    with torch.no_grad():\n        result = scheduler._schedule_chunked_prefill()\n    \n    # Extract scheduling order for comparison\n    scheduled_order = []\n    for seq_group in result.scheduled_seq_groups:\n        scheduled_order.append({\n            \"request_id\": seq_group.seq_group.request_id,\n            \"is_prefill\": seq_group.seq_group.is_prefill(),\n            \"token_chunk_size\": seq_group.token_chunk_size,\n        })\n    \n    return {\n        \"scheduled_order\": scheduled_order,\n        \"num_prefill_groups\": result.num_prefill_groups,\n        \"num_batched_tokens\": result.num_batched_tokens,\n        \"preempted\": result.preempted,\n    }\n\ndef simulate_chunked_prefill_scheduling(data: Dict[str, Any]) -> Any:\n    \"\"\"Simulate the scheduling behavior when vLLM is not available.\"\"\"\n    \n    # Simulate the optimized scheduling order:\n    # 1. Decode requests first (from running)\n    # 2. Swapped-in decode requests\n    # 3. Swapped-in prefill requests  \n    # 4. Running prefill requests (chunked)\n    # 5. New prefill requests\n    \n    scheduled_order = []\n    \n    # Schedule decode requests first (optimization)\n    for group in data[\"decode_groups\"]:\n        scheduled_order.append({\n            \"request_id\": group.request_id,\n            \"is_prefill\": False,\n            \"token_chunk_size\": 1,\n        })\n    \n    # Schedule swapped requests\n    for group in data[\"swapped_groups\"]:\n        scheduled_order.append({\n            \"request_id\": group.request_id,\n            \"is_prefill\": group.is_prefill(),\n            \"token_chunk_size\": 1 if not group.is_prefill() else 512,\n        })\n    \n    # Schedule new prefill requests (chunked)\n    token_budget = data[\"max_num_batched_tokens\"] - len(data[\"decode_groups\"])\n    for group in data[\"prefill_groups\"]:\n        if token_budget > 0:\n            chunk_size = min(512, token_budget)\n            scheduled_order.append({\n                \"request_id\": group.request_id,\n                \"is_prefill\": True,\n                \"token_chunk_size\": chunk_size,\n            })\n            token_budget -= chunk_size\n    \n    return {\n        \"scheduled_order\": scheduled_order,\n        \"num_prefill_groups\": len([s for s in scheduled_order if s[\"is_prefill\"]]),\n        \"num_batched_tokens\": sum(s[\"token_chunk_size\"] for s in scheduled_order),\n        \"preempted\": 0,\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    \n    # Check that both results have the same structure\n    assert set(current_result.keys()) == set(reference_result.keys()), \\\n        f\"Result keys mismatch: {current_result.keys()} vs {reference_result.keys()}\"\n    \n    # Check scheduling order maintains decode-first priority\n    current_order = current_result[\"scheduled_order\"]\n    reference_order = reference_result[\"scheduled_order\"]\n    \n    # Verify decode requests are scheduled before prefills\n    def get_first_prefill_index(order):\n        for i, item in enumerate(order):\n            if item[\"is_prefill\"]:\n                return i\n        return len(order)\n    \n    current_first_prefill = get_first_prefill_index(current_order)\n    reference_first_prefill = get_first_prefill_index(reference_order)\n    \n    # The optimization should schedule decodes first\n    assert current_first_prefill > 0, \"No decode requests scheduled before prefills\"\n    \n    # Check numerical values\n    assert abs(current_result[\"num_batched_tokens\"] - reference_result[\"num_batched_tokens\"]) <= 512, \\\n        f\"Token count mismatch: {current_result['num_batched_tokens']} vs {reference_result['num_batched_tokens']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"6e36f4fa6ce64619b9ea94c88a157f5783a63a65\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "commit_short": "70b808fe",
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "63d635d17962377df089cdc9d4a2684f0b007208",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 4199.745561122894,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:09:07 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev364+g63d635d1",
    "human_version": "INFO 01-02 17:15:07 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev365+g70b808fe",
    "agent_version": null,
    "model": "Qwen/Qwen2-VL-7B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 59.81,
    "baseline_ttft_median": 57.77,
    "baseline_ttft_p99": 90.17,
    "baseline_tpot_mean": 10.38,
    "baseline_tpot_median": 10.18,
    "baseline_tpot_p99": 12.34,
    "baseline_itl_mean": 10.38,
    "baseline_itl_median": 9.9,
    "baseline_itl_p99": 22.12,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 58.71,
    "human_ttft_median": 57.56,
    "human_ttft_p99": 85.33,
    "human_tpot_mean": 10.25,
    "human_tpot_median": 10.16,
    "human_tpot_p99": 11.51,
    "human_itl_mean": 10.25,
    "human_itl_median": 9.89,
    "human_itl_p99": 24.46,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 58.19,
    "agent_ttft_median": 56.7,
    "agent_ttft_p99": 85.91,
    "agent_tpot_mean": 9.96,
    "agent_tpot_median": 9.89,
    "agent_tpot_p99": 11.2,
    "agent_itl_mean": 9.96,
    "agent_itl_median": 9.6,
    "agent_itl_p99": 20.83,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 1.8391573315499103,
    "human_improvement_tpot_mean": 1.2524084778420113,
    "human_improvement_itl_mean": 1.2524084778420113,
    "agent_improvement_ttft_mean": 2.708577161009872,
    "agent_improvement_tpot_mean": 4.046242774566473,
    "agent_improvement_itl_mean": 4.046242774566473,
    "agent_vs_human_ttft_mean": 0.8857094191790208,
    "agent_vs_human_tpot_mean": 2.8292682926829187,
    "agent_vs_human_itl_mean": 2.8292682926829187,
    "human_improvement_ttft_median": 0.3635104725636158,
    "human_improvement_ttft_p99": 5.367638904291897,
    "agent_improvement_ttft_median": 1.8521724078241306,
    "agent_improvement_ttft_p99": 4.7244094488189035,
    "agent_vs_human_ttft_median": 1.4940931202223755,
    "agent_vs_human_ttft_p99": -0.6797140513301281,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:12:59 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.87    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12590     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         121.20    \nTotal Token throughput (tok/s):          1107.01   \n---------------Time to First Token----------------\nMean TTFT (ms):                          59.81     \nMedian TTFT (ms):                        57.77     \nP99 TTFT (ms):                           90.17     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          10.38     \nMedian TPOT (ms):                        10.18     \nP99 TPOT (ms):                           12.34     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.38     \nMedian ITL (ms):                         9.90      \nP99 ITL (ms):                            22.12     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:15,  1.37s/it]\n  2%|\u258f         | 2/100 [00:02<01:49,  1.12s/it]\n  3%|\u258e         | 3/100 [00:02<01:11,  1.36it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:53,  1.77it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:08<01:06,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:02,  1.42it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.44it/s]\n 13%|\u2588\u258e        | 13/1",
    "human_raw": "INFO 01-02 17:17:13 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.83    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12590     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         121.26    \nTotal Token throughput (tok/s):          1107.53   \n---------------Time to First Token----------------\nMean TTFT (ms):                          58.71     \nMedian TTFT (ms):                        57.56     \nP99 TTFT (ms):                           85.33     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          10.25     \nMedian TPOT (ms):                        10.16     \nP99 TPOT (ms):                           11.51     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           10.25     \nMedian ITL (ms):                         9.89      \nP99 ITL (ms):                            24.46     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:16,  1.38s/it]\n  2%|\u258f         | 2/100 [00:02<01:49,  1.11s/it]\n  3%|\u258e         | 3/100 [00:02<01:11,  1.35it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:52,  1.77it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 10%|\u2588         | 10/100 [00:08<01:06,  1.36it/s]\n 11%|\u2588         | 11/100 [00:09<01:02,  1.43it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.44it/s]\n 13%|\u2588\u258e        | 13/1",
    "agent_raw": "INFO 01-02 17:21:24 [__init__.py:256] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='Qwen/Qwen2-VL-7B', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=1.0, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 1.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.79    \nTotal input tokens:                      102400    \nTotal generated tokens:                  12537     \nRequest throughput (req/s):              0.96      \nOutput token throughput (tok/s):         120.80    \nTotal Token throughput (tok/s):          1107.45   \n---------------Time to First Token----------------\nMean TTFT (ms):                          58.19     \nMedian TTFT (ms):                        56.70     \nP99 TTFT (ms):                           85.91     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          9.96      \nMedian TPOT (ms):                        9.89      \nP99 TPOT (ms):                           11.20     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           9.96      \nMedian ITL (ms):                         9.60      \nP99 ITL (ms):                            20.83     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:11,  1.33s/it]\n  2%|\u258f         | 2/100 [00:02<01:46,  1.09s/it]\n  3%|\u258e         | 3/100 [00:02<01:10,  1.38it/s]\n  4%|\u258d         | 4/100 [00:03<01:10,  1.36it/s]\n  6%|\u258c         | 6/100 [00:04<00:52,  1.78it/s]\n  7%|\u258b         | 7/100 [00:06<01:46,  1.14s/it]\n  8%|\u258a         | 8/100 [00:07<01:26,  1.06it/s]\n  9%|\u2589         | 9/100 [00:08<01:29,  1.02it/s]\n 11%|\u2588         | 11/100 [00:09<01:03,  1.39it/s]\n 12%|\u2588\u258f        | 12/100 [00:09<01:01,  1.43it/s]\n 13%|\u2588\u258e        | 13/100 [00:10<00:53,  1.61it/s]\n 14%|\u2588\u258d        | 14/1",
    "test_script": null
  },
  {
    "commit_hash": "7661e92ef85e552936195ae4b803e292b9a96776",
    "commit_short": "7661e92e",
    "commit_subject": "[Model] Optimize nemotron_h implementation (#19249)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model nvidia/Nemotron-4-340B-Instruct --dataset-name sharegpt --request-rate 1",
    "files_changed": [
      "vllm/model_executor/models/nemotron_h.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/19249",
    "models": [
      "nvidia/Nemotron-4-340B-Instruct"
    ],
    "parent_commit": "f168b85725202915b5719c62b46d310a608b13dd",
    "status": "exception",
    "gpu_config": "H100:8",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.457069396972656e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "nvidia/Nemotron-4-340B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 7661e92ef85e552936195ae4b803e292b9a96776\nMessage: [Model] Optimize nemotron_h implementation (#19249)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on diff, the main change is in NemotronHMLP\n        module_path = \"vllm.model_executor.models.nemotron_h\"\n        symbol_name = \"NemotronHMLP\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required config class\n    try:\n        from vllm.transformers_utils.configs.nemotron_h import NemotronHConfig\n    except ImportError:\n        # Fallback to creating a mock config with the needed attributes\n        class NemotronHConfig:\n            def __init__(self):\n                self.hidden_size = 4096\n                self.intermediate_size = 11008  # Typical for 7B models\n                self.mlp_bias = False\n                self.rms_norm_eps = 1e-5\n        \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Model configuration for realistic workload\n    config = NemotronHConfig()\n    config.hidden_size = 4096\n    config.intermediate_size = 11008\n    config.mlp_bias = False\n    \n    # Adjust workload for available memory\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = 2\n        seq_len = 1024\n    else:\n        batch_size = 4\n        seq_len = 2048\n    \n    # Create input tensor\n    hidden_states = torch.randn(\n        batch_size, seq_len, config.hidden_size,\n        device=device, dtype=dtype\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"config\": config,\n        \"hidden_states\": hidden_states,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    NemotronHMLP, fq_name = resolve_target()\n    \n    # Create MLP instance\n    mlp = NemotronHMLP(\n        config=data[\"config\"],\n        quant_config=None,\n        bias=data[\"config\"].mlp_bias,\n        prefix=\"test_mlp\"\n    )\n    \n    # Move to correct device and dtype\n    mlp = mlp.to(data[\"device\"])\n    if data[\"dtype\"] == torch.float16:\n        mlp = mlp.half()\n    \n    # Execute forward pass\n    with torch.no_grad():\n        result = mlp(data[\"hidden_states\"])\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        # Handle NaNs and Infs\n        if torch.isnan(current_result).any() or torch.isnan(reference_result).any():\n            assert torch.isnan(current_result).equal(torch.isnan(reference_result)), \"NaN position mismatch\"\n            mask = ~torch.isnan(current_result)\n            torch.testing.assert_close(\n                current_result[mask].cpu(),\n                reference_result[mask].cpu(),\n                rtol=rtol, atol=atol\n            )\n        else:\n            torch.testing.assert_close(\n                current_result.cpu(),\n                reference_result.cpu(),\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Create experiment function\n    def run_experiment():\n        return experiment(data)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(run_experiment, warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(run_experiment, warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"7661e92ef85e552936195ae4b803e292b9a96776\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "7c01f706418d593b3cf23d2ec9110dca7151c539",
    "commit_short": "7c01f706",
    "commit_subject": "[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/5974",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2673.7804181575775,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 51e971d39e12",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 7c01f706418d593b3cf23d2ec9110dca7151c539\nMessage: [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimization is in SequenceStatus.is_finished\n        module_path = \"vllm.sequence\"\n        symbol_name = \"SequenceStatus\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # The optimization is for SequenceStatus.is_finished which checks if a status is finished\n    # We need to create a workload that tests this method with various status values\n    \n    SequenceStatus, _ = resolve_target()\n    \n    # Create all possible status values to test\n    all_statuses = [\n        SequenceStatus.WAITING,\n        SequenceStatus.RUNNING,\n        SequenceStatus.SWAPPED,\n        SequenceStatus.FINISHED_STOPPED,\n        SequenceStatus.FINISHED_LENGTH_CAPPED,\n        SequenceStatus.FINISHED_ABORTED,\n        SequenceStatus.FINISHED_IGNORED,\n    ]\n    \n    # Create a large test set with repeated status checks to measure performance\n    # Simulate realistic usage patterns with more finished statuses (common in batch processing)\n    test_statuses = []\n    # 30% waiting/running/swapped, 70% finished (realistic for batch inference)\n    for _ in range(10000):\n        if np.random.random() < 0.3:\n            test_statuses.append(np.random.choice(all_statuses[:3]))\n        else:\n            test_statuses.append(np.random.choice(all_statuses[3:]))\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": None,  # Not applicable for this optimization\n        \"hw_info\": hw_info,\n        \"SequenceStatus\": SequenceStatus,\n        \"test_statuses\": test_statuses,\n        \"all_statuses\": all_statuses,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    SequenceStatus = data[\"SequenceStatus\"]\n    test_statuses = data[\"test_statuses\"]\n    \n    # The optimization is in the is_finished static method\n    # We'll call it many times to measure the performance improvement\n    results = []\n    for status in test_statuses:\n        result = SequenceStatus.is_finished(status)\n        results.append(result)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store as JSON since results are boolean values\n    import pickle\n    with open(filepath, 'wb') as f:\n        pickle.dump(result, f)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    import pickle\n    with open(filepath, 'rb') as f:\n        return pickle.load(f)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, list), f\"Expected list, got {type(current_result)}\"\n    assert isinstance(reference_result, list), f\"Expected list, got {type(reference_result)}\"\n    assert len(current_result) == len(reference_result), f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr == ref, f\"Mismatch at index {i}: {curr} vs {ref}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-only optimization (enum comparison)\n    warmup = 5\n    iters = 100\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"7c01f706418d593b3cf23d2ec9110dca7151c539\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pkl\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU-only optimization\n        \"dtype\": \"None\",  # Not applicable\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
    "commit_short": "80aa7e91",
    "commit_subject": "[Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "Dockerfile.cpu",
      "README.md",
      "docs/source/getting_started/cpu-installation.rst",
      "requirements-cpu.txt",
      "vllm/attention/backends/torch_sdpa.py",
      "vllm/attention/ops/ipex_attn.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4971",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2155.2908067703247,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for bd43973522ea",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 80aa7e91fcd547a7a1396f71b9bdce18e5c92245\nMessage: [Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target the new IPEX attention\n    if not (module_path and symbol_name):\n        module_path = \"vllm.attention.ops.ipex_attn\"\n        symbol_name = \"PagedAttention\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        # Fallback to regular PagedAttention if IPEX not available\n        try:\n            module_path = \"vllm.attention.ops.paged_attn\"\n            module = importlib.import_module(module_path)\n            target = getattr(module, \"PagedAttention\")\n            return target, f\"{module_path}.PagedAttention\"\n        except (ImportError, AttributeError) as e2:\n            error_data = {\n                \"target_resolved\": False,\n                \"error\": str(e),\n                \"attempted_module\": module_path,\n                \"attempted_symbol\": symbol_name\n            }\n            print(json.dumps(error_data))\n            sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # CPU-optimized workload for PagedAttention decode\n    device = torch.device(\"cpu\")  # Force CPU to test IPEX optimization\n    dtype = torch.float32  # CPU typically uses FP32\n    \n    # Decode workload parameters\n    batch_size = 32\n    num_heads = 32\n    head_size = 128\n    num_kv_heads = 32\n    block_size = 16\n    max_context_len = 1024\n    \n    # Create decode query\n    query = torch.randn(batch_size, num_heads * head_size, device=device, dtype=dtype)\n    \n    # Create KV cache\n    num_blocks = (max_context_len + block_size - 1) // block_size\n    # Shape: [2, num_blocks, block_size * num_kv_heads * head_size]\n    kv_cache_shape = (2, num_blocks * batch_size, block_size * num_kv_heads * head_size)\n    kv_cache = torch.randn(kv_cache_shape, device=device, dtype=dtype)\n    \n    # Split into key and value caches\n    key_cache = kv_cache[0].view(num_blocks * batch_size, num_kv_heads, block_size, head_size)\n    value_cache = kv_cache[1].view(num_blocks * batch_size, num_kv_heads, block_size, head_size)\n    \n    # Create block tables\n    block_tables = torch.arange(num_blocks, device=device, dtype=torch.int32).unsqueeze(0).repeat(batch_size, 1)\n    \n    # Context lengths\n    context_lens = torch.randint(512, max_context_len, (batch_size,), device=device, dtype=torch.int32)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"query\": query,\n        \"key_cache\": key_cache,\n        \"value_cache\": value_cache,\n        \"block_tables\": block_tables,\n        \"context_lens\": context_lens,\n        \"max_context_len\": max_context_len,\n        \"num_kv_heads\": num_kv_heads,\n        \"scale\": 1.0 / math.sqrt(head_size),\n        \"alibi_slopes\": None,\n        \"kv_scale\": 1.0,\n        \"kv_cache_dtype\": \"auto\",\n        \"block_size\": block_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call PagedAttention.forward_decode\n    with torch.no_grad():\n        result = target.forward_decode(\n            query=data[\"query\"],\n            key_cache=data[\"key_cache\"],\n            value_cache=data[\"value_cache\"],\n            block_tables=data[\"block_tables\"],\n            context_lens=data[\"context_lens\"],\n            max_context_len=data[\"max_context_len\"],\n            kv_cache_dtype=data[\"kv_cache_dtype\"],\n            num_kv_heads=data[\"num_kv_heads\"],\n            scale=data[\"scale\"],\n            alibi_slopes=data[\"alibi_slopes\"],\n            kv_scale=data[\"kv_scale\"]\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if IPEX is available\n    ipex_available = False\n    try:\n        import intel_extension_for_pytorch\n        ipex_available = True\n    except ImportError:\n        pass\n    \n    # Timing - always use CPU timing for this optimization\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"80aa7e91fcd547a7a1396f71b9bdce18e5c92245\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": ipex_available  # True if IPEX is available\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "83450458339b07765b0e72a822e5fe93eeaf5258",
    "commit_short": "83450458",
    "commit_subject": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150",
    "files_changed": [
      "vllm/spec_decode/ngram_worker.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/9333",
    "models": [
      "N/A"
    ],
    "parent_commit": "5b8a1fde84224e24ec121e0dc149d775330d911b",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 1181.7687542438507,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.dev22+g5b8a1fde.d20241016",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model='[ngram]', num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-02 16:18:48 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n`torch_dtype` is deprecated! Use `dtype` instead!\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 1247, in maybe_create_spec_config\n    raise ValueError(f\"{ngram_prompt_lookup_max=} must be > 0\")\nValueError: ngram_prompt_lookup_max=None must be > 0\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 83450458339b07765b0e72a822e5fe93eeaf5258\nMessage: [Performance][Spec Decode] Optimize ngram lookup performance (#9333)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom unittest.mock import MagicMock\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on commit diff\n        module_path = \"vllm.spec_decode.ngram_worker\"\n        symbol_name = \"NGramWorker\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create different sequence lengths to test the optimization\n    # Some sequences < 3072 (use CPU), some >= 3072 (use GPU)\n    test_configs = [\n        {\"seq_len\": 1024, \"batch_size\": 8},   # Small sequence (CPU path)\n        {\"seq_len\": 2048, \"batch_size\": 4},   # Medium sequence (CPU path)\n        {\"seq_len\": 4096, \"batch_size\": 2},   # Large sequence (GPU path)\n    ]\n    \n    vocab_size = 32000  # Typical LLM vocab size\n    sample_len = 5  # Number of tokens to speculate\n    \n    # Create mock execute model requests\n    execute_model_reqs = []\n    \n    for config in test_configs:\n        seq_len = config[\"seq_len\"]\n        batch_size = config[\"batch_size\"]\n        \n        # Create token sequences with repeating patterns for ngram matching\n        token_lists = []\n        for b in range(batch_size):\n            # Create sequence with repeating ngrams\n            base_pattern = torch.randint(0, vocab_size, (20,), dtype=torch.long)\n            tokens = base_pattern.repeat((seq_len // 20) + 1)[:seq_len]\n            # Insert the pattern again near the end for matching\n            tokens[-40:-20] = base_pattern\n            token_lists.append(tokens.tolist())\n        \n        # Create mock SequenceGroupMetadata\n        seq_group_metadata_list = []\n        for token_ids in token_lists:\n            # Mock sequence data\n            seq_data = MagicMock()\n            seq_data.get_token_ids.return_value = token_ids\n            seq_data.get_len.return_value = len(token_ids)\n            \n            # Mock sequence group metadata\n            seq_group_metadata = MagicMock()\n            seq_group_metadata.seq_data = {0: seq_data}\n            seq_group_metadata_list.append(seq_group_metadata)\n        \n        # Mock ExecuteModelRequest\n        execute_model_req = MagicMock()\n        execute_model_req.seq_group_metadata_list = seq_group_metadata_list\n        execute_model_req.blocks_to_swap_in = []\n        execute_model_req.blocks_to_swap_out = []\n        execute_model_req.blocks_to_copy = []\n        \n        execute_model_reqs.append(execute_model_req)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"execute_model_reqs\": execute_model_reqs,\n        \"sample_len\": sample_len,\n        \"vocab_size\": vocab_size,\n        \"ngram_prompt_lookup_min\": 1,\n        \"ngram_prompt_lookup_max\": 15,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    NGramWorker, fq_name = resolve_target()\n    \n    # Create NGramWorker instance\n    local_rank = 0 if data[\"hw_info\"][\"device\"] == \"cuda\" else -1\n    \n    # Mock model config\n    model_config = MagicMock()\n    model_config.get_vocab_size.return_value = data[\"vocab_size\"]\n    \n    # Create worker\n    worker = NGramWorker(local_rank=local_rank, model_config=model_config)\n    worker.set_ngram_window_size(\n        data[\"ngram_prompt_lookup_min\"],\n        data[\"ngram_prompt_lookup_max\"]\n    )\n    \n    # Initialize device\n    if data[\"hw_info\"][\"device\"] == \"cuda\":\n        worker.device = torch.device(\"cuda:0\")\n    else:\n        worker.device = torch.device(\"cpu\")\n    \n    # Mock proposer to avoid initialization issues\n    worker._proposer = MagicMock()\n    worker.vocab_size = data[\"vocab_size\"]\n    \n    results = []\n    \n    with torch.no_grad():\n        for execute_model_req in data[\"execute_model_reqs\"]:\n            # Call the optimized sampler_output method\n            outputs, transposed = worker.sampler_output(\n                execute_model_req,\n                data[\"sample_len\"],\n                set()  # seq_ids_with_bonus_token_in_last_step\n            )\n            results.append((outputs, transposed))\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store sampler outputs as structured data\n    stored_data = []\n    for outputs, transposed in result:\n        if outputs is None:\n            stored_data.append(None)\n        else:\n            batch_outputs = []\n            for output in outputs:\n                if output is None:\n                    batch_outputs.append(None)\n                else:\n                    batch_outputs.append({\n                        \"sampled_token_ids\": output.sampled_token_ids.cpu() if output.sampled_token_ids is not None else None,\n                        \"transposed\": transposed\n                    })\n            stored_data.append(batch_outputs)\n    \n    torch.save({\"type\": \"ngram_outputs\", \"data\": stored_data}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert len(current_result) == len(reference_result), f\"Result count mismatch\"\n    \n    for i, ((curr_outputs, curr_trans), ref_data) in enumerate(zip(current_result, reference_result)):\n        if curr_outputs is None:\n            assert ref_data is None, f\"Output {i}: Expected None, got data\"\n        else:\n            assert ref_data is not None, f\"Output {i}: Expected data, got None\"\n            assert len(curr_outputs) == len(ref_data), f\"Output {i}: Batch size mismatch\"\n            \n            for j, (curr_out, ref_out) in enumerate(zip(curr_outputs, ref_data)):\n                if curr_out is None:\n                    assert ref_out is None, f\"Output {i},{j}: Expected None\"\n                else:\n                    assert ref_out is not None, f\"Output {i},{j}: Expected data\"\n                    if curr_out.sampled_token_ids is not None and ref_out[\"sampled_token_ids\"] is not None:\n                        ref_tensor = ref_out[\"sampled_token_ids\"]\n                        if curr_out.sampled_token_ids.device.type == \"cuda\":\n                            ref_tensor = ref_tensor.cuda()\n                        torch.testing.assert_close(\n                            curr_out.sampled_token_ids,\n                            ref_tensor,\n                            rtol=0, atol=0  # Exact match for token IDs\n                        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 20\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"83450458339b07765b0e72a822e5fe93eeaf5258\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}ngram_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "886936837ca89e5645bc1f71cc0e1492b65b1590",
    "commit_short": "88693683",
    "commit_subject": "[Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion (#7209)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --enable-prefix-caching",
    "files_changed": [
      "vllm/core/evictor.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7209",
    "models": [
      "N/A"
    ],
    "parent_commit": "6d917d0eebd03990edf2443780a5f2506026ea78",
    "status": "version_bug",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 26.96428608894348,
    "error": "vLLM 0.6.4.post2.dev368+g6d917d0e has known port binding bug (issue #8791) - serving benchmarks not supported",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev368+g6d917d0e",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 886936837ca89e5645bc1f71cc0e1492b65b1590\nMessage: [Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion (#7209)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the primary target is LRUEvictor\n        module_path = \"vllm.core.evictor\"\n        symbol_name = \"LRUEvictor\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Realistic evictor workload simulating cache operations\n    # This represents a typical vLLM block cache scenario\n    \n    # Number of blocks to simulate (representing KV cache blocks)\n    num_blocks = 10000  # Large number to stress the evictor\n    num_operations = 50000  # Mix of add/update/evict operations\n    \n    # Generate block metadata\n    block_data = []\n    for i in range(num_blocks):\n        block_data.append({\n            \"block_id\": i,\n            \"content_hash\": i * 7919,  # Prime multiplier for hash\n            \"num_hashed_tokens\": np.random.randint(1, 129),  # Typical token counts\n            \"last_accessed\": float(i)  # Initial access times\n        })\n    \n    # Generate operations sequence (realistic cache usage pattern)\n    operations = []\n    current_time = float(num_blocks)\n    \n    # Initial population phase\n    for i in range(min(1000, num_blocks)):\n        operations.append({\n            \"type\": \"add\",\n            \"block_id\": block_data[i][\"block_id\"],\n            \"content_hash\": block_data[i][\"content_hash\"],\n            \"num_hashed_tokens\": block_data[i][\"num_hashed_tokens\"],\n            \"last_accessed\": block_data[i][\"last_accessed\"]\n        })\n    \n    # Mixed operations phase\n    np.random.seed(42)  # Ensure reproducibility\n    for _ in range(num_operations):\n        op_type = np.random.choice([\"add\", \"update\", \"evict\", \"remove\"], \n                                   p=[0.3, 0.4, 0.2, 0.1])\n        \n        if op_type == \"add\":\n            # Add new blocks\n            idx = np.random.randint(0, num_blocks)\n            operations.append({\n                \"type\": \"add\",\n                \"block_id\": block_data[idx][\"block_id\"],\n                \"content_hash\": block_data[idx][\"content_hash\"],\n                \"num_hashed_tokens\": block_data[idx][\"num_hashed_tokens\"],\n                \"last_accessed\": current_time\n            })\n            current_time += 0.1\n            \n        elif op_type == \"update\":\n            # Update access time for existing blocks\n            operations.append({\n                \"type\": \"update\",\n                \"block_id\": np.random.randint(0, num_blocks),\n                \"last_accessed\": current_time\n            })\n            current_time += 0.1\n            \n        elif op_type == \"evict\":\n            # Trigger eviction\n            operations.append({\n                \"type\": \"evict\"\n            })\n            \n        elif op_type == \"remove\":\n            # Remove specific blocks\n            operations.append({\n                \"type\": \"remove\",\n                \"block_id\": np.random.randint(0, num_blocks)\n            })\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float32  # Not GPU-specific, using float32\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"operations\": operations,\n        \"num_blocks\": num_blocks,\n        \"num_operations\": len(operations)\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    LRUEvictor, fq_name = resolve_target()\n    \n    # Create evictor instance\n    evictor = LRUEvictor()\n    \n    operations = data[\"operations\"]\n    results = []\n    \n    # Track blocks in evictor\n    blocks_in_evictor = set()\n    \n    # Execute operations\n    for op in operations:\n        try:\n            if op[\"type\"] == \"add\":\n                # Check if block already exists to avoid duplicates\n                if op[\"block_id\"] not in blocks_in_evictor:\n                    evictor.add(\n                        op[\"block_id\"],\n                        op[\"content_hash\"],\n                        op[\"num_hashed_tokens\"],\n                        op[\"last_accessed\"]\n                    )\n                    blocks_in_evictor.add(op[\"block_id\"])\n                    \n            elif op[\"type\"] == \"update\":\n                if op[\"block_id\"] in blocks_in_evictor:\n                    evictor.update(op[\"block_id\"], op[\"last_accessed\"])\n                    \n            elif op[\"type\"] == \"evict\":\n                if len(blocks_in_evictor) > 0:\n                    evicted_id, content_hash = evictor.evict()\n                    blocks_in_evictor.discard(evicted_id)\n                    results.append((\"evict\", evicted_id, content_hash))\n                    \n            elif op[\"type\"] == \"remove\":\n                if op[\"block_id\"] in blocks_in_evictor:\n                    evictor.remove(op[\"block_id\"])\n                    blocks_in_evictor.discard(op[\"block_id\"])\n                    \n        except (ValueError, KeyError):\n            # Handle expected errors gracefully\n            pass\n    \n    # Return summary statistics\n    return {\n        \"total_operations\": len(operations),\n        \"final_blocks\": evictor.num_blocks,\n        \"evictions\": len([r for r in results if r[0] == \"evict\"]),\n        \"results_sample\": results[:100]  # First 100 results for verification\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check key statistics\n        assert current_result[\"total_operations\"] == reference_result[\"total_operations\"]\n        assert current_result[\"final_blocks\"] == reference_result[\"final_blocks\"]\n        assert current_result[\"evictions\"] == reference_result[\"evictions\"]\n        \n        # Check sample results\n        current_sample = current_result.get(\"results_sample\", [])\n        ref_sample = reference_result.get(\"results_sample\", [])\n        assert len(current_sample) == len(ref_sample)\n        \n        for i, (curr, ref) in enumerate(zip(current_sample, ref_sample)):\n            assert curr == ref, f\"Mismatch at result {i}: {curr} vs {ref}\"\n    else:\n        assert current_result == reference_result\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_operation(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # CPU-based timing (evictor is not GPU-accelerated)\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu_operation(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"886936837ca89e5645bc1f71cc0e1492b65b1590\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Evictor runs on CPU\n        \"dtype\": \"torch.float32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "commit_short": "89a84b0b",
    "commit_subject": "[Core] Use array to speedup padding (#6779)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py",
      "vllm/model_executor/sampling_metadata.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/6779",
    "models": [
      "N/A"
    ],
    "parent_commit": "084a01fd3544557990f8af8af6fd3c1185bae848",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2147.820925951004,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 084a01fd3544",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen1.5-0.5B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 89a84b0bb7b30706a02836234a94493ea8f780bf\nMessage: [Core] Use array to speedup padding (#6779)\n\nThis script measures the actual performance impact of using arrays instead of lists\nfor token storage in vLLM's sampling metadata preparation.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom array import array\nfrom typing import Dict, Any, Tuple, Optional, List\nimport random\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.model_executor.sampling_metadata\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"SamplingTensors.from_lists\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Realistic vLLM workload parameters\n    batch_size = 32  # Number of sequences\n    vocab_size = 32000  # Llama vocab size\n    max_prompt_len = 2048\n    max_output_len = 512\n    \n    # Generate token lists that would be used in sampling\n    prompt_tokens = []\n    output_tokens = []\n    \n    for _ in range(batch_size):\n        # Generate varying length prompts and outputs\n        prompt_len = random.randint(128, max_prompt_len)\n        output_len = random.randint(1, max_output_len)\n        \n        # Use arrays as per the optimization\n        prompt_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(prompt_len)])\n        output_seq = array('l', [random.randint(0, vocab_size-1) for _ in range(output_len)])\n        \n        prompt_tokens.append(prompt_seq)\n        output_tokens.append(output_seq)\n    \n    # Other sampling parameters\n    temperatures = [0.7] * batch_size\n    top_ps = [0.9] * batch_size\n    top_ks = [40] * batch_size\n    min_ps = [0.0] * batch_size\n    presence_penalties = [0.0] * batch_size\n    frequency_penalties = [0.0] * batch_size\n    repetition_penalties = [1.0] * batch_size\n    sampling_seeds = [random.randint(0, 2**31-1) for _ in range(batch_size)]\n    sample_indices = list(range(batch_size))\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"temperatures\": temperatures,\n        \"top_ps\": top_ps,\n        \"top_ks\": top_ks,\n        \"min_ps\": min_ps,\n        \"presence_penalties\": presence_penalties,\n        \"frequency_penalties\": frequency_penalties,\n        \"repetition_penalties\": repetition_penalties,\n        \"sampling_seeds\": sampling_seeds,\n        \"sample_indices\": sample_indices,\n        \"prompt_tokens\": prompt_tokens,\n        \"output_tokens\": output_tokens,\n        \"vocab_size\": vocab_size,\n        \"extra_seeds_to_generate\": 0,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call SamplingTensors.from_lists with the prepared data\n    result = target(\n        temperatures=data[\"temperatures\"],\n        top_ps=data[\"top_ps\"],\n        top_ks=data[\"top_ks\"],\n        min_ps=data[\"min_ps\"],\n        presence_penalties=data[\"presence_penalties\"],\n        frequency_penalties=data[\"frequency_penalties\"],\n        repetition_penalties=data[\"repetition_penalties\"],\n        sampling_seeds=data[\"sampling_seeds\"],\n        sample_indices=data[\"sample_indices\"],\n        prompt_tokens=data[\"prompt_tokens\"],\n        output_tokens=data[\"output_tokens\"],\n        vocab_size=data[\"vocab_size\"],\n        extra_seeds_to_generate=data[\"extra_seeds_to_generate\"],\n        device=data[\"device\"],\n        dtype=data[\"dtype\"]\n    )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store the tensor attributes of SamplingTensors\n    tensors_dict = {\n        \"temperatures\": result.temperatures.cpu(),\n        \"top_ps\": result.top_ps.cpu(),\n        \"top_ks\": result.top_ks.cpu(),\n        \"min_ps\": result.min_ps.cpu(),\n        \"presence_penalties\": result.presence_penalties.cpu(),\n        \"frequency_penalties\": result.frequency_penalties.cpu(),\n        \"repetition_penalties\": result.repetition_penalties.cpu(),\n        \"prompt_tokens\": result.prompt_tokens.cpu(),\n        \"output_tokens\": result.output_tokens.cpu(),\n        \"sampling_seeds\": result.sampling_seeds.cpu(),\n        \"sample_indices\": result.sample_indices.cpu(),\n    }\n    torch.save({\"type\": \"sampling_tensors\", \"data\": tensors_dict}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Check each tensor attribute\n    attrs_to_check = [\n        \"temperatures\", \"top_ps\", \"top_ks\", \"min_ps\",\n        \"presence_penalties\", \"frequency_penalties\", \"repetition_penalties\",\n        \"prompt_tokens\", \"output_tokens\", \"sampling_seeds\", \"sample_indices\"\n    ]\n    \n    for attr in attrs_to_check:\n        current_tensor = getattr(current_result, attr).cpu()\n        ref_tensor = reference_result[attr]\n        \n        assert current_tensor.shape == ref_tensor.shape, f\"{attr} shape mismatch\"\n        assert current_tensor.dtype == ref_tensor.dtype, f\"{attr} dtype mismatch\"\n        \n        if current_tensor.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_tensor,\n            ref_tensor,\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This optimization primarily affects CPU operations (array vs list)\n    # so we time on CPU\n    warmup = 5\n    iters = 20\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"89a84b0bb7b30706a02836234a94493ea8f780bf\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization affects CPU operations\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "commit_short": "8a4e5c5f",
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "docs/design/v1/p2p_nccl_connector.md",
      "examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py",
      "vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py",
      "vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/20906",
    "models": [
      "N/A"
    ],
    "parent_commit": "76b494444fd864ffc53a623420668d1865c804b9",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3467.6758530139923,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.9.2rc2.dev305+g76b494444",
    "human_version": "0.9.2rc2.dev306+g8a4e5c5f3",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 898.64,
    "baseline_ttft_median": 843.28,
    "baseline_ttft_p99": 1408.63,
    "baseline_tpot_mean": 20.31,
    "baseline_tpot_median": 18.6,
    "baseline_tpot_p99": 48.61,
    "baseline_itl_mean": 18.32,
    "baseline_itl_median": 14.46,
    "baseline_itl_p99": 190.43,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 924.66,
    "human_ttft_median": 882.64,
    "human_ttft_p99": 1431.22,
    "human_tpot_mean": 20.54,
    "human_tpot_median": 18.61,
    "human_tpot_p99": 47.54,
    "human_itl_mean": 18.45,
    "human_itl_median": 14.58,
    "human_itl_p99": 193.75,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 908.59,
    "agent_ttft_median": 829.03,
    "agent_ttft_p99": 1411.53,
    "agent_tpot_mean": 20.49,
    "agent_tpot_median": 18.71,
    "agent_tpot_p99": 51.4,
    "agent_itl_mean": 18.23,
    "agent_itl_median": 14.23,
    "agent_itl_p99": 194.44,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": -2.8954865129529046,
    "human_improvement_tpot_mean": -1.132447070408668,
    "human_improvement_itl_mean": -0.7096069868995578,
    "agent_improvement_ttft_mean": -1.107228701148407,
    "agent_improvement_tpot_mean": -0.8862629246676501,
    "agent_improvement_itl_mean": 0.4912663755458508,
    "agent_vs_human_ttft_mean": 1.7379361062444507,
    "agent_vs_human_tpot_mean": 0.24342745861733553,
    "agent_vs_human_itl_mean": 1.192411924119235,
    "human_improvement_ttft_median": -4.6674888530499965,
    "human_improvement_ttft_p99": -1.6036858507911884,
    "agent_improvement_ttft_median": 1.6898301868892895,
    "agent_improvement_ttft_p99": -0.2058737922662348,
    "agent_vs_human_ttft_median": 6.073823982597664,
    "agent_vs_human_ttft_p99": 1.3757493606852933,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:47:44 [__init__.py:253] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2aee47f323e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 16:47:53 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.26      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12284     \nRequest throughput (req/s):              30.65     \nOutput token throughput (tok/s):         3764.88   \nTotal Token throughput (tok/s):          19426.36  \n---------------Time to First Token----------------\nMean TTFT (ms):                          898.64    \nMedian TTFT (ms):                        843.28    \nP99 TTFT (ms):                           1408.63   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.31     \nMedian TPOT (ms):                        18.60     \nP99 TPOT (ms):                           48.61     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           18.32     \nMedian ITL (ms):                         14.46     \nP99 ITL (ms):                            190.43    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:23,  1.4",
    "human_raw": "INFO 01-02 16:52:48 [__init__.py:253] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ac35163a3e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 16:52:58 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.30      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12133     \nRequest throughput (req/s):              30.27     \nOutput token throughput (tok/s):         3672.87   \nTotal Token throughput (tok/s):          19141.72  \n---------------Time to First Token----------------\nMean TTFT (ms):                          924.66    \nMedian TTFT (ms):                        882.64    \nP99 TTFT (ms):                           1431.22   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.54     \nMedian TPOT (ms):                        18.61     \nP99 TPOT (ms):                           47.54     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           18.45     \nMedian ITL (ms):                         14.58     \nP99 ITL (ms):                            193.75    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:26,  1.4",
    "agent_raw": "INFO 01-02 16:56:35 [__init__.py:253] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b2dee2fe3e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 16:56:44 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.26      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12234     \nRequest throughput (req/s):              30.66     \nOutput token throughput (tok/s):         3751.26   \nTotal Token throughput (tok/s):          19419.83  \n---------------Time to First Token----------------\nMean TTFT (ms):                          908.59    \nMedian TTFT (ms):                        829.03    \nP99 TTFT (ms):                           1411.53   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          20.49     \nMedian TPOT (ms):                        18.71     \nP99 TPOT (ms):                           51.40     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           18.23     \nMedian ITL (ms):                         14.23     \nP99 ITL (ms):                            194.44    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:23,  1.4",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532\nMessage: [V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, target the extract_kv_from_layer static method\n        module_path = \"vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_engine\"\n        symbol_name = \"P2pNcclEngine.extract_kv_from_layer\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # KV cache extraction workload\n    # Typical shapes for KV cache in vLLM with paged attention\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Configuration for 7B model KV cache\n    batch_size = 32\n    num_pages = 128  # Number of KV cache pages\n    page_size = 16   # Tokens per page\n    num_heads = 32   # Number of attention heads\n    head_dim = 128   # Dimension per head\n    \n    # Create test cases for both MLA and non-MLA scenarios\n    test_cases = []\n    \n    # Non-MLA case: shape is (2, num_pages, page_size, num_heads * head_dim)\n    # The 2 represents K and V caches\n    kv_shape_non_mla = (2, num_pages, page_size, num_heads * head_dim)\n    layer_non_mla = torch.randn(kv_shape_non_mla, device=device, dtype=dtype)\n    \n    # MLA case: shape is (num_pages, page_size, hidden_dim)\n    # In MLA, K and V are combined/compressed\n    hidden_dim = num_heads * head_dim\n    kv_shape_mla = (num_pages, page_size, hidden_dim)\n    layer_mla = torch.randn(kv_shape_mla, device=device, dtype=dtype)\n    \n    # Create slot mapping for extraction\n    # Simulating extraction of a subset of tokens\n    num_tokens_to_extract = batch_size * 64  # Extract 64 tokens per request\n    max_slots = num_pages * page_size\n    slot_mapping = torch.randint(0, max_slots, (num_tokens_to_extract,), \n                                device=device, dtype=torch.long)\n    \n    # Sort slot_mapping for better memory access pattern\n    slot_mapping, _ = torch.sort(slot_mapping)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"layer_non_mla\": layer_non_mla,\n        \"layer_mla\": layer_mla,\n        \"slot_mapping\": slot_mapping,\n        \"test_cases\": [\n            {\"is_mla\": False, \"layer\": layer_non_mla},\n            {\"is_mla\": True, \"layer\": layer_mla}\n        ]\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Execute extraction for both MLA and non-MLA cases\n    results = []\n    \n    for test_case in data[\"test_cases\"]:\n        is_mla = test_case[\"is_mla\"]\n        layer = test_case[\"layer\"]\n        slot_mapping = data[\"slot_mapping\"]\n        \n        with torch.no_grad():\n            # Call the static method\n            result = target(is_mla, layer, slot_mapping)\n            results.append(result)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, list) and all(isinstance(r, torch.Tensor) for r in result):\n        torch.save({\"type\": \"tensor_list\", \"data\": [r.cpu() for r in result]}, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, list) and isinstance(reference_result, list):\n        assert len(current_result) == len(reference_result), \\\n            f\"Result list length mismatch: {len(current_result)} vs {len(reference_result)}\"\n        \n        for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n            assert curr.shape == ref.shape, \\\n                f\"Shape mismatch at index {i}: {curr.shape} vs {ref.shape}\"\n            assert curr.dtype == ref.dtype, \\\n                f\"Dtype mismatch at index {i}: {curr.dtype} vs {ref.dtype}\"\n            \n            # Determine tolerances based on dtype\n            if curr.dtype in (torch.float16, torch.bfloat16):\n                rtol, atol = 1e-3, 1e-4\n            else:\n                rtol, atol = 1e-5, 1e-7\n            \n            torch.testing.assert_close(\n                curr.cpu(),\n                ref.cpu(),\n                rtol=rtol, atol=atol\n            )\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Ensure clean state\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Resolve target early to check if it exists\n    try:\n        target, fq_name = resolve_target()\n    except SystemExit:\n        raise\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "commit_short": "8aa1485f",
    "commit_subject": "[Perf] Disable chunked local attention by default with llama4 (#21761)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --trust-remote-code --max-model-len 16384",
    "files_changed": [
      "vllm/config.py",
      "vllm/envs.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21761",
    "models": [
      "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    ],
    "parent_commit": "89ac266b262f08d25ebf25fc66122d1b2367ae64",
    "status": "baseline_failed",
    "gpu_config": "H100:4",
    "benchmark_mode": "serving",
    "patch_type": null,
    "duration_s": 6572.790939331055,
    "error": "BASELINE server failed to start",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.1.dev149+g89ac266b2",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8\nMessage: [Perf] Disable chunked local attention by default with llama4 (#21761)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.config\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"VllmConfig\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    try:\n        from vllm.config import (\n            ModelConfig, CacheConfig, ParallelConfig, \n            SchedulerConfig, DeviceConfig, LoadConfig,\n            LoRAConfig, PromptAdapterConfig, SpeculativeConfig,\n            TokenizerPoolConfig, ObservabilityConfig, DecodingConfig\n        )\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block.utils import SequenceGroup\n        from vllm.compilation.backends import Sequence\n        from vllm.core.block_manager import SequenceStatus\n        from vllm.block import LogicalTokenBlock\n        from vllm import SamplingParams\n    except ImportError as e:\n        print(json.dumps({\n            \"target_resolved\": False,\n            \"error\": f\"Failed to import vLLM components: {e}\"\n        }))\n        sys.exit(1)\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create model config with chunked local attention enabled (llama4-style)\n    model_config = ModelConfig(\n        model=\"meta-llama/Llama-3.2-3B\",  # Use a Llama model\n        tokenizer=\"meta-llama/Llama-3.2-3B\",\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        dtype=dtype,\n        seed=42,\n        max_model_len=4096,\n        attention_chunk_size=1024,  # Enable chunked local attention\n        quantization=None,\n        enforce_eager=True,  # Disable CUDA graphs for testing\n        max_context_len_to_capture=None,\n        max_seq_len_to_capture=8192,\n        max_logprobs=20,\n        disable_sliding_window=False,\n        skip_tokenizer_init=True,\n        served_model_name=\"llama4-test\"\n    )\n    \n    # Create cache config\n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=dtype,\n        num_gpu_blocks_override=1024,  # Fixed number for testing\n        sliding_window=None,\n        enable_prefix_caching=False,\n        cpu_offload_gb=0\n    )\n    \n    # Create scheduler config\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=2048,\n        max_num_seqs=64,\n        max_model_len=4096,\n        enable_hybrid_kv_cache_manager=True,  # Will be toggled by environment\n        disable_hybrid_kv_cache_manager=False,  # Initial value\n        use_v2_block_manager=True,\n        num_lookahead_slots=0,\n        delay_factor=0.0,\n        enable_chunked_prefill=False,\n        is_multimodal_model=False,\n        send_delta_data=False,\n        policy=\"fcfs\",\n        use_async_output_proc=False,\n        multi_step_stream_outputs=False,\n        recompute_depth=0,\n        use_kv_compression=False\n    )\n    \n    # Create other configs\n    parallel_config = ParallelConfig(\n        pipeline_parallel_size=1,\n        tensor_parallel_size=1,\n        worker_use_ray=False,\n        max_parallel_loading_workers=None,\n        disable_custom_all_reduce=False,\n        tokenizer_pool_config=None,\n        ray_workers_use_nsight=False,\n        placement_group=None,\n        distributed_executor_backend=None\n    )\n    \n    device_config = DeviceConfig(device=\"cuda\" if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    \n    # Create sequences for scheduling test\n    num_sequences = 32\n    sequences = []\n    for i in range(num_sequences):\n        seq_id = i\n        prompt_tokens = list(range(100, 100 + 512))  # 512 prompt tokens\n        sampling_params = SamplingParams(\n            temperature=0.7,\n            top_p=0.95,\n            max_tokens=128\n        )\n        \n        seq = Sequence(\n            seq_id=seq_id,\n            inputs={\"prompt_token_ids\": prompt_tokens},\n            block_size=cache_config.block_size,\n            eos_token_id=2,\n            lora_request=None,\n            prompt_adapter_request=None\n        )\n        \n        seq_group = SequenceGroup(\n            request_id=f\"request_{i}\",\n            seqs=[seq],\n            arrival_time=time.time() - (num_sequences - i) * 0.1,\n            sampling_params=sampling_params,\n            lora_request=None,\n            trace_headers=None,\n            prompt_adapter_request=None,\n            encoder_seq=None,\n            priority=0\n        )\n        \n        sequences.append(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"model_config\": model_config,\n        \"cache_config\": cache_config,\n        \"scheduler_config\": scheduler_config,\n        \"parallel_config\": parallel_config,\n        \"device_config\": device_config,\n        \"sequences\": sequences,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    VllmConfig, _ = resolve_target()\n    \n    # Create VllmConfig with the configurations\n    # This will trigger the optimization logic in the commit\n    try:\n        vllm_config = VllmConfig(\n            model_config=data[\"model_config\"],\n            cache_config=data[\"cache_config\"],\n            parallel_config=data[\"parallel_config\"],\n            scheduler_config=data[\"scheduler_config\"],\n            device_config=data[\"device_config\"],\n            lora_config=None,\n            speculative_config=None,\n            load_config=LoadConfig(load_format=\"auto\"),\n            decoding_config=None,\n            observability_config=None,\n            prompt_adapter_config=None,\n            tokenizer_pool_config=None,\n            kv_transfer_config=None,\n            compilation_config=None\n        )\n        \n        # The optimization affects the scheduler configuration\n        # Check if hybrid KV cache manager was disabled due to chunked local attention\n        result = {\n            \"hybrid_kv_disabled\": vllm_config.scheduler_config.disable_hybrid_kv_cache_manager,\n            \"attention_chunk_size\": vllm_config.model_config.attention_chunk_size,\n            \"env_allow_chunked\": os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\") == \"1\"\n        }\n        \n        # Create scheduler to measure actual performance impact\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n        \n        # Create block manager\n        block_manager = BlockSpaceManagerV2(\n            block_size=data[\"cache_config\"].block_size,\n            num_gpu_blocks=data[\"cache_config\"].num_gpu_blocks_override,\n            num_cpu_blocks=0,\n            watermark=0.01,\n            sliding_window=None,\n            enable_caching=False,\n            hybrid_enabled=not vllm_config.scheduler_config.disable_hybrid_kv_cache_manager\n        )\n        \n        # Create scheduler\n        scheduler = Scheduler(\n            scheduler_config=vllm_config.scheduler_config,\n            cache_config=data[\"cache_config\"],\n            lora_config=None,\n            parallel_config=data[\"parallel_config\"],\n            pipeline_parallel_size=1,\n            output_proc_callback=None\n        )\n        \n        # Add sequences to scheduler\n        for seq_group in data[\"sequences\"]:\n            scheduler.add_seq_group(seq_group)\n        \n        # Run scheduling iterations\n        schedule_times = []\n        for _ in range(10):\n            start = time.perf_counter()\n            scheduler_outputs = scheduler.schedule()\n            end = time.perf_counter()\n            schedule_times.append((end - start) * 1000)\n        \n        result[\"avg_schedule_ms\"] = sum(schedule_times) / len(schedule_times)\n        result[\"scheduler_outputs\"] = len(scheduler_outputs.scheduled_seq_groups) if scheduler_outputs else 0\n        \n    except Exception as e:\n        # If there's an error, return minimal result\n        result = {\n            \"hybrid_kv_disabled\": data[\"scheduler_config\"].disable_hybrid_kv_cache_manager,\n            \"attention_chunk_size\": data[\"model_config\"].attention_chunk_size,\n            \"env_allow_chunked\": os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\") == \"1\",\n            \"error\": str(e)\n        }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # For configuration results, check key fields\n        assert current_result.get(\"attention_chunk_size\") == reference_result.get(\"attention_chunk_size\")\n        # The hybrid_kv_disabled flag may differ between commits (that's the optimization)\n        # So we don't assert equality on that field\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Always use CPU timing since this is a configuration/scheduling test\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": result.get(\"hybrid_kv_disabled\", False) if isinstance(result, dict) else True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
    "commit_short": "8bc68e19",
    "commit_subject": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      ".buildkite/test-pipeline.yaml",
      "examples/tensorize_vllm_model.py",
      "requirements-dev.txt",
      "setup.py",
      "tests/tensorizer_loader/tensorize_vllm_model_for_testing.py",
      "tests/tensorizer_loader/test_tensorizer.py",
      "vllm/engine/arg_utils.py",
      "vllm/envs.py",
      "vllm/model_executor/model_loader/loader.py",
      "vllm/model_executor/model_loader/tensorizer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4208",
    "models": [
      "N/A"
    ],
    "parent_commit": "0fca3cdcf265cd375bca684d951702b6b7adf65a",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 870.9689390659332,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 0fca3cdcf265",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd\nMessage: [Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nimport tempfile\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target is is_vllm_tensorized\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.model_loader.tensorizer\"\n        symbol_name = \"is_vllm_tensorized\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Mock Model Creation\n# =======================\nclass MockVLLMModel(nn.Module):\n    \"\"\"Mock vLLM model for testing serialization/detection.\"\"\"\n    def __init__(self, hidden_size=4096, num_layers=32):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        \n        # Create some realistic layers\n        self.embed_tokens = nn.Embedding(32000, hidden_size)\n        self.layers = nn.ModuleList([\n            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n        ])\n        self.norm = nn.LayerNorm(hidden_size)\n        self.lm_head = nn.Linear(hidden_size, 32000)\n    \n    def forward(self, x):\n        x = self.embed_tokens(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.norm(x)\n        return self.lm_head(x)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create a mock model and serialize it\n    model = MockVLLMModel(hidden_size=2048, num_layers=8)\n    model = model.to(device).to(dtype)\n    \n    # Create temporary file for serialized model\n    temp_file = tempfile.NamedTemporaryFile(suffix=\".tensors\", delete=False)\n    temp_path = temp_file.name\n    temp_file.close()\n    \n    # Import tensorizer components\n    try:\n        from tensorizer import TensorSerializer\n        from vllm.config import TensorizerConfig\n        \n        # Add vLLM marker to simulate new serialization method\n        model.register_parameter(\n            \"vllm_tensorized_marker\",\n            nn.Parameter(torch.tensor([1.0], device=device), requires_grad=False)\n        )\n        \n        # Serialize the model\n        with open(temp_path, \"wb\") as f:\n            serializer = TensorSerializer(f)\n            serializer.write_module(model)\n            serializer.close()\n        \n        # Create TensorizerConfig for detection\n        config = TensorizerConfig(\n            tensorizer_uri=temp_path,\n            vllm_tensorized=False  # Test auto-detection\n        )\n        \n        data = {\n            \"device\": device,\n            \"dtype\": dtype,\n            \"hw_info\": hw_info,\n            \"model\": model,\n            \"temp_path\": temp_path,\n            \"config\": config,\n        }\n        \n    except ImportError as e:\n        # Fallback if tensorizer not available\n        data = {\n            \"device\": device,\n            \"dtype\": dtype,\n            \"hw_info\": hw_info,\n            \"model\": None,\n            \"temp_path\": None,\n            \"config\": None,\n            \"error\": str(e)\n        }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # If setup failed, return early\n    if data.get(\"error\"):\n        return {\"detected\": False, \"error\": data[\"error\"]}\n    \n    # Call the auto-detection function\n    config = data[\"config\"]\n    \n    with torch.no_grad():\n        # The optimization is the automatic detection of vLLM-tensorized models\n        is_vllm_model = target(config)\n    \n    # Clean up temp file\n    if data[\"temp_path\"] and os.path.exists(data[\"temp_path\"]):\n        os.unlink(data[\"temp_path\"])\n    \n    return {\"detected\": is_vllm_model, \"config\": str(config.tensorizer_uri)}\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"detection_result\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For this optimization, we check that detection works correctly\n    assert isinstance(current_result, dict), f\"Result should be dict, got {type(current_result)}\"\n    assert isinstance(reference_result, dict), f\"Reference should be dict, got {type(reference_result)}\"\n    \n    # The detection result should be the same\n    if \"detected\" in current_result and \"detected\" in reference_result:\n        assert current_result[\"detected\"] == reference_result[\"detected\"], \\\n            f\"Detection mismatch: {current_result['detected']} vs {reference_result['detected']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if we can actually run the test\n    if data.get(\"error\"):\n        # Tensorizer not available, report gracefully\n        summary = {\n            \"impl_tag\": os.getenv(\"IMPL_TAG\", \"child\"),\n            \"commit_hash\": os.getenv(\"COMMIT_HASH\", \"8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd\"),\n            \"device\": str(hw_info[\"device\"]),\n            \"dtype\": \"torch.float32\",\n            \"iters\": 0,\n            \"warmup\": 0,\n            \"avg_ms\": 0.0,\n            \"p50_ms\": 0.0,\n            \"p95_ms\": 0.0,\n            \"eq_level\": \"skip\",\n            \"opt_path_hit\": False,\n            \"error\": \"tensorizer_not_available\"\n        }\n        print(json.dumps(summary))\n        return 0.0\n    \n    # Timing - this is primarily a CPU operation (model detection)\n    warmup = 3\n    iters = 20  # More iterations since this is fast\n    \n    # Time the detection operation\n    times = []\n    for _ in range(warmup):\n        _ = experiment(data)\n        # Recreate data for each warmup to ensure clean state\n        data = setup()\n    \n    for _ in range(iters):\n        data = setup()  # Fresh setup for each iteration\n        start = time.perf_counter()\n        result = experiment(data)\n        times.append((time.perf_counter() - start) * 1000)\n    \n    times.sort()\n    avg_ms = sum(times) / len(times)\n    p50_ms = times[len(times) // 2]\n    p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": result.get(\"detected\", False)\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "commit_short": "8c1e77fb",
    "commit_subject": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "files_changed": [
      "CMakeLists.txt"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/10742",
    "models": [
      "N/A"
    ],
    "parent_commit": "5fc5ce0fe45f974fc8840175e8321652238400f0",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 395.22964119911194,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev181+g5fc5ce0f",
    "human_version": "0.6.4.post2.dev182+g8c1e77fb",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": "Patch contains only non-Python files (skipped: ['CMakeLists.txt'])",
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1674.9860915333252,
    "baseline_throughput": 10117.1,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1655.657326799989,
    "human_throughput": 10206.3,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": 1.1539656855085934,
    "human_improvement_throughput": 0.8816755789702474,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-02 11:46:24 __init__.py:42] No plugins found.\nINFO 01-02 11:46:35 config.py:373] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-02 11:46:35 arg_utils.py:1057] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some",
    "human_raw": "Namespace(input_len=512, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-02 11:48:59 __init__.py:42] No plugins found.\nINFO 01-02 11:49:09 config.py:373] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\nWARNING 01-02 11:49:09 arg_utils.py:1057] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some",
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f\nMessage: [Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_flash_attn\"] = major >= 7\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_flash_attn\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Use vllm-flash-attn if available\n    if not (module_path and symbol_name):\n        module_path = \"vllm_flash_attn.flash_attn_interface\"\n        symbol_name = \"flash_attn_func\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        # Fallback to vllm's flash attention wrapper\n        try:\n            from vllm.attention.backends.flash_attn import FlashAttentionBackend\n            return FlashAttentionBackend, \"vllm.attention.backends.flash_attn.FlashAttentionBackend\"\n        except ImportError:\n            error_data = {\n                \"target_resolved\": False,\n                \"error\": str(e),\n                \"attempted_module\": module_path,\n                \"attempted_symbol\": symbol_name\n            }\n            print(json.dumps(error_data))\n            sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Flash attention workload - prefill scenario\n    batch_size = 4\n    seq_len = 2048\n    num_heads = 32\n    head_dim = 128\n    \n    # Adjust for hardware constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = 2\n        seq_len = 1024\n    \n    # Create Q, K, V tensors for attention\n    q = torch.randn(batch_size, seq_len, num_heads, head_dim, \n                    device=device, dtype=dtype, requires_grad=False)\n    k = torch.randn(batch_size, seq_len, num_heads, head_dim,\n                    device=device, dtype=dtype, requires_grad=False)\n    v = torch.randn(batch_size, seq_len, num_heads, head_dim,\n                    device=device, dtype=dtype, requires_grad=False)\n    \n    # Reshape to flash attention format (batch, seqlen, nheads, headdim)\n    q = q.contiguous()\n    k = k.contiguous()\n    v = v.contiguous()\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"q\": q,\n        \"k\": k,\n        \"v\": v,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_heads\": num_heads,\n        \"head_dim\": head_dim,\n        \"dropout_p\": 0.0,\n        \"softmax_scale\": 1.0 / math.sqrt(head_dim),\n        \"causal\": True,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Try to use flash attention directly\n    try:\n        from vllm_flash_attn.flash_attn_interface import flash_attn_func\n        \n        with torch.no_grad():\n            result = flash_attn_func(\n                data[\"q\"],\n                data[\"k\"],\n                data[\"v\"],\n                dropout_p=data[\"dropout_p\"],\n                softmax_scale=data[\"softmax_scale\"],\n                causal=data[\"causal\"]\n            )\n        return result\n    except ImportError:\n        pass\n    \n    # Fallback to standard scaled dot product attention\n    try:\n        with torch.no_grad():\n            # Use PyTorch's optimized SDPA which may use Flash Attention internally\n            result = torch.nn.functional.scaled_dot_product_attention(\n                data[\"q\"].transpose(1, 2),  # (batch, nheads, seqlen, headdim)\n                data[\"k\"].transpose(1, 2),\n                data[\"v\"].transpose(1, 2),\n                dropout_p=data[\"dropout_p\"],\n                is_causal=data[\"causal\"],\n                scale=data[\"softmax_scale\"]\n            )\n            # Transpose back to match flash attention output format\n            result = result.transpose(1, 2).contiguous()\n        return result\n    except Exception as e:\n        # Final fallback: manual attention computation\n        with torch.no_grad():\n            q = data[\"q\"].transpose(1, 2)\n            k = data[\"k\"].transpose(1, 2)\n            v = data[\"v\"].transpose(1, 2)\n            \n            scores = torch.matmul(q, k.transpose(-2, -1)) * data[\"softmax_scale\"]\n            \n            if data[\"causal\"]:\n                mask = torch.triu(torch.ones_like(scores, dtype=torch.bool), diagonal=1)\n                scores.masked_fill_(mask, float('-inf'))\n            \n            attn_weights = torch.softmax(scores, dim=-1)\n            result = torch.matmul(attn_weights, v)\n            result = result.transpose(1, 2).contiguous()\n        \n        return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if we can run flash attention\n    if not hw_info.get(\"supports_flash_attn\", False) and hw_info[\"device\"] == \"cuda\":\n        print(json.dumps({\n            \"error\": \"Flash attention requires GPU with compute capability >= 7.0\",\n            \"device\": str(hw_info[\"device\"]),\n            \"capability\": hw_info.get(\"capability\", (0, 0)),\n            \"target_resolved\": False,\n            \"opt_path_hit\": False\n        }))\n        sys.exit(2)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
    "commit_short": "8d75fe48",
    "commit_subject": "[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
    "files_changed": [
      "vllm/_custom_ops.py",
      "vllm/model_executor/layers/quantization/fp8.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/5183",
    "models": [
      "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "nm-testing/Meta-Llama-3-70B-Instruct-FP8",
      "nm-testing/Meta-Llama-3-8B-Instruct-FP8-KV"
    ],
    "parent_commit": "388596c91437a51d428a447594e9faec340c29b2",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 826.0435643196106,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
    "has_agent_patch": true,
    "baseline_install_method": "docker_fallback",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Server failed to start. Server output: most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 4, in <module>\n    from transformers.generation.logits_process import LogitsWarper, PrefixConstrainedLogitsProcessor\nImportError: cannot import name 'LogitsWarper' from 'transformers.generation.logits_process' (/usr/local/lib/python3.10/dist-packages/transformers/generation/logits_process.py)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 27, in <module>\n    from vllm.entrypoints.openai.serving_chat import OpenAIServingChat\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 25, in <module>\n    from vllm.model_executor.guided_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/__init__.py\", line 6, in <module>\n    from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/guided_decoding/lm_format_enforcer_decoding.py\", line 8, in <module>\n    from lmformatenforcer.integrations.vllm import (\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/vllm.py\", line 8, in <module>\n    from lmformatenforcer.integrations.transformers import build_token_enforcer_tokenizer_data\n  File \"/usr/local/lib/python3.10/dist-packages/lmformatenforcer/integrations/transformers.py\", line 7, in <module>\n    raise ImportError('transformers is not installed. Please install it with \"pip install transformers[torch]\"')\nImportError: transformers is not installed. Please install it with \"pip install transformers[torch]\"\n",
    "human_raw": "",
    "agent_raw": "",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8d75fe48ca5f46b7af0f5201d8500b9604eed769\nMessage: [Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)\n\nThis script measures the actual performance impact of switching from\ntorch._scaled_mm to vLLM's CUTLASS FP8 kernels.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_fp8\"] = major >= 9  # Hopper+\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_fp8\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target is cutlass_scaled_mm_dq\n    if not (module_path and symbol_name):\n        module_path = \"vllm._custom_ops\"\n        symbol_name = \"cutlass_scaled_mm_dq\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for FP8 GEMM optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # FP8 requires CUDA\n    if hw_info[\"device\"] != \"cuda\":\n        error_data = {\n            \"target_resolved\": True,\n            \"error\": \"FP8 operations require CUDA device\",\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Typical LLM linear layer dimensions (7B model)\n    batch_size = 8\n    seq_len = 2048\n    hidden_size = 4096\n    intermediate_size = 11008\n    \n    # Total tokens\n    m = batch_size * seq_len\n    n = intermediate_size\n    k = hidden_size\n    \n    # Create FP16 inputs for quantization\n    input_fp16 = torch.randn(m, k, device=device, dtype=torch.float16)\n    weight_fp16 = torch.randn(k, n, device=device, dtype=torch.float16)\n    \n    # Quantize to FP8\n    # Input quantization\n    input_scale = torch.tensor(input_fp16.abs().max() / 448.0, device=device, dtype=torch.float32)\n    a = (input_fp16 / input_scale).clamp(-448, 448).to(torch.float8_e4m3fn)\n    scale_a = input_scale\n    \n    # Weight quantization  \n    weight_scale = torch.tensor(weight_fp16.abs().max() / 448.0, device=device, dtype=torch.float32)\n    b = (weight_fp16 / weight_scale).clamp(-448, 448).to(torch.float8_e4m3fn)\n    scale_b = weight_scale\n    \n    # Output dtype\n    out_dtype = torch.float16\n    \n    data = {\n        \"device\": device,\n        \"dtype\": out_dtype,\n        \"hw_info\": hw_info,\n        \"a\": a,\n        \"b\": b,\n        \"scale_a\": scale_a,\n        \"scale_b\": scale_b,\n        \"out_dtype\": out_dtype,\n        \"m\": m,\n        \"n\": n,\n        \"k\": k\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized CUTLASS FP8 GEMM operation.\"\"\"\n    \n    # Get the cutlass_scaled_mm_dq function\n    target, fq_name = resolve_target()\n    \n    # Call the CUTLASS kernel\n    with torch.no_grad():\n        result = target(\n            a=data[\"a\"],\n            b=data[\"b\"],\n            scale_a=data[\"scale_a\"],\n            scale_b=data[\"scale_b\"],\n            out_dtype=data[\"out_dtype\"]\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # FP8 operations have higher tolerance\n        rtol, atol = 5e-2, 1e-2\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) if len(times_ms) > 1 else -1],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        # Should not reach here for FP8 tests\n        error_data = {\n            \"target_resolved\": True,\n            \"error\": \"FP8 operations require CUDA\",\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8d75fe48ca5f46b7af0f5201d8500b9604eed769\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Check if CUTLASS is available\n    opt_path_hit = True  # We're directly calling cutlass_scaled_mm_dq\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float16\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": opt_path_hit\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
    "commit_short": "9323a315",
    "commit_subject": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)",
    "repo": "vllm",
    "perf_command": "python benchmark_guided.py --model meta-llama/Llama-3.1-8B-Instruct --dataset xgrammar_bench --async-engine --output-len 512 --num-prompts 20 --enable-chunked-prefill --guided-decoding-ratio 1",
    "files_changed": [
      "docs/source/conf.py",
      "requirements-common.txt",
      "tests/entrypoints/llm/test_guided_generate.py",
      "tests/model_executor/test_guided_processors.py",
      "vllm/config.py",
      "vllm/engine/arg_utils.py",
      "vllm/engine/async_llm_engine.py",
      "vllm/engine/llm_engine.py",
      "vllm/engine/multiprocessing/client.py",
      "vllm/model_executor/guided_decoding/__init__.py",
      "vllm/model_executor/guided_decoding/xgrammar_decoding.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/10785",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "parent_commit": "3257d449fa0fd3e05aa20cc8c5fff79ad101984f",
    "status": "version_bug",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 26.236889839172363,
    "error": "vLLM 0.6.4.post2.dev218+g3257d449 has known port binding bug (issue #8791) - serving benchmarks not supported",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev218+g3257d449",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 9323a3153b20d4a2ca7ac04a2784609d6ce656e0\nMessage: [Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)\n\nThis script measures the actual performance impact of the XGrammar guided decoding optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use xgrammar_decoding\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.guided_decoding.xgrammar_decoding\"\n        symbol_name = \"get_local_xgrammar_guided_decoding_logits_processor\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Setup for guided decoding workload\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Typical model configuration\n    vocab_size = 32000  # Llama vocabulary size\n    batch_size = 8\n    \n    # Create a sample JSON schema for guided decoding\n    json_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"integer\"},\n            \"email\": {\"type\": \"string\", \"format\": \"email\"},\n            \"address\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"street\": {\"type\": \"string\"},\n                    \"city\": {\"type\": \"string\"},\n                    \"country\": {\"type\": \"string\"}\n                },\n                \"required\": [\"street\", \"city\", \"country\"]\n            }\n        },\n        \"required\": [\"name\", \"age\", \"email\", \"address\"]\n    }\n    \n    # Mock tokenizer for testing\n    from transformers import AutoTokenizer\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", use_fast=True)\n    except:\n        # Fallback to a simpler model if Llama is not available\n        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n        vocab_size = tokenizer.vocab_size\n    \n    # Mock model config\n    class MockModelConfig:\n        class HFConfig:\n            vocab_size = vocab_size\n        hf_config = HFConfig()\n    \n    # Create guided decoding parameters\n    from vllm.sampling_params import GuidedDecodingParams\n    guided_params = GuidedDecodingParams(\n        json=json_schema,\n        backend=\"xgrammar\"  # Use the new XGrammar backend\n    )\n    \n    # Create sample input tokens and logits\n    input_ids = [tokenizer.encode(\"The user information is: \")[-10:]] * batch_size\n    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"guided_params\": guided_params,\n        \"tokenizer\": tokenizer,\n        \"model_config\": MockModelConfig(),\n        \"input_ids\": input_ids,\n        \"logits\": logits,\n        \"batch_size\": batch_size,\n        \"vocab_size\": vocab_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Get the logits processor using XGrammar\n    processor = target(\n        guided_params=data[\"guided_params\"],\n        tokenizer=data[\"tokenizer\"],\n        model_config=data[\"model_config\"]\n    )\n    \n    # Apply the processor to the logits\n    result_logits = []\n    with torch.no_grad():\n        for i in range(data[\"batch_size\"]):\n            # Process each batch item\n            processed = processor(data[\"input_ids\"][i], data[\"logits\"][i])\n            result_logits.append(processed.clone())\n    \n    return torch.stack(result_logits)\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        # For guided decoding, we check that the masking is similar\n        # but allow for some differences in implementation\n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"9323a3153b20d4a2ca7ac04a2784609d6ce656e0\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66",
    "commit_short": "93e5f3c5",
    "commit_subject": "[Perf] Optimize Preparing Inputs for GPU Model Run",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "70363bccfac1a6a0818ea577ad9cf8123a0ec3ae",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 4522.116349697113,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:10:53 [__init__.py:239] Automatically detected platform cuda.\n0.8.3rc2.dev173+g70363bccf",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:14:22 [__init__.py:239] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
    "commit_short": "9474e89b",
    "commit_subject": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)",
    "repo": "vllm",
    "perf_command": "python benchmark_throughput_cache.py --backend vllm --model huggyllama/llama-7b --dataset ../data/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 2000",
    "files_changed": [
      "tests/core/test_block_manager.py",
      "tests/prefix_caching/test_prefix_caching.py",
      "vllm/core/block_manager.py",
      "vllm/core/evictor.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3357",
    "models": [
      "huggyllama/llama-7b"
    ],
    "parent_commit": "20478c4d3abcd0aa8a1d9ace9c76ea3a2e04cb5e",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 32.59974122047424,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 20478c4d3abc",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "huggyllama/llama-7b",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 9474e89ba4ecae253b585eb6b3e1d85f4e108f01\nMessage: [PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.core.block_manager\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"UncachedBlockAllocator\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Block allocator configuration\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Realistic vLLM block manager parameters\n    block_size = 16  # Tokens per block\n    num_blocks = 1024  # Number of physical blocks\n    \n    # Simulate typical allocation patterns\n    # Mix of prefill (large allocations) and decode (single block) patterns\n    allocation_patterns = []\n    \n    # Prefill requests (allocate multiple blocks at once)\n    for i in range(32):\n        # Simulate different prompt lengths\n        num_blocks_needed = (i % 8) + 2  # 2-9 blocks per request\n        allocation_patterns.append({\n            \"type\": \"prefill\",\n            \"blocks_needed\": num_blocks_needed,\n            \"request_id\": i\n        })\n    \n    # Decode requests (allocate single blocks)\n    for i in range(64):\n        allocation_patterns.append({\n            \"type\": \"decode\", \n            \"blocks_needed\": 1,\n            \"request_id\": 32 + i\n        })\n    \n    # Mixed pattern with frees\n    for i in range(32):\n        allocation_patterns.append({\n            \"type\": \"mixed\",\n            \"blocks_needed\": (i % 4) + 1,  # 1-4 blocks\n            \"request_id\": 96 + i\n        })\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"block_size\": block_size,\n        \"num_blocks\": num_blocks,\n        \"allocation_patterns\": allocation_patterns,\n        \"num_iterations\": 100  # Number of allocation/free cycles\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Import Device enum from vllm.utils\n    try:\n        from vllm.config import Device\n        device_enum = Device.GPU if data[\"device\"].type == \"cuda\" else Device.CPU\n    except ImportError:\n        # Fallback if Device enum not available\n        device_enum = 0 if data[\"device\"].type == \"cuda\" else 1\n    \n    block_size = data[\"block_size\"]\n    num_blocks = data[\"num_blocks\"]\n    patterns = data[\"allocation_patterns\"]\n    num_iterations = data[\"num_iterations\"]\n    \n    # Create allocator instance\n    allocator = target(device_enum, block_size, num_blocks)\n    \n    # Track allocated blocks for freeing\n    allocated_blocks = []\n    allocation_times = []\n    free_times = []\n    \n    # Run allocation/free cycles\n    for iteration in range(num_iterations):\n        # Allocation phase\n        pattern = patterns[iteration % len(patterns)]\n        blocks_to_allocate = pattern[\"blocks_needed\"]\n        \n        # Allocate blocks\n        iter_blocks = []\n        for _ in range(blocks_to_allocate):\n            if allocator.get_num_free_blocks() > 0:\n                start = time.perf_counter()\n                block = allocator.allocate()\n                end = time.perf_counter()\n                allocation_times.append(end - start)\n                iter_blocks.append(block)\n        \n        if iter_blocks:\n            allocated_blocks.append(iter_blocks)\n        \n        # Free some blocks periodically to maintain steady state\n        if len(allocated_blocks) > 10 and iteration % 5 == 0:\n            # Free oldest allocation\n            blocks_to_free = allocated_blocks.pop(0)\n            for block in blocks_to_free:\n                start = time.perf_counter()\n                allocator.free(block)\n                end = time.perf_counter()\n                free_times.append(end - start)\n    \n    # Free remaining blocks\n    for block_list in allocated_blocks:\n        for block in block_list:\n            start = time.perf_counter()\n            allocator.free(block)\n            end = time.perf_counter()\n            free_times.append(end - start)\n    \n    # Return timing statistics\n    result = {\n        \"num_allocations\": len(allocation_times),\n        \"num_frees\": len(free_times),\n        \"avg_alloc_us\": np.mean(allocation_times) * 1e6 if allocation_times else 0,\n        \"avg_free_us\": np.mean(free_times) * 1e6 if free_times else 0,\n        \"median_alloc_us\": np.median(allocation_times) * 1e6 if allocation_times else 0,\n        \"median_free_us\": np.median(free_times) * 1e6 if free_times else 0,\n        \"total_operations\": len(allocation_times) + len(free_times)\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For this performance test, we check that the allocator behavior is consistent\n    assert isinstance(current_result, dict), \"Result should be a dictionary\"\n    assert isinstance(reference_result, dict), \"Reference should be a dictionary\"\n    \n    # Check that we performed similar number of operations\n    current_ops = current_result.get(\"total_operations\", 0)\n    reference_ops = reference_result.get(\"total_operations\", 0)\n    \n    # Allow small variance in operation count due to allocation failures\n    assert abs(current_ops - reference_ops) <= 10, \\\n        f\"Operation count mismatch: {current_ops} vs {reference_ops}\"\n    \n    # Check that allocation/free counts are similar\n    assert abs(current_result[\"num_allocations\"] - reference_result[\"num_allocations\"]) <= 5\n    assert abs(current_result[\"num_frees\"] - reference_result[\"num_frees\"]) <= 5\n\n# =======================\n# Timing Implementation  \n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-bound operation (block allocation)\n    # Always use CPU timing\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"9474e89ba4ecae253b585eb6b3e1d85f4e108f01\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Block allocation is CPU-bound\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True,\n        \"total_operations\": result.get(\"total_operations\", 0),\n        \"avg_alloc_us\": result.get(\"avg_alloc_us\", 0),\n        \"avg_free_us\": result.get(\"avg_free_us\", 0)\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "commit_short": "98f47f2a",
    "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py",
    "files_changed": [
      "vllm/v1/attention/backends/flash_attn.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/10733",
    "models": [
      "N/A"
    ],
    "parent_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 443.7458448410034,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev182+g8c1e77fb",
    "human_version": "0.6.4.post2.dev183+g98f47f2a",
    "agent_version": null,
    "model": "unknown",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 258.8026435333319,
    "baseline_throughput": 972.5,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 262.09803716666516,
    "human_throughput": 972.5,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 264.7257783333373,
    "agent_throughput": 1023.7,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -1.2733230187847187,
    "human_improvement_throughput": 0.0,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": -2.2886685851192077,
    "agent_improvement_throughput": 5.264781491002575,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": -1.0025794908953045,
    "agent_vs_human_throughput": 5.264781491002575,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-01 06:55:25 __init__.py:42] No plugins found.\nWARNING 01-01 06:55:39 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
    "human_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-01 06:57:30 __init__.py:42] No plugins found.\nWARNING 01-01 06:57:43 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
    "agent_raw": "Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='facebook/opt-125m', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, worker_cls='auto')\nINFO 01-01 06:59:39 __init__.py:42] No plugins found.\nWARNING 01-01 06:59:51 arg_utils.py:1123] [DEPRECATED] Block manager v1 has been removed, and setting --use-v2-block-manager to True or False has no effect on vLLM behavior. Please remove --use-v2-block-manager in your engine argument. If your use case is not supported by SelfAttnBlockSpaceManager (i.e. bloc",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 98f47f2a4032f8c395268de80858c64ffcfc60fa\nMessage: [V1] Optimize the CPU overheads in FlashAttention custom op (#10733)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_flash_attn\"] = major >= 7\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_flash_attn\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, the optimization is in FlashAttentionImpl.forward\n        module_path = \"vllm.v1.attention.backends.flash_attn\"\n        symbol_name = \"FlashAttentionImpl\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # FlashAttention workload - prefill scenario\n    batch_size = 4\n    seq_len = 2048\n    num_heads = 32\n    head_size = 128\n    num_kv_heads = 32  # No GQA for this test\n    \n    # Adjust for hardware constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = 2\n        seq_len = 1024\n    \n    # Create attention inputs\n    num_tokens = batch_size * seq_len\n    query = torch.randn(num_tokens, num_heads * head_size, device=device, dtype=dtype)\n    key = torch.randn(num_tokens, num_kv_heads * head_size, device=device, dtype=dtype)\n    value = torch.randn(num_tokens, num_kv_heads * head_size, device=device, dtype=dtype)\n    \n    # KV cache setup for paged attention\n    block_size = 16\n    num_blocks = (seq_len + block_size - 1) // block_size * batch_size * 2\n    kv_cache = torch.zeros(2, num_blocks, block_size, num_kv_heads, head_size, \n                          device=device, dtype=dtype)\n    \n    # Create metadata\n    from vllm.attention.backends.dual_chunk_flash_attn import FlashAttentionMetadata\n    \n    # Query and sequence start locations\n    query_start_loc = torch.tensor([i * seq_len for i in range(batch_size + 1)], \n                                   device=device, dtype=torch.int32)\n    seq_start_loc = query_start_loc.clone()\n    \n    # Block table for paged attention\n    blocks_per_seq = (seq_len + block_size - 1) // block_size\n    block_table = torch.arange(batch_size * blocks_per_seq, device=device, dtype=torch.int32)\n    block_table = block_table.view(batch_size, blocks_per_seq)\n    \n    # Slot mapping\n    slot_mapping = torch.arange(num_tokens, device=device, dtype=torch.int32)\n    \n    attn_metadata = FlashAttentionMetadata(\n        num_actual_tokens=num_tokens,\n        max_query_len=seq_len,\n        query_start_loc=query_start_loc,\n        max_seq_len=seq_len,\n        seq_start_loc=seq_start_loc,\n        block_table=block_table,\n        slot_mapping=slot_mapping\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"query\": query,\n        \"key\": key,\n        \"value\": value,\n        \"kv_cache\": kv_cache,\n        \"attn_metadata\": attn_metadata,\n        \"num_heads\": num_heads,\n        \"head_size\": head_size,\n        \"num_kv_heads\": num_kv_heads,\n        \"scale\": 1.0 / math.sqrt(head_size),\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    FlashAttentionImpl, _ = resolve_target()\n    \n    # Create FlashAttentionImpl instance\n    impl = FlashAttentionImpl(\n        num_heads=data[\"num_heads\"],\n        head_size=data[\"head_size\"],\n        scale=data[\"scale\"],\n        num_kv_heads=data[\"num_kv_heads\"],\n        alibi_slopes=None,\n        sliding_window=None,\n        kv_cache_dtype=\"auto\",\n        blocksparse_params=None,\n        logits_soft_cap=None\n    )\n    \n    # Execute the forward pass\n    with torch.no_grad():\n        output = impl.forward(\n            query=data[\"query\"].clone(),\n            key=data[\"key\"].clone(),\n            value=data[\"value\"].clone(),\n            kv_cache=data[\"kv_cache\"].clone(),\n            attn_metadata=data[\"attn_metadata\"],\n            k_scale=1.0,\n            v_scale=1.0\n        )\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        # Move to CPU for comparison\n        current_cpu = current_result.cpu()\n        reference_cpu = reference_result.cpu()\n        \n        # Handle NaN and Inf\n        if torch.isnan(current_cpu).any() or torch.isnan(reference_cpu).any():\n            assert torch.isnan(current_cpu).equal(torch.isnan(reference_cpu)), \"NaN mismatch\"\n            mask = ~torch.isnan(current_cpu)\n            torch.testing.assert_close(\n                current_cpu[mask],\n                reference_cpu[mask],\n                rtol=rtol, atol=atol\n            )\n        else:\n            torch.testing.assert_close(\n                current_cpu,\n                reference_cpu,\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check hardware support\n    if not hw_info.get(\"supports_flash_attn\", False):\n        error_data = {\n            \"error_code\": 2,\n            \"error_name\": \"CAPABILITY_UNSUPPORTED\",\n            \"error_message\": \"FlashAttention requires GPU with compute capability >= 7.0\",\n            \"target_resolved\": True,\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"98f47f2a4032f8c395268de80858c64ffcfc60fa\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "commit_short": "99abb8b6",
    "commit_subject": "[V1][Spec Decode] Optimize Rejection Sampler with ",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 1000",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "3a1e6481586ed7f079275b5d5072a6e246af691e",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.314018249511719e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "9a3b88328f7e434cac35b90ee463de6689f9a833",
    "commit_short": "9a3b8832",
    "commit_subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct --dataset-name random --num-prompts 1000",
    "files_changed": [
      "vllm/model_executor/layers/rotary_embedding.py",
      "vllm/v1/worker/gpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/19939",
    "models": [
      "Qwen/Qwen2.5-VL-3B-Instruct"
    ],
    "parent_commit": "3014c920dae5a2360b9b4141395522cc52b59193",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3382.993711948395,
    "error": "BASELINE server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nINFO 01-02 16:45:38 [__init__.py:244] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 38, in <module>\n    from vllm.config import VllmConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 43, in <module>\n    from vllm.transformers_utils.config import (\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 33, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py\", line 28, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py\", line 76, in <module>\n    AutoConfig.register(\"aimv2\", AIMv2Config)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1081, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\nValueError: 'aimv2' is already used by a Transformers config, pick another name.\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.9.2.dev225+g3014c920d",
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen2.5-VL-3B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 9a3b88328f7e434cac35b90ee463de6689f9a833\nMessage: [PERF] Speedup of MRoPE prepare inputs (#19939)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on commit diff, the target is MRotaryEmbedding.get_next_input_positions_tensor\n        module_path = \"vllm.model_executor.layers.rotary_embedding\"\n        symbol_name = \"MRotaryEmbedding\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # MRoPE position preparation workload\n    # Simulating batched request processing in vLLM\n    batch_size = 64  # Number of concurrent requests\n    max_seq_len = 2048  # Maximum sequence length\n    num_iterations = 100  # Number of position updates to simulate\n    \n    # Pre-allocate arrays for new implementation\n    # 3 dimensions for MRoPE (temporal, height, width)\n    mrope_positions_np = np.zeros((3, max_seq_len), dtype=np.int64)\n    \n    # Generate random request parameters to simulate real workload\n    np.random.seed(42)\n    requests = []\n    for i in range(num_iterations):\n        req = {\n            'mrope_position_delta': np.random.randint(0, 100),\n            'context_len': np.random.randint(1, 1024),\n            'num_new_tokens': np.random.randint(1, 128),\n            'out_offset': np.random.randint(0, max_seq_len - 128)\n        }\n        requests.append(req)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.int64,\n        \"hw_info\": hw_info,\n        \"mrope_positions_np\": mrope_positions_np,\n        \"requests\": requests,\n        \"max_seq_len\": max_seq_len,\n        \"batch_size\": batch_size,\n        \"num_iterations\": num_iterations\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    MRotaryEmbedding, fq_name = resolve_target()\n    \n    # Check which version of the API we have\n    import inspect\n    method = MRotaryEmbedding.get_next_input_positions_tensor\n    sig = inspect.signature(method)\n    params = list(sig.parameters.keys())\n    \n    # New version has 'out' parameter, old version doesn't\n    is_new_api = 'out' in params\n    \n    results = []\n    \n    if is_new_api:\n        # New optimized implementation - in-place numpy operations\n        mrope_positions_np = data[\"mrope_positions_np\"].copy()\n        \n        for req in data[\"requests\"]:\n            MRotaryEmbedding.get_next_input_positions_tensor(\n                out=mrope_positions_np,\n                out_offset=req['out_offset'],\n                mrope_position_delta=req['mrope_position_delta'],\n                context_len=req['context_len'],\n                num_new_tokens=req['num_new_tokens']\n            )\n            # Store a snapshot of the relevant region\n            snapshot = mrope_positions_np[:, req['out_offset']:req['out_offset']+req['num_new_tokens']].copy()\n            results.append(snapshot)\n    else:\n        # Old implementation - creates new tensors\n        for req in data[\"requests\"]:\n            # Old API: returns a tensor\n            result_tensor = MRotaryEmbedding.get_next_input_positions_tensor(\n                mrope_position_delta=req['mrope_position_delta'],\n                context_len=req['context_len'],\n                seq_len=req['context_len'] + req['num_new_tokens']\n            )\n            # Convert to numpy for consistent comparison\n            if isinstance(result_tensor, torch.Tensor):\n                result_np = result_tensor.cpu().numpy()\n            else:\n                result_np = result_tensor\n            results.append(result_np)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert to serializable format\n    if isinstance(result, list) and all(isinstance(x, np.ndarray) for x in result):\n        # List of numpy arrays\n        torch.save({\n            \"type\": \"numpy_list\",\n            \"data\": [torch.from_numpy(arr.copy()) for arr in result]\n        }, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath, weights_only=True)\n    if data.get(\"type\") == \"numpy_list\":\n        # Convert back to numpy arrays\n        return [t.numpy() for t in data[\"data\"]]\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, list) and isinstance(reference_result, list):\n        assert len(current_result) == len(reference_result), f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n        \n        for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n            if isinstance(curr, np.ndarray) and isinstance(ref, np.ndarray):\n                # Compare numpy arrays\n                assert curr.shape == ref.shape, f\"Shape mismatch at index {i}: {curr.shape} vs {ref.shape}\"\n                assert curr.dtype == ref.dtype, f\"Dtype mismatch at index {i}: {curr.dtype} vs {ref.dtype}\"\n                \n                # For integer arrays, require exact match\n                if np.issubdtype(curr.dtype, np.integer):\n                    np.testing.assert_array_equal(curr, ref, err_msg=f\"Value mismatch at index {i}\")\n                else:\n                    np.testing.assert_allclose(curr, ref, rtol=1e-5, atol=1e-7, \n                                             err_msg=f\"Value mismatch at index {i}\")\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_operation(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations with perf_counter.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)] if len(times_ms) >= 100 else times_ms[-1],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU optimization (numpy operations)\n    warmup = 5\n    iters = 20  # More iterations since this is a fast CPU operation\n    \n    # Time the experiment\n    result, timing_stats = time_cpu_operation(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"9a3b88328f7e434cac35b90ee463de6689f9a833\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU optimization\n        \"dtype\": \"int64\",  # Position indices are integers\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),  # Integer arrays require exact match\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "commit_short": "9badee53",
    "commit_subject": "Fix performance when `--generation-config` is not ",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "beebf4742af80296d3c3a657c66d512615c550c1",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 1670.1454713344574,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 16:39:34 [__init__.py:253] Automatically detected platform cuda.\n0.7.4.dev208+gbeebf474",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:41:24 [__init__.py:253] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', max_concurrency=None, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1315, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 876, in main\n    input_requests = sample_sharegpt_requests(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 102, in sample_sharegpt_requests\n    with open(dataset_path, encoding='utf-8') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: 'ShareGPT_V3_unfiltered_cleaned_split.json'\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "9d72daf4ced05a5fec1ad8ea2914a39296f402da",
    "commit_short": "9d72daf4",
    "commit_subject": "[V1][Perf] Simpler request output queues (#15156)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "6dd55af6c9dde9174e0616739d783133f5e45d42",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2533.9087586402893,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 16:38:35 [__init__.py:239] Automatically detected platform cuda.\n0.8.3.dev20+g6dd55af6",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:43:11 [__init__.py:239] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "9ed82e7074a18e25680ab106fc846364ad97bc00",
    "commit_short": "9ed82e70",
    "commit_subject": "[Misc] Small perf improvements (#6520)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/core/block/test_block_manager_v2.py",
      "tests/core/block/test_cpu_gpu_block_allocator.py",
      "vllm/core/block/block_table.py",
      "vllm/core/block/prefix_caching_block.py",
      "vllm/model_executor/models/__init__.py",
      "vllm/sequence.py",
      "vllm/utils.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/6520",
    "models": [
      "N/A"
    ],
    "parent_commit": "51f8aa90ad409cc77bfab208be7f5907bf7d5330",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2275.3659660816193,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 51f8aa90ad40",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 9ed82e7074a18e25680ab106fc846364ad97bc00\nMessage: [Misc] Small perf improvements (#6520)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target BlockTable class\n    if not (module_path and symbol_name):\n        module_path = \"vllm.core.block.block_table\"\n        symbol_name = \"BlockTable\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import necessary classes\n    from vllm.core.block.naive_block import NaiveBlockAllocator\n    from vllm.core.block.block_table import BlockTable\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Typical block and sequence configurations\n    block_size = 16  # Common block size\n    num_blocks = 1024\n    \n    # Create a block allocator\n    allocator = NaiveBlockAllocator(\n        create_block=lambda prev, token_ids, block_size, allocator, block_id=None: None,\n        num_blocks=num_blocks,\n        block_size=block_size\n    )\n    \n    # Test with various sequence lengths and lookahead slots\n    test_cases = []\n    \n    # Different token_ids lengths to test\n    token_lengths = [128, 256, 512, 1024, 2048]\n    lookahead_slots = [0, 10, 20, 50]\n    \n    for token_len in token_lengths:\n        for lookahead in lookahead_slots:\n            # Create token_ids\n            token_ids = list(range(token_len))\n            \n            # Create BlockTable instance\n            block_table = BlockTable(\n                block_size=block_size,\n                block_allocator=allocator,\n                max_block_sliding_window=None\n            )\n            \n            # Simulate partial filling\n            num_full_slots = token_len // 2  # Simulate half-filled\n            block_table._num_full_slots = num_full_slots\n            \n            test_cases.append({\n                \"block_table\": block_table,\n                \"token_ids\": token_ids,\n                \"num_lookahead_slots\": lookahead,\n                \"block_size\": block_size\n            })\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"test_cases\": test_cases,\n        \"block_size\": block_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    results = []\n    \n    # Run get_num_blocks_touched_by_append_slots for each test case\n    for test_case in data[\"test_cases\"]:\n        block_table = test_case[\"block_table\"]\n        token_ids = test_case[\"token_ids\"]\n        num_lookahead_slots = test_case[\"num_lookahead_slots\"]\n        \n        # Call the optimized method\n        num_blocks = block_table.get_num_blocks_touched_by_append_slots(\n            token_ids, num_lookahead_slots\n        )\n        \n        results.append(num_blocks)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"list\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, list), f\"Expected list, got {type(current_result)}\"\n    assert isinstance(reference_result, list), f\"Expected list, got {type(reference_result)}\"\n    assert len(current_result) == len(reference_result), \\\n        f\"Length mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr == ref, f\"Mismatch at index {i}: {curr} vs {ref}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Always use CPU timing for this optimization\n    warmup = 10\n    iters = 1000  # More iterations since this is a fast operation\n    \n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"9ed82e7074a18e25680ab106fc846364ad97bc00\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization is CPU-bound\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": \"exact\",\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "commit_short": "a3223766",
    "commit_subject": "[Core] Optimize update checks in LogitsProcessor (#21245)",
    "repo": "vllm",
    "perf_command": "vllm bench serve --dataset-name random --model facebook/opt-125m --served-model-name facebook/opt-125m --random-input-len 700 --random-output-len 1 --endpoint /v1/completions --ignore-eos --host localhost --port 8000 --request-rate 200 --num-prompts 100",
    "files_changed": [
      "vllm/v1/sample/logits_processor.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21245",
    "models": [
      "N/A"
    ],
    "parent_commit": "bc8a8ce5ec374dd18e86f59be7cb0057a4b21992",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 329.321843624115,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.0rc2.dev36+gbc8a8ce5e",
    "human_version": "0.10.0rc2.dev37+ga32237665",
    "agent_version": null,
    "model": "facebook/opt-125m",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 35.75,
    "baseline_ttft_median": 32.32,
    "baseline_ttft_p99": 66.42,
    "baseline_tpot_mean": 0.0,
    "baseline_tpot_median": 0.0,
    "baseline_tpot_p99": 0.0,
    "baseline_itl_mean": 0.0,
    "baseline_itl_median": 0.0,
    "baseline_itl_p99": 0.0,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 33.52,
    "human_ttft_median": 29.98,
    "human_ttft_p99": 64.89,
    "human_tpot_mean": 0.0,
    "human_tpot_median": 0.0,
    "human_tpot_p99": 0.0,
    "human_itl_mean": 0.0,
    "human_itl_median": 0.0,
    "human_itl_p99": 0.0,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 30.78,
    "agent_ttft_median": 25.52,
    "agent_ttft_p99": 63.29,
    "agent_tpot_mean": 0.0,
    "agent_tpot_median": 0.0,
    "agent_tpot_p99": 0.0,
    "agent_itl_mean": 0.0,
    "agent_itl_median": 0.0,
    "agent_itl_p99": 0.0,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 6.237762237762229,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": 13.902097902097898,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": 8.174224343675423,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": 7.240099009900989,
    "human_improvement_ttft_p99": 2.303523035230354,
    "agent_improvement_ttft_median": 21.039603960396043,
    "agent_improvement_ttft_p99": 4.712436013249025,
    "agent_vs_human_ttft_median": 14.876584389593065,
    "agent_vs_human_ttft_p99": 2.4657112035752835,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 12-30 05:42:11 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b2e6eb4e8e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:42:17 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              192.74    \nOutput token throughput (tok/s):         192.74    \nTotal Token throughput (tok/s):          134917.17 \n---------------Time to First Token----------------\nMean TTFT (ms):                          35.75     \nMedian TTFT (ms):                        32.32     \nP99 TTFT (ms):                           66.42     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 10%|\u2588         | 10/100 [00:00<00:00, 96.06it",
    "human_raw": "INFO 12-30 05:44:06 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae159a728e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:44:12 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              193.28    \nOutput token throughput (tok/s):         193.28    \nTotal Token throughput (tok/s):          135295.36 \n---------------Time to First Token----------------\nMean TTFT (ms):                          33.52     \nMedian TTFT (ms):                        29.98     \nP99 TTFT (ms):                           64.89     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 11%|\u2588         | 11/100 [00:00<00:00, 103.16i",
    "agent_raw": "INFO 12-30 05:45:43 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2bab79e3e8e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 12-30 05:45:49 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: 200.0\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  0.52      \nTotal input tokens:                      69900     \nTotal generated tokens:                  100       \nRequest throughput (req/s):              193.00    \nOutput token throughput (tok/s):         193.00    \nTotal Token throughput (tok/s):          135099.76 \n---------------Time to First Token----------------\nMean TTFT (ms):                          30.78     \nMedian TTFT (ms):                        25.52     \nP99 TTFT (ms):                           63.29     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          0.00      \nMedian TPOT (ms):                        0.00      \nP99 TPOT (ms):                           0.00      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           0.00      \nMedian ITL (ms):                         0.00      \nP99 ITL (ms):                            0.00      \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n 10%|\u2588         | 10/100 [00:00<00:00, 98.53it",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: a32237665df876fcb51196dc209e8aff9fd89d29\nMessage: [Core] Optimize update checks in LogitsProcessor (#21245)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, we're optimizing update_state in two processor classes\n        module_path = \"vllm.v1.sample.logits_processor\"\n        symbol_name = \"LogitBiasLogitsProcessor\"  # We'll test both processors\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        \n        # Get both processor classes\n        logit_bias_processor = getattr(module, \"LogitBiasLogitsProcessor\")\n        min_tokens_processor = getattr(module, \"MinTokensLogitsProcessor\")\n        \n        # Also need the BatchUpdateRequest class\n        batch_update_cls = getattr(module, \"BatchUpdateRequest\")\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return (logit_bias_processor, min_tokens_processor, batch_update_cls), fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import necessary classes\n    try:\n        from vllm import SamplingParams\n    except ImportError:\n        # Fallback mock if module structure differs\n        class SamplingParams:\n            def __init__(self, logit_bias=None, min_tokens=None, all_stop_token_ids=None):\n                self.logit_bias = logit_bias\n                self.min_tokens = min_tokens\n                self.all_stop_token_ids = all_stop_token_ids or []\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float32  # Logits are typically float32\n    \n    # Typical vocab size for language models\n    vocab_size = 32000\n    batch_size = 128  # Large batch to stress the update logic\n    \n    # Create diverse set of requests to simulate real workload\n    requests_with_bias = []\n    requests_with_min_tokens = []\n    requests_without_special = []\n    \n    # 1/3 of requests have logit bias\n    for i in range(batch_size // 3):\n        # Create realistic logit bias dictionary\n        bias_dict = {j: np.random.randn() * 10 for j in range(100)}  # Bias 100 tokens\n        params = SamplingParams(logit_bias=bias_dict)\n        requests_with_bias.append((i, params, []))\n    \n    # 1/3 of requests have min_tokens constraint\n    for i in range(batch_size // 3, 2 * batch_size // 3):\n        params = SamplingParams(\n            min_tokens=np.random.randint(10, 100),\n            all_stop_token_ids=[0, 1, 2]  # EOS tokens\n        )\n        output_tokens = list(range(np.random.randint(0, 50)))  # Some tokens already generated\n        requests_with_min_tokens.append((i, params, output_tokens))\n    \n    # 1/3 of requests have no special requirements\n    for i in range(2 * batch_size // 3, batch_size):\n        params = SamplingParams()\n        requests_without_special.append((i, params, []))\n    \n    # Create removal indices - simulate removing some requests\n    removed_indices = list(range(0, batch_size, 5))  # Remove every 5th request\n    \n    # Create logits tensor\n    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"vocab_size\": vocab_size,\n        \"batch_size\": batch_size,\n        \"logits\": logits,\n        \"requests_with_bias\": requests_with_bias,\n        \"requests_with_min_tokens\": requests_with_min_tokens,\n        \"requests_without_special\": requests_without_special,\n        \"removed_indices\": removed_indices,\n        \"SamplingParams\": SamplingParams\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    targets, fq_name = resolve_target()\n    logit_bias_processor_cls, min_tokens_processor_cls, batch_update_cls = targets\n    \n    device = data[\"device\"]\n    dtype = data[\"dtype\"]\n    vocab_size = data[\"vocab_size\"]\n    batch_size = data[\"batch_size\"]\n    logits = data[\"logits\"].clone()  # Clone to avoid modification\n    \n    # Initialize processors\n    logit_bias_processor = logit_bias_processor_cls(\n        vocab_size=vocab_size,\n        max_batch_size=batch_size,\n        device=device,\n        dtype=dtype\n    )\n    \n    min_tokens_processor = min_tokens_processor_cls(\n        vocab_size=vocab_size,\n        max_batch_size=batch_size,\n        device=device,\n        dtype=dtype\n    )\n    \n    # Simulate batch updates - this is what we're optimizing\n    results = []\n    \n    # Test 1: Add requests with bias\n    batch_update_bias = batch_update_cls(\n        added=data[\"requests_with_bias\"],\n        removed=[]\n    )\n    logit_bias_processor.update_state(batch_update_bias)\n    \n    # Test 2: Add requests with min tokens\n    batch_update_min = batch_update_cls(\n        added=data[\"requests_with_min_tokens\"],\n        removed=[]\n    )\n    min_tokens_processor.update_state(batch_update_min)\n    \n    # Test 3: Remove some requests\n    batch_update_remove = batch_update_cls(\n        added=[],\n        removed=data[\"removed_indices\"]\n    )\n    logit_bias_processor.update_state(batch_update_remove)\n    min_tokens_processor.update_state(batch_update_remove)\n    \n    # Test 4: Replace requests (mix of operations)\n    # This tests the optimization where we check if pop() returns None\n    batch_update_replace = batch_update_cls(\n        added=data[\"requests_without_special\"],\n        removed=[]\n    )\n    logit_bias_processor.update_state(batch_update_replace)\n    min_tokens_processor.update_state(batch_update_replace)\n    \n    # Apply processors to logits (actual computation)\n    with torch.no_grad():\n        processed_logits_bias = logit_bias_processor(logits.clone())\n        processed_logits_min = min_tokens_processor(logits.clone())\n    \n    results = {\n        \"processed_logits_bias\": processed_logits_bias,\n        \"processed_logits_min\": processed_logits_min,\n        \"bias_state_size\": len(logit_bias_processor.biases),\n        \"min_tokens_state_size\": len(min_tokens_processor.min_toks)\n    }\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    elif isinstance(result, dict):\n        # Handle dict with tensors\n        saved_data = {}\n        for k, v in result.items():\n            if isinstance(v, torch.Tensor):\n                saved_data[k] = v.cpu()\n            else:\n                saved_data[k] = v\n        torch.save({\"type\": \"dict\", \"data\": saved_data}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        \n        for key in current_result:\n            curr_val = current_result[key]\n            ref_val = reference_result[key]\n            \n            if isinstance(curr_val, torch.Tensor):\n                assert curr_val.shape == ref_val.shape, f\"Shape mismatch for {key}\"\n                assert curr_val.dtype == ref_val.dtype, f\"Dtype mismatch for {key}\"\n                \n                # Determine tolerances based on dtype\n                if curr_val.dtype in (torch.float16, torch.bfloat16):\n                    rtol, atol = 1e-3, 1e-4\n                else:\n                    rtol, atol = 1e-5, 1e-7\n                \n                torch.testing.assert_close(\n                    curr_val.cpu(),\n                    ref_val.cpu(),\n                    rtol=rtol, atol=atol\n                )\n            else:\n                assert curr_val == ref_val, f\"Value mismatch for {key}: {curr_val} vs {ref_val}\"\n    \n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - this is primarily CPU work with some GPU tensor ops\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 20  # More iterations for CPU timing\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95)]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"a32237665df876fcb51196dc209e8aff9fd89d29\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "commit_short": "ac45c44d",
    "commit_subject": "[Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2",
    "files_changed": [
      "vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21837",
    "models": [
      "deepseek-ai/DeepSeek-V2",
      "deepseek-ai/DeepSeek-V3"
    ],
    "parent_commit": null,
    "status": "error",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3822.6821501255035,
    "error": null,
    "error_message": "Baseline server failed to start",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "serving",
    "agent_error": null,
    "patch_path": "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0058/model_patch.diff",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ac45c44d98e77f30e47b8fb69134f4635183070d\nMessage: [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, target is DeepEPHTPrepareAndFinalize.prepare\n        module_path = \"vllm.model_executor.layers.fused_moe.deepep_ht_prepare_finalize\"\n        symbol_name = \"DeepEPHTPrepareAndFinalize\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # MoE configuration for DeepSeek-like models\n    batch_size = 4\n    seq_len = 512  # Reduced for performance testing\n    hidden_size = 4096\n    num_experts = 8\n    top_k = 2\n    expert_intermediate_size = 14336\n    \n    # Create mock quantization config classes\n    class BlockQuantConfig:\n        def __init__(self, is_block=True):\n            self.is_block_quantized = is_block\n            self.per_act_token_quant = not is_block\n            self.quant_dtype = torch.int8\n            self.block_shape = (128, 128) if is_block else None\n    \n    # Create input tensors\n    hidden_states = torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype)\n    \n    # Router logits for expert selection\n    router_logits = torch.randn(batch_size * seq_len, num_experts, device=device, dtype=dtype)\n    \n    # Get top-k experts\n    topk_weights, topk_ids = torch.topk(router_logits, top_k, dim=-1)\n    topk_weights = torch.softmax(topk_weights, dim=-1)\n    \n    # Create expert weights (for MoE)\n    w1 = torch.randn(num_experts, hidden_size, expert_intermediate_size, device=device, dtype=dtype)\n    w2 = torch.randn(num_experts, expert_intermediate_size, hidden_size, device=device, dtype=dtype)\n    \n    # Mock scale parameters\n    a1_scale = None\n    a2_scale = None\n    \n    # Create mock config for block quantization (triggers the optimization)\n    quant_config_block = BlockQuantConfig(is_block=True)\n    # Create mock config for non-block (original path)\n    quant_config_orig = BlockQuantConfig(is_block=False)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_states\": hidden_states,\n        \"topk_weights\": topk_weights,\n        \"topk_ids\": topk_ids,\n        \"num_experts\": num_experts,\n        \"top_k\": top_k,\n        \"w1\": w1,\n        \"w2\": w2,\n        \"a1_scale\": a1_scale,\n        \"a2_scale\": a2_scale,\n        \"quant_config_block\": quant_config_block,\n        \"quant_config_orig\": quant_config_orig,\n        \"apply_router_weight_on_input\": True,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target_class, fq_name = resolve_target()\n    \n    # Instantiate the target class\n    try:\n        prepare_finalize = target_class()\n    except:\n        # Fallback: return mock result if class cannot be instantiated\n        return {\n            \"expert_x\": data[\"hidden_states\"].clone(),\n            \"expert_x_scale\": None,\n            \"opt_path_hit\": False\n        }\n    \n    # Use block quantization config to trigger the optimized path\n    quant_config = data[\"quant_config_block\"]\n    \n    with torch.no_grad():\n        try:\n            # Call the prepare method with the workload\n            result = prepare_finalize.prepare(\n                a1=data[\"hidden_states\"],\n                topk_ids=data[\"topk_ids\"],\n                topk_weights=data[\"topk_weights\"],\n                num_experts=data[\"num_experts\"],\n                a1_scale=data[\"a1_scale\"],\n                a2_scale=data[\"a2_scale\"],\n                quant_config=quant_config,\n                apply_router_weight_on_input=data[\"apply_router_weight_on_input\"]\n            )\n            \n            # Result is a tuple: (expert_x, expert_x_scale, ...)\n            if isinstance(result, tuple):\n                result = {\n                    \"expert_x\": result[0],\n                    \"expert_x_scale\": result[1] if len(result) > 1 else None,\n                    \"opt_path_hit\": True\n                }\n        except Exception as e:\n            # Fallback for missing dependencies\n            result = {\n                \"expert_x\": data[\"hidden_states\"].clone(),\n                \"expert_x_scale\": None,\n                \"opt_path_hit\": False,\n                \"error\": str(e)\n            }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, dict):\n        save_dict = {}\n        for k, v in result.items():\n            if isinstance(v, torch.Tensor):\n                save_dict[k] = v.cpu()\n            else:\n                save_dict[k] = v\n        torch.save({\"type\": \"dict\", \"data\": save_dict}, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check dictionary keys match\n        assert set(current_result.keys()) == set(reference_result.keys()), \\\n            f\"Keys mismatch: {current_result.keys()} vs {reference_result.keys()}\"\n        \n        for key in current_result:\n            if key in [\"opt_path_hit\", \"error\"]:\n                continue  # Skip metadata fields\n            \n            curr_val = current_result[key]\n            ref_val = reference_result[key]\n            \n            if isinstance(curr_val, torch.Tensor) and isinstance(ref_val, torch.Tensor):\n                assert curr_val.shape == ref_val.shape, f\"Shape mismatch for {key}\"\n                assert curr_val.dtype == ref_val.dtype, f\"Dtype mismatch for {key}\"\n                \n                # Determine tolerances based on dtype\n                if curr_val.dtype in (torch.float16, torch.bfloat16):\n                    rtol, atol = 1e-3, 1e-4\n                else:\n                    rtol, atol = 1e-5, 1e-7\n                \n                torch.testing.assert_close(\n                    curr_val.cpu(),\n                    ref_val.cpu(),\n                    rtol=rtol, atol=atol\n                )\n    elif isinstance(current_result, torch.Tensor) and isinstance(reference_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ac45c44d98e77f30e47b8fb69134f4635183070d\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Check if optimization path was hit\n    opt_path_hit = True\n    if isinstance(result, dict) and \"opt_path_hit\" in result:\n        opt_path_hit = result[\"opt_path_hit\"]\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": opt_path_hit\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
    "commit_short": "ad8d696a",
    "commit_subject": "[Core] Scheduler perf fix (#4270)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/core/test_scheduler.py",
      "vllm/core/scheduler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4270",
    "models": [
      "N/A"
    ],
    "parent_commit": "3d925165f2b18379640a63fbb42de95440d63b64",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 868.2518367767334,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 3d925165f2b1",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ad8d696a99ca1eee19f1404e16e8e82df592ff85\nMessage: [Core] Scheduler perf fix (#4270)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.core.scheduler\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"Scheduler\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    from vllm.config import CacheConfig, SchedulerConfig\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm import SamplingParams\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create scheduler configuration\n    block_size = 16\n    num_gpu_blocks = 1024\n    num_cpu_blocks = 512\n    max_num_seqs = 256\n    max_model_len = 2048\n    max_num_batched_tokens = 2048\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=max_num_batched_tokens,\n        max_num_seqs=max_num_seqs,\n        max_model_len=max_model_len\n    )\n    \n    cache_config = CacheConfig(\n        block_size=block_size,\n        gpu_memory_utilization=0.9,\n        swap_space_bytes=1,\n        cache_dtype=\"auto\"\n    )\n    cache_config.num_gpu_blocks = num_gpu_blocks\n    cache_config.num_cpu_blocks = num_cpu_blocks\n    \n    # Create sequence groups for testing\n    num_seq_groups = 64  # Simulate multiple concurrent requests\n    seq_groups = []\n    \n    for i in range(num_seq_groups):\n        # Create sequences with varying prompt lengths\n        prompt_length = 128 + (i % 5) * 64  # Vary from 128 to 384\n        \n        # Create a sequence\n        seq_id = i\n        prompt_token_ids = list(range(prompt_length))\n        \n        seq = Sequence(\n            seq_id=seq_id,\n            prompt=None,\n            prompt_token_ids=prompt_token_ids,\n            block_size=block_size\n        )\n        \n        # Create sampling params\n        sampling_params = SamplingParams(\n            temperature=0.7,\n            top_p=0.9,\n            max_tokens=128\n        )\n        \n        # Create sequence group\n        seq_group = SequenceGroup(\n            request_id=str(i),\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time() - (num_seq_groups - i) * 0.01  # Stagger arrival times\n        )\n        \n        seq_groups.append(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"scheduler_config\": scheduler_config,\n        \"cache_config\": cache_config,\n        \"seq_groups\": seq_groups,\n        \"num_iterations\": 100  # Number of scheduling iterations to test\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create scheduler instance\n    scheduler = target(\n        scheduler_config=data[\"scheduler_config\"],\n        cache_config=data[\"cache_config\"],\n        lora_config=None\n    )\n    \n    seq_groups = data[\"seq_groups\"]\n    num_iterations = data[\"num_iterations\"]\n    \n    # Add sequence groups to scheduler\n    for seq_group in seq_groups[:32]:  # Start with half the groups\n        scheduler.add_seq_group(seq_group)\n    \n    results = {\n        \"scheduled_counts\": [],\n        \"allocation_times\": [],\n        \"total_scheduled\": 0\n    }\n    \n    # Simulate scheduling iterations\n    for iteration in range(num_iterations):\n        # Schedule requests\n        seq_group_metadata_list, scheduler_outputs = scheduler.schedule()\n        \n        results[\"scheduled_counts\"].append(len(scheduler_outputs.scheduled_seq_groups))\n        results[\"total_scheduled\"] += len(scheduler_outputs.scheduled_seq_groups)\n        \n        # Add more requests progressively\n        if iteration < len(seq_groups) - 32:\n            scheduler.add_seq_group(seq_groups[32 + iteration])\n        \n        # Simulate token generation for running sequences\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            seq_group = scheduled_seq_group.seq_group\n            for seq in seq_group.get_seqs():\n                if seq.get_len() < seq.get_prompt_len() + 50:  # Generate up to 50 tokens\n                    # Simulate appending a generated token\n                    seq.append_token_id(token_id=100, logprobs={100: -0.5})\n        \n        # Free finished sequences periodically\n        if iteration % 10 == 0:\n            scheduler.free_finished_seq_groups()\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, dict):\n        # Store as JSON for dictionaries\n        import json\n        with open(filepath, 'w') as f:\n            json.dump(result, f)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    if filepath.endswith('.json'):\n        import json\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    else:\n        data = torch.load(filepath)\n        return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check that both scheduled similar numbers of sequences\n        curr_total = current_result.get(\"total_scheduled\", 0)\n        ref_total = reference_result.get(\"total_scheduled\", 0)\n        \n        # Allow some variance in scheduling decisions\n        if abs(curr_total - ref_total) > ref_total * 0.1:  # 10% tolerance\n            raise AssertionError(f\"Total scheduled mismatch: {curr_total} vs {ref_total}\")\n        \n        # Check scheduled counts have similar patterns\n        curr_counts = current_result.get(\"scheduled_counts\", [])\n        ref_counts = reference_result.get(\"scheduled_counts\", [])\n        \n        if len(curr_counts) != len(ref_counts):\n            raise AssertionError(f\"Count length mismatch: {len(curr_counts)} vs {len(ref_counts)}\")\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - scheduler operations are CPU-bound\n    warmup = 3\n    iters = 10\n    \n    # Time the experiment\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ad8d696a99ca1eee19f1404e16e8e82df592ff85\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.json\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Scheduler is CPU-bound\n        \"dtype\": \"N/A\",  # Not applicable for scheduler\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True,\n        \"total_scheduled\": result[\"total_scheduled\"]\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
    "commit_short": "aea94362",
    "commit_subject": "[Frontend][V1] Online serving performance improvements (#12287)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/entrypoints/openai/api_server.py",
      "vllm/entrypoints/openai/protocol.py",
      "vllm/envs.py",
      "vllm/v1/engine/async_llm.py",
      "vllm/v1/engine/core_client.py",
      "vllm/v1/engine/output_processor.py",
      "vllm/v1/request.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12287",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "meta-llama/Llama-3.2-1B-Instruct"
    ],
    "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 302.348726272583,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 11:17:43 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev337+g7206ce4c",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 11:19:30 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='sharegpt', dataset_path=None, max_concurrency=400, model='meta-llama/Llama-3.2-1B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=6000, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 1248, in <module>\n    main(args)\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 821, in main\n    input_requests = sample_sharegpt_requests(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm-commit/benchmarks/benchmark_serving.py\", line 97, in sample_sharegpt_requests\n    with open(dataset_path, encoding='utf-8') as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: aea94362c9bdd08ed2b346701bdc09d278e85f66\nMessage: [Frontend][V1] Online serving performance improvements (#12287)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport asyncio\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.v1.engine.output_processor\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"OutputProcessor\")\n    \n    # Import with error handling\n    try:\n        import importlib\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required modules\n    from vllm.v1.engine import EngineCoreOutput\n    from vllm.engine.llm_engine import BaseTokenizerGroup\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Simulate high-concurrency streaming scenario\n    batch_size = 256  # High concurrency\n    seq_len = 128  # Typical generation length\n    vocab_size = 32000\n    \n    # Create mock tokenizer\n    class MockTokenizer:\n        def get_lora_tokenizer(self, lora_request):\n            return self\n        \n        def decode(self, token_ids):\n            return \"test \" * len(token_ids)\n    \n    # Create mock outputs simulating streaming tokens\n    outputs = []\n    for i in range(batch_size):\n        output = EngineCoreOutput(\n            request_id=f\"req_{i}\",\n            new_token_ids=[np.random.randint(0, vocab_size)],\n            finished=i % 10 == 0,  # 10% finish rate\n            finish_reason=\"stop\" if i % 10 == 0 else None,\n            stop_reason=None,\n            logprobs=None,\n            prompt_logprobs=None,\n            prompt_logprobs_token_ids=None,\n        )\n        outputs.append(output)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"outputs\": outputs,\n        \"tokenizer\": MockTokenizer(),\n        \"batch_size\": batch_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    OutputProcessor, _ = resolve_target()\n    \n    # Create processor instance\n    processor = OutputProcessor(\n        tokenizer=data[\"tokenizer\"],\n        log_stats=False\n    )\n    \n    # Add mock requests to processor\n    from vllm.v1.engine import EngineCoreRequest\n    from vllm import SamplingParams\n    \n    for i in range(data[\"batch_size\"]):\n        request = EngineCoreRequest(\n            request_id=f\"req_{i}\",\n            prompt=\"Test prompt\",\n            prompt_token_ids=[1, 2, 3, 4, 5],\n            sampling_params=SamplingParams(max_tokens=100),\n            eos_token_id=2,\n            arrival_time=time.time(),\n            lora_request=None,\n            mm_inputs=None,\n            mm_hashes=None,\n            mm_placeholders=None,\n        )\n        processor.add_request(request)\n    \n    # Process outputs - this is the optimized path\n    result = processor.process_outputs(data[\"outputs\"])\n    \n    return {\n        \"num_outputs\": len(data[\"outputs\"]),\n        \"reqs_to_abort\": len(result.reqs_to_abort),\n        \"iteration_stats\": result.iteration_stats,\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store only comparable data\n    stored_data = {\n        \"num_outputs\": result[\"num_outputs\"],\n        \"reqs_to_abort\": result[\"reqs_to_abort\"],\n    }\n    torch.save({\"type\": \"dict\", \"data\": stored_data}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert current_result[\"num_outputs\"] == reference_result[\"num_outputs\"]\n    assert current_result[\"reqs_to_abort\"] == reference_result[\"reqs_to_abort\"]\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This optimization is CPU-bound (async processing)\n    warmup = 5\n    iters = 50\n    \n    # Time the operation\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"aea94362c9bdd08ed2b346701bdc09d278e85f66\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization is CPU-bound\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "b10e51989551cd80dd74079429ccf91f0807bd92",
    "commit_short": "b10e5198",
    "commit_subject": "[V1][Minor] Optimize get_cached_block (#16135)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "9bde5ba12709ea0fe9e1a1eeee1e8d7b4c7ea668",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 4818.583622217178,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:17:10 [__init__.py:239] Automatically detected platform cuda.\n0.8.3rc2.dev22+g9bde5ba1",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:20:54 [__init__.py:239] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: vllm [-h] [-v] {chat,complete,serve,bench} ...\nvllm: error: unrecognized arguments: --backend vllm\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "commit_short": "b2e0ad3b",
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
    "status": "version_bug",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 35.60082292556763,
    "error": "vLLM 0.6.3.post2.dev398+g4a18fd14 has known port binding bug (issue #8791) - serving benchmarks not supported",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.3.post2.dev398+g4a18fd14",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "commit_short": "b55ed6ef",
    "commit_subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "files_changed": [
      "vllm/v1/worker/gpu_input_batch.py",
      "vllm/v1/worker/gpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11692",
    "models": [
      "N/A"
    ],
    "parent_commit": "2f385183f35497e030ef22c9820d83b83bc4f6db",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 435.72700119018555,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.6.post2.dev52+g2f385183",
    "human_version": "0.6.6.post2.dev53+gb55ed6ef",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 1145.21,
    "baseline_ttft_median": 1103.73,
    "baseline_ttft_p99": 1921.62,
    "baseline_tpot_mean": 35.59,
    "baseline_tpot_median": 33.85,
    "baseline_tpot_p99": 72.67,
    "baseline_itl_mean": 45.87,
    "baseline_itl_median": 30.05,
    "baseline_itl_p99": 429.74,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 1031.57,
    "human_ttft_median": 1037.16,
    "human_ttft_p99": 1775.55,
    "human_tpot_mean": 31.13,
    "human_tpot_median": 29.46,
    "human_tpot_p99": 80.48,
    "human_itl_mean": 39.84,
    "human_itl_median": 25.85,
    "human_itl_p99": 263.58,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 1056.14,
    "agent_ttft_median": 1036.15,
    "agent_ttft_p99": 1811.85,
    "agent_tpot_mean": 30.92,
    "agent_tpot_median": 29.82,
    "agent_tpot_p99": 59.59,
    "agent_itl_mean": 40.08,
    "agent_itl_median": 25.15,
    "agent_itl_p99": 297.41,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 9.923070877830275,
    "human_improvement_tpot_mean": 12.53161000280979,
    "human_improvement_itl_mean": 13.145846958796586,
    "agent_improvement_ttft_mean": 7.777612839566536,
    "agent_improvement_tpot_mean": 13.121663388592305,
    "agent_improvement_itl_mean": 12.622629169391757,
    "agent_vs_human_ttft_mean": -2.3818063728103924,
    "agent_vs_human_tpot_mean": 0.6745904272405953,
    "agent_vs_human_itl_mean": -0.6024096385542039,
    "human_improvement_ttft_median": 6.031366366774478,
    "human_improvement_ttft_p99": 7.601398819745836,
    "agent_improvement_ttft_median": 6.122874253667104,
    "agent_improvement_ttft_p99": 5.712367689761763,
    "agent_vs_human_ttft_median": 0.0973813105017539,
    "agent_vs_human_ttft_p99": -2.0444369350342124,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  5.54      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11217     \nRequest throughput (req/s):              17.15     \nOutput token throughput (tok/s):         2024.57   \nTotal Token throughput (tok/s):          10803.67  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1145.21   \nMedian TTFT (ms):                        1103.73   \nP99 TTFT (ms):                           1921.62   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          35.59     \nMedian TPOT (ms):                        33.85     \nP99 TPOT (ms):                           72.67     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           45.87     \nMedian ITL (ms):                         30.05     \nP99 ITL (ms):                            429.74    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:23,  1.19it/s]\n  3%|\u258e         | 3/100 [00:01<00:41,  2.31it/s]\n  4%|\u258d         | 4/100 [00:01<00:31,  3.06it/s]\n  5%|\u258c         | 5/100 [00:03<01:33,  1.02it/s]\n  6%|\u258c         | 6/100 [00:04<01:11,  1.31it/s]\n  7%|\u258b         | 7/100 [00:04<01:11,  1.29it/s]\n  9%|\u2589         | 9/100 [00:05<00:39,  2.31it/s]\n 25%|\u2588\u2588\u258c       | 25/100 [00:05<00:05, 13.72it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:05<00:02, 27.89it/s]\n 63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 63/100 [00:05<00:00, 48.17it/s]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 86/100 [00:05<00:00, 73.65it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 18.05it/s]\n",
    "human_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  4.86      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11222     \nRequest throughput (req/s):              19.56     \nOutput token throughput (tok/s):         2310.42   \nTotal Token throughput (tok/s):          12324.58  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1031.57   \nMedian TTFT (ms):                        1037.16   \nP99 TTFT (ms):                           1775.55   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          31.13     \nMedian TPOT (ms):                        29.46     \nP99 TPOT (ms):                           80.48     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           39.84     \nMedian ITL (ms):                         25.85     \nP99 ITL (ms):                            263.58    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<00:38,  2.57it/s]\n  2%|\u258f         | 2/100 [00:00<00:49,  1.96it/s]\n  3%|\u258e         | 3/100 [00:01<00:48,  2.01it/s]\n  4%|\u258d         | 4/100 [00:01<00:42,  2.28it/s]\n  5%|\u258c         | 5/100 [00:03<01:29,  1.06it/s]\n  6%|\u258c         | 6/100 [00:03<01:03,  1.48it/s]\n  8%|\u258a         | 8/100 [00:03<00:35,  2.61it/s]\n 10%|\u2588         | 10/100 [00:04<00:28,  3.20it/s]\n 24%|\u2588\u2588\u258d       | 24/100 [00:04<00:05, 14.96it/s]\n 40%|\u2588\u2588\u2588\u2588      | 40/100 [00:04<00:01, 30.68it/s]\n 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 61/100 [00:04<00:00, 54.06it/s]\n 90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 90/100 [00:04<00:00, 90.24it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 20.59it/s]\n",
    "agent_raw": "Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto')\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     95        \nBenchmark duration (s):                  4.93      \nTotal input tokens:                      48640     \nTotal generated tokens:                  11265     \nRequest throughput (req/s):              19.25     \nOutput token throughput (tok/s):         2282.95   \nTotal Token throughput (tok/s):          12140.28  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1056.14   \nMedian TTFT (ms):                        1036.15   \nP99 TTFT (ms):                           1811.85   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30.92     \nMedian TPOT (ms):                        29.82     \nP99 TPOT (ms):                           59.59     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           40.08     \nMedian ITL (ms):                         25.15     \nP99 ITL (ms):                            297.41    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:00<01:09,  1.42it/s]\n  2%|\u258f         | 2/100 [00:01<00:54,  1.79it/s]\n  3%|\u258e         | 3/100 [00:01<00:52,  1.87it/s]\n  4%|\u258d         | 4/100 [00:01<00:35,  2.74it/s]\n  5%|\u258c         | 5/100 [00:03<01:20,  1.17it/s]\n  6%|\u258c         | 6/100 [00:03<01:00,  1.54it/s]\n  7%|\u258b         | 7/100 [00:03<00:44,  2.11it/s]\n  8%|\u258a         | 8/100 [00:04<00:46,  1.98it/s]\n 26%|\u2588\u2588\u258c       | 26/100 [00:04<00:04, 16.49it/s]\n 42%|\u2588\u2588\u2588\u2588\u258f     | 42/100 [00:04<00:01, 31.47it/s]\n 57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 57/100 [00:04<00:00, 46.56it/s]\n 79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 79/100 [00:04<00:00, 72.63it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:04<00:00, 20.27it/s]\n",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c\nMessage: [V1][Minor] Optimize token_ids_cpu copy (#11692)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Mock CachedRequestState\n# =======================\nfrom dataclasses import dataclass\nfrom typing import Set\n\n@dataclass\nclass MockCachedRequestState:\n    req_id: str\n    prompt_token_ids: List[int]\n    prompt: Optional[str]\n    mm_inputs: List\n    mm_positions: List\n    sampling_params: Any\n    generator: Optional[torch.Generator]\n    block_ids: List[int]\n    num_computed_tokens: int\n    output_token_ids: List[int]\n    \n    @property\n    def num_tokens(self) -> int:\n        return len(self.prompt_token_ids) + len(self.output_token_ids)\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.v1.worker.gpu_input_batch\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"InputBatch\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Create InputBatch parameters that trigger the optimization\n    # The optimization is about copying only necessary tokens during condense()\n    max_num_reqs = 256  # Typical batch size\n    max_model_len = 4096  # Large model context to make copy cost visible\n    max_num_blocks_per_req = 256\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    pin_memory = torch.cuda.is_available()\n    vocab_size = 32000  # Typical vocab size\n    \n    # Create mock requests with varying token counts\n    requests = []\n    for i in range(32):  # Create 32 active requests\n        prompt_len = 256 + i * 16  # Varying prompt lengths\n        output_len = 128 + i * 8   # Varying output lengths\n        req = MockCachedRequestState(\n            req_id=f\"req_{i}\",\n            prompt_token_ids=list(range(prompt_len)),\n            prompt=None,\n            mm_inputs=[],\n            mm_positions=[],\n            sampling_params=type('SamplingParams', (), {\n                'temperature': 0.7,\n                'top_p': 0.9,\n                'top_k': 40,\n                'frequency_penalty': 0.0,\n                'presence_penalty': 0.0,\n                'repetition_penalty': 1.0,\n                'min_tokens': 0,\n                'all_stop_token_ids': set(),\n                'sampling_type': 0,  # GREEDY\n                'logprobs': None,\n                'prompt_logprobs': False\n            })(),\n            generator=None,\n            block_ids=list(range(16)),\n            num_computed_tokens=prompt_len,\n            output_token_ids=list(range(output_len))\n        )\n        requests.append(req)\n    \n    # Create indices to remove (simulate request completion)\n    # This will trigger condense() operation\n    indices_to_remove = [3, 7, 11, 15, 19, 23, 27]  # Remove every 4th request\n    \n    data = {\n        \"device\": device,\n        \"dtype\": torch.float32,\n        \"hw_info\": hw_info,\n        \"max_num_reqs\": max_num_reqs,\n        \"max_model_len\": max_model_len,\n        \"max_num_blocks_per_req\": max_num_blocks_per_req,\n        \"pin_memory\": pin_memory,\n        \"vocab_size\": vocab_size,\n        \"requests\": requests,\n        \"indices_to_remove\": indices_to_remove,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    InputBatch, _ = resolve_target()\n    \n    # Create InputBatch instance\n    batch = InputBatch(\n        max_num_reqs=data[\"max_num_reqs\"],\n        max_model_len=data[\"max_model_len\"],\n        max_num_blocks_per_req=data[\"max_num_blocks_per_req\"],\n        device=data[\"device\"],\n        pin_memory=data[\"pin_memory\"],\n        vocab_size=data[\"vocab_size\"],\n    )\n    \n    # Add all requests\n    for i, req in enumerate(data[\"requests\"]):\n        batch.add_request(req, req_index=i)\n    \n    # Remove some requests to create empty indices\n    empty_indices = []\n    for idx in data[\"indices_to_remove\"]:\n        req_id = data[\"requests\"][idx].req_id\n        removed_idx = batch.remove_request(req_id)\n        if removed_idx is not None:\n            empty_indices.append(removed_idx)\n    \n    # Sort in descending order as required by condense()\n    empty_indices.sort(reverse=True)\n    \n    # Time the condense operation which contains the optimization\n    # This is where the optimized token copying happens\n    start_state = {\n        \"num_reqs\": batch.num_reqs,\n        \"empty_indices\": empty_indices.copy(),\n        \"token_ids_snapshot\": batch.token_ids_cpu.copy() if hasattr(batch, 'token_ids_cpu') else None\n    }\n    \n    # Execute the optimized condense operation\n    batch.condense(empty_indices)\n    \n    # Return state for verification\n    result = {\n        \"num_reqs_after\": batch.num_reqs,\n        \"req_ids\": [req_id for req_id in batch.req_ids if req_id is not None],\n        \"start_state\": start_state,\n        \"batch\": batch  # Keep reference for multiple iterations\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Store only the verifiable parts, not the batch object\n    storable = {\n        \"num_reqs_after\": result[\"num_reqs_after\"],\n        \"req_ids\": result[\"req_ids\"],\n    }\n    torch.save({\"type\": \"dict\", \"data\": storable}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # Compare the state after condense operation\n    assert current_result[\"num_reqs_after\"] == reference_result[\"num_reqs_after\"], \\\n        f\"Number of requests mismatch: {current_result['num_reqs_after']} vs {reference_result['num_reqs_after']}\"\n    \n    assert set(current_result[\"req_ids\"]) == set(reference_result[\"req_ids\"]), \\\n        f\"Request IDs mismatch: {current_result['req_ids']} vs {reference_result['req_ids']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_condense(data: Dict[str, Any], warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time the condense operation on CPU.\"\"\"\n    InputBatch, _ = resolve_target()\n    \n    # Warmup\n    for _ in range(warmup):\n        batch = InputBatch(\n            max_num_reqs=data[\"max_num_reqs\"],\n            max_model_len=data[\"max_model_len\"],\n            max_num_blocks_per_req=data[\"max_num_blocks_per_req\"],\n            device=data[\"device\"],\n            pin_memory=data[\"pin_memory\"],\n            vocab_size=data[\"vocab_size\"],\n        )\n        for i, req in enumerate(data[\"requests\"]):\n            batch.add_request(req, req_index=i)\n        empty_indices = []\n        for idx in data[\"indices_to_remove\"]:\n            req_id = data[\"requests\"][idx].req_id\n            removed_idx = batch.remove_request(req_id)\n            if removed_idx is not None:\n                empty_indices.append(removed_idx)\n        empty_indices.sort(reverse=True)\n        batch.condense(empty_indices)\n    \n    # Timing\n    times_ms = []\n    result = None\n    for _ in range(iterations):\n        # Fresh setup for each iteration\n        batch = InputBatch(\n            max_num_reqs=data[\"max_num_reqs\"],\n            max_model_len=data[\"max_model_len\"],\n            max_num_blocks_per_req=data[\"max_num_blocks_per_req\"],\n            device=data[\"device\"],\n            pin_memory=data[\"pin_memory\"],\n            vocab_size=data[\"vocab_size\"],\n        )\n        for i, req in enumerate(data[\"requests\"]):\n            batch.add_request(req, req_index=i)\n        empty_indices = []\n        for idx in data[\"indices_to_remove\"]:\n            req_id = data[\"requests\"][idx].req_id\n            removed_idx = batch.remove_request(req_id)\n            if removed_idx is not None:\n                empty_indices.append(removed_idx)\n        empty_indices.sort(reverse=True)\n        \n        # Time the condense operation\n        start = time.perf_counter()\n        batch.condense(empty_indices)\n        end = time.perf_counter()\n        \n        times_ms.append((end - start) * 1000)\n        \n        # Save last result\n        if result is None:\n            result = {\n                \"num_reqs_after\": batch.num_reqs,\n                \"req_ids\": [req_id for req_id in batch.req_ids if req_id is not None],\n            }\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # For this CPU-based optimization, we always time on CPU\n    warmup = 5\n    iters = 20  # More iterations since operation is fast\n    result, timing_stats = time_cpu_condense(data, warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This optimization affects CPU operations\n        \"dtype\": \"torch.int32\",  # token_ids dtype\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "commit_short": "b690e348",
    "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "tests/kernels/mamba/test_mamba_ssm.py",
      "tests/kernels/mamba/test_mamba_ssm_ssd.py",
      "vllm/model_executor/layers/mamba/mamba_mixer.py",
      "vllm/model_executor/layers/mamba/mamba_mixer2.py",
      "vllm/model_executor/layers/mamba/ops/mamba_ssm.py",
      "vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py",
      "vllm/model_executor/layers/mamba/ops/ssd_combined.py",
      "vllm/model_executor/models/phi4flash.py",
      "vllm/model_executor/models/plamo2.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21075",
    "models": [
      "ibm-ai-platform/Bamba-9B-v2",
      "microsoft/Phi-4-mini-flash-reasoning"
    ],
    "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 657.343558549881,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.1.dev312+g25373b6c6",
    "human_version": "0.10.1.dev313+gb690e3482",
    "agent_version": null,
    "model": "ibm-ai-platform/Bamba-9B-v2",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 37803.3,
    "baseline_ttft_median": 38250.43,
    "baseline_ttft_p99": 46512.18,
    "baseline_tpot_mean": 78.67,
    "baseline_tpot_median": 76.26,
    "baseline_tpot_p99": 318.64,
    "baseline_itl_mean": 78.67,
    "baseline_itl_median": 58.17,
    "baseline_itl_p99": 126.91,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 9130.87,
    "human_ttft_median": 8385.2,
    "human_ttft_p99": 16448.2,
    "human_tpot_mean": 69.8,
    "human_tpot_median": 74.61,
    "human_tpot_p99": 106.18,
    "human_itl_mean": 69.8,
    "human_itl_median": 57.76,
    "human_itl_p99": 114.79,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 9640.47,
    "agent_ttft_median": 8944.8,
    "agent_ttft_p99": 19501.5,
    "agent_tpot_mean": 85.02,
    "agent_tpot_median": 86.85,
    "agent_tpot_p99": 104.32,
    "agent_itl_mean": 85.02,
    "agent_itl_median": 61.42,
    "agent_itl_p99": 123.19,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 75.84636790967984,
    "human_improvement_tpot_mean": 11.274945976865393,
    "human_improvement_itl_mean": 11.274945976865393,
    "agent_improvement_ttft_mean": 74.49833744673084,
    "agent_improvement_tpot_mean": -8.071691877462813,
    "agent_improvement_itl_mean": -8.071691877462813,
    "agent_vs_human_ttft_mean": -5.581067302458567,
    "agent_vs_human_tpot_mean": -21.80515759312321,
    "agent_vs_human_itl_mean": -21.80515759312321,
    "human_improvement_ttft_median": 78.07815493838892,
    "human_improvement_ttft_p99": 64.63678976130554,
    "agent_improvement_ttft_median": 76.6151648491272,
    "agent_improvement_ttft_p99": 58.07227268212326,
    "agent_vs_human_ttft_median": -6.673663120736519,
    "agent_vs_human_ttft_p99": -18.563125448377324,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-01 09:07:09 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ae180054fe0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 09:07:18 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  50.12     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              5.99      \nOutput token throughput (tok/s):         766.22    \nTotal Token throughput (tok/s):          3822.30   \n---------------Time to First Token----------------\nMean TTFT (ms):                          37803.30  \nMedian TTFT (ms):                        38250.43  \nP99 TTFT (ms):                           46512.18  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          78.67     \nMedian TPOT (ms):                        76.26     \nP99 TPOT (ms):                           318.64    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           78.67     \nMedian ITL (ms):                         58.17     \nP99 ITL (ms):                            126.91    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:45<3:45:23, 45.23s/",
    "human_raw": "INFO 01-01 09:10:20 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b73a9724fe0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 09:10:27 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  21.20     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              14.15     \nOutput token throughput (tok/s):         1811.68   \nTotal Token throughput (tok/s):          9037.63   \n---------------Time to First Token----------------\nMean TTFT (ms):                          9130.87   \nMedian TTFT (ms):                        8385.20   \nP99 TTFT (ms):                           16448.20  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          69.80     \nMedian TPOT (ms):                        74.61     \nP99 TPOT (ms):                           106.18    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           69.80     \nMedian ITL (ms):                         57.76     \nP99 ITL (ms):                            114.79    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:15<1:15:10, 15.08s/",
    "agent_raw": "INFO 01-01 09:13:09 [__init__.py:241] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2ac82f698fe0>, seed=0, num_prompts=300, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-01 09:13:16 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  23.32     \nTotal input tokens:                      153160    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              12.87     \nOutput token throughput (tok/s):         1646.82   \nTotal Token throughput (tok/s):          8215.21   \n---------------Time to First Token----------------\nMean TTFT (ms):                          9640.47   \nMedian TTFT (ms):                        8944.80   \nP99 TTFT (ms):                           19501.50  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          85.02     \nMedian TPOT (ms):                        86.85     \nP99 TPOT (ms):                           104.32    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           85.02     \nMedian ITL (ms):                         61.42     \nP99 ITL (ms):                            123.19    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:15<1:18:01, 15.66s/",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: b690e34824fd5a5c4054a0c0468ebfb6aa1dd215\nMessage: [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)\n\nThis script measures the actual performance impact of preallocating output tensors\nto avoid device-to-device memory copy overhead in Mamba2 SSM operations.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Default to the main optimized function\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.mamba.ops.mamba_ssm\"\n        symbol_name = \"selective_state_update\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic Mamba2 SSM workload.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Mamba2 SSM configuration for a typical 7B model\n    batch_size = 32  # Batched decode scenario\n    nheads = 32  # Number of attention heads\n    dim = 128  # Head dimension\n    dstate = 16  # SSM state dimension\n    \n    # Create SSM state and inputs\n    state = torch.randn(batch_size, nheads, dim, dstate, dtype=dtype, device=device)\n    x = torch.randn(batch_size, nheads, dim, device=device, dtype=dtype)\n    \n    # Preallocate output tensor (key optimization)\n    out = torch.empty_like(x)\n    \n    # SSM parameters\n    dt = torch.randn(batch_size, nheads, dim, device=device, dtype=dtype)\n    dt_bias = torch.rand(nheads, dim, device=device, dtype=torch.float32) - 4.0\n    A = -torch.rand(nheads, dim, dstate, device=device, dtype=torch.float32) - 1.0\n    B = torch.randn(batch_size, nheads, dstate, device=device, dtype=dtype)\n    C = torch.randn(batch_size, nheads, dstate, device=device, dtype=dtype)\n    D = torch.randn(nheads, dim, device=device, dtype=torch.float32)\n    z = torch.randn_like(x)\n    \n    # State batch indices for continuous batching\n    state_batch_indices = torch.arange(batch_size, dtype=torch.int32, device=device)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"state\": state,\n        \"x\": x,\n        \"out\": out,\n        \"dt\": dt,\n        \"dt_bias\": dt_bias,\n        \"A\": A,\n        \"B\": B,\n        \"C\": C,\n        \"D\": D,\n        \"z\": z,\n        \"state_batch_indices\": state_batch_indices,\n        \"batch_size\": batch_size,\n        \"nheads\": nheads,\n        \"dim\": dim,\n        \"dstate\": dstate,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized SSM operation with preallocated output.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Clone state to avoid side effects between iterations\n    state_copy = data[\"state\"].clone()\n    \n    with torch.no_grad():\n        # Call with preallocated output tensor (the optimization)\n        target(\n            state_copy,\n            data[\"x\"],\n            data[\"dt\"],\n            data[\"A\"],\n            data[\"B\"],\n            data[\"C\"],\n            D=data[\"D\"],\n            z=data[\"z\"],\n            dt_bias=data[\"dt_bias\"],\n            dt_softplus=True,\n            state_batch_indices=data[\"state_batch_indices\"],\n            out=data[\"out\"]\n        )\n    \n    # Return both output and final state for equivalence checking\n    return {\"output\": data[\"out\"].clone(), \"state\": state_copy.clone()}\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\n        \"type\": \"ssm_result\",\n        \"output\": result[\"output\"].cpu(),\n        \"state\": result[\"state\"].cpu()\n    }, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return {\n        \"output\": data[\"output\"],\n        \"state\": data[\"state\"]\n    }\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence of SSM outputs.\"\"\"\n    # Check output tensor\n    current_out = current_result[\"output\"]\n    ref_out = reference_result[\"output\"]\n    \n    assert current_out.shape == ref_out.shape, f\"Output shape mismatch: {current_out.shape} vs {ref_out.shape}\"\n    assert current_out.dtype == ref_out.dtype, f\"Output dtype mismatch: {current_out.dtype} vs {ref_out.dtype}\"\n    \n    # Check state tensor\n    current_state = current_result[\"state\"]\n    ref_state = reference_result[\"state\"]\n    \n    assert current_state.shape == ref_state.shape, f\"State shape mismatch: {current_state.shape} vs {ref_state.shape}\"\n    assert current_state.dtype == ref_state.dtype, f\"State dtype mismatch: {current_state.dtype} vs {ref_state.dtype}\"\n    \n    # Determine tolerances based on dtype\n    if current_out.dtype in (torch.float16, torch.bfloat16):\n        rtol, atol = 1e-3, 1e-4\n    else:\n        rtol, atol = 1e-5, 1e-7\n    \n    # Check output equivalence\n    torch.testing.assert_close(\n        current_out.cpu(),\n        ref_out.cpu(),\n        rtol=rtol, atol=atol\n    )\n    \n    # Check state equivalence\n    torch.testing.assert_close(\n        current_state.cpu(),\n        ref_state.cpu(),\n        rtol=rtol, atol=atol\n    )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"b690e34824fd5a5c4054a0c0468ebfb6aa1dd215\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "b6d103542c654fb63013a1e45a586d654ae36a2a",
    "commit_short": "b6d10354",
    "commit_subject": "[Kernel] Layernorm performance optimization (#3662)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-2-70b-hf --dtype float16 --tensor-parallel-size 1",
    "files_changed": [
      "cmake/utils.cmake",
      "csrc/layernorm_kernels.cu",
      "csrc/reduction_utils.cuh",
      "tests/kernels/test_layernorm.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3662",
    "models": [
      "N/A"
    ],
    "parent_commit": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "c_cuda",
    "duration_s": 879.8014242649078,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 51c31bc10ca7",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-2-70b-hf",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: b6d103542c654fb63013a1e45a586d654ae36a2a\nMessage: [Kernel] Layernorm performance optimization (#3662)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use RMSNorm from vllm\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.layernorm\"\n        symbol_name = \"RMSNorm\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # Use FP16 for CUDA to trigger the optimized kernel path\n    # The optimization specifically targets FP16/BF16 vectorized operations\n    if hw_info[\"device\"] == \"cuda\":\n        dtype = torch.float16\n    else:\n        dtype = torch.float32\n    \n    # Realistic LLM configurations - test both aligned and unaligned cases\n    # The optimization works best when hidden_size % 8 == 0\n    model_configs = {\n        \"7B\": {\"hidden_size\": 4096},   # Aligned to 8\n        \"13B\": {\"hidden_size\": 5120},  # Aligned to 8  \n        \"70B\": {\"hidden_size\": 8192},  # Aligned to 8\n    }\n    \n    # Choose a configuration that triggers the optimized path\n    config = model_configs[\"7B\"]\n    hidden_size = config[\"hidden_size\"]\n    \n    # Test with various batch sizes and sequence lengths\n    # Large num_tokens (>= 256) triggers different block size in kernel\n    batch_size = 4\n    seq_len = 512\n    num_tokens = batch_size * seq_len  # 2048 tokens\n    \n    # Create input tensors with proper alignment for vectorized ops\n    # Scale inputs to prevent overflow in FP16\n    scale = 1.0 / math.sqrt(2 * hidden_size)\n    \n    # Main input tensor\n    x = torch.randn(num_tokens, hidden_size, device=device, dtype=dtype) * scale\n    \n    # Residual tensor for fused_add_rms_norm\n    residual = torch.randn(num_tokens, hidden_size, device=device, dtype=dtype) * scale\n    \n    # RMSNorm weight parameter\n    weight = torch.ones(hidden_size, device=device, dtype=dtype)\n    # Add some variation to weight\n    weight.data.normal_(mean=1.0, std=0.1)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_size\": hidden_size,\n        \"num_tokens\": num_tokens,\n        \"x\": x,\n        \"residual\": residual,\n        \"weight\": weight,\n        \"epsilon\": 1e-5,\n        \"add_residual\": True,  # Test the fused_add_rms_norm path\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    RMSNorm, fq_name = resolve_target()\n    \n    # Create RMSNorm layer instance\n    layer = RMSNorm(data[\"hidden_size\"]).to(device=data[\"device\"], dtype=data[\"dtype\"])\n    \n    # Set the weight to our prepared weight\n    layer.weight.data = data[\"weight\"].clone()\n    \n    # Clone inputs since the kernel is in-place\n    x = data[\"x\"].clone()\n    residual = data[\"residual\"].clone() if data[\"add_residual\"] else None\n    \n    with torch.no_grad():\n        # Call the forward method which internally calls fused_add_rms_norm kernel\n        if data[\"add_residual\"]:\n            # This calls the optimized fused_add_rms_norm kernel\n            result = layer(x, residual)\n            # Return both output and modified residual for equivalence checking\n            return (result[0], result[1])\n        else:\n            # This calls the rms_norm kernel\n            result = layer(x)\n            return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, tuple):\n        # Store tuple of tensors\n        torch.save({\n            \"type\": \"tensor_tuple\",\n            \"data\": tuple(t.cpu() for t in result)\n        }, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    result = data.get(\"data\", data)\n    \n    # Convert back to GPU if needed\n    if isinstance(result, tuple):\n        return tuple(t.cuda() if torch.cuda.is_available() else t for t in result)\n    elif isinstance(result, torch.Tensor):\n        return result.cuda() if torch.cuda.is_available() else result\n    return result\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    \n    # LayerNorm operations have higher numerical errors due to reductions\n    # Use relaxed tolerances as done in the original test\n    rtol = 1e-2\n    atol = 1e-2\n    \n    if isinstance(current_result, tuple) and isinstance(reference_result, tuple):\n        # Check both output and residual for fused_add_rms_norm\n        assert len(current_result) == len(reference_result), f\"Tuple length mismatch\"\n        \n        for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n            assert curr.shape == ref.shape, f\"Shape mismatch at index {i}\"\n            assert curr.dtype == ref.dtype, f\"Dtype mismatch at index {i}\"\n            \n            torch.testing.assert_close(\n                curr.cpu(),\n                ref.cpu(),\n                rtol=rtol,\n                atol=atol\n            )\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol,\n            atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"b6d103542c654fb63013a1e45a586d654ae36a2a\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Check if optimization path was triggered\n    opt_path_hit = True\n    if hw_info[\"device\"] == \"cuda\" and data[\"dtype\"] in [torch.float16, torch.bfloat16]:\n        # Optimization is triggered for FP16/BF16 with aligned pointers and hidden_size % 8 == 0\n        if data[\"hidden_size\"] % 8 == 0:\n            opt_path_hit = True\n        else:\n            opt_path_hit = False  # Fallback to generic kernel\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": \"numeric\",\n        \"opt_path_hit\": opt_path_hit\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "baeded25699f9f4851843306f27f685c4d4ee7c5",
    "commit_short": "baeded25",
    "commit_subject": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3 --dtype float16",
    "files_changed": [
      "vllm/attention/backends/mla/utils.py",
      "vllm/attention/backends/triton_mla.py",
      "vllm/attention/layer.py",
      "vllm/config.py",
      "vllm/envs.py",
      "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
      "vllm/model_executor/layers/quantization/utils/quant_utils.py",
      "vllm/model_executor/model_loader/loader.py",
      "vllm/model_executor/models/deepseek_v3.py",
      "vllm/worker/cache_engine.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12601",
    "models": [
      "deepseek-ai/DeepSeek-V3"
    ],
    "parent_commit": null,
    "status": "error",
    "gpu_config": null,
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3722.2263786792755,
    "error": null,
    "error_message": "Baseline server failed to start",
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": null,
    "has_agent_patch": null,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": "serving",
    "agent_error": null,
    "patch_path": "/home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0066/model_patch.diff",
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: baeded25699f9f4851843306f27f685c4d4ee7c5\nMessage: [Attention] Deepseek v3 MLA support with FP8 compute (#12601)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_fp8\"] = major >= 9  # Hopper+\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_fp8\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target the FP8 utilities\n    if not (module_path and symbol_name):\n        # Target the new FP8 linear generic function\n        module_path = \"vllm.model_executor.layers.quantization.utils.fp8_utils\"\n        symbol_name = \"apply_fp8_linear_generic\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Deepseek V3 MLA configuration\n    # Based on the commit, these are the relevant dimensions\n    device = torch.device(hw_info[\"device\"])\n    \n    # Use FP8 if supported, otherwise fall back to FP16\n    if hw_info.get(\"supports_fp8\", False):\n        # E4M3 format for FP8\n        dtype = torch.float8_e4m3fn\n        compute_dtype = torch.float16\n    else:\n        dtype = torch.float16\n        compute_dtype = torch.float16\n    \n    # Deepseek V3 dimensions from the model\n    batch_size = 4\n    seq_len = 512  # Reduced for testing\n    num_heads = 32  # Local heads after TP\n    kv_lora_rank = 512  # From Deepseek V3 config\n    qk_nope_head_dim = 128\n    v_head_dim = 128\n    hidden_size = 4096\n    \n    # Create test tensors for MLA attention\n    # Input for the absorbed matrices\n    x = torch.randn(batch_size * seq_len, kv_lora_rank, \n                     device=device, dtype=compute_dtype)\n    \n    # Absorbed weight matrices (W_Q_UK and W_UV_O)\n    # W_Q_UK: (kv_lora_rank, num_heads * kv_lora_rank)\n    w_q_uk_shape = (kv_lora_rank, num_heads * kv_lora_rank)\n    \n    # W_UV_O: (num_heads * kv_lora_rank, hidden_size)  \n    w_uv_o_shape = (num_heads * kv_lora_rank, hidden_size)\n    \n    if hw_info.get(\"supports_fp8\", False):\n        # Create FP8 weights with scales\n        w_q_uk = torch.randn(w_q_uk_shape, device=device, dtype=torch.float32)\n        w_q_uk_fp8 = w_q_uk.to(dtype)\n        w_q_uk_scale = torch.tensor([1.0 / 448.0], device=device)  # E4M3 max scale\n        \n        w_uv_o = torch.randn(w_uv_o_shape, device=device, dtype=torch.float32)\n        w_uv_o_fp8 = w_uv_o.to(dtype)\n        w_uv_o_scale = torch.tensor([1.0 / 448.0], device=device)\n    else:\n        # Use FP16 weights\n        w_q_uk_fp8 = torch.randn(w_q_uk_shape, device=device, dtype=dtype)\n        w_q_uk_scale = torch.ones(1, device=device)\n        \n        w_uv_o_fp8 = torch.randn(w_uv_o_shape, device=device, dtype=dtype)  \n        w_uv_o_scale = torch.ones(1, device=device)\n    \n    # Transpose weights for the operation (as done in the code)\n    w_q_uk_fp8_t = w_q_uk_fp8.T.contiguous()\n    w_q_uk_scale_t = w_q_uk_scale\n    \n    w_uv_o_fp8_t = w_uv_o_fp8.T.contiguous()\n    w_uv_o_scale_t = w_uv_o_scale\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"compute_dtype\": compute_dtype,\n        \"hw_info\": hw_info,\n        \"x\": x,\n        \"w_q_uk\": w_q_uk_fp8_t,\n        \"w_q_uk_scale\": w_q_uk_scale_t,\n        \"w_uv_o\": w_uv_o_fp8_t,\n        \"w_uv_o_scale\": w_uv_o_scale_t,\n        \"input_group_shape\": (1, -1),  # per-token quantization\n        \"weight_group_shape\": (-1, -1),  # per-tensor quantization\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_heads\": num_heads,\n        \"kv_lora_rank\": kv_lora_rank,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Execute the FP8 linear operation\n    # This is the core optimization: FP8 matrix absorption\n    with torch.no_grad():\n        if data[\"hw_info\"].get(\"supports_fp8\", False):\n            # Use the new FP8 linear generic function\n            output = target(\n                data[\"x\"],\n                data[\"w_q_uk\"],\n                data[\"w_q_uk_scale\"],\n                data[\"input_group_shape\"],\n                data[\"weight_group_shape\"]\n            )\n        else:\n            # Fallback to regular matmul for non-FP8\n            output = torch.matmul(data[\"x\"], data[\"w_q_uk\"])\n    \n    # Reshape output as done in the actual code\n    output = output.view(-1, data[\"num_heads\"], data[\"kv_lora_rank\"])\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, \\\n            f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, \\\n            f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 5e-3, 5e-4  # Relaxed for FP8 operations\n        elif str(current_result.dtype).startswith(\"torch.float8\"):\n            rtol, atol = 1e-2, 1e-3  # Very relaxed for FP8\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Ensure clean state\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"baeded25699f9f4851843306f27f685c4d4ee7c5\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "commit_short": "bc7c4d20",
    "commit_subject": "[Kernel][ROCM] Upstream prefix prefill speed up fo",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "f67e9e9f221e9791733b827585d6eb6dbc23133c",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 5920.347839593887,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:27:01 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev156+gf67e9e9f2",
    "human_version": "INFO 01-02 17:31:14 [__init__.py:239] Automatically detected platform cuda.\n0.8.5.dev157+gbc7c4d206",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 2435.9,
    "baseline_ttft_median": 2491.88,
    "baseline_ttft_p99": 4335.24,
    "baseline_tpot_mean": 40.71,
    "baseline_tpot_median": 37.21,
    "baseline_tpot_p99": 201.86,
    "baseline_itl_mean": 36.79,
    "baseline_itl_median": 23.42,
    "baseline_itl_p99": 208.4,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 2520.72,
    "human_ttft_median": 2487.94,
    "human_ttft_p99": 4477.38,
    "human_tpot_mean": 41.47,
    "human_tpot_median": 37.75,
    "human_tpot_p99": 205.33,
    "human_itl_mean": 37.55,
    "human_itl_median": 24.23,
    "human_itl_p99": 210.08,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 2454.66,
    "agent_ttft_median": 2379.54,
    "agent_ttft_p99": 4394.01,
    "agent_tpot_mean": 40.8,
    "agent_tpot_median": 37.32,
    "agent_tpot_p99": 201.45,
    "agent_itl_mean": 36.85,
    "agent_itl_median": 23.54,
    "agent_itl_p99": 210.19,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": -3.4820805451783614,
    "human_improvement_tpot_mean": -1.8668631785801966,
    "human_improvement_itl_mean": -2.0657787442239686,
    "agent_improvement_ttft_mean": -0.7701465577404558,
    "agent_improvement_tpot_mean": -0.22107590272659375,
    "agent_improvement_itl_mean": -0.1630877955966357,
    "agent_vs_human_ttft_mean": 2.620679805769778,
    "agent_vs_human_tpot_mean": 1.6156257535567924,
    "agent_vs_human_itl_mean": 1.8641810918774853,
    "human_improvement_ttft_median": 0.1581135528195601,
    "human_improvement_ttft_p99": -3.2787112132200367,
    "agent_improvement_ttft_median": 4.5082427725251675,
    "agent_improvement_ttft_p99": -1.3556342901431164,
    "agent_vs_human_ttft_median": 4.357018256067272,
    "agent_vs_human_ttft_p99": 1.8620264529702613,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:30:28 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b8bc3d6f740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.26      \nTotal input tokens:                      153600    \nTotal generated tokens:                  36968     \nRequest throughput (req/s):              41.34     \nOutput token throughput (tok/s):         5093.69   \nTotal Token throughput (tok/s):          26257.67  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2435.90   \nMedian TTFT (ms):                        2491.88   \nP99 TTFT (ms):                           4335.24   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          40.71     \nMedian TPOT (ms):                        37.21     \nP99 TPOT (ms):                           201.86    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.79     \nMedian ITL (ms):                         23.42     \nP99 ITL (ms):                            208.40    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<06:37,  1.33s/it]\n  1%|          | 3/300 [00:01<02:26,  2.03it/s]\n  1%|\u258f         | 4/300 [00:02<02:16,  2.17it/s]\n  2%|\u258f         | 6/300 [00:03<02:22,  2.07it/s]\n  2%|\u258f         | 7/300 [00:03<02:32,  1.92it/s]\n  3%|\u258e         | 8/300 [00:05<04:21,  1.11it/s]\n  3%|\u258e         | 9/300 [00:05<03:19,  1.46it/s]\n  4%|\u258e         | 11/300 [00:05<01:59,  2.41it/s]\n  4%|\u258d         | 13/300 [00:06<01:30,  3.17it/s]\n  5%|\u258c         | 16/300 [00:06<00:55,  5.13it/s]\n  7%|\u258b         | 20/300 [00:06<00:33,  8.30it/s]\n  8%|\u258a         ",
    "human_raw": "INFO 01-02 17:34:30 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b29c415f740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.43      \nTotal input tokens:                      153600    \nTotal generated tokens:                  37000     \nRequest throughput (req/s):              40.35     \nOutput token throughput (tok/s):         4976.58   \nTotal Token throughput (tok/s):          25636.13  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2520.72   \nMedian TTFT (ms):                        2487.94   \nP99 TTFT (ms):                           4477.38   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          41.47     \nMedian TPOT (ms):                        37.75     \nP99 TPOT (ms):                           205.33    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           37.55     \nMedian ITL (ms):                         24.23     \nP99 ITL (ms):                            210.08    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<07:33,  1.52s/it]\n  1%|          | 3/300 [00:01<02:19,  2.13it/s]\n  1%|\u258f         | 4/300 [00:02<02:11,  2.24it/s]\n  2%|\u258f         | 5/300 [00:02<02:08,  2.30it/s]\n  2%|\u258f         | 6/300 [00:03<02:24,  2.03it/s]\n  2%|\u258f         | 7/300 [00:03<02:55,  1.67it/s]\n  3%|\u258e         | 8/300 [00:05<04:46,  1.02it/s]\n  3%|\u258e         | 9/300 [00:06<03:36,  1.34it/s]\n  4%|\u258e         | 11/300 [00:06<02:02,  2.36it/s]\n  4%|\u258d         | 12/300 [00:06<01:54,  2.51it/s]\n  5%|\u258c         | 15/300 [00:06<01:01,  4.66it/s]\n  7%|\u258b         |",
    "agent_raw": "INFO 01-02 17:37:27 [__init__.py:239] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b7a9feef740>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  7.29      \nTotal input tokens:                      153600    \nTotal generated tokens:                  37000     \nRequest throughput (req/s):              41.17     \nOutput token throughput (tok/s):         5077.53   \nTotal Token throughput (tok/s):          26156.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2454.66   \nMedian TTFT (ms):                        2379.54   \nP99 TTFT (ms):                           4394.01   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          40.80     \nMedian TPOT (ms):                        37.32     \nP99 TPOT (ms):                           201.45    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.85     \nMedian ITL (ms):                         23.54     \nP99 ITL (ms):                            210.19    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:01<07:03,  1.42s/it]\n  1%|          | 3/300 [00:01<02:10,  2.27it/s]\n  1%|\u258f         | 4/300 [00:02<02:06,  2.34it/s]\n  2%|\u258f         | 5/300 [00:02<01:43,  2.84it/s]\n  2%|\u258f         | 6/300 [00:03<02:27,  1.99it/s]\n  2%|\u258f         | 7/300 [00:03<02:57,  1.65it/s]\n  3%|\u258e         | 8/300 [00:05<04:49,  1.01it/s]\n  3%|\u258e         | 9/300 [00:05<03:36,  1.34it/s]\n  3%|\u258e         | 10/300 [00:06<02:39,  1.82it/s]\n  4%|\u258d         | 12/300 [00:06<01:45,  2.72it/s]\n  5%|\u258c         | 15/300 [00:06<00:58,  4.87it/s]\n  6%|\u258c         |",
    "test_script": null
  },
  {
    "commit_hash": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
    "commit_short": "bd6028d6",
    "commit_subject": "Optimized topk for topk=1 (Llama-4) (#16512)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000 --batch-size 1 --num-iters-warmup 5 --num-iters 5",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "802329dee9e5b70c0c73df93c9db1ecdc4632664",
    "status": "baseline_failed",
    "gpu_config": "H100:2",
    "benchmark_mode": "standalone",
    "patch_type": null,
    "duration_s": 2244.361783027649,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 16:36:47 [__init__.py:239] Automatically detected platform cuda.\n0.8.3rc2.dev163+g802329dee",
    "human_version": null,
    "agent_version": null,
    "model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "commit_short": "bfdb1ba5",
    "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)",
    "repo": "vllm",
    "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
    "files_changed": [
      "tests/tokenization/test_detokenize.py",
      "vllm/engine/llm_engine.py",
      "vllm/transformers_utils/detokenizer.py",
      "vllm/transformers_utils/tokenizer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3469",
    "models": [
      "N/A"
    ],
    "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 893.6335244178772,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for cf2f084d56a1",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: bfdb1ba5c3fb14387c69acb1f5067102d8028e56\nMessage: [Core] Improve detokenization performance for prefill (#3469)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use Detokenizer.decode_sequence_inplace\n    if not (module_path and symbol_name):\n        module_path = \"vllm.transformers_utils.detokenizer\"\n        symbol_name = \"Detokenizer\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required components\n    from transformers import AutoTokenizer\n    from vllm.compilation.backends import Sequence\n    from vllm.beam_search import Logprob\n    from vllm import SamplingParams\n    from vllm.transformers_utils.tokenizer_group import get_tokenizer_group\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Setup tokenizer\n    tokenizer_name = \"facebook/opt-125m\"  # Fast tokenizer for testing\n    tokenizer_group = get_tokenizer_group(\n        tokenizer_pool_config=None,\n        tokenizer_id=tokenizer_name,\n        enable_lora=False,\n        max_num_seqs=100,\n        max_input_length=None,\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        revision=None,\n    )\n    \n    # Create test sequences with varying lengths\n    test_prompts = [\n        \"The quick brown fox jumps over the lazy dog. \" * 10,  # Medium\n        \"Hello world! \" * 50,  # Long repetitive\n        \"In a hole in the ground there lived a hobbit. Not a nasty, dirty, wet hole, filled with the ends of worms and an oozy smell, nor yet a dry, bare, sandy hole with nothing in it to sit down on or to eat: it was a hobbit-hole, and that means comfort. \" * 5,  # Long narrative\n    ]\n    \n    # Tokenize prompts\n    tokenizer = tokenizer_group.get_lora_tokenizer(None)\n    sequences = []\n    \n    for i, prompt in enumerate(test_prompts):\n        prompt_token_ids = tokenizer.encode(prompt, add_special_tokens=True)\n        \n        # Create sequence\n        seq = Sequence(\n            seq_id=i,\n            prompt=prompt,\n            prompt_token_ids=prompt_token_ids,\n            block_size=16,\n            eos_token_id=tokenizer.eos_token_id,\n            lora_request=None\n        )\n        \n        # Simulate generation by adding tokens\n        generated_tokens = [42, 123, 456, 789, 1011, 1213, 1415, 1617, 1819, 2021]  # Dummy tokens\n        for token_id in generated_tokens:\n            # Create logprobs for each token\n            logprobs = {\n                token_id: Logprob(logprob=-0.5),\n                token_id + 1: Logprob(logprob=-1.0),\n                token_id + 2: Logprob(logprob=-2.0),\n            }\n            seq.append_token_id(token_id, logprobs)\n        \n        sequences.append(seq)\n    \n    # Create sampling params\n    sampling_params = SamplingParams(\n        skip_special_tokens=True,\n        spaces_between_special_tokens=True,\n        logprobs=3\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"tokenizer_group\": tokenizer_group,\n        \"sequences\": sequences,\n        \"sampling_params\": sampling_params,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    Detokenizer, _ = resolve_target()\n    \n    # Create detokenizer instance\n    detokenizer = Detokenizer(data[\"tokenizer_group\"])\n    \n    # Run detokenization on all sequences\n    results = []\n    for seq in data[\"sequences\"]:\n        # Reset sequence state for consistent testing\n        seq.output_text = \"\"\n        seq.tokens = None\n        seq.prefix_offset = 0\n        seq.read_offset = 0\n        \n        # Decode each generated token incrementally\n        all_token_ids = seq.get_token_ids()\n        num_prompt_tokens = len(seq.prompt_token_ids)\n        \n        for i in range(num_prompt_tokens, len(all_token_ids)):\n            # Simulate incremental generation\n            seq_view = Sequence(\n                seq_id=seq.seq_id,\n                prompt=seq.prompt,\n                prompt_token_ids=seq.prompt_token_ids,\n                block_size=seq.block_size,\n                eos_token_id=seq.eos_token_id,\n                lora_request=seq.lora_request\n            )\n            \n            # Copy state\n            seq_view.output_text = seq.output_text\n            seq_view.tokens = seq.tokens\n            seq_view.prefix_offset = seq.prefix_offset\n            seq_view.read_offset = seq.read_offset\n            \n            # Add tokens up to current position\n            for j in range(num_prompt_tokens, i + 1):\n                seq_view.append_token_id(all_token_ids[j], seq.output_logprobs[j - num_prompt_tokens] if j - num_prompt_tokens < len(seq.output_logprobs) else {})\n            \n            # Decode current token\n            detokenizer.decode_sequence_inplace(seq_view, data[\"sampling_params\"])\n            \n            # Update state\n            seq.output_text = seq_view.output_text\n            seq.tokens = seq_view.tokens\n            seq.prefix_offset = seq_view.prefix_offset\n            seq.read_offset = seq_view.read_offset\n        \n        results.append({\n            \"output_text\": seq.output_text,\n            \"tokens\": seq.tokens,\n            \"prefix_offset\": seq.prefix_offset,\n            \"read_offset\": seq.read_offset,\n            \"num_tokens\": len(all_token_ids)\n        })\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert tokens to serializable format\n    serializable_result = []\n    for item in result:\n        serializable_item = item.copy()\n        if serializable_item.get(\"tokens\"):\n            serializable_item[\"tokens\"] = list(serializable_item[\"tokens\"])\n        serializable_result.append(serializable_item)\n    \n    torch.save({\"type\": \"detokenizer_result\", \"data\": serializable_result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert len(current_result) == len(reference_result), f\"Result count mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (current, reference) in enumerate(zip(current_result, reference_result)):\n        # Check output text\n        assert current[\"output_text\"] == reference[\"output_text\"], f\"Seq {i}: output_text mismatch\"\n        \n        # Check tokens if present\n        if current.get(\"tokens\") and reference.get(\"tokens\"):\n            assert current[\"tokens\"] == reference[\"tokens\"], f\"Seq {i}: tokens mismatch\"\n        \n        # Check offsets\n        assert current[\"prefix_offset\"] == reference[\"prefix_offset\"], f\"Seq {i}: prefix_offset mismatch\"\n        assert current[\"read_offset\"] == reference[\"read_offset\"], f\"Seq {i}: read_offset mismatch\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - detokenization is CPU-bound\n    warmup = 3\n    iters = 20  # More iterations since this is a fast operation\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"bfdb1ba5c3fb14387c69acb1f5067102d8028e56\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Detokenization is CPU-bound\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "c0569dbc82b5e945a77878190114d1b68027828b",
    "commit_short": "c0569dbc",
    "commit_subject": "[Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py",
      "vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py",
      "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
      "vllm/model_executor/layers/fused_moe/deep_gemm_moe.py",
      "vllm/model_executor/layers/fused_moe/fused_batched_moe.py",
      "vllm/model_executor/layers/fused_moe/fused_moe.py",
      "vllm/model_executor/layers/fused_moe/modular_kernel.py",
      "vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py",
      "vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/20725",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "parent_commit": "8bb43b9c9ee878e07038d3f36aaf279ffb2fabab",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.361701965332031e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: c0569dbc82b5e945a77878190114d1b68027828b\nMessage: [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use TritonExperts as main target\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.fused_moe.fused_moe\"\n        symbol_name = \"TritonExperts\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # MoE configuration for typical model (e.g., Mixtral)\n    batch_size = 4\n    seq_len = 512  # Reduced for memory constraints\n    hidden_size = 4096\n    num_experts = 8\n    top_k = 2\n    expert_intermediate_size = 14336\n    \n    # Create tensors\n    M = batch_size * seq_len\n    N = expert_intermediate_size\n    K = hidden_size\n    \n    # Hidden states input\n    hidden_states = torch.randn(M, K, device=device, dtype=dtype)\n    \n    # Expert weights (gate and up projections combined)\n    w1 = torch.randn(num_experts, N * 2, K, device=device, dtype=dtype)\n    # Down projection weights\n    w2 = torch.randn(num_experts, K, N, device=device, dtype=dtype)\n    \n    # Router outputs\n    topk_ids = torch.randint(0, num_experts, (M, top_k), device=device, dtype=torch.int32)\n    topk_weights = torch.randn(M, top_k, device=device, dtype=dtype)\n    topk_weights = torch.softmax(topk_weights, dim=-1)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_states\": hidden_states,\n        \"w1\": w1,\n        \"w2\": w2,\n        \"topk_ids\": topk_ids,\n        \"topk_weights\": topk_weights,\n        \"num_experts\": num_experts,\n        \"top_k\": top_k,\n        \"M\": M,\n        \"N\": N,\n        \"K\": K,\n        \"activation\": \"silu\"\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create the TritonExperts instance\n    triton_experts = target(\n        use_fp8_w8a8=False,\n        use_int8_w8a8=False,\n        use_int8_w8a16=False,\n        use_int4_w4a16=False,\n        per_act_token_quant=False,\n        per_channel_quant=False,\n        block_shape=(16, 256, 64),\n    )\n    \n    # Prepare workspace tensors\n    M, N, K = data[\"M\"], data[\"N\"], data[\"K\"]\n    top_k = data[\"top_k\"]\n    \n    workspace1 = torch.zeros(M, top_k, max(N // 2, K), \n                             device=data[\"device\"], dtype=data[\"dtype\"])\n    workspace2 = torch.zeros(M, top_k, max(N, K), \n                            device=data[\"device\"], dtype=data[\"dtype\"])\n    output = torch.zeros(M, K, device=data[\"device\"], dtype=data[\"dtype\"])\n    \n    with torch.no_grad():\n        # Call the optimized apply function\n        triton_experts.apply(\n            output=output,\n            hidden_states=data[\"hidden_states\"],\n            w1=data[\"w1\"],\n            w2=data[\"w2\"],\n            topk_weights=data[\"topk_weights\"],\n            topk_ids=data[\"topk_ids\"],\n            activation=data[\"activation\"],\n            global_num_experts=data[\"num_experts\"],\n            expert_map=None,\n            w1_scale=None,\n            w2_scale=None,\n            w1_zp=None,\n            w2_zp=None,\n            a1q_scale=None,\n            a2_scale=None,\n            workspace13=workspace1,\n            workspace2=workspace2,\n            expert_tokens_meta=None,\n            apply_router_weight_on_input=False\n        )\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"c0569dbc82b5e945a77878190114d1b68027828b\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "commit_short": "c45f3c3a",
    "commit_subject": "Optimize tensor parallel execution speed (#17)",
    "repo": "vllm",
    "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
    "files_changed": [
      "benchmark/benchmark_latency.py",
      "cacheflow/parallel_utils/tensor_parallel/__init__.py",
      "cacheflow/parallel_utils/tensor_parallel/layers.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/17",
    "models": [
      "N/A"
    ],
    "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 117.65412759780884,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 7a7929abe8e2",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "facebook/opt-13b",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd\nMessage: Optimize tensor parallel execution speed (#17)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - using ColumnParallelLinear as primary target\n    if not (module_path and symbol_name):\n        module_path = \"cacheflow.parallel_utils.tensor_parallel.layers\"\n        symbol_name = \"ColumnParallelLinear\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        # Also get RowParallelLinear for comprehensive testing\n        row_parallel = getattr(module, \"RowParallelLinear\", None)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return (target, row_parallel), fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Tensor parallel configuration\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Model dimensions typical for LLMs\n    batch_size = 4\n    seq_len = 512  # Reduced for faster testing\n    hidden_size = 4096\n    intermediate_size = 11008\n    \n    # Set tensor parallel size to 1 for single GPU testing\n    # In production this would be > 1\n    os.environ[\"RANK\"] = \"0\"\n    os.environ[\"WORLD_SIZE\"] = \"1\"\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"12355\"\n    \n    # Initialize distributed if not already\n    if not torch.distributed.is_initialized():\n        torch.distributed.init_process_group(backend=\"nccl\" if hw_info[\"device\"] == \"cuda\" else \"gloo\")\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"hidden_size\": hidden_size,\n        \"intermediate_size\": intermediate_size,\n        # Input tensors\n        \"input_tensor\": torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype),\n        \"column_input\": torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype),\n        \"row_input\": torch.randn(batch_size * seq_len, intermediate_size, device=device, dtype=dtype),\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    (ColumnParallelLinear, RowParallelLinear), fq_name = resolve_target()\n    \n    device = data[\"device\"]\n    dtype = data[\"dtype\"]\n    hidden_size = data[\"hidden_size\"]\n    intermediate_size = data[\"intermediate_size\"]\n    \n    # Create layer instances\n    column_layer = ColumnParallelLinear(\n        input_size=hidden_size,\n        output_size=intermediate_size,\n        bias=True,\n        gather_output=False,\n        skip_bias_add=False,\n        params_dtype=dtype,\n        use_cpu_initialization=False,\n        perform_initialization=True\n    ).to(device)\n    \n    row_layer = RowParallelLinear(\n        input_size=intermediate_size,\n        output_size=hidden_size,\n        bias=True,\n        input_is_parallel=True,\n        skip_bias_add=False,\n        params_dtype=dtype,\n        use_cpu_initialization=False,\n        perform_initialization=True\n    ).to(device)\n    \n    # Execute forward passes - this is what was optimized\n    with torch.no_grad():\n        # Column parallel forward (hidden -> intermediate)\n        column_output = column_layer(data[\"column_input\"])\n        \n        # Row parallel forward (intermediate -> hidden)\n        row_output = row_layer(column_output)\n    \n    return {\"column_output\": column_output, \"row_output\": row_output}\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, dict):\n        # Store each tensor separately\n        stored_data = {}\n        for key, value in result.items():\n            if isinstance(value, torch.Tensor):\n                stored_data[key] = value.cpu()\n            else:\n                stored_data[key] = value\n        torch.save({\"type\": \"dict\", \"data\": stored_data}, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        \n        for key in current_result:\n            current_val = current_result[key]\n            reference_val = reference_result[key]\n            \n            if isinstance(current_val, torch.Tensor):\n                assert current_val.shape == reference_val.shape\n                assert current_val.dtype == reference_val.dtype\n                \n                # Determine tolerances based on dtype\n                if current_val.dtype in (torch.float16, torch.bfloat16):\n                    rtol, atol = 1e-3, 1e-4\n                else:\n                    rtol, atol = 1e-5, 1e-7\n                \n                torch.testing.assert_close(\n                    current_val.cpu(),\n                    reference_val.cpu(),\n                    rtol=rtol, atol=atol\n                )\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "ca7a2d5f28eac9621474563cdda0e08596222755",
    "commit_short": "ca7a2d5f",
    "commit_subject": "Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "333681408feabb97193880303b23f6571ba39045",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.1948089599609375e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
    "commit_short": "ccf02fcb",
    "commit_subject": "Revert \"[Model] Mamba2 Prefill Performance Tweaks:",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "acaea3bb07883c80b71643ebee1cd08d555797bc",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.1948089599609375e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "ibm-ai-platform/Bamba-9B",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "commit_short": "ce6bf3a2",
    "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
    "files_changed": [
      ".buildkite/run-tpu-test.sh",
      ".buildkite/test-pipeline.yaml",
      "tests/compile/test_wrapper.py",
      "tests/tpu/__init__.py",
      "tests/tpu/test_custom_dispatcher.py",
      "vllm/compilation/__init__.py",
      "vllm/compilation/wrapper.py",
      "vllm/envs.py",
      "vllm/worker/tpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7898",
    "models": [
      "N/A"
    ],
    "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 522.2120008468628,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "google/gemma-2b",
    "has_agent_patch": true,
    "baseline_install_method": "docker_fallback",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": "Agent benchmark produced no metrics",
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": 54.71,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": 55.5,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": 1.4439773350392966,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:40:58 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:41:01 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:41:01 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:41:01 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:41:23 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:41:28 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:41:29 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:41:29 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-01 14:41:52 model_runner.py:1331] Graph capturing finished in 23 secs.\nThroughpu",
    "human_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:42:31 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:42:33 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:42:33 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:42:33 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:42:34 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:42:39 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:42:40 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:42:40 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-01 14:43:00 model_runner.py:1331] Graph capturing finished in 19 secs.\nThroughpu",
    "agent_raw": "Namespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='google/gemma-2b', tokenizer='google/gemma-2b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='auto', num_scheduler_steps=1, use_v2_block_manager=False, enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None, output_json=None, distributed_executor_backend=None, load_format='auto', disable_async_output_proc=False)\nINFO 01-01 14:43:39 llm_engine.py:210] Initializing an LLM engine (v0.5.5) with config: model='google/gemma-2b', speculative_config=None, tokenizer='google/gemma-2b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2b, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\nINFO 01-01 14:43:41 model_runner.py:906] Starting to load model google/gemma-2b...\nWARNING 01-01 14:43:41 gemma.py:56] Gemma's activation function was incorrectly set to exact GeLU in the config JSON file when it was initially released. Changing the activation function to approximate GeLU (`gelu_pytorch_tanh`). If you want to use the legacy `gelu`, edit the config JSON to set `hidden_activation=gelu` instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\nINFO 01-01 14:43:41 weight_utils.py:236] Using model weights format ['*.safetensors']\nINFO 01-01 14:43:42 model_runner.py:917] Loading model weights took 4.6720 GB\nINFO 01-01 14:43:47 gpu_executor.py:121] # GPU blocks: 231137, # CPU blocks: 14563\nINFO 01-01 14:43:48 model_runner.py:1212] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-01 14:43:48 model_runner.py:1216] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\n/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:111: FutureWarning: U",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ce6bf3a2cff4860c5661cac2280e0a28bedb6440\nMessage: [torch.compile] avoid Dynamo guard evaluation overhead (#7898)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, the main optimization is TorchCompileWrapperWithCustomDispatcher\n        module_path = \"vllm.compilation.wrapper\"\n        symbol_name = \"TorchCompileWrapperWithCustomDispatcher\"  # Note: typo in original\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float32  # Use float32 for CPU compatibility\n    \n    # Create a simple model that will be compiled multiple times\n    # This simulates the TPU model runner scenario\n    class TestModel(torch.nn.Module):\n        def __init__(self, hidden_size=512):\n            super().__init__()\n            self.linear1 = torch.nn.Linear(hidden_size, hidden_size)\n            self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n            \n        def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n            x = self.linear1(x)\n            if cache is not None:\n                x = x + cache\n            x = torch.relu(x)\n            x = self.linear2(x)\n            return x\n    \n    # Create test inputs simulating different dispatch scenarios\n    batch_size = 8\n    seq_len = 128\n    hidden_size = 512\n    \n    model = TestModel(hidden_size).to(device).to(dtype)\n    \n    # Prefill inputs (prompt processing)\n    prefill_input = torch.randn(batch_size, seq_len, hidden_size, \n                                device=device, dtype=dtype)\n    \n    # Decode inputs (token generation) \n    decode_input = torch.randn(batch_size, 1, hidden_size,\n                               device=device, dtype=dtype)\n    \n    # Cache for decode phase\n    cache = torch.randn(batch_size, 1, hidden_size,\n                        device=device, dtype=dtype)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"model\": model,\n        \"prefill_input\": prefill_input,\n        \"decode_input\": decode_input,\n        \"cache\": cache,\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"hidden_size\": hidden_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create wrapper with custom dispatcher\n    class TestWrapper(target):\n        def __init__(self, model):\n            self.model = model\n            # Use eager backend for CPU compatibility\n            backend = \"eager\" if data[\"device\"].type == \"cpu\" else \"inductor\"\n            compiled_callable = torch.compile(self.forward, backend=backend)\n            super().__init__(compiled_callable)\n        \n        def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n            return self.model(x, cache)\n        \n        def __call__(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n            # Simulate the dispatch logic from ModelWrapper\n            if len(self.compiled_codes) >= 2 and self.use_custom_dispatcher:\n                # Dispatch based on whether we have cache (decode) or not (prefill)\n                dispatch_id = 0 if cache is None else 1\n                with self.dispatch_to_code(dispatch_id):\n                    return self.forward(x, cache)\n            else:\n                return self.compiled_callable(x, cache)\n    \n    wrapper = TestWrapper(data[\"model\"])\n    \n    # Warmup to compile both paths\n    with torch.no_grad():\n        # Compile prefill path\n        _ = wrapper(data[\"prefill_input\"], None)\n        # Compile decode path  \n        _ = wrapper(data[\"decode_input\"], data[\"cache\"])\n    \n    # Measure dispatch overhead with many calls\n    results = []\n    with torch.no_grad():\n        # Alternate between prefill and decode to trigger dispatch logic\n        for i in range(100):\n            if i % 2 == 0:\n                output = wrapper(data[\"prefill_input\"], None)\n            else:\n                output = wrapper(data[\"decode_input\"], data[\"cache\"])\n            results.append(output)\n    \n    return results[-1]  # Return last output for equivalence checking\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ce6bf3a2cff4860c5661cac2280e0a28bedb6440\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "commit_short": "cf2f084d",
    "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/core/test_scheduler.py",
      "vllm/config.py",
      "vllm/core/scheduler.py",
      "vllm/engine/arg_utils.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3279",
    "models": [
      "N/A"
    ],
    "parent_commit": "f721096d48a7e3b98dffcb9b400bf58989cef64d",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 850.165488243103,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for f721096d48a7",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: cf2f084d56a1293cb08da2393984cdc7685ac019\nMessage: Dynamic scheduler delay to improve ITL performance  (#3279)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, the main changes are in Scheduler._passed_delay\n        module_path = \"vllm.core.scheduler\"\n        symbol_name = \"Scheduler\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required classes for scheduler setup\n    try:\n        from vllm.config import SchedulerConfig, CacheConfig\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block.utils import SequenceGroup\n        from vllm.core.scheduler import SequenceGroupMetadata\n        from vllm.compilation.backends import Sequence\n        from vllm import SamplingParams\n        from vllm.block import LogicalTokenBlock\n    except ImportError as e:\n        print(json.dumps({\"target_resolved\": False, \"error\": f\"Import error: {e}\"}))\n        sys.exit(1)\n    \n    # Create scheduler configurations\n    block_size = 16\n    max_num_batched_tokens = 4096\n    max_num_seqs = 256\n    max_model_len = 2048\n    \n    # Test with delay factor (the optimization parameter)\n    delay_factor = float(os.getenv(\"DELAY_FACTOR\", \"0.5\"))\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=max_num_batched_tokens,\n        max_num_seqs=max_num_seqs,\n        max_model_len=max_model_len,\n        delay_factor=delay_factor\n    )\n    \n    cache_config = CacheConfig(\n        block_size=block_size,\n        gpu_memory_utilization=0.9,\n        swap_space_bytes=0,\n        cache_dtype=\"auto\"\n    )\n    cache_config.num_cpu_blocks = 512\n    cache_config.num_gpu_blocks = 1024\n    \n    # Create scheduler instance\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    \n    # Create simulated requests queue\n    num_requests = 100\n    requests = []\n    \n    for i in range(num_requests):\n        # Mix of different prompt lengths\n        prompt_length = np.random.choice([128, 256, 512, 1024])\n        request_id = f\"req_{i}\"\n        \n        # Create sequence and sequence group\n        prompt_tokens = list(range(prompt_length))\n        seq = Sequence(\n            seq_id=i,\n            inputs={\"prompt_token_ids\": prompt_tokens},\n            block_size=block_size\n        )\n        \n        sampling_params = SamplingParams(\n            temperature=0.7,\n            max_tokens=128\n        )\n        \n        # Create sequence group with arrival time\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time() + i * 0.01  # Stagger arrivals\n        )\n        \n        requests.append(seq_group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float16,\n        \"hw_info\": hw_info,\n        \"scheduler\": scheduler,\n        \"requests\": requests,\n        \"delay_factor\": delay_factor,\n        \"scheduler_config\": scheduler_config,\n        \"cache_config\": cache_config\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    scheduler = data[\"scheduler\"]\n    requests = data[\"requests\"]\n    \n    # Reset scheduler state\n    scheduler.waiting = deque()\n    scheduler.running = []\n    scheduler.swapped = deque()\n    scheduler.prev_time = 0.0\n    scheduler.prev_prompt = False\n    scheduler.last_prompt_latency = 0.0\n    \n    # Simulate scheduling with delay factor\n    results = {\n        \"scheduled_prompts\": [],\n        \"schedule_times\": [],\n        \"waiting_times\": [],\n        \"batch_sizes\": []\n    }\n    \n    # Add requests progressively and schedule\n    request_idx = 0\n    total_scheduled = 0\n    \n    # Run scheduling iterations\n    for iteration in range(50):\n        # Add some new requests\n        while request_idx < len(requests) and request_idx < (iteration + 1) * 2:\n            scheduler.add_seq_group(requests[request_idx])\n            request_idx += 1\n        \n        # Record time before scheduling\n        start_time = time.perf_counter()\n        \n        # Call schedule method (the optimized function)\n        seq_group_meta, scheduler_outputs = scheduler.schedule()\n        \n        # Record scheduling time\n        schedule_time = time.perf_counter() - start_time\n        results[\"schedule_times\"].append(schedule_time * 1000)  # Convert to ms\n        \n        if scheduler_outputs.scheduled_seq_groups:\n            total_scheduled += len(scheduler_outputs.scheduled_seq_groups)\n            results[\"scheduled_prompts\"].append(len(scheduler_outputs.scheduled_seq_groups))\n            results[\"batch_sizes\"].append(scheduler_outputs.num_batched_tokens)\n            \n            # Simulate processing time for prompts\n            if scheduler_outputs.prompt_run:\n                # Simulate prompt processing latency\n                time.sleep(0.01)  # 10ms simulated processing\n        \n        # Record waiting queue size\n        results[\"waiting_times\"].append(len(scheduler.waiting))\n        \n        # Break if all requests scheduled\n        if total_scheduled >= len(requests):\n            break\n        \n        # Small delay between iterations\n        time.sleep(0.001)\n    \n    # Calculate metrics\n    results[\"total_scheduled\"] = total_scheduled\n    results[\"avg_schedule_time_ms\"] = np.mean(results[\"schedule_times\"]) if results[\"schedule_times\"] else 0\n    results[\"avg_batch_size\"] = np.mean(results[\"batch_sizes\"]) if results[\"batch_sizes\"] else 0\n    results[\"max_waiting_queue\"] = max(results[\"waiting_times\"]) if results[\"waiting_times\"] else 0\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For scheduler, check that key metrics are similar\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check that total scheduled is the same\n        assert current_result.get(\"total_scheduled\") == reference_result.get(\"total_scheduled\"), \\\n            f\"Total scheduled mismatch: {current_result.get('total_scheduled')} vs {reference_result.get('total_scheduled')}\"\n        \n        # Check that scheduling times are reasonable (within 2x)\n        curr_time = current_result.get(\"avg_schedule_time_ms\", 0)\n        ref_time = reference_result.get(\"avg_schedule_time_ms\", 0)\n        if ref_time > 0:\n            ratio = curr_time / ref_time\n            assert 0.5 <= ratio <= 2.0, f\"Schedule time ratio {ratio} out of bounds\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_scheduler(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU scheduler operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - scheduler is CPU-based\n    warmup = 3\n    iters = 10\n    \n    result, timing_stats = time_cpu_scheduler(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"cf2f084d56a1293cb08da2393984cdc7685ac019\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Scheduler runs on CPU\n        \"dtype\": \"torch.float16\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True,\n        \"delay_factor\": data[\"delay_factor\"],\n        \"avg_schedule_time_ms\": result.get(\"avg_schedule_time_ms\", 0),\n        \"total_scheduled\": result.get(\"total_scheduled\", 0)\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
    "commit_short": "d4bc1a4d",
    "commit_subject": "Add unoptimized OPT Attention",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
    "files_changed": [
      "cacheflow/models/attention.py",
      "cacheflow/models/opt.py"
    ],
    "pr_url": "No PR found",
    "models": [
      "facebook/opt-125m",
      "facebook/opt-350m",
      "facebook/opt-1.3b",
      "facebook/opt-2.7b",
      "facebook/opt-6.7b"
    ],
    "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 96.37895154953003,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for b56b6ca0d650",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "facebook/opt-125m",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc\nMessage: Add unoptimized OPT Attention\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use OPTCacheFlowAttention\n    if not (module_path and symbol_name):\n        module_path = \"cacheflow.models.attention\"\n        symbol_name = \"OPTCacheFlowAttention\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # OPT attention workload - prefill scenario\n    batch_size = 4\n    seq_len = 512  # Reduced for stability\n    num_heads = 32\n    head_dim = 64\n    embed_dim = num_heads * head_dim\n    \n    # Adjust for hardware constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        batch_size = 2\n        seq_len = 256\n    \n    # Create input tensors for attention\n    hidden_states = torch.randn(batch_size * seq_len, embed_dim, device=device, dtype=dtype)\n    \n    # Query, Key, Value projections (simulating what OPTAttention does)\n    query = torch.randn(batch_size * seq_len, num_heads, head_dim, device=device, dtype=dtype)\n    key = torch.randn(batch_size * seq_len, num_heads, head_dim, device=device, dtype=dtype)\n    value = torch.randn(batch_size * seq_len, num_heads, head_dim, device=device, dtype=dtype)\n    \n    # KV cache for decode phase\n    block_size = 16\n    num_blocks = 64\n    key_cache = torch.zeros(num_blocks, num_heads, head_dim, block_size, device=device, dtype=dtype)\n    value_cache = torch.zeros(num_blocks, num_heads, block_size, head_dim, device=device, dtype=dtype)\n    \n    # Input metadata\n    from cacheflow.models import InputMetadata\n    \n    # Create metadata for prefill\n    prompt_lens = [seq_len] * batch_size\n    slot_mapping = torch.arange(batch_size * seq_len, device=device, dtype=torch.long)\n    context_lens = torch.tensor([seq_len] * batch_size, device=device, dtype=torch.long)\n    block_tables = torch.zeros(batch_size, num_blocks // batch_size, device=device, dtype=torch.int32)\n    \n    input_metadata = InputMetadata(\n        num_prompts=batch_size,\n        num_generation_tokens=0,\n        prompt_lens=prompt_lens,\n        slot_mapping=slot_mapping,\n        context_lens=context_lens,\n        block_tables=block_tables\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"query\": query,\n        \"key\": key,\n        \"value\": value,\n        \"key_cache\": key_cache,\n        \"value_cache\": value_cache,\n        \"input_metadata\": input_metadata,\n        \"cache_event\": None,\n        \"scale\": 1.0 / math.sqrt(head_dim),\n        \"batch_size\": batch_size,\n        \"seq_len\": seq_len,\n        \"num_heads\": num_heads,\n        \"head_dim\": head_dim\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Create attention instance\n    attn = target(scale=data[\"scale\"])\n    \n    # Execute multi_query_kv_attention for prefill\n    output = torch.empty_like(data[\"query\"])\n    \n    with torch.no_grad():\n        # Call the unoptimized attention\n        attn.multi_query_kv_attention(\n            output,\n            data[\"query\"],\n            data[\"key\"],\n            data[\"value\"]\n        )\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
    "commit_short": "d55e446d",
    "commit_subject": "[V1][Spec Decode] Small refactors to improve eagle",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --batch-size 2",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "ec82c3e388b962a30a02fa376c222cef787b3c14",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 6243.7799236774445,
    "error": "BASELINE server failed to start. Logs: eturn value is None\nWARNING 01-02 17:33:13 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-02 17:33:13 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-02 17:33:13 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-02 17:33:13 [__init__.py:246] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/usr/local/lib/python3.11/site-packages/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 20, in <module>\n    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 38, in <module>\n    from vllm.transformers_utils.config import (\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 31, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py\", line 26, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py\", line 75, in <module>\n    AutoConfig.register(\"aimv2\", AIMv2Config)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1081, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\nValueError: 'aimv2' is already used by a Transformers config, pick another name.\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "WARNING 01-02 17:32:57 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-02 17:32:57 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-02 17:32:57 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-02 17:32:57 [__init__.py:246] Automatically detected platform cuda.",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
    "commit_short": "d7740ea4",
    "commit_subject": "[Core] Optimize sampler get_logprobs (#4594)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.1-8B-Instruct --input-len 256 --output-len 256",
    "files_changed": [
      "vllm/model_executor/layers/sampler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/4594",
    "models": [
      "N/A"
    ],
    "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 1069.4858162403107,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for cc466a32903d",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: d7740ea4dcee4ab75d7d6eef723f33cae957b288\nMessage: [Core] Optimize sampler get_logprobs (#4594)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target is _get_logprobs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.sampler\"\n        symbol_name = \"_get_logprobs\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create realistic LLM sampling scenario\n    batch_size = 32  # Multiple sequences\n    vocab_size = 32000  # Llama vocab size\n    num_query_tokens = 64  # Tokens across batch\n    \n    # Generate logprobs tensor (post-softmax log probabilities)\n    logits = torch.randn(num_query_tokens, vocab_size, device=device, dtype=dtype)\n    logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n    \n    # Create mock sampling metadata\n    from vllm.model_executor.sampling_metadata import SamplingMetadata, SequenceGroupToSample\n    from vllm import SamplingParams\n    from vllm.sampling_params import SamplingType\n    from vllm.core.scheduler import SequenceData\n    \n    # Create sequence groups with various sampling configurations\n    seq_groups = []\n    sample_results = []\n    \n    # Mix of prompt and generation sequences\n    for i in range(batch_size):\n        # Create sampling params with logprobs requested\n        sampling_params = SamplingParams(\n            temperature=0.7,\n            top_p=0.9,\n            top_k=40,\n            logprobs=5,  # Request top-5 logprobs\n            prompt_logprobs=5 if i % 4 == 0 else None,  # Some with prompt logprobs\n        )\n        \n        # Create sequence data\n        seq_data = {\n            0: SequenceData(prompt_token_ids=list(range(100)))\n        }\n        \n        # Create sequence group\n        is_prompt = i % 4 == 0\n        seq_group = SequenceGroupToSample(\n            seq_ids=[0],\n            sampling_params=sampling_params,\n            seq_data=seq_data,\n            sample_indices=[i * 2] if not is_prompt else [],\n            prompt_logprob_indices=list(range(i * 2, i * 2 + 2)) if is_prompt else [],\n            do_sample=True,\n            is_prompt=is_prompt,\n            query_len=2 if is_prompt else None\n        )\n        seq_groups.append(seq_group)\n        \n        # Create sample results (next_token_ids, parent_ids)\n        if is_prompt:\n            sample_results.append(([np.random.randint(0, vocab_size)], [0]))\n        else:\n            sample_results.append(([np.random.randint(0, vocab_size)], [0]))\n    \n    # Create sampling metadata\n    sampling_metadata = SamplingMetadata(\n        seq_groups=seq_groups,\n        selected_token_indices=torch.arange(num_query_tokens, device=device),\n    )\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"logprobs\": logprobs,\n        \"sampling_metadata\": sampling_metadata,\n        \"sample_results\": sample_results,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    with torch.no_grad():\n        # Call the optimized _get_logprobs function\n        prompt_logprobs, sample_logprobs = target(\n            data[\"logprobs\"],\n            data[\"sampling_metadata\"],\n            data[\"sample_results\"]\n        )\n    \n    return (prompt_logprobs, sample_logprobs)\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert to serializable format\n    prompt_logprobs, sample_logprobs = result\n    \n    # Store as pickle since these are complex nested structures\n    import pickle\n    with open(filepath, 'wb') as f:\n        pickle.dump(result, f)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    import pickle\n    with open(filepath, 'rb') as f:\n        return pickle.load(f)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    current_prompt, current_sample = current_result\n    ref_prompt, ref_sample = reference_result\n    \n    # Check lengths match\n    assert len(current_prompt) == len(ref_prompt), f\"Prompt logprobs length mismatch\"\n    assert len(current_sample) == len(ref_sample), f\"Sample logprobs length mismatch\"\n    \n    # Check each sequence group's logprobs\n    for i, (curr_p, ref_p) in enumerate(zip(current_prompt, ref_prompt)):\n        if curr_p is None:\n            assert ref_p is None, f\"Prompt logprobs mismatch at group {i}\"\n        else:\n            assert len(curr_p) == len(ref_p), f\"Prompt logprobs token count mismatch at group {i}\"\n    \n    for i, (curr_s, ref_s) in enumerate(zip(current_sample, ref_sample)):\n        assert len(curr_s) == len(ref_s), f\"Sample logprobs length mismatch at group {i}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"d7740ea4dcee4ab75d7d6eef723f33cae957b288\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pkl\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float32\",  # logprobs are always float32\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "dae68969774e41b93b01cd31171ca033a92b574a",
    "commit_short": "dae68969",
    "commit_subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)",
    "repo": "vllm-project/vllm",
    "perf_command": "VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHMLA python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "c34eeec58d3a94437c5311e256f8ba21d1912a39",
    "status": "baseline_failed",
    "gpu_config": "H100:8",
    "benchmark_mode": "serving",
    "patch_type": null,
    "duration_s": 6726.257014274597,
    "error": "BASELINE server failed to start",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:05:27 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev281+gc34eeec5",
    "human_version": null,
    "agent_version": null,
    "model": "deepseek-ai/DeepSeek-R1",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
    "commit_short": "dcc6cfb9",
    "commit_subject": "[Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21193",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "parent_commit": "dd572c0ab3effa539b74f9a1288bb61ce83ada76",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.337860107421875e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: dcc6cfb991cd76369aad96e04424f29c8fecdbd8\nMessage: [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n        major, minor = hw_info[\"capability\"]\n        hw_info[\"supports_fp8\"] = major >= 9  # Hopper+\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n        hw_info[\"supports_fp8\"] = False\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit, the target is silu_mul_fp8_quant_deep_gemm\n        module_path = \"vllm.model_executor.layers.fused_moe.batched_deep_gemm_moe\"\n        symbol_name = \"silu_mul_fp8_quant_deep_gemm\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # MoE configuration for FP8 quantization kernel\n    # Typical MoE settings\n    num_experts = 8\n    num_tokens = 128  # Batch of tokens\n    hidden_size = 4096  # Model hidden dimension\n    intermediate_size = 14336  # MoE expert intermediate size (usually ~3.5x hidden)\n    \n    # FP8 specific settings\n    group_size = 128  # Quantization group size\n    \n    device = torch.device(hw_info[\"device\"])\n    \n    # Check if we can use FP8\n    if hw_info.get(\"supports_fp8\", False):\n        # Use float32 for inputs (will be quantized internally)\n        dtype = torch.float32\n    else:\n        # Fallback to float16 if FP8 not supported\n        dtype = torch.float16\n    \n    # Adjust workload for memory constraints\n    if hw_info.get(\"memory_gb\", float('inf')) < 16:\n        num_tokens = 64\n        intermediate_size = 11008  # Smaller intermediate size\n    \n    # Create input tensor (E, T, 2*H) - gate and up projections concatenated\n    y = torch.randn(num_experts, num_tokens, 2 * intermediate_size, \n                    device=device, dtype=dtype)\n    \n    # Output tensors for quantized results\n    y_q = torch.zeros(num_experts, num_tokens, intermediate_size, \n                      device=device, dtype=torch.int8)\n    y_s = torch.zeros(num_experts, num_tokens, \n                      device=device, dtype=torch.float32)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"y\": y,\n        \"y_q\": y_q,\n        \"y_s\": y_s,\n        \"fp8_max\": 448.0,  # E4M3 max value\n        \"group_size\": group_size,\n        \"num_experts\": num_experts,\n        \"num_tokens\": num_tokens,\n        \"intermediate_size\": intermediate_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the FP8 quantization kernel\n    with torch.no_grad():\n        # The function signature is:\n        # silu_mul_fp8_quant_deep_gemm(y, y_q, y_s, fp8_max, group_size)\n        y_q_out, y_s_out = target(\n            data[\"y\"],\n            data[\"y_q\"],\n            data[\"y_s\"],\n            data[\"fp8_max\"],\n            data[\"group_size\"]\n        )\n    \n    return (y_q_out, y_s_out)\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, tuple) and len(result) == 2:\n        # Store both quantized output and scales\n        torch.save({\n            \"type\": \"fp8_quant_result\",\n            \"y_q\": result[0].cpu(),\n            \"y_s\": result[1].cpu()\n        }, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    if data.get(\"type\") == \"fp8_quant_result\":\n        return (data[\"y_q\"], data[\"y_s\"])\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, tuple) and isinstance(reference_result, tuple):\n        # Check both quantized values and scales\n        y_q_curr, y_s_curr = current_result\n        y_q_ref, y_s_ref = reference_result\n        \n        # Check quantized values (exact match for int8)\n        assert y_q_curr.shape == y_q_ref.shape, f\"y_q shape mismatch\"\n        assert y_q_curr.dtype == y_q_ref.dtype, f\"y_q dtype mismatch\"\n        assert torch.equal(y_q_curr.cpu(), y_q_ref.cpu()), \"Quantized values mismatch\"\n        \n        # Check scales (with tolerance for float32)\n        assert y_s_curr.shape == y_s_ref.shape, f\"y_s shape mismatch\"\n        assert y_s_curr.dtype == y_s_ref.dtype, f\"y_s dtype mismatch\"\n        torch.testing.assert_close(\n            y_s_curr.cpu(),\n            y_s_ref.cpu(),\n            rtol=1e-5, atol=1e-7\n        )\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        elif current_result.dtype == torch.int8:\n            # Exact match for quantized values\n            assert torch.equal(current_result.cpu(), reference_result.cpu())\n            return\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache before timing\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check if FP8 is supported\n    if not hw_info.get(\"supports_fp8\", False) and hw_info[\"device\"] == \"cuda\":\n        # Try to proceed anyway, kernel might have fallback\n        pass\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        # This kernel requires CUDA\n        error_data = {\n            \"target_resolved\": True,\n            \"error\": \"FP8 quantization kernel requires CUDA device\",\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"dcc6cfb991cd76369aad96e04424f29c8fecdbd8\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),  # Exact for quantized values\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "e206b5433109d298e53451015465b2bf8f03ef0a",
    "commit_short": "e206b543",
    "commit_subject": "[v0][Core] Use xgrammar shared context to avoid co",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --guided-decoding-backend xgrammar",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "1d35662e6dc199431bfe4004cc84d66fd9b297b1",
    "status": "baseline_failed",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3055.2186255455017,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:02:53 [__init__.py:207] Automatically detected platform cuda.\n0.7.4.dev103+g1d35662e",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 17:04:36 [__init__.py:207] Automatically detected platform cuda.\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nusage: benchmark_serving.py [-h]\n                            [--backend {tgi,vllm,lmdeploy,deepspeed-mii,openai,openai-chat,tensorrt-llm,scalellm,sglang}]\n                            [--base-url BASE_URL] [--host HOST] [--port PORT]\n                            [--endpoint ENDPOINT]\n                            [--dataset-name {sharegpt,burstgpt,sonnet,random,hf}]\n                            [--dataset-path DATASET_PATH]\n                            [--max-concurrency MAX_CONCURRENCY] --model MODEL\n                            [--tokenizer TOKENIZER] [--best-of BEST_OF]\n                            [--use-beam-search] [--num-prompts NUM_PROMPTS]\n                            [--logprobs LOGPROBS]\n                            [--request-rate REQUEST_RATE]\n                            [--burstiness BURSTINESS] [--seed SEED]\n                            [--trust-remote-code] [--disable-tqdm] [--profile]\n                            [--save-result] [--metadata [KEY=VALUE ...]]\n                            [--result-dir RESULT_DIR]\n                            [--result-filename RESULT_FILENAME] [--ignore-eos]\n                            [--percentile-metrics PERCENTILE_METRICS]\n                            [--metric-percentiles METRIC_PERCENTILES]\n                            [--goodput GOODPUT [GOODPUT ...]]\n                            [--sonnet-input-len SONNET_INPUT_LEN]\n                            [--sonnet-output-len SONNET_OUTPUT_LEN]\n                            [--sonnet-prefix-len SONNET_PREFIX_LEN]\n                            [--sharegpt-output-len SHAREGPT_OUTPUT_LEN]\n                            [--random-input-len RANDOM_INPUT_LEN]\n                            [--random-output-len RANDOM_OUTPUT_LEN]\n                            [--random-range-ratio RANDOM_RANGE_RATIO]\n                            [--random-prefix-len RANDOM_PREFIX_LEN]\n                            [--hf-subset HF_SUBSET] [--hf-split HF_SPLIT]\n                            [--hf-output-len HF_OUTPUT_LEN]\n                            [--tokenizer-mode {auto,slow,mistral,custom}]\n                            [--served-model-name SERVED_MODEL_NAME]\n                            [--lora-modules LORA_MODULES [LORA_MODULES ...]]\nbenchmark_serving.py: error: unrecognized arguments: --guided-decoding-backend xgrammar\n",
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
    "commit_short": "e3580537",
    "commit_subject": "[Performance] Enable chunked prefill and prefix caching together (#7753)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048",
    "files_changed": [
      "tests/basic_correctness/test_chunked_prefill.py",
      "tests/core/test_block_manager.py",
      "tests/core/test_chunked_prefill_scheduler.py",
      "vllm/core/block_manager_v1.py",
      "vllm/core/block_manager_v2.py",
      "vllm/core/embedding_model_block_manager.py",
      "vllm/core/interfaces.py",
      "vllm/core/scheduler.py",
      "vllm/worker/model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7753",
    "models": [
      "N/A"
    ],
    "parent_commit": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2931.0759365558624,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for f508e03e7f2d",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: e3580537a41a46b0f3cd750b86b633c1857a8c90\nMessage: [Performance] Enable chunked prefill and prefix caching together (#7753)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target mark_blocks_as_computed\n    if not (module_path and symbol_name):\n        # Based on the diff, the key change is in mark_blocks_as_computed method\n        module_path = \"vllm.core.block_manager_v1\"\n        symbol_name = \"BlockSpaceManagerV1.mark_blocks_as_computed\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create a block manager setup that exercises chunked prefill + prefix caching\n    block_size = 16\n    num_gpu_blocks = 256\n    num_cpu_blocks = 0\n    \n    # Import the block manager class\n    from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm import SamplingParams\n    \n    # Create block manager with prefix caching enabled\n    block_manager = BlockSpaceManagerV1(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=num_cpu_blocks,\n        watermark=0.01,\n        enable_caching=True  # Enable prefix caching\n    )\n    \n    # Create a sequence group with a long prompt to test chunked prefill\n    prompt_length = 512  # Long enough to require multiple chunks\n    token_chunk_size = 64  # Chunk size for chunked prefill\n    \n    # Create sequence\n    seq = Sequence(\n        seq_id=0,\n        inputs={\"prompt_token_ids\": list(range(prompt_length))},\n        block_size=block_size\n    )\n    \n    # Create sequence group\n    seq_group = SequenceGroup(\n        request_id=\"test_request\",\n        seqs=[seq],\n        arrival_time=time.time(),\n        sampling_params=SamplingParams()\n    )\n    \n    # Allocate blocks for the sequence\n    block_manager.allocate(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"block_manager\": block_manager,\n        \"seq_group\": seq_group,\n        \"token_chunk_size\": token_chunk_size,\n        \"block_size\": block_size,\n        \"prompt_length\": prompt_length\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    block_manager = data[\"block_manager\"]\n    seq_group = data[\"seq_group\"]\n    token_chunk_size = data[\"token_chunk_size\"]\n    prompt_length = data[\"prompt_length\"]\n    \n    # Simulate chunked prefill by marking blocks as computed in chunks\n    results = []\n    num_chunks = (prompt_length + token_chunk_size - 1) // token_chunk_size\n    \n    for chunk_idx in range(num_chunks):\n        # Update the number of computed tokens for the sequence\n        for seq in seq_group.get_seqs():\n            current_computed = min((chunk_idx + 1) * token_chunk_size, prompt_length)\n            seq.data.update_num_computed_tokens(current_computed)\n        \n        # Call the optimized function with token_chunk_size\n        with torch.no_grad():\n            block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n            \n            # Get computed blocks for verification\n            computed_blocks = []\n            for seq in seq_group.get_seqs():\n                blocks = block_manager.get_all_computed_blocks(seq)\n                computed_blocks.append(len(blocks))\n        \n        results.append({\n            \"chunk_idx\": chunk_idx,\n            \"computed_blocks\": computed_blocks[0] if computed_blocks else 0\n        })\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"list\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert type(current_result) == type(reference_result), f\"Type mismatch\"\n    assert len(current_result) == len(reference_result), f\"Length mismatch\"\n    \n    for i, (curr, ref) in enumerate(zip(current_result, reference_result)):\n        assert curr[\"chunk_idx\"] == ref[\"chunk_idx\"], f\"Chunk index mismatch at {i}\"\n        assert curr[\"computed_blocks\"] == ref[\"computed_blocks\"], f\"Computed blocks mismatch at {i}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            times_ms.append((time.perf_counter() - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"e3580537a41a46b0f3cd750b86b633c1857a8c90\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "e493e48524e9e78ab33eafec6461b3940e361189",
    "commit_short": "e493e485",
    "commit_subject": "[V0][Bugfix] Fix parallel sampling performance reg",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model microsoft/phi-1_5 --backend vllm --num-prompts 100",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "4ce64e2df48649c4873f828b8bf71790aa1e56ee",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 3236.9154798984528,
    "error": "BASELINE server failed to start. Logs: eturn value is None\nWARNING 01-02 16:43:06 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-02 16:43:06 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-02 16:43:06 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-02 16:43:06 [__init__.py:246] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/usr/local/lib/python3.11/site-packages/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 20, in <module>\n    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 38, in <module>\n    from vllm.transformers_utils.config import (\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 31, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py\", line 26, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py\", line 75, in <module>\n    AutoConfig.register(\"aimv2\", AIMv2Config)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1081, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\nValueError: 'aimv2' is already used by a Transformers config, pick another name.\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "WARNING 01-02 16:42:51 [__init__.py:221] Platform plugin tpu function's return value is None\nINFO 01-02 16:42:51 [__init__.py:220] Platform plugin cuda loaded.\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin cuda function's return value is None\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin rocm function's return value is None\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin hpu function's return value is None\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin xpu function's return value is None\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin cpu function's return value is None\nWARNING 01-02 16:42:51 [__init__.py:221] Platform plugin neuron function's return value is None\nINFO 01-02 16:42:51 [__init__.py:246] Automatically detected platform cuda.",
    "human_version": null,
    "agent_version": null,
    "model": "microsoft/phi-1_5",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "e7523c2e031bc96740723ab63833d1cf94229ab4",
    "commit_short": "e7523c2e",
    "commit_subject": "[V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs (#18608)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --backend openai-chat --model google/gemma-3-12b-it --endpoint /v1/chat/completions --dataset-name hf --dataset-path lmarena-ai/VisionArena-Chat --hf-split train --num-prompts 1000",
    "files_changed": [
      "vllm/v1/sample/ops/topk_topp_sampler.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/18608",
    "models": [
      "google/gemma-3-12b-it"
    ],
    "parent_commit": "a869baca73eb90ae7bd18402915dc4bfc36cf06b",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 5997.983085632324,
    "error": "BASELINE server failed to start. Logs: /usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nINFO 01-02 17:29:07 [__init__.py:243] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/usr/local/lib/python3.11/site-packages/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/usr/local/lib/python3.11/site-packages/vllm/engine/arg_utils.py\", line 20, in <module>\n    from vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/config.py\", line 38, in <module>\n    from vllm.transformers_utils.config import (\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/config.py\", line 31, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/__init__.py\", line 26, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File \"/usr/local/lib/python3.11/site-packages/vllm/transformers_utils/configs/ovis.py\", line 75, in <module>\n    AutoConfig.register(\"aimv2\", AIMv2Config)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File \"/usr/local/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py\", line 1081, in register\n    raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\nValueError: 'aimv2' is already used by a Transformers config, pick another name.\n",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:28:51 [__init__.py:243] Automatically detected platform cuda.",
    "human_version": null,
    "agent_version": null,
    "model": "google/gemma-3-12b-it",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: e7523c2e031bc96740723ab63833d1cf94229ab4\nMessage: [V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs (#18608)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, the optimized function is flashinfer_sample\n        module_path = \"vllm.v1.sample.ops.topk_topp_sampler\"\n        symbol_name = \"flashinfer_sample\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Sampling workload for LLM decode\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Realistic decode batch configuration\n    batch_size = 64  # Many concurrent requests\n    vocab_size = 32000  # Llama vocabulary size\n    \n    # Create logits tensor (typical output from LLM)\n    logits = torch.randn(batch_size, vocab_size, device=device, dtype=dtype)\n    \n    # Apply temperature scaling (common in sampling)\n    temperature = 0.7\n    logits = logits / temperature\n    \n    # Top-k and top-p parameters (both set to trigger optimized path)\n    # The optimization specifically improves the case where both k and p are set\n    k = torch.full((batch_size,), 40, dtype=torch.int32, device=device)  # Top-40 sampling\n    p = torch.full((batch_size,), 0.95, dtype=torch.float32, device=device)  # Top-p 0.95\n    \n    # No per-request generators for FlashInfer path\n    generators = {}\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"logits\": logits,\n        \"k\": k,\n        \"p\": p,\n        \"generators\": generators,\n        \"batch_size\": batch_size,\n        \"vocab_size\": vocab_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the flashinfer_sample function with both k and p (optimized path)\n    with torch.no_grad():\n        result = target(\n            data[\"logits\"],\n            data[\"k\"],\n            data[\"p\"],\n            data[\"generators\"]\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        \n        # For sampling, we check that results are valid token indices\n        # Since sampling is stochastic, we verify statistical properties\n        assert torch.all(current_result >= 0), \"Negative token indices found\"\n        assert torch.all(current_result < 32000), \"Token indices exceed vocabulary size\"\n        \n        # For deterministic mode, results should match exactly\n        if current_result.dtype in (torch.int32, torch.int64):\n            # Token indices should match exactly in deterministic mode\n            torch.testing.assert_close(\n                current_result.cpu(),\n                reference_result.cpu(),\n                rtol=0, atol=0\n            )\n        else:\n            # Should not happen for token indices\n            rtol, atol = 1e-5, 1e-7\n            torch.testing.assert_close(\n                current_result.cpu(),\n                reference_result.cpu(),\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache before timing\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        # CPU fallback (though FlashInfer is CUDA-only)\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"e7523c2e031bc96740723ab63833d1cf94229ab4\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
    "commit_short": "e7b20426",
    "commit_subject": "Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model 01-ai/Yi-1.5-9B-Chat --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "benchmarks/kernels/benchmark_grouped_gemm_cutlass.py",
      "csrc/moe/moe_permute_unpermute_op.cu",
      "tests/kernels/moe/test_cutlass_moe.py",
      "tests/kernels/moe/test_pplx_cutlass_moe.py",
      "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
      "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21334",
    "models": [
      "01-ai/Yi-1.5-9B-Chat"
    ],
    "parent_commit": "90f1e55421f1b61394ba25abe34bf5abd82a71af",
    "status": "exception",
    "gpu_config": "H100:1",
    "benchmark_mode": null,
    "patch_type": null,
    "duration_s": 3.2901763916015625e-05,
    "error": "[Errno 32] Broken pipe",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "01-ai/Yi-1.5-9B-Chat",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: e7b204268132cb775c139574c1ff4ad7e15c8f66\nMessage: Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, target cutlass_moe_fp8 function\n        module_path = \"vllm.model_executor.layers.fused_moe.cutlass_moe\"\n        symbol_name = \"cutlass_moe_fp8\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # MoE configuration for FP8 CUTLASS kernel\n    batch_size = 4\n    seq_len = 512  # Reduced for stability\n    num_tokens = batch_size * seq_len\n    hidden_size = 4096  # Standard for 7B models\n    intermediate_size = 11008  # Standard for 7B models\n    num_experts = 8\n    top_k = 2\n    \n    # Create FP8 quantized weights\n    w1_q = torch.randint(0, 127, (num_experts, 2 * intermediate_size, hidden_size), \n                          device=device, dtype=torch.int8)\n    w2_q = torch.randint(0, 127, (num_experts, hidden_size, intermediate_size),\n                          device=device, dtype=torch.int8)\n    \n    # Scales for dequantization\n    w1_scale = torch.randn(num_experts, device=device, dtype=torch.float32) * 0.01 + 0.1\n    w2_scale = torch.randn(num_experts, device=device, dtype=torch.float32) * 0.01 + 0.1\n    \n    # Input activations\n    a = torch.randn(num_tokens, hidden_size, device=device, dtype=dtype)\n    \n    # Top-k routing\n    router_logits = torch.randn(num_tokens, num_experts, device=device, dtype=dtype)\n    topk_weights, topk_ids = torch.topk(router_logits, top_k, dim=-1)\n    topk_weights = torch.softmax(topk_weights, dim=-1)\n    \n    # Activation function (silu is default)\n    activation = \"silu\"\n    \n    # Optional scales for activation quantization\n    a1_scale = torch.tensor(0.1, device=device, dtype=torch.float32)\n    a2_scale = torch.tensor(0.1, device=device, dtype=torch.float32)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"a\": a,\n        \"w1_q\": w1_q,\n        \"w2_q\": w2_q,\n        \"topk_weights\": topk_weights,\n        \"topk_ids\": topk_ids,\n        \"w1_scale\": w1_scale,\n        \"w2_scale\": w2_scale,\n        \"activation\": activation,\n        \"a1_scale\": a1_scale,\n        \"a2_scale\": a2_scale,\n        \"per_act_token\": False,\n        \"per_out_ch\": False,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call cutlass_moe_fp8 with parameters\n    # Note: After the revert, the function no longer takes stride parameters\n    with torch.no_grad():\n        result = target(\n            a=data[\"a\"],\n            w1_q=data[\"w1_q\"],\n            w2_q=data[\"w2_q\"],\n            topk_weights=data[\"topk_weights\"],\n            topk_ids=data[\"topk_ids\"],\n            w1_scale=data[\"w1_scale\"],\n            w2_scale=data[\"w2_scale\"],\n            per_act_token=data[\"per_act_token\"],\n            activation=data[\"activation\"],\n            a1_scale=data[\"a1_scale\"],\n            a2_scale=data[\"a2_scale\"],\n            per_out_ch=data[\"per_out_ch\"],\n        )\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"e7b204268132cb775c139574c1ff4ad7e15c8f66\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
    "commit_short": "ec3b5ce9",
    "commit_subject": "Improve detokenization performance (#1338)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100",
    "files_changed": [
      "vllm/transformers_utils/tokenizer.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/1338",
    "models": [
      "N/A"
    ],
    "parent_commit": "6368e777a8ead7fb62054d3779c6237361ec0d86",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 107.71910548210144,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 6368e777a8ea",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9\nMessage: Improve detokenization performance (#1338)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on commit diff, the optimized function is detokenize_incrementally\n        module_path = \"vllm.transformers_utils.tokenizer\"\n        symbol_name = \"detokenize_incrementally\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required tokenizer utilities\n    try:\n        from transformers import AutoTokenizer\n    except ImportError:\n        print(json.dumps({\"error\": \"transformers not installed\", \"target_resolved\": False}))\n        sys.exit(1)\n    \n    # Use a fast tokenizer for testing (LLaMA tokenizer as mentioned in code)\n    tokenizer_name = \"hf-internal-testing/llama-tokenizer\"\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n    except Exception:\n        # Fallback to a common tokenizer if the specific one is not available\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n        except Exception as e:\n            print(json.dumps({\"error\": f\"Failed to load tokenizer: {e}\", \"target_resolved\": False}))\n            sys.exit(1)\n    \n    # Create realistic token sequences for detokenization\n    # Simulate incremental decoding scenario with various sequence lengths\n    test_sequences = [\n        \"The quick brown fox jumps over the lazy dog.\",\n        \"In the realm of artificial intelligence, large language models have revolutionized natural language processing.\",\n        \"def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\",\n        \"\ud83d\ude80 Emojis and special characters: \u00f1, \u00e9, \u00fc, \u4e2d\u6587, \u65e5\u672c\u8a9e, \ud55c\uad6d\uc5b4 \ud83c\udf0d\",\n        \" \".join([\"token\"] * 500),  # Long repetitive sequence\n    ]\n    \n    # Convert to token IDs\n    all_token_ids = []\n    for text in test_sequences:\n        ids = tokenizer.encode(text, add_special_tokens=True)\n        all_token_ids.append(ids)\n    \n    # Prepare test cases for incremental detokenization\n    test_cases = []\n    for ids in all_token_ids:\n        # Simulate incremental generation by processing tokens one by one\n        for i in range(1, min(len(ids), 100)):  # Limit to 100 tokens per sequence\n            test_cases.append({\n                \"all_input_ids\": ids[:i+1],\n                \"prev_tokens\": None if i == 0 else tokenizer.convert_ids_to_tokens(ids[:i]),\n                \"prefix_offset\": max(i - 5, 0) if i > 0 else 0,\n                \"read_offset\": i if i > 0 else 0,\n                \"skip_special_tokens\": False\n            })\n    \n    data = {\n        \"tokenizer\": tokenizer,\n        \"test_cases\": test_cases,\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float32,  # Tokenization is CPU-bound\n        \"hw_info\": hw_info\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    tokenizer = data[\"tokenizer\"]\n    test_cases = data[\"test_cases\"]\n    \n    # Run detokenization for all test cases\n    results = []\n    for case in test_cases:\n        result = target(\n            tokenizer=tokenizer,\n            all_input_ids=case[\"all_input_ids\"],\n            prev_tokens=case[\"prev_tokens\"],\n            prefix_offset=case[\"prefix_offset\"],\n            read_offset=case[\"read_offset\"],\n            skip_special_tokens=case[\"skip_special_tokens\"]\n        )\n        results.append(result)\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert tuples to lists for JSON serialization\n    serializable_results = []\n    for r in result:\n        serializable_results.append({\n            \"new_tokens\": r[0],\n            \"new_text\": r[1],\n            \"read_offset\": r[2],\n            \"output_length\": r[3]\n        })\n    \n    import pickle\n    with open(filepath, 'wb') as f:\n        pickle.dump(serializable_results, f)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    import pickle\n    with open(filepath, 'rb') as f:\n        data = pickle.load(f)\n    \n    # Convert back to tuples\n    results = []\n    for r in data:\n        results.append((r[\"new_tokens\"], r[\"new_text\"], r[\"read_offset\"], r[\"output_length\"]))\n    return results\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert len(current_result) == len(reference_result), f\"Result count mismatch: {len(current_result)} vs {len(reference_result)}\"\n    \n    for i, (current, reference) in enumerate(zip(current_result, reference_result)):\n        # Each result is a tuple: (new_tokens, new_text, read_offset, output_length)\n        assert len(current) == 4, f\"Result {i}: Invalid current result format\"\n        assert len(reference) == 4, f\"Result {i}: Invalid reference result format\"\n        \n        # Check new_tokens (list of strings)\n        assert current[0] == reference[0], f\"Result {i}: Token mismatch\"\n        \n        # Check new_text (string)\n        assert current[1] == reference[1], f\"Result {i}: Text mismatch: '{current[1]}' vs '{reference[1]}'\"\n        \n        # Check offsets (integers)\n        assert current[2] == reference[2], f\"Result {i}: Read offset mismatch: {current[2]} vs {reference[2]}\"\n        assert current[3] == reference[3], f\"Result {i}: Output length mismatch: {current[3]} vs {reference[3]}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_operation(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU-bound operation (tokenization)\n    warmup = 3\n    iters = 10\n    \n    result, timing_stats = time_cpu_operation(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pkl\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Tokenization is CPU-bound\n        \"dtype\": \"str\",  # Working with strings/tokens\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "commit_short": "ed250545",
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21222)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "tests/v1/core/test_kv_cache_utils.py",
      "vllm/v1/core/block_pool.py",
      "vllm/v1/core/kv_cache_utils.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21222",
    "models": [
      "N/A"
    ],
    "parent_commit": "10904e6d755051260a7c3ce98659d8907c74caa9",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 11444.386620759964,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.0rc2.dev38+g10904e6d7",
    "human_version": "0.10.0rc2.dev39+ged2505457",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 818.34,
    "baseline_ttft_median": 774.19,
    "baseline_ttft_p99": 1322.6,
    "baseline_tpot_mean": 19.01,
    "baseline_tpot_median": 17.09,
    "baseline_tpot_p99": 46.04,
    "baseline_itl_mean": 16.93,
    "baseline_itl_median": 13.04,
    "baseline_itl_p99": 187.54,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 799.24,
    "human_ttft_median": 769.55,
    "human_ttft_p99": 1309.2,
    "human_tpot_mean": 18.86,
    "human_tpot_median": 16.82,
    "human_tpot_p99": 46.04,
    "human_itl_mean": 16.77,
    "human_itl_median": 12.78,
    "human_itl_p99": 187.44,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 807.86,
    "agent_ttft_median": 738.59,
    "agent_ttft_p99": 1309.53,
    "agent_tpot_mean": 18.67,
    "agent_tpot_median": 17.01,
    "agent_tpot_p99": 48.02,
    "agent_itl_mean": 16.61,
    "agent_itl_median": 12.79,
    "agent_itl_p99": 189.65,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 2.333993205758001,
    "human_improvement_tpot_mean": 0.7890583903208949,
    "human_improvement_itl_mean": 0.9450679267572365,
    "agent_improvement_ttft_mean": 1.2806412982378983,
    "agent_improvement_tpot_mean": 1.7885323513940024,
    "agent_improvement_itl_mean": 1.890135853514473,
    "agent_vs_human_ttft_mean": -1.0785245983684506,
    "agent_vs_human_tpot_mean": 1.007423117709426,
    "agent_vs_human_itl_mean": 0.9540846750149083,
    "human_improvement_ttft_median": 0.5993360802903809,
    "human_improvement_ttft_p99": 1.0131559050355259,
    "agent_improvement_ttft_median": 4.5983544091243775,
    "agent_improvement_ttft_p99": 0.9882050506577904,
    "agent_vs_human_ttft_median": 4.023130400883623,
    "agent_vs_human_ttft_p99": -0.025206232813926616,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 19:03:43 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b49abe0f7e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 19:03:49 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.99      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12178     \nRequest throughput (req/s):              33.46     \nOutput token throughput (tok/s):         4075.29   \nTotal Token throughput (tok/s):          21175.56  \n---------------Time to First Token----------------\nMean TTFT (ms):                          818.34    \nMedian TTFT (ms):                        774.19    \nP99 TTFT (ms):                           1322.60   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          19.01     \nMedian TPOT (ms):                        17.09     \nP99 TPOT (ms):                           46.04     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.93     \nMedian ITL (ms):                         13.04     \nP99 ITL (ms):                            187.54    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:13,  1.3",
    "human_raw": "INFO 01-02 19:07:06 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b6198c177e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 19:07:13 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.95      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12178     \nRequest throughput (req/s):              33.89     \nOutput token throughput (tok/s):         4127.45   \nTotal Token throughput (tok/s):          21446.62  \n---------------Time to First Token----------------\nMean TTFT (ms):                          799.24    \nMedian TTFT (ms):                        769.55    \nP99 TTFT (ms):                           1309.20   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          18.86     \nMedian TPOT (ms):                        16.82     \nP99 TPOT (ms):                           46.04     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.77     \nMedian ITL (ms):                         12.78     \nP99 ITL (ms):                            187.44    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:12,  1.3",
    "agent_raw": "INFO 01-02 19:09:36 [__init__.py:235] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2af0ce90f7e0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=inf, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nINFO 01-02 19:09:42 [datasets.py:355] Sampling input_len from [511, 511] and output_len from [128, 128]\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.94      \nTotal input tokens:                      51100     \nTotal generated tokens:                  12284     \nRequest throughput (req/s):              33.97     \nOutput token throughput (tok/s):         4172.30   \nTotal Token throughput (tok/s):          21528.60  \n---------------Time to First Token----------------\nMean TTFT (ms):                          807.86    \nMedian TTFT (ms):                        738.59    \nP99 TTFT (ms):                           1309.53   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          18.67     \nMedian TPOT (ms):                        17.01     \nP99 TPOT (ms):                           48.02     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           16.61     \nMedian ITL (ms):                         12.79     \nP99 ITL (ms):                            189.65    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:12,  1.3",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ed25054577f7abca2aee32a5290200c4a1aed561\nMessage: [Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21222)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use BlockPool since it calls the optimized methods\n    if not (module_path and symbol_name):\n        module_path = \"vllm.v1.core.block_pool\"\n        symbol_name = \"BlockPool\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        # Also import KVCacheBlock for creating test data\n        kv_module = importlib.import_module(\"vllm.v1.core.kv_cache_utils\")\n        kv_cache_block = getattr(kv_module, \"KVCacheBlock\")\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, kv_cache_block, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Resolve target classes\n    BlockPool, KVCacheBlock, fq_name = resolve_target()\n    \n    # Realistic KV cache configuration for a 7B model\n    num_layers = 32\n    num_heads = 32\n    head_dim = 128\n    block_size = 16  # Common block size in vLLM\n    \n    # Total blocks to simulate a reasonable GPU memory allocation\n    # For 16GB GPU: ~8192 blocks (each block holds 16 tokens of KV cache)\n    num_total_blocks = 8192\n    \n    # Create block pool\n    blocks = []\n    for i in range(num_total_blocks):\n        block = KVCacheBlock(\n            block_id=i,\n            prev_token_id=-1,\n            token_ids=[-1] * block_size,\n            num_tokens=0,\n            prev_block=None,\n            next_free_block=None,\n            prev_free_block=None,\n            ref_cnt=0,\n            is_full=False,\n            is_cached=False,\n            is_null=False\n        )\n        blocks.append(block)\n    \n    # Initialize block pool\n    block_pool = BlockPool(\n        blocks=blocks,\n        enable_caching=False  # Start with caching disabled for cleaner comparison\n    )\n    \n    # Workload patterns - simulate various batch sizes for allocation/deallocation\n    # These represent different request patterns in continuous batching\n    allocation_sizes = [\n        1,    # Single block allocations (old decode pattern)\n        4,    # Small batch\n        16,   # Medium batch (prefill for short sequence)\n        64,   # Large batch (prefill for medium sequence)\n        128,  # Very large batch (prefill for long sequence)\n        256,  # Maximum batch (stress test)\n    ]\n    \n    # Create allocation/deallocation pattern\n    operations = []\n    for size in allocation_sizes:\n        # Multiple rounds of alloc/free for each size\n        for _ in range(10):\n            operations.append(('alloc', size))\n            operations.append(('free', size))\n    \n    data = {\n        \"device\": \"cpu\",  # Block pool operations are CPU-bound\n        \"dtype\": torch.float32,\n        \"hw_info\": hw_info,\n        \"block_pool\": block_pool,\n        \"BlockPool\": BlockPool,\n        \"KVCacheBlock\": KVCacheBlock,\n        \"blocks\": blocks,\n        \"operations\": operations,\n        \"num_iterations\": 100,  # Number of times to repeat the operation pattern\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    block_pool = data[\"block_pool\"]\n    operations = data[\"operations\"]\n    num_iterations = data[\"num_iterations\"]\n    \n    # Track allocated blocks for proper cleanup\n    allocated_blocks_list = []\n    \n    # Execute the operation pattern multiple times\n    for _ in range(num_iterations):\n        for op_type, size in operations:\n            if op_type == 'alloc':\n                # Ensure we have enough free blocks\n                if block_pool.free_block_queue.num_free_blocks >= size:\n                    allocated = block_pool.get_new_blocks(size)\n                    allocated_blocks_list.append(allocated)\n            elif op_type == 'free':\n                # Free the oldest allocated blocks if any\n                if allocated_blocks_list:\n                    blocks_to_free = allocated_blocks_list.pop(0)\n                    block_pool.free_blocks(blocks_to_free)\n    \n    # Clean up any remaining allocated blocks\n    while allocated_blocks_list:\n        blocks_to_free = allocated_blocks_list.pop(0)\n        block_pool.free_blocks(blocks_to_free)\n    \n    # Return final state for verification\n    return {\n        \"num_free_blocks\": block_pool.free_block_queue.num_free_blocks,\n        \"total_operations\": len(operations) * num_iterations\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # For block pool state, check that free blocks match\n        assert current_result.get(\"num_free_blocks\") == reference_result.get(\"num_free_blocks\"), \\\n            f\"Free blocks mismatch: {current_result.get('num_free_blocks')} vs {reference_result.get('num_free_blocks')}\"\n        assert current_result.get(\"total_operations\") == reference_result.get(\"total_operations\"), \\\n            f\"Total operations mismatch: {current_result.get('total_operations')} vs {reference_result.get('total_operations')}\"\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Block pool operations are CPU-bound\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ed25054577f7abca2aee32a5290200c4a1aed561\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",\n        \"dtype\": \"torch.float32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "commit_short": "eefbf4a6",
    "commit_subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [
      "benchmarks/kernels/benchmark_reshape_and_cache_flash.py",
      "csrc/cache_kernels.cu"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/22036",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "c_cuda",
    "duration_s": 3272.4722554683685,
    "error": "Command '['git', 'reset', '--hard', 'HEAD']' timed out after 60 seconds",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.10.1.dev295+g88faa466d",
    "human_version": null,
    "agent_version": null,
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 2026.694461566668,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 16:44:35 [__init__.py:241] Automatically detected platform cuda.\nINFO 01-02 16:44:40 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-30B-A3B-FP8', 'enable_prefix_caching': False, 'enable_lora': None}\nINFO 01-02 16:44:51 [config.py:723] Resolved architecture: Qwen3MoeForCausalLM\nINFO 01-02 16:44:51 [config.py:1756] Using max model len 40960\nINFO 01-02 16:44:51 [config.py:2582] Chunked prefill is enabled with max_num_batched_tokens=16384.\nINFO 01-02 16:45:02 [__init__.py:241] Automatically detected platform cuda.\n\u001b[1;36m(EngineCore_0 pid=522)\u001b[0;0m INFO 01-02 16:45:06 [core.py:619] Waiting for init message from front-end.\n\u001b[1;36m(EngineCore_0 pid=522)\u001b[0;0m INFO 01-02 16:45:06 [core.py:71] Initializing a V1 LLM engine (v0.10.1.dev295+g88faa466d) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n\u001b[1;36m(EngineCore_0 pid=522)\u001b[0;0m INFO 01-02 16:45:08 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n\u001b[1;36m(EngineCore_0 pid=522)\u001b[0;0m WARNING 01-02 16:45:08 [topk_topp_sampler.py:60] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n\u001b[1;36m(EngineCore_0 pid=522)\u001b[0;0m INFO 01-02 16:45:08 [gpu_model_runner.py",
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: eefbf4a68b7b0a5b8364a59647906be1b7f043e2\nMessage: [Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport random\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm._custom_ops\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"reshape_and_cache_flash\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Cache Creation Helper\n# =======================\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    kv_cache_dtype: str,\n    dtype: torch.dtype,\n    device: str,\n    cache_layout: str\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    \"\"\"Create KV caches with specified layout.\"\"\"\n    \n    if kv_cache_dtype == \"fp8\":\n        cache_dtype = torch.uint8\n    else:\n        cache_dtype = dtype\n    \n    key_caches = []\n    value_caches = []\n    \n    for _ in range(num_layers):\n        if cache_layout == \"NHD\":\n            # [num_blocks, block_size, num_heads, head_size]\n            key_cache = torch.randn(\n                num_blocks, block_size, num_heads, head_size,\n                dtype=cache_dtype, device=device\n            )\n            value_cache = torch.randn(\n                num_blocks, block_size, num_heads, head_size,\n                dtype=cache_dtype, device=device\n            )\n        else:  # HND\n            # [num_blocks, num_heads, block_size, head_size]\n            key_cache = torch.randn(\n                num_blocks, num_heads, block_size, head_size,\n                dtype=cache_dtype, device=device\n            )\n            value_cache = torch.randn(\n                num_blocks, num_heads, block_size, head_size,\n                dtype=cache_dtype, device=device\n            )\n        \n        key_caches.append(key_cache)\n        value_caches.append(value_cache)\n    \n    return key_caches, value_caches\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # KV cache parameters\n    num_tokens = 256  # Moderate batch for stable timing\n    num_heads = 32\n    head_size = 128\n    block_size = 16\n    num_blocks = 512\n    \n    # Adjust for memory constraints\n    if hw_info.get(\"memory_gb\", 0) < 16:\n        num_tokens = 128\n        num_blocks = 256\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    kv_cache_dtype = \"auto\"  # Use same dtype as inputs\n    cache_layout = \"NHD\"  # Default to NHD layout\n    \n    # Override from environment if specified\n    cache_layout = os.getenv(\"CACHE_LAYOUT\", cache_layout)\n    \n    # Create key/value tensors [T, H, D]\n    key = torch.randn(num_tokens, num_heads, head_size, dtype=dtype, device=device)\n    value = torch.randn(num_tokens, num_heads, head_size, dtype=dtype, device=device)\n    \n    # Create slot mapping\n    num_slots = block_size * num_blocks\n    if num_tokens > num_slots:\n        num_tokens = num_slots\n        key = key[:num_tokens]\n        value = value[:num_tokens]\n    \n    slot_mapping_lst = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping_lst, dtype=torch.long, device=device)\n    \n    # Create KV caches\n    key_caches, value_caches = create_kv_caches_with_random_flash(\n        num_blocks,\n        block_size,\n        1,  # num_layers\n        num_heads,\n        head_size,\n        kv_cache_dtype,\n        dtype,\n        device=str(device),\n        cache_layout=cache_layout,\n    )\n    key_cache = key_caches[0]\n    value_cache = value_caches[0]\n    \n    # Compute scaling factors for fp8 (even if not used)\n    k_scale = (key.amax() / 64.0).to(torch.float32)\n    v_scale = (value.amax() / 64.0).to(torch.float32)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"key\": key,\n        \"value\": value,\n        \"key_cache\": key_cache,\n        \"value_cache\": value_cache,\n        \"slot_mapping\": slot_mapping,\n        \"kv_cache_dtype\": kv_cache_dtype,\n        \"k_scale\": k_scale,\n        \"v_scale\": v_scale,\n        \"cache_layout\": cache_layout,\n        \"num_tokens\": num_tokens,\n        \"num_heads\": num_heads,\n        \"head_size\": head_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target, fq_name = resolve_target()\n    \n    # Call the optimized kernel\n    with torch.no_grad():\n        # Clone caches to avoid modifying input data\n        key_cache_copy = data[\"key_cache\"].clone()\n        value_cache_copy = data[\"value_cache\"].clone()\n        \n        target(\n            data[\"key\"],\n            data[\"value\"],\n            key_cache_copy,\n            value_cache_copy,\n            data[\"slot_mapping\"],\n            data[\"kv_cache_dtype\"],\n            data[\"k_scale\"],\n            data[\"v_scale\"],\n        )\n    \n    return {\n        \"key_cache\": key_cache_copy,\n        \"value_cache\": value_cache_copy,\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\n        \"type\": \"dict\",\n        \"data\": {\n            \"key_cache\": result[\"key_cache\"].cpu(),\n            \"value_cache\": result[\"value_cache\"].cpu(),\n        }\n    }, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, dict) and isinstance(reference_result, dict)\n    \n    for cache_name in [\"key_cache\", \"value_cache\"]:\n        current_cache = current_result[cache_name]\n        reference_cache = reference_result[cache_name]\n        \n        assert current_cache.shape == reference_cache.shape, f\"{cache_name} shape mismatch\"\n        assert current_cache.dtype == reference_cache.dtype, f\"{cache_name} dtype mismatch\"\n        \n        # Determine tolerances based on dtype\n        if current_cache.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        elif current_cache.dtype == torch.uint8:  # fp8\n            rtol, atol = 5e-2, 1e-2\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_cache.cpu(),\n            reference_cache.cpu(),\n            rtol=rtol,\n            atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    result = None\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Check hardware support\n    if hw_info[\"device\"] != \"cuda\":\n        error_data = {\n            \"error_code\": 2,\n            \"error_name\": \"CAPABILITY_UNSUPPORTED\",\n            \"error_message\": \"CUDA device required for reshape_and_cache_flash kernel\",\n            \"target_resolved\": True,\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(2)\n    \n    # Timing\n    warmup = 5\n    iters = 50\n    \n    # Adjust iterations based on workload size\n    if data[\"num_tokens\"] < 64:\n        iters = 100  # More iterations for small workloads\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"eefbf4a68b7b0a5b8364a59647906be1b7f043e2\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "commit_short": "f092153f",
    "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/v1/worker/gpu_input_batch.py",
      "vllm/v1/worker/gpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11111",
    "models": [
      "N/A"
    ],
    "parent_commit": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03",
    "status": "version_bug",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 33.359105587005615,
    "error": "vLLM 0.6.4.post2.dev330+g1da8f0e1 has known port binding bug (issue #8791) - serving benchmarks not supported",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.4.post2.dev330+g1da8f0e1",
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: f092153fbe349a9a1742940e3703bfcff6aa0a6d\nMessage: [V1] Use more persistent buffers to optimize input preparation overheads\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - target _prepare_inputs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.v1.worker.gpu_model_runner\"\n        symbol_name = \"GPUModelRunner._prepare_inputs\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Simulate a realistic batch of requests for input preparation\n    max_num_reqs = 256  # Typical max batch size\n    max_model_len = 4096  # Typical model context length\n    max_num_blocks_per_req = 256  # max_model_len / block_size\n    block_size = 16\n    \n    # Create mock InputBatch\n    from vllm.v1.worker.gpu_input_batch import InputBatch\n    input_batch = InputBatch(\n        max_num_reqs=max_num_reqs,\n        max_model_len=max_model_len,\n        max_num_blocks_per_req=max_num_blocks_per_req,\n        device=device,\n        pin_memory=torch.cuda.is_available(),\n    )\n    \n    # Simulate active requests with varying sequence lengths\n    num_active_reqs = 32  # Typical active batch\n    for i in range(num_active_reqs):\n        req_id = f\"req_{i}\"\n        input_batch.req_ids[i] = req_id\n        input_batch.req_id_to_index[req_id] = i\n        \n        # Random sequence lengths\n        prompt_len = np.random.randint(128, 1024)\n        output_len = np.random.randint(0, 512)\n        \n        # Fill token ids\n        input_batch.token_ids_cpu[i, :prompt_len] = np.random.randint(0, 32000, prompt_len)\n        input_batch.token_ids_cpu[i, prompt_len:prompt_len+output_len] = np.random.randint(0, 32000, output_len)\n        \n        # Set computed tokens\n        input_batch.num_computed_tokens_cpu[i] = prompt_len\n        \n        # Fill block table\n        num_blocks = (prompt_len + output_len + block_size - 1) // block_size\n        input_batch.block_table_cpu[i, :num_blocks] = np.arange(i * max_num_blocks_per_req, i * max_num_blocks_per_req + num_blocks)\n    \n    # Create mock scheduler output\n    class MockSchedulerOutput:\n        def __init__(self, num_reqs, input_batch):\n            self.total_num_scheduled_tokens = 0\n            self.num_scheduled_tokens = {}\n            \n            # Simulate scheduling some tokens for each request\n            for i in range(num_reqs):\n                req_id = input_batch.req_ids[i]\n                if req_id:\n                    # Schedule 1-16 tokens per request (typical decode)\n                    num_tokens = np.random.randint(1, 17)\n                    self.num_scheduled_tokens[req_id] = num_tokens\n                    self.total_num_scheduled_tokens += num_tokens\n    \n    scheduler_output = MockSchedulerOutput(num_active_reqs, input_batch)\n    \n    # Create GPUModelRunner instance\n    from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n    from vllm.config import VllmConfig, ModelConfig, CacheConfig, SchedulerConfig, ParallelConfig\n    \n    # Mock minimal config\n    model_config = ModelConfig(\n        model=\"mock\",\n        tokenizer=\"mock\",\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        dtype=dtype,\n        seed=42,\n        max_model_len=max_model_len,\n    )\n    \n    cache_config = CacheConfig(\n        block_size=block_size,\n        cache_dtype=\"auto\",\n    )\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=2048,\n        max_num_seqs=max_num_reqs,\n    )\n    \n    parallel_config = ParallelConfig()\n    \n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        scheduler_config=scheduler_config,\n        parallel_config=parallel_config,\n    )\n    \n    runner = GPUModelRunner(vllm_config, device)\n    runner.input_batch = input_batch\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"runner\": runner,\n        \"scheduler_output\": scheduler_output,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    runner = data[\"runner\"]\n    scheduler_output = data[\"scheduler_output\"]\n    \n    # Call the optimized _prepare_inputs method\n    with torch.no_grad():\n        attn_metadata, logits_indices = runner._prepare_inputs(scheduler_output)\n    \n    # Return the results for equivalence checking\n    return {\n        \"query_start_loc\": attn_metadata.query_start_loc.cpu(),\n        \"seq_start_loc\": attn_metadata.seq_start_loc.cpu(),\n        \"slot_mapping\": attn_metadata.slot_mapping.cpu(),\n        \"max_query_len\": attn_metadata.max_query_len,\n        \"max_seq_len\": attn_metadata.max_seq_len,\n        \"logits_indices\": logits_indices.cpu(),\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, dict) and isinstance(reference_result, dict)\n    assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n    \n    for key in current_result:\n        current_val = current_result[key]\n        reference_val = reference_result[key]\n        \n        if isinstance(current_val, torch.Tensor):\n            assert current_val.shape == reference_val.shape, f\"{key} shape mismatch\"\n            assert current_val.dtype == reference_val.dtype, f\"{key} dtype mismatch\"\n            \n            # Integer tensors should match exactly\n            if current_val.dtype in (torch.int32, torch.int64):\n                torch.testing.assert_close(current_val, reference_val, rtol=0, atol=0)\n            else:\n                torch.testing.assert_close(current_val, reference_val, rtol=1e-5, atol=1e-7)\n        else:\n            assert current_val == reference_val, f\"{key} value mismatch: {current_val} vs {reference_val}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"f092153fbe349a9a1742940e3703bfcff6aa0a6d\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
    "commit_short": "f26c4aee",
    "commit_subject": "[Misc] Optimize ray worker initialization time (#11275)",
    "repo": "vllm",
    "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
    "files_changed": [
      "vllm/executor/ray_gpu_executor.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/11275",
    "models": [
      "N/A"
    ],
    "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0",
    "status": "success",
    "gpu_config": "H100:4",
    "benchmark_mode": "standalone",
    "patch_type": null,
    "duration_s": 529.6115574836731,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "0.6.6.dev17+g8936316d",
    "human_version": "0.6.6.dev18+gf26c4aee",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1372.7327409000054,
    "baseline_throughput": 818.8,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1379.1739461499901,
    "human_throughput": 818.9,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 1380.1756272999683,
    "agent_throughput": 818.7,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": -0.46922500338716,
    "human_improvement_throughput": 0.012212994626285143,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": -0.5421948627147235,
    "agent_improvement_throughput": -0.012212994626271257,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": -0.07262906559207893,
    "agent_vs_human_throughput": -0.02442300647208839,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "Namespace(input_len=128, output_len=256, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=5, num_iters=20, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-01 06:45:26 config.py:477] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-01 06:45:33 llm_engine.py:249] Initializing an LLM engine (v0.6.6.dev17+g8936316d) with conf",
    "human_raw": "Namespace(input_len=128, output_len=256, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=5, num_iters=20, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-01 06:48:32 config.py:477] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-01 06:48:39 llm_engine.py:249] Initializing an LLM engine (v0.6.6.dev18+gf26c4aee) with conf",
    "agent_raw": "Namespace(input_len=128, output_len=256, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=5, num_iters=20, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Llama-3.1-8B-Instruct', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=2048, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend='ray', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto')\nINFO 01-01 06:51:13 config.py:477] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\nINFO 01-01 06:51:20 llm_engine.py:249] Initializing an LLM engine (v0.6.6.dev17+g8936316d) with conf",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: f26c4aeecba481ce1445be7a998b0b97460a13bb\nMessage: [Misc] Optimize ray worker initialization time (#11275)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\n\n# Ray import with fallback\ntry:\n    import ray\n    RAY_AVAILABLE = True\nexcept ImportError:\n    RAY_AVAILABLE = False\n    \nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    hw_info[\"device\"] = \"cpu\"  # This is a CPU optimization (Ray initialization)\n    hw_info[\"device_name\"] = \"CPU\"\n    hw_info[\"memory_gb\"] = 0\n    hw_info[\"ray_available\"] = RAY_AVAILABLE\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.executor.ray_gpu_executor\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"RayGPUExecutor\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        # Fallback to simulation if vLLM not available\n        return None, \"simulation\"\n\n# =======================\n# Ray Worker Simulation\n# =======================\n@ray.remote\nclass MockRayWorker:\n    \"\"\"Simulates a Ray worker with get_node_ip method.\"\"\"\n    def __init__(self, worker_id: int, node_ip: str):\n        self.worker_id = worker_id\n        self.node_ip = node_ip\n        # Simulate some initialization overhead\n        time.sleep(0.001)  # 1ms per worker init\n    \n    def get_node_ip(self):\n        # Simulate network latency for IP retrieval\n        time.sleep(0.002)  # 2ms network call\n        return self.node_ip\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Number of workers to simulate (typical vLLM deployment)\n    num_workers = 8  # Common multi-GPU setup\n    \n    # Initialize Ray if available and not already initialized\n    if RAY_AVAILABLE:\n        if not ray.is_initialized():\n            ray.init(ignore_reinit_error=True, num_cpus=num_workers+2)\n    \n    # Create IP addresses for simulation\n    driver_ip = \"192.168.1.1\"\n    worker_ips = []\n    for i in range(num_workers):\n        # Distribute workers across nodes\n        node_id = i // 2  # 2 workers per node\n        worker_ips.append(f\"192.168.1.{node_id + 2}\")\n    \n    data = {\n        \"device\": \"cpu\",\n        \"dtype\": torch.float32,\n        \"hw_info\": hw_info,\n        \"num_workers\": num_workers,\n        \"driver_ip\": driver_ip,\n        \"worker_ips\": worker_ips,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    if not RAY_AVAILABLE:\n        # Simulate the optimization pattern without Ray\n        num_workers = data[\"num_workers\"]\n        worker_ips = data[\"worker_ips\"]\n        \n        # Simulate old approach: sequential calls\n        old_time = 0\n        for i in range(num_workers):\n            # Each call has overhead\n            time.sleep(0.002)  # Simulate network latency\n            old_time += 0.002\n        \n        # Simulate new approach: batched call\n        new_time = 0.002  # Single batched call\n        \n        return {\"old_approach_ms\": old_time * 1000, \"new_approach_ms\": new_time * 1000, \"speedup\": old_time / new_time}\n    \n    # With Ray available, test actual pattern\n    num_workers = data[\"num_workers\"]\n    worker_ips = data[\"worker_ips\"]\n    \n    # Create mock workers\n    workers = []\n    for i in range(num_workers):\n        worker = MockRayWorker.remote(i, worker_ips[i])\n        workers.append(worker)\n    \n    # Test the optimization pattern\n    start_time = time.perf_counter()\n    \n    # NEW APPROACH (optimized): Batch all IP retrievals\n    worker_ip_refs = [\n        worker.get_node_ip.remote()\n        for worker in workers\n    ]\n    retrieved_ips = ray.get(worker_ip_refs)  # Single batched ray.get()\n    \n    end_time = time.perf_counter()\n    new_approach_time = end_time - start_time\n    \n    # Clean up Ray actors\n    for worker in workers:\n        ray.kill(worker)\n    \n    # For comparison, we would test old approach but that would double test time\n    # Instead, we know the pattern saves approximately (n-1) * network_latency\n    estimated_old_time = num_workers * 0.003  # Sequential calls\n    \n    result = {\n        \"num_workers\": num_workers,\n        \"worker_ips\": retrieved_ips if RAY_AVAILABLE else worker_ips,\n        \"new_approach_time\": new_approach_time,\n        \"estimated_old_time\": estimated_old_time,\n        \"speedup\": estimated_old_time / new_approach_time if new_approach_time > 0 else 1.0\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    elif isinstance(result, dict):\n        # Save as JSON for dictionaries with simple types\n        import json\n        with open(filepath, 'w') as f:\n            # Convert numpy/torch types to native Python types\n            json_safe = {}\n            for k, v in result.items():\n                if isinstance(v, (list, str, int, float, bool)):\n                    json_safe[k] = v\n                elif isinstance(v, (np.ndarray, torch.Tensor)):\n                    json_safe[k] = v.tolist() if hasattr(v, 'tolist') else list(v)\n                else:\n                    json_safe[k] = str(v)\n            json.dump(json_safe, f)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    if filepath.endswith('.json'):\n        import json\n        with open(filepath, 'r') as f:\n            return json.load(f)\n    else:\n        data = torch.load(filepath)\n        return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # For this optimization, we check that the same workers/IPs are retrieved\n        assert set(current_result.keys()) == set(reference_result.keys()), f\"Keys mismatch\"\n        \n        # Check worker count\n        if \"num_workers\" in current_result:\n            assert current_result[\"num_workers\"] == reference_result[\"num_workers\"]\n        \n        # Check that IPs match (order may vary but content should be same)\n        if \"worker_ips\" in current_result:\n            current_ips = sorted(current_result[\"worker_ips\"])\n            ref_ips = sorted(reference_result[\"worker_ips\"])\n            assert current_ips == ref_ips, f\"Worker IPs mismatch: {current_ips} vs {ref_ips}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)] if len(times_ms) >= 20 else times_ms[-1],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)] if len(times_ms) >= 100 else times_ms[-1],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # For this CPU optimization, we measure initialization time\n    warmup = 1 if RAY_AVAILABLE else 3  # Ray actors are stateful, less warmup needed\n    iters = 5 if RAY_AVAILABLE else 10  # Ray tests are slower\n    \n    # Execute and time\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    \n    # Extract timing from result (this optimization measures init time directly)\n    if isinstance(result, dict) and \"new_approach_time\" in result:\n        avg_ms = result[\"new_approach_time\"] * 1000\n        p50_ms = avg_ms  # Single measurement\n        p95_ms = avg_ms\n    else:\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"f26c4aeecba481ce1445be7a998b0b97460a13bb\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.json\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",\n        \"dtype\": \"None\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": RAY_AVAILABLE,\n        \"speedup\": result.get(\"speedup\", 1.0) if isinstance(result, dict) else 1.0\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    try:\n        run_test(args.eqcheck, args.reference, args.prefix)\n    except Exception as e:\n        # Output error in expected format\n        error_data = {\n            \"error_code\": 6,\n            \"error_name\": \"INVALID_CONFIG\",\n            \"error_message\": str(e),\n            \"target_resolved\": False,\n            \"opt_path_hit\": False\n        }\n        print(json.dumps(error_data))\n        sys.exit(6)"
  },
  {
    "commit_hash": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "commit_short": "fa63e710",
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)",
    "repo": "vllm",
    "perf_command": "VLLM_USE_V1=1 python3 benchmarks/benchmark_latency.py --model \"/data/users/ktong/llama/llm_8b_oss\" --tensor-parallel-size 1 --input_len 1000 --batch_size 32",
    "files_changed": [
      "vllm/v1/outputs.py",
      "vllm/v1/sample/sampler.py",
      "vllm/v1/worker/gpu_model_runner.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12094",
    "models": [
      "N/A"
    ],
    "parent_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "standalone",
    "patch_type": "python_only",
    "duration_s": 755.4508941173553,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-01 20:44:34 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev379+g2a0309a6",
    "human_version": "INFO 01-01 20:48:52 __init__.py:183] Automatically detected platform cuda.\n0.6.6.post2.dev380+gfa63e710",
    "agent_version": null,
    "model": "meta-llama/Meta-Llama-3-8B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": 1331.7060970666603,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": 1323.820436133345,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": 1329.9196108666706,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": 0.5921472425999162,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": 0.13415018553453711,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": -0.4607252288037025,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-01 20:45:20 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:45:22 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:45:32 config.py:520] This model ",
    "human_raw": "INFO 01-01 20:49:36 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:49:38 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:49:48 config.py:520] This model ",
    "agent_raw": "INFO 01-01 20:53:44 __init__.py:183] Automatically detected platform cuda.\nNamespace(input_len=1000, output_len=128, batch_size=32, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-8B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False)\nWARNING 01-01 20:53:46 arg_utils.py:1296] Setting max_num_batched_tokens to 16384 for LLM_CLASS usage context.\nINFO 01-01 20:53:56 config.py:520] This model ",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412\nMessage: [V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Main optimization is in gpu_model_runner\n        module_path = \"vllm.v1.worker.gpu_model_runner\"\n        symbol_name = \"GPUModelRunner\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        # Fallback to testing the data structure change\n        try:\n            module_path = \"vllm.v1.outputs\"\n            symbol_name = \"SamplerOutput\"\n            module = importlib.import_module(module_path)\n            target = getattr(module, symbol_name)\n            fq_name = f\"{module_path}.{symbol_name}\"\n            return target, fq_name\n        except (ImportError, AttributeError) as e2:\n            error_data = {\n                \"target_resolved\": False,\n                \"error\": str(e2),\n                \"attempted_module\": module_path,\n                \"attempted_symbol\": symbol_name\n            }\n            print(json.dumps(error_data))\n            sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # This optimization is about CPU-GPU sync overhead\n    # We simulate the sampler output and measure the sync overhead\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float32\n    \n    # Simulate a batch of requests\n    batch_size = 64  # Typical batch size for continuous batching\n    vocab_size = 32000  # Common vocab size for LLMs\n    \n    # Create sampled token IDs tensor (simulating sampler output)\n    sampled_token_ids = torch.randint(0, vocab_size, (batch_size,), \n                                     device=device, dtype=torch.int32)\n    \n    # Simulate logprobs output (optional in real sampler)\n    max_num_logprobs = 5\n    logprob_token_ids = torch.randint(0, vocab_size, \n                                      (batch_size, max_num_logprobs + 1),\n                                      device=device, dtype=torch.int32)\n    logprobs = torch.randn(batch_size, max_num_logprobs + 1,\n                          device=device, dtype=torch.float32)\n    \n    # Create mock request states to simulate the CPU operations\n    class MockRequestState:\n        def __init__(self):\n            self.num_tokens = np.random.randint(100, 1000)\n            self.output_token_ids = []\n    \n    request_states = [MockRequestState() for _ in range(batch_size)]\n    \n    # Create mock input batch\n    class MockInputBatch:\n        def __init__(self, batch_size):\n            self.num_reqs = batch_size\n            self.req_ids = [f\"req_{i}\" for i in range(batch_size)]\n            self.token_ids_cpu = np.zeros((batch_size, 2048), dtype=np.int32)\n            self.num_tokens = np.random.randint(100, 1000, batch_size)\n            self.req_id_to_index = {req_id: i for i, req_id in enumerate(self.req_ids)}\n    \n    input_batch = MockInputBatch(batch_size)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"sampled_token_ids\": sampled_token_ids,\n        \"logprob_token_ids\": logprob_token_ids,\n        \"logprobs\": logprobs,\n        \"request_states\": request_states,\n        \"input_batch\": input_batch,\n        \"batch_size\": batch_size,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Import the SamplerOutput class\n    try:\n        from vllm.engine.async_llm_engine import SamplerOutput\n    except ImportError:\n        # Fallback definition for testing\n        class SamplerOutput:\n            def __init__(self, sampled_token_ids, logprob_token_ids=None, \n                        logprobs=None, prompt_logprob_token_ids=None,\n                        prompt_logprobs=None):\n                self.sampled_token_ids = sampled_token_ids\n                self.logprob_token_ids = logprob_token_ids\n                self.logprobs = logprobs\n                self.prompt_logprob_token_ids = prompt_logprob_token_ids\n                self.prompt_logprobs = prompt_logprobs\n    \n    # Create sampler output with tensor (optimized version)\n    sampler_output = SamplerOutput(\n        sampled_token_ids=data[\"sampled_token_ids\"],\n        logprob_token_ids=data[\"logprob_token_ids\"],\n        logprobs=data[\"logprobs\"],\n        prompt_logprob_token_ids=None,\n        prompt_logprobs=None\n    )\n    \n    # Simulate the optimized CPU operations before sync\n    request_seq_lens = []\n    num_reqs = data[\"input_batch\"].num_reqs\n    \n    for i in range(num_reqs):\n        req_state = data[\"request_states\"][i]\n        seq_len = data[\"input_batch\"].num_tokens[i]\n        if seq_len == req_state.num_tokens:\n            data[\"input_batch\"].num_tokens[i] += 1\n            # Optimization: append placeholder, update later\n            req_state.output_token_ids.append(0)\n            request_seq_lens.append((i, req_state, seq_len))\n    \n    # The key optimization: delay .tolist() until here\n    # This is the GPU->CPU sync point\n    if hasattr(sampler_output.sampled_token_ids, 'tolist'):\n        sampled_token_ids_list = sampler_output.sampled_token_ids.tolist()\n    else:\n        sampled_token_ids_list = sampler_output.sampled_token_ids\n    \n    # Update with actual token ids after sync\n    for i, req_state, seq_len in request_seq_lens:\n        token_id = sampled_token_ids_list[i]\n        data[\"input_batch\"].token_ids_cpu[i, seq_len] = token_id\n        req_state.output_token_ids[-1] = token_id\n    \n    # Move logprobs to CPU if needed\n    if sampler_output.logprob_token_ids is not None:\n        logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n    else:\n        logprob_token_ids = None\n    \n    if sampler_output.logprobs is not None:\n        logprobs = sampler_output.logprobs.cpu()\n    else:\n        logprobs = None\n    \n    result = {\n        \"sampled_token_ids\": sampled_token_ids_list,\n        \"logprob_token_ids\": logprob_token_ids,\n        \"logprobs\": logprobs,\n        \"num_updated\": len(request_seq_lens)\n    }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    # Convert tensors to CPU for storage\n    stored_result = {}\n    for key, value in result.items():\n        if isinstance(value, torch.Tensor):\n            stored_result[key] = value.cpu()\n        else:\n            stored_result[key] = value\n    torch.save({\"type\": \"dict\", \"data\": stored_result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert type(current_result) == type(reference_result), f\"Type mismatch\"\n    \n    if isinstance(current_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        for key in current_result:\n            curr_val = current_result[key]\n            ref_val = reference_result[key]\n            \n            if isinstance(curr_val, torch.Tensor):\n                assert curr_val.shape == ref_val.shape, f\"Shape mismatch for {key}\"\n                assert curr_val.dtype == ref_val.dtype, f\"Dtype mismatch for {key}\"\n                \n                rtol, atol = 1e-5, 1e-7\n                torch.testing.assert_close(\n                    curr_val.cpu(),\n                    ref_val.cpu(),\n                    rtol=rtol, atol=atol\n                )\n            elif isinstance(curr_val, list):\n                assert len(curr_val) == len(ref_val), f\"Length mismatch for {key}\"\n                assert curr_val == ref_val, f\"Value mismatch for {key}\"\n            else:\n                assert curr_val == ref_val, f\"Value mismatch for {key}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        if torch.cuda.is_available():\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            torch.cuda.synchronize()\n            start.record()\n            result = func()\n            end.record()\n            torch.cuda.synchronize()\n            \n            times_ms.append(start.elapsed_time(end))\n        else:\n            start = time.perf_counter()\n            result = func()\n            end = time.perf_counter()\n            times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n    else:\n        warmup = 3\n        iters = 10\n    \n    result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": \"torch.float32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "fb0acb6c72874e98617cabee4ff4851569374fc9",
    "commit_short": "fb0acb6c",
    "commit_subject": "[Perf] Improve MLA on V1 (#14540)",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 6000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "92b0ce2ac75e251fe683f5b720f07001782054ff",
    "status": "baseline_failed",
    "gpu_config": "H100:8",
    "benchmark_mode": "standalone",
    "patch_type": null,
    "duration_s": 3332.4809260368347,
    "error": "Baseline benchmark produced no metrics",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 17:06:43 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev351+g92b0ce2a",
    "human_version": null,
    "agent_version": null,
    "model": "deepseek-ai/DeepSeek-R1",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": null
  },
  {
    "commit_hash": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "commit_short": "fc542144",
    "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
    "files_changed": [
      "vllm/model_executor/guided_decoding/xgrammar_decoding.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/12563",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 438.1914894580841,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-01 06:10:41 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev57+geb5741ad",
    "human_version": "INFO 01-01 06:12:54 __init__.py:183] Automatically detected platform cuda.\n0.7.1.dev58+gfc542144",
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 32.05,
    "baseline_ttft_median": 32.05,
    "baseline_ttft_p99": 32.05,
    "baseline_tpot_mean": 8.04,
    "baseline_tpot_median": 8.04,
    "baseline_tpot_p99": 8.04,
    "baseline_itl_mean": 8.04,
    "baseline_itl_median": 7.99,
    "baseline_itl_p99": 12.97,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 34.47,
    "human_ttft_median": 34.47,
    "human_ttft_p99": 34.47,
    "human_tpot_mean": 8.04,
    "human_tpot_median": 8.04,
    "human_tpot_p99": 8.04,
    "human_itl_mean": 8.04,
    "human_itl_median": 8.03,
    "human_itl_p99": 11.99,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 34.58,
    "agent_ttft_median": 34.58,
    "agent_ttft_p99": 34.58,
    "agent_tpot_mean": 8.17,
    "agent_tpot_median": 8.17,
    "agent_tpot_p99": 8.17,
    "agent_itl_mean": 8.17,
    "agent_itl_median": 8.17,
    "agent_itl_p99": 13.73,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": -7.55070202808113,
    "human_improvement_tpot_mean": 0.0,
    "human_improvement_itl_mean": 0.0,
    "agent_improvement_ttft_mean": -7.893915756630269,
    "agent_improvement_tpot_mean": -1.6169154228855822,
    "agent_improvement_itl_mean": -1.6169154228855822,
    "agent_vs_human_ttft_mean": -0.31911807368726264,
    "agent_vs_human_tpot_mean": -1.6169154228855822,
    "agent_vs_human_itl_mean": -1.6169154228855822,
    "human_improvement_ttft_median": -7.55070202808113,
    "human_improvement_ttft_p99": -7.55070202808113,
    "agent_improvement_ttft_median": -7.893915756630269,
    "agent_improvement_ttft_p99": -7.893915756630269,
    "agent_vs_human_ttft_median": -0.31911807368726264,
    "agent_vs_human_ttft_p99": -0.31911807368726264,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-01 06:12:25 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.05      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.95      \nOutput token throughput (tok/s):         121.37    \nTotal Token throughput (tok/s):          606.87    \n---------------Time to First Token----------------\nMean TTFT (ms):                          32.05     \nMedian TTFT (ms):                        32.05     \nP99 TTFT (ms):                           32.05     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.04      \nMedian TPOT (ms):                        8.04      \nP99 TPOT (ms):                           8.04      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.04      \nMedian ITL (ms):                         7.99      \nP99 ITL (ms):                            12.97     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/it]\n",
    "human_raw": "INFO 01-01 06:14:27 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.06      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.95      \nOutput token throughput (tok/s):         121.17    \nTotal Token throughput (tok/s):          605.83    \n---------------Time to First Token----------------\nMean TTFT (ms):                          34.47     \nMedian TTFT (ms):                        34.47     \nP99 TTFT (ms):                           34.47     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.04      \nMedian TPOT (ms):                        8.04      \nP99 TPOT (ms):                           8.04      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.04      \nMedian ITL (ms):                         8.03      \nP99 ITL (ms):                            11.99     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.06s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.06s/it]\n",
    "agent_raw": "INFO 01-01 06:16:35 __init__.py:183] Automatically detected platform cuda.\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=1, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     1         \nBenchmark duration (s):                  1.07      \nTotal input tokens:                      512       \nTotal generated tokens:                  128       \nRequest throughput (req/s):              0.93      \nOutput token throughput (tok/s):         119.17    \nTotal Token throughput (tok/s):          595.85    \n---------------Time to First Token----------------\nMean TTFT (ms):                          34.58     \nMedian TTFT (ms):                        34.58     \nP99 TTFT (ms):                           34.58     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          8.17      \nMedian TPOT (ms):                        8.17      \nP99 TPOT (ms):                           8.17      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           8.17      \nMedian ITL (ms):                         8.17      \nP99 ITL (ms):                            13.73     \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/1 [00:00<?, ?it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.07s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.07s/it]\n",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: fc542144c4477ffec1d3de6fa43e54f8fb5351e8\nMessage: [Feature] Fix guided decoding blocking bitmask memcpy (#12563)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, the target is XGrammarLogitsProcessor\n        module_path = \"vllm.model_executor.guided_decoding.xgrammar_decoding\"\n        symbol_name = \"XGrammarLogitsProcessor\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Simulate guided decoding workload\n    batch_size = 32  # Multiple users with guided decoding\n    vocab_size = 32000  # Llama vocab size\n    \n    # Create scores tensor (logits from model)\n    scores = torch.randn(batch_size, vocab_size, dtype=dtype, device='cpu')\n    \n    # Create token bitmask for grammar constraints (on CPU initially)\n    # Bitmask is typically sparse - most tokens are masked\n    token_bitmask = torch.zeros(batch_size, vocab_size, dtype=torch.bool, device='cpu')\n    # Allow only specific tokens per position (simulate grammar constraints)\n    for i in range(batch_size):\n        # Allow 5-10% of tokens\n        num_allowed = int(vocab_size * 0.05)\n        allowed_indices = torch.randperm(vocab_size)[:num_allowed]\n        token_bitmask[i, allowed_indices] = True\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"scores\": scores,\n        \"token_bitmask\": token_bitmask,\n        \"batch_size\": batch_size,\n        \"vocab_size\": vocab_size\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Simulate the critical path: moving bitmask to GPU and applying it\n    scores = data[\"scores\"].clone()\n    token_bitmask = data[\"token_bitmask\"].clone()\n    device = data[\"device\"]\n    \n    # Move scores to device first (simulating they come from model on GPU)\n    if device.type == \"cuda\":\n        scores = scores.to(device)\n    \n    # The optimization: non_blocking=True for bitmask transfer\n    # Check if we're on the optimized commit (with non_blocking support)\n    impl_tag = os.getenv(\"IMPL_TAG\", \"\")\n    \n    if impl_tag == \"child\":\n        # Optimized version with non_blocking\n        device_bitmask = token_bitmask.to(scores.device, non_blocking=True)\n    else:\n        # Original version without non_blocking\n        device_bitmask = token_bitmask.to(scores.device)\n    \n    # Apply the bitmask (simulate xgr.apply_token_bitmask_inplace)\n    # Set scores to -inf where bitmask is False\n    scores[~device_bitmask] = float('-inf')\n    \n    # Ensure synchronization for timing accuracy\n    if device.type == \"cuda\":\n        torch.cuda.synchronize()\n    \n    return scores\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Handle -inf values specially\n        curr_cpu = current_result.cpu()\n        ref_cpu = reference_result.cpu()\n        \n        # Check that -inf positions match\n        curr_inf_mask = torch.isinf(curr_cpu)\n        ref_inf_mask = torch.isinf(ref_cpu)\n        assert torch.equal(curr_inf_mask, ref_inf_mask), \"Inf mask mismatch\"\n        \n        # Check non-inf values\n        if not curr_inf_mask.all():\n            finite_mask = ~curr_inf_mask\n            # Determine tolerances based on dtype\n            if current_result.dtype in (torch.float16, torch.bfloat16):\n                rtol, atol = 1e-3, 1e-4\n            else:\n                rtol, atol = 1e-5, 1e-7\n            \n            torch.testing.assert_close(\n                curr_cpu[finite_mask],\n                ref_cpu[finite_mask],\n                rtol=rtol, atol=atol\n            )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Ensure clean state\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"fc542144c4477ffec1d3de6fa43e54f8fb5351e8\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
    "commit_short": "fc7b8d1e",
    "commit_subject": "[Performance] e2e overheads reduction: Small followup diff (#7364)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "files_changed": [
      "vllm/core/block_manager_v1.py",
      "vllm/sequence.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/7364",
    "models": [
      "N/A"
    ],
    "parent_commit": "67abdbb42fdbb59c274130368981c0d0ac3539e3",
    "status": "error",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2677.219157934189,
    "error": "Baseline install failed: No wheel available and no ancestor wheel found for 67abdbb42fdb",
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": null,
    "human_version": null,
    "agent_version": null,
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "has_agent_patch": true,
    "baseline_install_method": null,
    "human_install_method": null,
    "agent_install_method": null,
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": null,
    "baseline_ttft_median": null,
    "baseline_ttft_p99": null,
    "baseline_tpot_mean": null,
    "baseline_tpot_median": null,
    "baseline_tpot_p99": null,
    "baseline_itl_mean": null,
    "baseline_itl_median": null,
    "baseline_itl_p99": null,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": null,
    "human_ttft_median": null,
    "human_ttft_p99": null,
    "human_tpot_mean": null,
    "human_tpot_median": null,
    "human_tpot_p99": null,
    "human_itl_mean": null,
    "human_itl_median": null,
    "human_itl_p99": null,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": null,
    "agent_ttft_median": null,
    "agent_ttft_p99": null,
    "agent_tpot_mean": null,
    "agent_tpot_median": null,
    "agent_tpot_p99": null,
    "agent_itl_mean": null,
    "agent_itl_median": null,
    "agent_itl_p99": null,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": null,
    "human_improvement_tpot_mean": null,
    "human_improvement_itl_mean": null,
    "agent_improvement_ttft_mean": null,
    "agent_improvement_tpot_mean": null,
    "agent_improvement_itl_mean": null,
    "agent_vs_human_ttft_mean": null,
    "agent_vs_human_tpot_mean": null,
    "agent_vs_human_itl_mean": null,
    "human_improvement_ttft_median": null,
    "human_improvement_ttft_p99": null,
    "agent_improvement_ttft_median": null,
    "agent_improvement_ttft_p99": null,
    "agent_vs_human_ttft_median": null,
    "agent_vs_human_ttft_p99": null,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": null,
    "human_raw": null,
    "agent_raw": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: fc7b8d1eefcbe837a56b7c080509417fe5167e6c\nMessage: [Performance] e2e overheads reduction: Small followup diff (#7364)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - use SequenceGroup.get_finished_seqs\n    if not (module_path and symbol_name):\n        module_path = \"vllm.sequence\"\n        symbol_name = \"SequenceGroup\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    from vllm.compilation.backends import Sequence\n    from vllm.core.block.utils import SequenceGroup\n    from vllm.core.block_manager import SequenceStatus\n    from vllm.core.scheduler import SequenceData\n    from vllm import SamplingParams\n    from vllm.beam_search import LoRARequest\n    \n    # Create sampling params\n    sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=100)\n    \n    # Number of test scenarios\n    num_single_seq_groups = 500  # Single sequence groups (fast path)\n    num_multi_seq_groups = 100   # Multi-sequence groups (beam search)\n    \n    single_seq_groups = []\n    multi_seq_groups = []\n    \n    # Create single sequence groups (common case)\n    for i in range(num_single_seq_groups):\n        seq_id = f\"single_{i}\"\n        seq_data = SequenceData([1, 2, 3, 4, 5])  # Mock prompt tokens\n        seq = Sequence(\n            seq_id=seq_id,\n            inputs={\"prompt_token_ids\": [1, 2, 3, 4, 5]},\n            block_size=16\n        )\n        # Mark some as finished\n        if i % 3 == 0:\n            seq.status = SequenceStatus.FINISHED_STOPPED\n        \n        seq_group = SequenceGroup(\n            request_id=f\"req_single_{i}\",\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time()\n        )\n        single_seq_groups.append(seq_group)\n    \n    # Create multi-sequence groups (beam search case)\n    for i in range(num_multi_seq_groups):\n        seqs = []\n        beam_width = 4\n        for j in range(beam_width):\n            seq_id = f\"multi_{i}_{j}\"\n            seq_data = SequenceData([1, 2, 3, 4, 5])\n            seq = Sequence(\n                seq_id=seq_id,\n                inputs={\"prompt_token_ids\": [1, 2, 3, 4, 5]},\n                block_size=16\n            )\n            # Mark some beams as finished\n            if j < 2 and i % 2 == 0:\n                seq.status = SequenceStatus.FINISHED_STOPPED\n            seqs.append(seq)\n        \n        seq_group = SequenceGroup(\n            request_id=f\"req_multi_{i}\",\n            seqs=seqs,\n            sampling_params=sampling_params,\n            arrival_time=time.time()\n        )\n        multi_seq_groups.append(seq_group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float32,  # CPU optimization\n        \"hw_info\": hw_info,\n        \"single_seq_groups\": single_seq_groups,\n        \"multi_seq_groups\": multi_seq_groups,\n        \"all_seq_groups\": single_seq_groups + multi_seq_groups\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Test get_finished_seqs() which was optimized\n    results = {\n        \"single_finished\": [],\n        \"multi_finished\": []\n    }\n    \n    # Test single sequence groups (optimized fast path)\n    for seq_group in data[\"single_seq_groups\"]:\n        finished = seq_group.get_finished_seqs()\n        results[\"single_finished\"].append(len(finished))\n    \n    # Test multi-sequence groups\n    for seq_group in data[\"multi_seq_groups\"]:\n        finished = seq_group.get_finished_seqs()\n        results[\"multi_finished\"].append(len(finished))\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict):\n        assert current_result.keys() == reference_result.keys(), f\"Keys mismatch\"\n        for key in current_result:\n            if isinstance(current_result[key], list):\n                assert len(current_result[key]) == len(reference_result[key]), f\"Length mismatch for {key}\"\n                assert current_result[key] == reference_result[key], f\"Value mismatch for {key}\"\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n    else:\n        assert current_result == reference_result, f\"Value mismatch\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=100) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations with high precision.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # This is a CPU optimization - use CPU timing\n    warmup = 5\n    iters = 100  # More iterations for CPU timing stability\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"fc7b8d1eefcbe837a56b7c080509417fe5167e6c\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # This is a CPU optimization\n        \"dtype\": \"torch.float32\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"
  },
  {
    "commit_hash": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "commit_short": "fe66b347",
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing ",
    "repo": "vllm-project/vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "files_changed": [],
    "pr_url": null,
    "models": [],
    "parent_commit": "270a5da495d24e947a71e2fa0c56635f4fad2dc3",
    "status": "success",
    "gpu_config": "H100:1",
    "benchmark_mode": "serving",
    "patch_type": "python_only",
    "duration_s": 2762.1037936210632,
    "error": null,
    "error_message": null,
    "agent_name": "claude-code",
    "agent_model": "claude-sonnet-4-20250514",
    "benchmark_date": "2026-01-06",
    "baseline_version": "INFO 01-02 15:28:20 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev450+g270a5da4",
    "human_version": "INFO 01-02 15:34:23 [__init__.py:256] Automatically detected platform cuda.\n0.7.4.dev451+gfe66b347",
    "agent_version": null,
    "model": "ibm-ai-platform/Bamba-9B",
    "has_agent_patch": true,
    "baseline_install_method": "wheel",
    "human_install_method": "wheel",
    "agent_install_method": "python_overlay",
    "benchmark_type": null,
    "agent_error": null,
    "patch_path": null,
    "baseline_ttft_mean": 6225.57,
    "baseline_ttft_median": 4998.48,
    "baseline_ttft_p99": 17311.88,
    "baseline_tpot_mean": 82.23,
    "baseline_tpot_median": 87.68,
    "baseline_tpot_p99": 117.5,
    "baseline_itl_mean": 82.23,
    "baseline_itl_median": 68.35,
    "baseline_itl_p99": 101.29,
    "baseline_latency_avg": null,
    "baseline_throughput": null,
    "baseline_request_throughput": null,
    "baseline_output_throughput": null,
    "baseline_total_throughput": null,
    "human_ttft_mean": 5722.95,
    "human_ttft_median": 4649.01,
    "human_ttft_p99": 15374.29,
    "human_tpot_mean": 71.34,
    "human_tpot_median": 75.78,
    "human_tpot_p99": 102.94,
    "human_itl_mean": 71.34,
    "human_itl_median": 61.28,
    "human_itl_p99": 110.49,
    "human_latency_avg": null,
    "human_throughput": null,
    "human_request_throughput": null,
    "human_output_throughput": null,
    "human_total_throughput": null,
    "agent_ttft_mean": 5874.58,
    "agent_ttft_median": 4777.7,
    "agent_ttft_p99": 15940.41,
    "agent_tpot_mean": 74.58,
    "agent_tpot_median": 79.08,
    "agent_tpot_p99": 106.43,
    "agent_itl_mean": 74.58,
    "agent_itl_median": 63.35,
    "agent_itl_p99": 112.78,
    "agent_latency_avg": null,
    "agent_throughput": null,
    "agent_request_throughput": null,
    "agent_output_throughput": null,
    "agent_total_throughput": null,
    "human_improvement_ttft_mean": 8.073477609279148,
    "human_improvement_tpot_mean": 13.24334184604159,
    "human_improvement_itl_mean": 13.24334184604159,
    "agent_improvement_ttft_mean": 5.637877334926759,
    "agent_improvement_tpot_mean": 9.30317402407881,
    "agent_improvement_itl_mean": 9.30317402407881,
    "agent_vs_human_ttft_mean": -2.6495076839741762,
    "agent_vs_human_tpot_mean": -4.541631623212776,
    "agent_vs_human_itl_mean": -4.541631623212776,
    "human_improvement_ttft_median": 6.991525423728801,
    "human_improvement_ttft_p99": 11.192256415825433,
    "agent_improvement_ttft_median": 4.416942750596176,
    "agent_improvement_ttft_p99": 7.92213208501908,
    "agent_vs_human_ttft_median": -2.7681162225936187,
    "agent_vs_human_ttft_p99": -3.6822513429888404,
    "human_improvement_latency_avg": null,
    "human_improvement_throughput": null,
    "human_improvement_request_throughput": null,
    "human_improvement_output_throughput": null,
    "human_improvement_total_throughput": null,
    "agent_improvement_latency_avg": null,
    "agent_improvement_throughput": null,
    "agent_improvement_request_throughput": null,
    "agent_improvement_output_throughput": null,
    "agent_improvement_total_throughput": null,
    "agent_vs_human_latency_avg": null,
    "agent_vs_human_throughput": null,
    "agent_vs_human_request_throughput": null,
    "agent_vs_human_output_throughput": null,
    "agent_vs_human_total_throughput": null,
    "baseline_raw": "INFO 01-02 15:32:47 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2af84c127c40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18001, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  19.83     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              15.13     \nOutput token throughput (tok/s):         1936.46   \nTotal Token throughput (tok/s):          9682.32   \n---------------Time to First Token----------------\nMean TTFT (ms):                          6225.57   \nMedian TTFT (ms):                        4998.48   \nP99 TTFT (ms):                           17311.88  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          82.23     \nMedian TPOT (ms):                        87.68     \nP99 TPOT (ms):                           117.50    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           82.23     \nMedian ITL (ms):                         68.35     \nP99 ITL (ms):                            101.29    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:16<1:20:49, 16.22s/it]\n 86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 257/300 [00:19<00:02, 17.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:19<00:00, 15.13it/s]\n",
    "human_raw": "INFO 01-02 15:36:29 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b317384fc40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18002, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  17.86     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              16.80     \nOutput token throughput (tok/s):         2150.62   \nTotal Token throughput (tok/s):          10753.09  \n---------------Time to First Token----------------\nMean TTFT (ms):                          5722.95   \nMedian TTFT (ms):                        4649.01   \nP99 TTFT (ms):                           15374.29  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          71.34     \nMedian TPOT (ms):                        75.78     \nP99 TPOT (ms):                           102.94    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           71.34     \nMedian ITL (ms):                         61.28     \nP99 ITL (ms):                            110.49    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:11:00, 14.25s/it]\n 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 201/300 [00:14<00:04, 19.92it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:17<00:00, 16.80it/s]\n",
    "agent_raw": "INFO 01-02 15:40:15 [__init__.py:256] Automatically detected platform cuda.\nNamespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x2b4892867c40>, endpoint_type='openai-comp', label=None, base_url=None, host='127.0.0.1', port=18003, endpoint='/v1/completions', dataset_name='random', max_concurrency=None, model='ibm-ai-platform/Bamba-9B', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, random_input_len=512, random_output_len=128, random_range_ratio=1.0, random_prefix_len=0, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n============ Serving Benchmark Result ============\nSuccessful requests:                     300       \nBenchmark duration (s):                  18.49     \nTotal input tokens:                      153600    \nTotal generated tokens:                  38400     \nRequest throughput (req/s):              16.22     \nOutput token throughput (tok/s):         2076.40   \nTotal Token throughput (tok/s):          10382.02  \n---------------Time to First Token----------------\nMean TTFT (ms):                          5874.58   \nMedian TTFT (ms):                        4777.70   \nP99 TTFT (ms):                           15940.41  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          74.58     \nMedian TPOT (ms):                        79.08     \nP99 TPOT (ms):                           106.43    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           74.58     \nMedian ITL (ms):                         63.35     \nP99 ITL (ms):                            112.78    \n==================================================\n\n/usr/local/lib/python3.11/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n\n  0%|          | 0/300 [00:00<?, ?it/s]\n  0%|          | 1/300 [00:14<1:13:46, 14.80s/it]\n 70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 209/300 [00:14<00:04, 19.94it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:18<00:00, 16.22it/s]\n",
    "test_script": null
  }
]