{
  "human_commit": "83450458",
  "human_commit_full": "83450458339b07765b0e72a822e5fe93eeaf5258",
  "parent_commit": "5b8a1fde84224e24ec121e0dc149d775330d911b",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "No latency metrics in agent output",
  "duration_s": 147.10440802574158,
  "metrics": {},
  "raw_output": "tching file vllm/spec_decode/ngram_worker.py\nAGENT_PATCH_APPLIED\nFound vLLM at: /usr/bin/python3\n=== Running AGENT latency benchmark (offline) ===\nOriginal perf command: python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\nUsing baseline benchmark_latency.py: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\n`torch_dtype` is deprecated! Use `dtype` instead!\nNamespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model=\"'[ngram]'\", num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-14 18:44:58 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 1262, in maybe_create_spec_config\n    draft_model_config = ModelConfig(\n                         ^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 162, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 148, in get_config\n    if is_gguf or file_or_path_exists(model,\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 76, in file_or_path_exists\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n    validate_repo_id(arg_value)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n    raise HFValidationError(\nhuggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''[ngram]''.\nBENCHMARK_DONE\n`torch_dtype` is deprecated! Use `dtype` instead!\nNamespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model=\"'[ngram]'\", num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-14 18:44:58 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 1262, in maybe_create_spec_config\n    draft_model_config = ModelConfig(\n                         ^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 162, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 148, in get_config\n    if is_gguf or file_or_path_exists(model,\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 76, in file_or_path_exists\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n    validate_repo_id(arg_value)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n    raise HFValidationError(\nhuggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''[ngram]''.\nUnable to find image 'anonymous/vllm-baseline:baseline-5b8a1fde8422' locally\nbaseline-5b8a1fde8422: Pulling from anonymous/vllm-baseline\n3c645031de29: Already exists\n0d6448aff889: Already exists\n0a7674e3e8fe: Already exists\nb71b637b97c5: Already exists\n56dc85502937: Already exists\nb9839793d66a: Pulling fs layer\n2b5f54714bab: Pulling fs layer\n7fb1d733c143: Pulling fs layer\ndfe7effe1245: Pulling fs layer\n8f1341072e1b: Pulling fs layer\nd44b59559a3b: Pulling fs layer\n4a95ea0cab6c: Pulling fs layer\n7231a5ed91b2: Pulling fs layer\n61f1fb473dfe: Pulling fs layer\na71959b39201: Pulling fs layer\n58aa19ebc685: Pulling fs layer\n8986bcc6c0f8: Pulling fs layer\ndfe7effe1245: Waiting\n883162dfb86b: Pulling fs layer\n8f1341072e1b: Waiting\n7231a5ed91b2: Waiting\nd44b59559a3b: Waiting\n768e5c7afd08: Pulling fs layer\na71959b39201: Waiting\n58aa19ebc685: Waiting\n8dbd114f1dc5: Pulling fs layer\n883162dfb86b: Waiting\n57a8b593fc8e: Pulling fs layer\n768e5c7afd08: Waiting\n624612e31330: Pulling fs layer\n8986bcc6c0f8: Waiting\n8dbd114f1dc5: Waiting\n57a8b593fc8e: Waiting\n624612e31330: Waiting\nb9839793d66a: Verifying Checksum\nb9839793d66a: Download complete\n2b5f54714bab: Verifying Checksum\n2b5f54714bab: Download complete\nb9839793d66a: Pull complete\n2b5f54714bab: Pull complete\ndfe7effe1245: Verifying Checksum\ndfe7effe1245: Download complete\n7fb1d733c143: Verifying Checksum\n7fb1d733c143: Download complete\n4a95ea0cab6c: Verifying Checksum\n4a95ea0cab6c: Download complete\n7231a5ed91b2: Verifying Checksum\n7231a5ed91b2: Download complete\nd44b59559a3b: Verifying Checksum\nd44b59559a3b: Download complete\na71959b39201: Verifying Checksum\na71959b39201: Download complete\n7fb1d733c143: Pull complete\ndfe7effe1245: Pull complete\n58aa19ebc685: Verifying Checksum\n58aa19ebc685: Download complete\n8986bcc6c0f8: Verifying Checksum\n8986bcc6c0f8: Download complete\n883162dfb86b: Verifying Checksum\n883162dfb86b: Download complete\n768e5c7afd08: Verifying Checksum\n768e5c7afd08: Download complete\n8dbd114f1dc5: Verifying Checksum\n8dbd114f1dc5: Download complete\n57a8b593fc8e: Download complete\n624612e31330: Verifying Checksum\n624612e31330: Download complete\n8f1341072e1b: Verifying Checksum\n8f1341072e1b: Download complete\n61f1fb473dfe: Verifying Checksum\n61f1fb473dfe: Download complete\n8f1341072e1b: Pull complete\nd44b59559a3b: Pull complete\n4a95ea0cab6c: Pull complete\n7231a5ed91b2: Pull complete\n61f1fb473dfe: Pull complete\na71959b39201: Pull complete\n58aa19ebc685: Pull complete\n8986bcc6c0f8: Pull complete\n883162dfb86b: Pull complete\n768e5c7afd08: Pull complete\n8dbd114f1dc5: Pull complete\n57a8b593fc8e: Pull complete\n624612e31330: Pull complete\nDigest: sha256:22fc7addab807f1e407c375cea9370b2713de5f7158f90b7fda65c3774226c35\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-5b8a1fde8422\n",
  "timestamp": "2026-01-15 02:44:59"
}