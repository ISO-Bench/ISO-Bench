{
  "human_commit": "61b8cea3",
  "human_commit_full": "61b8cea3b42feab021d506e9143551de18f9165c",
  "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b",
  "model": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "error",
  "error": "No throughput metrics in agent output",
  "duration_s": 232.71533608436584,
  "metrics": {},
  "raw_output": "r_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \nINFO 01-14 16:45:21 [cuda.py:398] Using Flash Attention backend.\nINFO 01-14 16:45:21 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 01-14 16:45:21 [model_runner.py:1083] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\nINFO 01-14 16:45:22 [weight_utils.py:296] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.79s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.77s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.92s/it]\n\nINFO 01-14 16:45:26 [default_loader.py:262] Loading weights took 3.91 seconds\nINFO 01-14 16:45:27 [model_runner.py:1115] Model loading took 6.0160 GiB and 4.412654 seconds\nINFO 01-14 16:45:27 [worker.py:295] Memory profiling takes 0.52 seconds\nINFO 01-14 16:45:27 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\nINFO 01-14 16:45:27 [worker.py:295] model weights take 6.02GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is 63.83GiB.\nINFO 01-14 16:45:27 [executor_base.py:113] # cuda blocks: 37350, # CPU blocks: 2340\nINFO 01-14 16:45:27 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 4.56x\nINFO 01-14 16:45:32 [model_runner.py:1385] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\nCapturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\nCapturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:22,  1.54it/s]\nCapturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:27,  1.22it/s]\nCapturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:02<00:29,  1.09it/s]\nCapturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:03<00:28,  1.08it/s]\nCapturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:04<00:28,  1.05it/s]\nCapturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:05<00:28,  1.03it/s]\nCapturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:06<00:27,  1.03it/s]\nCapturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:07<00:26,  1.03it/s]\nCapturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:08<00:25,  1.03it/s]\nCapturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:09<00:23,  1.04it/s]\nCapturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:10<00:22,  1.05it/s]\nCapturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:11<00:21,  1.09it/s]\nCapturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:11<00:18,  1.17it/s]\nCapturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:12<00:16,  1.29it/s]\nCapturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:13<00:14,  1.37it/s]\nCapturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:13<00:12,  1.51it/s]\nCapturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:14<00:10,  1.64it/s]\nCapturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:14<00:09,  1.75it/s]\nCapturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:08,  1.82it/s]\nCapturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:15<00:07,  1.88it/s]\nCapturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:16<00:07,  1.91it/s]\nCapturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:16<00:06,  1.93it/s]\nCapturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:17<00:06,  1.95it/s]\nCapturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:17<00:05,  1.97it/s]\nCapturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:18<00:05,  1.99it/s]\nCapturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:18<00:04,  2.01it/s]\nCapturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:19<00:03,  2.02it/s]\nCapturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:19<00:03,  2.03it/s]\nCapturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:20<00:02,  2.04it/s]\nCapturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:20<00:02,  2.04it/s]\nCapturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:20<00:01,  2.05it/s]\nCapturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:21<00:01,  2.06it/s]\nCapturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:21<00:00,  2.07it/s]\nCapturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:22<00:00,  2.08it/s]\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:22<00:00,  2.09it/s]\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:22<00:00,  1.53it/s]\nINFO 01-14 16:45:55 [model_runner.py:1537] Graph capturing finished in 23 secs, took 0.33 GiB\nINFO 01-14 16:45:55 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 28.60 seconds\n\nAdding requests:   0%|          | 0/100 [00:00<?, ?it/s]\nAdding requests: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00<00:00, 1354.37it/s]\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:01<03:01,  1.83s/it, est. speed input: 139.91 toks/s, output: 69.95 toks/s]\nProcessed prompts:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 79/100 [00:01<00:00, 56.99it/s, est. speed input: 10478.36 toks/s, output: 5239.41 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 56.99it/s, est. speed input: 13161.51 toks/s, output: 6581.00 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 51.41it/s, est. speed input: 13161.51 toks/s, output: 6581.00 toks/s]\nThroughput: 49.51 requests/s, 19012.61 total tokens/s, 6337.70 output tokens/s\nTotal num prompt tokens:  25599\nTotal num output tokens:  12800\n[rank0]:[W114 16:45:58.757314770 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nUnable to find image 'anonymous/vllm-baseline:baseline-526078a96c52' locally\nbaseline-526078a96c52: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\n0b45fc65175e: Pulling fs layer\n21741161128c: Pulling fs layer\n9da69fd00e97: Pulling fs layer\nfff70fd02e66: Pulling fs layer\n6a6ca5ae8853: Pulling fs layer\na5da2d4e7468: Pulling fs layer\n96f2ec5191d2: Pulling fs layer\nbcb7d137089c: Pulling fs layer\n8497eafc3278: Pulling fs layer\nfff70fd02e66: Waiting\n65a47be34c52: Pulling fs layer\na0555c6d1173: Pulling fs layer\nc0096c57533f: Pulling fs layer\n8f7811bd0912: Pulling fs layer\n4b0b2601cf00: Pulling fs layer\n203b2b7dd2ec: Pulling fs layer\n874294342ba1: Pulling fs layer\n6a6ca5ae8853: Waiting\ne45c00c4d2c5: Pulling fs layer\na5da2d4e7468: Waiting\n6e784e87eb2c: Pulling fs layer\n96f2ec5191d2: Waiting\nbcb7d137089c: Waiting\n991a4d5b08cc: Pulling fs layer\n65a47be34c52: Waiting\n8497eafc3278: Waiting\na0555c6d1173: Waiting\n874294342ba1: Waiting\nc0096c57533f: Waiting\ne45c00c4d2c5: Waiting\n6e784e87eb2c: Waiting\n8f7811bd0912: Waiting\n4b0b2601cf00: Waiting\n991a4d5b08cc: Waiting\n203b2b7dd2ec: Waiting\n9da69fd00e97: Verifying Checksum\n9da69fd00e97: Download complete\nfff70fd02e66: Verifying Checksum\nfff70fd02e66: Download complete\n21741161128c: Verifying Checksum\n21741161128c: Download complete\na5da2d4e7468: Download complete\n6a6ca5ae8853: Verifying Checksum\n6a6ca5ae8853: Download complete\n96f2ec5191d2: Verifying Checksum\n96f2ec5191d2: Download complete\nbcb7d137089c: Verifying Checksum\nbcb7d137089c: Download complete\n8497eafc3278: Download complete\n65a47be34c52: Verifying Checksum\n65a47be34c52: Download complete\nc0096c57533f: Verifying Checksum\nc0096c57533f: Download complete\n8f7811bd0912: Verifying Checksum\n8f7811bd0912: Download complete\n4b0b2601cf00: Verifying Checksum\n4b0b2601cf00: Download complete\n203b2b7dd2ec: Verifying Checksum\n203b2b7dd2ec: Download complete\n874294342ba1: Verifying Checksum\n874294342ba1: Download complete\ne45c00c4d2c5: Verifying Checksum\ne45c00c4d2c5: Download complete\n6e784e87eb2c: Verifying Checksum\n6e784e87eb2c: Download complete\n991a4d5b08cc: Verifying Checksum\n991a4d5b08cc: Download complete\na0555c6d1173: Verifying Checksum\na0555c6d1173: Download complete\n0b45fc65175e: Verifying Checksum\n0b45fc65175e: Download complete\n0b45fc65175e: Pull complete\n21741161128c: Pull complete\n9da69fd00e97: Pull complete\nfff70fd02e66: Pull complete\n6a6ca5ae8853: Pull complete\na5da2d4e7468: Pull complete\n96f2ec5191d2: Pull complete\nbcb7d137089c: Pull complete\n8497eafc3278: Pull complete\n65a47be34c52: Pull complete\na0555c6d1173: Pull complete\nc0096c57533f: Pull complete\n8f7811bd0912: Pull complete\n4b0b2601cf00: Pull complete\n203b2b7dd2ec: Pull complete\n874294342ba1: Pull complete\ne45c00c4d2c5: Pull complete\n6e784e87eb2c: Pull complete\n991a4d5b08cc: Pull complete\nDigest: sha256:b578b87a18847931751d56fc86204b79046a536bbc6987ef54b1e0b13d9b61a9\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-526078a96c52\n",
  "timestamp": "2026-01-15 00:46:02"
}