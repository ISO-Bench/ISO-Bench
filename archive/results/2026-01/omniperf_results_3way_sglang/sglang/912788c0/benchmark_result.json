{
  "status": "error",
  "gpu_config": "H100:1",
  "baseline_metrics": {},
  "human_metrics": {},
  "agent_metrics": null,
  "human_improvement": {},
  "agent_improvement": null,
  "agent_vs_human": null,
  "error": "Human server failed to start. Logs: ned_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 322, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\", line 511, in cached_files\n    raise OSError(\nOSError: Llama4-Maverick-FP8 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
  "duration_s": 417.45758414268494,
  "perf_command": "python benchmarks/benchmark_serving.py --model Llama4-Maverick-FP8 --num-prompts 100",
  "install_method": "docker",
  "baseline_raw": "",
  "human_raw": "",
  "agent_raw": "",
  "commit": "912788c0",
  "full_commit": "912788c095c9306daabc996fd06e59cf062a783b",
  "parent_commit": "0f75b907c6bf6273dbb813b9e983757dab20751f",
  "model": "Llama4-Maverick-FP8",
  "subject": "perf: optimize local_block_table memory allocation (#6273)",
  "has_agent_patch": true
}