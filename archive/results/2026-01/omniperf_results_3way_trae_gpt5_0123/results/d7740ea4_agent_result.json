{
  "human_commit": "d7740ea4",
  "human_commit_full": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
  "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 165.30168962478638,
  "metrics": {
    "throughput_tok_s": 8088.76
  },
  "raw_output": "ig: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\nINFO 01-24 11:16:29 utils.py:638] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-24 11:16:29 selector.py:27] Using FlashAttention-2 backend.\nINFO 01-24 11:16:31 weight_utils.py:199] Using model weights format ['*.safetensors']\nINFO 01-24 11:16:36 model_runner.py:175] Loading model weights took 14.9595 GB\nINFO 01-24 11:16:37 gpu_executor.py:118] # GPU blocks: 27735, # CPU blocks: 2048\nINFO 01-24 11:16:39 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-24 11:16:39 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-24 11:16:44 model_runner.py:1017] Graph capturing finished in 5 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s]\nProcessed prompts:   0%|          | 1/1000 [00:15<4:17:03, 15.44s/it]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:31<01:16,  9.70it/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [00:47<00:38, 12.58it/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 768/1000 [01:00<00:18, 12.58it/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:02<00:16, 14.07it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:02<00:00, 15.91it/s]\nThroughput: 15.80 requests/s, 8088.76 tokens/s\nBENCHMARK_DONE\nNamespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None)\nINFO 01-24 11:16:28 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\nINFO 01-24 11:16:29 utils.py:638] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-24 11:16:29 selector.py:27] Using FlashAttention-2 backend.\nINFO 01-24 11:16:31 weight_utils.py:199] Using model weights format ['*.safetensors']\nINFO 01-24 11:16:36 model_runner.py:175] Loading model weights took 14.9595 GB\nINFO 01-24 11:16:37 gpu_executor.py:118] # GPU blocks: 27735, # CPU blocks: 2048\nINFO 01-24 11:16:39 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-24 11:16:39 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-24 11:16:44 model_runner.py:1017] Graph capturing finished in 5 secs.\n\nProcessed prompts:   0%|          | 0/1000 [00:00<?, ?it/s]\nProcessed prompts:   0%|          | 1/1000 [00:15<4:17:03, 15.44s/it]\nProcessed prompts:  26%|\u2588\u2588\u258c       | 257/1000 [00:31<01:16,  9.70it/s]\nProcessed prompts:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 513/1000 [00:47<00:38, 12.58it/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 768/1000 [01:00<00:18, 12.58it/s]\nProcessed prompts:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 769/1000 [01:02<00:16, 14.07it/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [01:02<00:00, 15.91it/s]\nThroughput: 15.80 requests/s, 8088.76 tokens/s\n",
  "timestamp": "2026-01-24 11:17:57"
}