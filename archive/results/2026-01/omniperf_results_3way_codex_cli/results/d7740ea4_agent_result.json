{
  "human_commit": "d7740ea4",
  "human_commit_full": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
  "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "No throughput metrics in agent output",
  "duration_s": 463.9017732143402,
  "metrics": {},
  "raw_output": "/libnccl.so.2.18.1\nINFO 01-21 21:06:52 selector.py:27] Using FlashAttention-2 backend.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 387, in <module>\n[rank0]:     main(args)\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 221, in main\n[rank0]:     elapsed_time = run_vllm(\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 85, in run_vllm\n[rank0]:     llm = LLM(\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 123, in __init__\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 292, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 160, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 41, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 23, in _init_executor\n[rank0]:     self._init_non_spec_worker()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 69, in _init_non_spec_worker\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 118, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 164, in load_model\n[rank0]:     self.model = get_model(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 222, in load_model\n[rank0]:     model = _initialize_model(model_config, self.load_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 88, in _initialize_model\n[rank0]:     return model_class(config=model_config.hf_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 338, in __init__\n[rank0]:     self.model = LlamaModel(config, quant_config, lora_config=lora_config)\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 267, in __init__\n[rank0]:     self.layers = nn.ModuleList([\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 268, in <listcomp>\n[rank0]:     LlamaDecoderLayer(config, quant_config)\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 195, in __init__\n[rank0]:     self.self_attn = LlamaAttention(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 144, in __init__\n[rank0]:     self.rotary_emb = get_rope(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\", line 523, in get_rope\n[rank0]:     raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n[rank0]: ValueError: Unknown RoPE scaling type llama3\nBENCHMARK_DONE\nNamespace(backend='vllm', dataset=None, input_len=256, output_len=256, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer='meta-llama/Llama-3.1-8B-Instruct', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto', gpu_memory_utilization=0.9, enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, device='cuda', enable_prefix_caching=False, enable_chunked_prefill=False, max_num_batched_tokens=None, download_dir=None)\nINFO 01-21 21:06:51 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1048576, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct)\nINFO 01-21 21:06:51 utils.py:638] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\nINFO 01-21 21:06:52 selector.py:27] Using FlashAttention-2 backend.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 387, in <module>\n[rank0]:     main(args)\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 221, in main\n[rank0]:     elapsed_time = run_vllm(\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 85, in run_vllm\n[rank0]:     llm = LLM(\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 123, in __init__\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 292, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 160, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 41, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 23, in _init_executor\n[rank0]:     self._init_non_spec_worker()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 69, in _init_non_spec_worker\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 118, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 164, in load_model\n[rank0]:     self.model = get_model(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 222, in load_model\n[rank0]:     model = _initialize_model(model_config, self.load_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 88, in _initialize_model\n[rank0]:     return model_class(config=model_config.hf_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 338, in __init__\n[rank0]:     self.model = LlamaModel(config, quant_config, lora_config=lora_config)\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 267, in __init__\n[rank0]:     self.layers = nn.ModuleList([\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 268, in <listcomp>\n[rank0]:     LlamaDecoderLayer(config, quant_config)\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 195, in __init__\n[rank0]:     self.self_attn = LlamaAttention(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 144, in __init__\n[rank0]:     self.rotary_emb = get_rope(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\", line 523, in get_rope\n[rank0]:     raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n[rank0]: ValueError: Unknown RoPE scaling type llama3\nUnable to find image 'anonymous/vllm-baseline:baseline-cc466a32903d' locally\nbaseline-cc466a32903d: Pulling from anonymous/vllm-baseline\n3c645031de29: Already exists\n0d6448aff889: Already exists\n0a7674e3e8fe: Already exists\nb71b637b97c5: Already exists\n56dc85502937: Already exists\n558a309afd19: Pulling fs layer\ndb5e0c2391d9: Pulling fs layer\nde0c85be2f0d: Pulling fs layer\n9dd10ad2d299: Pulling fs layer\n6bb111822e6b: Pulling fs layer\ne93d4d15c7b8: Pulling fs layer\nf6fd747944a7: Pulling fs layer\ncb9ce9a302c3: Pulling fs layer\na0375eb008f8: Pulling fs layer\ndedf9bee420f: Pulling fs layer\n9b984870bd81: Pulling fs layer\n9dd10ad2d299: Waiting\n6f725e269758: Pulling fs layer\n6bb111822e6b: Waiting\ne93d4d15c7b8: Waiting\n257613c813a1: Pulling fs layer\nf6fd747944a7: Waiting\ncb9ce9a302c3: Waiting\nae5a8ab5647c: Pulling fs layer\n246551bfe875: Pulling fs layer\na0375eb008f8: Waiting\ndedf9bee420f: Waiting\n9b984870bd81: Waiting\n6f725e269758: Waiting\n246551bfe875: Waiting\nae5a8ab5647c: Waiting\n257613c813a1: Waiting\nde0c85be2f0d: Verifying Checksum\nde0c85be2f0d: Download complete\n558a309afd19: Download complete\n558a309afd19: Pull complete\n6bb111822e6b: Verifying Checksum\n6bb111822e6b: Download complete\ne93d4d15c7b8: Verifying Checksum\ne93d4d15c7b8: Download complete\ndb5e0c2391d9: Verifying Checksum\ndb5e0c2391d9: Download complete\ncb9ce9a302c3: Verifying Checksum\ncb9ce9a302c3: Download complete\na0375eb008f8: Verifying Checksum\na0375eb008f8: Download complete\ndedf9bee420f: Download complete\n9b984870bd81: Verifying Checksum\n9b984870bd81: Download complete\n6f725e269758: Verifying Checksum\n6f725e269758: Download complete\n257613c813a1: Verifying Checksum\n257613c813a1: Download complete\nae5a8ab5647c: Verifying Checksum\nae5a8ab5647c: Download complete\ndb5e0c2391d9: Pull complete\nde0c85be2f0d: Pull complete\n246551bfe875: Verifying Checksum\n246551bfe875: Download complete\n9dd10ad2d299: Verifying Checksum\n9dd10ad2d299: Download complete\nf6fd747944a7: Download complete\n9dd10ad2d299: Pull complete\n6bb111822e6b: Pull complete\ne93d4d15c7b8: Pull complete\nf6fd747944a7: Pull complete\ncb9ce9a302c3: Pull complete\na0375eb008f8: Pull complete\ndedf9bee420f: Pull complete\n9b984870bd81: Pull complete\n6f725e269758: Pull complete\n257613c813a1: Pull complete\nae5a8ab5647c: Pull complete\n246551bfe875: Pull complete\nDigest: sha256:dadc52e40c762b6e4d7ee3e4952b92d907cdd7d88d00f0e1027b994d4bb8cbed\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-cc466a32903d\n",
  "timestamp": "2026-01-21 21:07:04"
}