{
  "human_commit": "2deb029d",
  "human_commit_full": "2deb029d115dadd012ce5ea70487a207cb025493",
  "parent_commit": "029c71de11bc3bcf84a1b3cf9d91e79ab6949799",
  "model": "RedHatAI/Meta-Llama-3-8B-Instruct-FP8",
  "status": "success",
  "error": null,
  "duration_s": 369.2482054233551,
  "metrics": {
    "input_throughput_tok_s": 10794.96,
    "throughput_tok_s": 3347.27,
    "elapsed_time_s": 6.093891143798828
  },
  "raw_output": "ze=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=RedHatAI/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=True)\nINFO 01-21 03:08:07 model_runner.py:879] Starting to load model RedHatAI/Meta-Llama-3-8B-Instruct-FP8...\nWARNING 01-21 03:08:07 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-21 03:08:07 weight_utils.py:236] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.06s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.79s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.68s/it]\n\nINFO 01-21 03:08:48 model_runner.py:890] Loading model weights took 8.4596 GB\nINFO 01-21 03:08:48 gpu_executor.py:121] # GPU blocks: 31185, # CPU blocks: 2048\nINFO 01-21 03:08:51 block_manager_v1.py:263] Automatic prefix caching is enabled.\nTesting filtered datasets\n------warm up------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<09:42,  5.89s/it, est. speed input: 109.60 toks/s, output: 33.98 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 16.98it/s, est. speed input: 10954.21 toks/s, output: 3396.65 toks/s]\ncost time 6.025847911834717\n------start generating------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<09:51,  5.97s/it, est. speed input: 108.00 toks/s, output: 33.49 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 16.74it/s, est. speed input: 10794.96 toks/s, output: 3347.27 toks/s]\ncost time 6.093891143798828\nBENCHMARK_DONE\nINFO 01-21 03:08:05 llm_engine.py:194] Initializing an LLM engine (v0.5.5) with config: model='RedHatAI/Meta-Llama-3-8B-Instruct-FP8', speculative_config=None, tokenizer='RedHatAI/Meta-Llama-3-8B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=RedHatAI/Meta-Llama-3-8B-Instruct-FP8, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=True)\nINFO 01-21 03:08:07 model_runner.py:879] Starting to load model RedHatAI/Meta-Llama-3-8B-Instruct-FP8...\nWARNING 01-21 03:08:07 fp8.py:46] Detected fp8 checkpoint. Please note that the format is experimental and subject to change.\nINFO 01-21 03:08:07 weight_utils.py:236] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.06s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.79s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.68s/it]\n\nINFO 01-21 03:08:48 model_runner.py:890] Loading model weights took 8.4596 GB\nINFO 01-21 03:08:48 gpu_executor.py:121] # GPU blocks: 31185, # CPU blocks: 2048\nINFO 01-21 03:08:51 block_manager_v1.py:263] Automatic prefix caching is enabled.\nTesting filtered datasets\n------warm up------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<09:42,  5.89s/it, est. speed input: 109.60 toks/s, output: 33.98 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 16.98it/s, est. speed input: 10954.21 toks/s, output: 3396.65 toks/s]\ncost time 6.025847911834717\n------start generating------\n\nProcessed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\nProcessed prompts:   1%|          | 1/100 [00:05<09:51,  5.97s/it, est. speed input: 108.00 toks/s, output: 33.49 toks/s]\nProcessed prompts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:05<00:00, 16.74it/s, est. speed input: 10794.96 toks/s, output: 3347.27 toks/s]\ncost time 6.093891143798828\n",
  "timestamp": "2026-01-21 11:09:19"
}