{
  "human_commit": "015069b0",
  "human_commit_full": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
  "parent_commit": "fbefc8a78d22b20eac042c586805c7dcbfc66b1c",
  "model": "Qwen/Qwen3-7B-Instruct",
  "status": "error",
  "error": "No serving metrics in output",
  "duration_s": 897.5778713226318,
  "metrics": {},
  "raw_output": "-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n    raise _format(RepositoryNotFoundError, message, response) from e\nhuggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-69708e36-6ff659c3562abb0539910a45;1aa725ec-6a1f-4137-a3ef-81ece508a6c9)\n\nRepository Not Found for url: https://huggingface.co/api/models/Qwen/Qwen3-7B-Instruct/tree/main?recursive=True&expand=False.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1130, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 941, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 830, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"<string>\", line 41, in __init__\n  File \"/opt/vllm_baseline/vllm/config.py\", line 487, in __post_init__\n    hf_config = get_config(self.hf_config_path or self.model,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 305, in get_config\n    raise ValueError(error_message) from e\nValueError: Invalid repository ID or local directory specified: 'Qwen/Qwen3-7B-Instruct'.\nPlease verify the following requirements:\n1. Provide a valid Hugging Face repository ID.\n2. Specify a local directory that contains a recognized configuration file.\n   - For Hugging Face models: ensure the presence of a 'config.json'.\n   - For Mistral models: ensure the presence of a 'params.json'.\n\nSERVER_START_FAILED\nUnable to find image 'anonymous/vllm-baseline:baseline-fbefc8a78d22' locally\nbaseline-fbefc8a78d22: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\na04181a7ff83: Already exists\nb89dfbaf9e2c: Already exists\nf221ee730b1b: Already exists\nab7bef329942: Already exists\n46bce9cdf2db: Already exists\n27f7c531d01c: Already exists\nd8bb65e3f5b5: Already exists\n3223ae3372cf: Already exists\nbd1777c8392b: Already exists\n324924804bf9: Already exists\n444f103aa742: Already exists\nda4fe45f4026: Already exists\n7566a439a21c: Already exists\n4a5b7095821f: Already exists\n1cc21fa2e007: Already exists\ne4e6ada9d900: Pulling fs layer\n6ae75f8c5ac2: Pulling fs layer\ne4e6ada9d900: Verifying Checksum\ne4e6ada9d900: Download complete\ne4e6ada9d900: Pull complete\n6ae75f8c5ac2: Download complete\n6ae75f8c5ac2: Pull complete\nDigest: sha256:4d11a5a146cdc501319991a5ebcd09aaee0ed5fefe9f2203aeb0bf765b3f1142\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-fbefc8a78d22\n",
  "timestamp": "2026-01-21 08:33:25"
}