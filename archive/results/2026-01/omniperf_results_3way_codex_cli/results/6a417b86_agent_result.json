{
  "human_commit": "6a417b86",
  "human_commit_full": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
  "parent_commit": "d3ea50113c08bdd3c5cfda42ec6ecbc72328d7d1",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 225.1060209274292,
  "metrics": {
    "ttft_mean_ms": 757.2,
    "ttft_median_ms": 828.15,
    "ttft_p99_ms": 1182.86,
    "tpot_mean_ms": 34.98,
    "tpot_median_ms": 30.65,
    "tpot_p99_ms": 128.68,
    "itl_mean_ms": 29.42,
    "itl_median_ms": 21.43,
    "itl_p99_ms": 443.85,
    "request_throughput_req_s": 37.07,
    "output_token_throughput_tok_s": 2296.38,
    "total_token_throughput_tok_s": 11787.38
  },
  "raw_output": "OST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48240 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48248 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48258 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48260 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48274 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48276 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48284 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48298 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48310 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48314 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48324 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48334 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48336 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48342 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48356 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48368 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48376 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48390 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:48402 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n  1%|          | 1/100 [00:01<02:11,  1.33s/it]\n  4%|\u258d         | 4/100 [00:02<00:48,  2.00it/s]\n  5%|\u258c         | 5/100 [00:02<00:44,  2.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 37.08it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.70      \nTotal input tokens:                      25600     \nTotal generated tokens:                  6194      \nRequest throughput (req/s):              37.07     \nOutput token throughput (tok/s):         2296.38   \nTotal Token throughput (tok/s):          11787.38  \n---------------Time to First Token----------------\nMean TTFT (ms):                          757.20    \nMedian TTFT (ms):                        828.15    \nP99 TTFT (ms):                           1182.86   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          34.98     \nMedian TPOT (ms):                        30.65     \nP99 TPOT (ms):                           128.68    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.42     \nMedian ITL (ms):                         21.43     \nP99 ITL (ms):                            443.85    \n==================================================\nBENCHMARK_DONE\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset=None, dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, best_of=1, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=256, random_output_len=64, random_range_ratio=1.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:11,  1.33s/it]\n  4%|\u258d         | 4/100 [00:02<00:48,  2.00it/s]\n  5%|\u258c         | 5/100 [00:02<00:44,  2.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:02<00:00, 37.08it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  2.70      \nTotal input tokens:                      25600     \nTotal generated tokens:                  6194      \nRequest throughput (req/s):              37.07     \nOutput token throughput (tok/s):         2296.38   \nTotal Token throughput (tok/s):          11787.38  \n---------------Time to First Token----------------\nMean TTFT (ms):                          757.20    \nMedian TTFT (ms):                        828.15    \nP99 TTFT (ms):                           1182.86   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          34.98     \nMedian TPOT (ms):                        30.65     \nP99 TPOT (ms):                           128.68    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.42     \nMedian ITL (ms):                         21.43     \nP99 ITL (ms):                            443.85    \n==================================================\n",
  "timestamp": "2026-01-21 22:11:13"
}