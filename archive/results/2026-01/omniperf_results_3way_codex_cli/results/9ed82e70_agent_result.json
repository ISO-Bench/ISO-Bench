{
  "human_commit": "9ed82e70",
  "human_commit_full": "9ed82e7074a18e25680ab106fc846364ad97bc00",
  "parent_commit": "51f8aa90ad409cc77bfab208be7f5907bf7d5330",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 759.7966012954712,
  "metrics": {},
  "raw_output": "ser can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n\n[notice] A new release of pip is available: 25.2 -> 25.3\n[notice] To update, run: python3 -m pip install --upgrade pip\nWarning: outlines.fsm fix failed - patching vLLM files directly...\nPatching all guided_decoding modules...\nGuided decoding modules patched\n=== Starting vLLM server for AGENT benchmark ===\nINFO 01-21 15:20:18 api_server.py:218] vLLM API server version 0.5.2\nINFO 01-21 15:20:18 api_server.py:219] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\nINFO 01-21 15:20:18 llm_engine.py:175] Initializing an LLM engine (v0.5.2) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n[rank0]:     return _run_code(code, main_globals, None,\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n[rank0]:     exec(code, run_globals)\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 292, in <module>\n[rank0]:     run_server(args)\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 230, in run_server\n[rank0]:     if llm_engine is not None else AsyncLLMEngine.from_engine_args(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 446, in from_engine_args\n[rank0]:     engine = cls(\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 374, in __init__\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/async_llm_engine.py\", line 528, in _init_engine\n[rank0]:     return engine_class(*args, **kwargs)\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 250, in __init__\n[rank0]:     self.model_executor = executor_class(\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 45, in __init__\n[rank0]:     self._init_executor()\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/gpu_executor.py\", line 34, in _init_executor\n[rank0]:     self.driver_worker.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 139, in load_model\n[rank0]:     self.model_runner.load_model()\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/model_runner.py\", line 553, in load_model\n[rank0]:     self.model = get_model(model_config=self.model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\n[rank0]:     return loader.load_model(model_config=model_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 275, in load_model\n[rank0]:     model = _initialize_model(model_config, self.load_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/model_loader/loader.py\", line 111, in _initialize_model\n[rank0]:     return model_class(config=model_config.hf_config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 382, in __init__\n[rank0]:     self.model = LlamaModel(config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 283, in __init__\n[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/utils.py\", line 144, in make_layers\n[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/utils.py\", line 145, in <listcomp>\n[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 285, in <lambda>\n[rank0]:     lambda prefix: LlamaDecoderLayer(config=config,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 201, in __init__\n[rank0]:     self.self_attn = LlamaAttention(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/llama.py\", line 149, in __init__\n[rank0]:     self.rotary_emb = get_rope(\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/rotary_embedding.py\", line 859, in get_rope\n[rank0]:     raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n[rank0]: ValueError: Unknown RoPE scaling type llama3\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-baseline:baseline-51f8aa90ad40' locally\nbaseline-51f8aa90ad40: Pulling from anonymous/vllm-baseline\n43cfb69dbb46: Pulling fs layer\nfbcd35dc5bc3: Pulling fs layer\nc7232af9ae05: Pulling fs layer\ndb6cdef1932a: Pulling fs layer\n56dc85502937: Pulling fs layer\n40b94ba5ab89: Pulling fs layer\n61fa7fc5e3d7: Pulling fs layer\nc18ca742cc7b: Pulling fs layer\n7c4c7b530b9c: Pulling fs layer\n4f4fb700ef54: Pulling fs layer\n517d5cf07ead: Pulling fs layer\n8e8f1a3dcc57: Pulling fs layer\ncc76f2b4edf6: Pulling fs layer\n2afb4bc38e0e: Pulling fs layer\n4864cd9af8e6: Pulling fs layer\n061231d3272b: Pulling fs layer\ndb6cdef1932a: Waiting\n56dc85502937: Waiting\n40b94ba5ab89: Waiting\n61fa7fc5e3d7: Waiting\nc18ca742cc7b: Waiting\n7c4c7b530b9c: Waiting\n4f4fb700ef54: Waiting\n517d5cf07ead: Waiting\n8e8f1a3dcc57: Waiting\ncc76f2b4edf6: Waiting\n2afb4bc38e0e: Waiting\n4864cd9af8e6: Waiting\n061231d3272b: Waiting\nfbcd35dc5bc3: Verifying Checksum\nfbcd35dc5bc3: Download complete\n43cfb69dbb46: Verifying Checksum\n43cfb69dbb46: Download complete\ndb6cdef1932a: Download complete\nc7232af9ae05: Verifying Checksum\nc7232af9ae05: Download complete\n56dc85502937: Download complete\n40b94ba5ab89: Download complete\n7c4c7b530b9c: Verifying Checksum\n7c4c7b530b9c: Download complete\n4f4fb700ef54: Download complete\n517d5cf07ead: Verifying Checksum\n517d5cf07ead: Download complete\nc18ca742cc7b: Verifying Checksum\nc18ca742cc7b: Download complete\n43cfb69dbb46: Pull complete\n61fa7fc5e3d7: Verifying Checksum\n61fa7fc5e3d7: Download complete\nfbcd35dc5bc3: Pull complete\nc7232af9ae05: Pull complete\ndb6cdef1932a: Pull complete\n56dc85502937: Pull complete\n40b94ba5ab89: Pull complete\ncc76f2b4edf6: Verifying Checksum\ncc76f2b4edf6: Download complete\n4864cd9af8e6: Verifying Checksum\n4864cd9af8e6: Download complete\n61fa7fc5e3d7: Pull complete\n2afb4bc38e0e: Verifying Checksum\n2afb4bc38e0e: Download complete\nc18ca742cc7b: Pull complete\n7c4c7b530b9c: Pull complete\n4f4fb700ef54: Pull complete\n517d5cf07ead: Pull complete\n8e8f1a3dcc57: Verifying Checksum\n8e8f1a3dcc57: Download complete\n061231d3272b: Download complete\n8e8f1a3dcc57: Pull complete\ncc76f2b4edf6: Pull complete\n2afb4bc38e0e: Pull complete\n4864cd9af8e6: Pull complete\n061231d3272b: Pull complete\nDigest: sha256:2f9371ec2d34341d3751c038e42f892b48160bba1d6c9b00efb5d731cd378032\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-51f8aa90ad40\n",
  "timestamp": "2026-01-21 15:20:33"
}