{
  "human_commit": "bc7c4d20",
  "human_commit_full": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
  "parent_commit": "f67e9e9f221e9791733b827585d6eb6dbc23133c",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 278.71200799942017,
  "metrics": {
    "ttft_mean_ms": 789.19,
    "ttft_median_ms": 754.45,
    "ttft_p99_ms": 1298.59,
    "tpot_mean_ms": 45.55,
    "tpot_median_ms": 34.81,
    "tpot_p99_ms": 414.34,
    "itl_mean_ms": 34.65,
    "itl_median_ms": 21.59,
    "itl_p99_ms": 397.28,
    "request_throughput_req_s": 32.96,
    "output_token_throughput_tok_s": 2041.24,
    "total_token_throughput_tok_s": 10477.73
  },
  "raw_output": "    127.0.0.1:55642 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55646 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55656 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55670 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55678 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55684 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55696 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55698 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:55714 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54790 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54794 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54810 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54818 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54826 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54834 - \"POST /v1/completions HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:54838 - \"POST /v1/completions HTTP/1.1\" 200 OK\n\n  1%|          | 1/100 [00:01<02:15,  1.36s/it]\n  2%|\u258f         | 2/100 [00:01<01:11,  1.37it/s]\n  4%|\u258d         | 4/100 [00:02<00:53,  1.80it/s]\n  5%|\u258c         | 5/100 [00:02<00:47,  2.01it/s]\n 14%|\u2588\u258d        | 14/100 [00:03<00:09,  9.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 32.96it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.03      \nTotal input tokens:                      25600     \nTotal generated tokens:                  6194      \nRequest throughput (req/s):              32.96     \nOutput token throughput (tok/s):         2041.24   \nTotal Token throughput (tok/s):          10477.73  \n---------------Time to First Token----------------\nMean TTFT (ms):                          789.19    \nMedian TTFT (ms):                        754.45    \nP99 TTFT (ms):                           1298.59   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          45.55     \nMedian TPOT (ms):                        34.81     \nP99 TPOT (ms):                           414.34    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           34.65     \nMedian ITL (ms):                         21.59     \nP99 ITL (ms):                            397.28    \n==================================================\nBENCHMARK_DONE\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=None, model='meta-llama/Llama-3.1-8B-Instruct', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=256, random_output_len=64, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)\nStarting initial single prompt test run...\nInitial test run completed. Starting main benchmark run...\nTraffic request rate: inf\nBurstiness factor: 1.0 (Poisson process)\nMaximum request concurrency: None\n\n  0%|          | 0/100 [00:00<?, ?it/s]\n  1%|          | 1/100 [00:01<02:15,  1.36s/it]\n  2%|\u258f         | 2/100 [00:01<01:11,  1.37it/s]\n  4%|\u258d         | 4/100 [00:02<00:53,  1.80it/s]\n  5%|\u258c         | 5/100 [00:02<00:47,  2.01it/s]\n 14%|\u2588\u258d        | 14/100 [00:03<00:09,  9.16it/s]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:03<00:00, 32.96it/s]\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  3.03      \nTotal input tokens:                      25600     \nTotal generated tokens:                  6194      \nRequest throughput (req/s):              32.96     \nOutput token throughput (tok/s):         2041.24   \nTotal Token throughput (tok/s):          10477.73  \n---------------Time to First Token----------------\nMean TTFT (ms):                          789.19    \nMedian TTFT (ms):                        754.45    \nP99 TTFT (ms):                           1298.59   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          45.55     \nMedian TPOT (ms):                        34.81     \nP99 TPOT (ms):                           414.34    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           34.65     \nMedian ITL (ms):                         21.59     \nP99 ITL (ms):                            397.28    \n==================================================\n",
  "timestamp": "2026-01-21 22:21:13"
}