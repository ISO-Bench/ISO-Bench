{
  "human_commit": "3b61cb45",
  "human_commit_full": "3b61cb450d899dc423feb264c297d4d18d701678",
  "parent_commit": "edc4fa31888b4a41060acb7b16250540f051ad59",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "success",
  "error": null,
  "duration_s": 217.60216069221497,
  "metrics": {
    "latency_avg_ms": 2481.586223166687,
    "throughput_tok_s": 7662.9
  },
  "raw_output": "0,  1.15it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.07it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.18it/s]\n\nINFO 01-21 00:00:02 model_runner.py:1094] Loading model weights took 14.9888 GB\nINFO 01-21 00:00:02 worker.py:237] Memory profiling results: duration=0.40 seconds, total_gpu_memory=79.19GiB, initial_memory_usage=15.53GiB, peak_torch_memory=16.20GiB, memory_usage_post_profile=15.63GiB, non_torch_memory=0.61GiB, kv_cache_size=54.46GiB, gpu_memory_utilization=0.90.\nINFO 01-21 00:00:02 gpu_executor.py:76] # GPU blocks: 27885, # CPU blocks: 2048\nINFO 01-21 00:00:02 gpu_executor.py:80] Maximum concurrency for 131072 tokens per request: 3.40x\nINFO 01-21 00:00:05 model_runner.py:1409] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\nINFO 01-21 00:00:05 model_runner.py:1413] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\nINFO 01-21 00:00:15 model_runner.py:1523] Graph capturing finished in 10 secs, took 0.32 GiB\nINFO 01-21 00:00:15 llm_engine.py:445] init engine (profile, create kv cache, warmup model) took 12.82 seconds\nSamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=True, max_tokens=128, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None)\nWarming up...\n\nWarmup iterations:   0%|          | 0/10 [00:00<?, ?it/s]\nWarmup iterations:  10%|\u2588         | 1/10 [00:02<00:21,  2.41s/it]\nWarmup iterations:  20%|\u2588\u2588        | 2/10 [00:04<00:19,  2.41s/it]INFO 01-21 00:00:20 metrics.py:460] Avg prompt throughput: 7662.9 tokens/s, Avg generation throughput: 1618.0 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 16 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.\n\nWarmup iterations:  30%|\u2588\u2588\u2588       | 3/10 [00:07<00:16,  2.42s/it]\nWarmup iterations:  40%|\u2588\u2588\u2588\u2588      | 4/10 [00:09<00:14,  2.42s/it]INFO 01-21 00:00:25 metrics.py:460] Avg prompt throughput: 7362.1 tokens/s, Avg generation throughput: 1644.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n\nWarmup iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 5/10 [00:12<00:12,  2.42s/it]\nWarmup iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 6/10 [00:14<00:09,  2.43s/it]INFO 01-21 00:00:30 metrics.py:460] Avg prompt throughput: 7309.6 tokens/s, Avg generation throughput: 1636.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nWarmup iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 7/10 [00:16<00:07,  2.43s/it]\nWarmup iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 8/10 [00:19<00:05,  2.54s/it]INFO 01-21 00:00:35 metrics.py:460] Avg prompt throughput: 5663.7 tokens/s, Avg generation throughput: 1606.0 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 8 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.\n\nWarmup iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 9/10 [00:22<00:02,  2.52s/it]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:24<00:00,  2.49s/it]\nWarmup iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:24<00:00,  2.47s/it]\n\nProfiling iterations:   0%|          | 0/30 [00:00<?, ?it/s]INFO 01-21 00:00:40 metrics.py:460] Avg prompt throughput: 7283.4 tokens/s, Avg generation throughput: 1630.3 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:   3%|\u258e         | 1/30 [00:02<01:10,  2.44s/it]\nProfiling iterations:   7%|\u258b         | 2/30 [00:04<01:08,  2.44s/it]INFO 01-21 00:00:45 metrics.py:460] Avg prompt throughput: 6969.2 tokens/s, Avg generation throughput: 1655.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  10%|\u2588         | 3/30 [00:07<01:05,  2.44s/it]\nProfiling iterations:  13%|\u2588\u258e        | 4/30 [00:09<01:03,  2.44s/it]INFO 01-21 00:00:50 metrics.py:460] Avg prompt throughput: 6543.6 tokens/s, Avg generation throughput: 1693.4 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  17%|\u2588\u258b        | 5/30 [00:12<01:01,  2.44s/it]\nProfiling iterations:  20%|\u2588\u2588        | 6/30 [00:14<00:58,  2.46s/it]INFO 01-21 00:00:55 metrics.py:460] Avg prompt throughput: 6552.1 tokens/s, Avg generation throughput: 1663.6 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  23%|\u2588\u2588\u258e       | 7/30 [00:17<00:57,  2.50s/it]\nProfiling iterations:  27%|\u2588\u2588\u258b       | 8/30 [00:19<00:55,  2.53s/it]INFO 01-21 00:01:00 metrics.py:460] Avg prompt throughput: 6538.3 tokens/s, Avg generation throughput: 1564.3 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  30%|\u2588\u2588\u2588       | 9/30 [00:22<00:53,  2.55s/it]\nProfiling iterations:  33%|\u2588\u2588\u2588\u258e      | 10/30 [00:25<00:51,  2.56s/it]INFO 01-21 00:01:05 metrics.py:460] Avg prompt throughput: 6080.5 tokens/s, Avg generation throughput: 1594.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  37%|\u2588\u2588\u2588\u258b      | 11/30 [00:27<00:48,  2.54s/it]\nProfiling iterations:  40%|\u2588\u2588\u2588\u2588      | 12/30 [00:30<00:45,  2.52s/it]INFO 01-21 00:01:10 metrics.py:460] Avg prompt throughput: 6871.5 tokens/s, Avg generation throughput: 1624.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  43%|\u2588\u2588\u2588\u2588\u258e     | 13/30 [00:32<00:42,  2.50s/it]\nProfiling iterations:  47%|\u2588\u2588\u2588\u2588\u258b     | 14/30 [00:34<00:39,  2.49s/it]INFO 01-21 00:01:15 metrics.py:460] Avg prompt throughput: 6565.7 tokens/s, Avg generation throughput: 1668.1 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  50%|\u2588\u2588\u2588\u2588\u2588     | 15/30 [00:37<00:37,  2.48s/it]\nProfiling iterations:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 16/30 [00:39<00:34,  2.48s/it]INFO 01-21 00:01:20 metrics.py:460] Avg prompt throughput: 6548.5 tokens/s, Avg generation throughput: 1681.9 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 17/30 [00:42<00:32,  2.47s/it]\nProfiling iterations:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 18/30 [00:44<00:29,  2.46s/it]INFO 01-21 00:01:25 metrics.py:460] Avg prompt throughput: 6551.4 tokens/s, Avg generation throughput: 1676.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 19/30 [00:47<00:27,  2.51s/it]\nProfiling iterations:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 20/30 [00:49<00:24,  2.49s/it]INFO 01-21 00:01:30 metrics.py:460] Avg prompt throughput: 6538.8 tokens/s, Avg generation throughput: 1609.2 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588   | 21/30 [00:52<00:22,  2.48s/it]\nProfiling iterations:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 22/30 [00:54<00:19,  2.48s/it]INFO 01-21 00:01:35 metrics.py:460] Avg prompt throughput: 6550.5 tokens/s, Avg generation throughput: 1676.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 23/30 [00:57<00:17,  2.47s/it]\nProfiling iterations:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 24/30 [00:59<00:14,  2.47s/it]INFO 01-21 00:01:40 metrics.py:460] Avg prompt throughput: 6536.3 tokens/s, Avg generation throughput: 1678.7 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 25/30 [01:02<00:12,  2.46s/it]\nProfiling iterations:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 26/30 [01:04<00:09,  2.46s/it]INFO 01-21 00:01:45 metrics.py:460] Avg prompt throughput: 6538.0 tokens/s, Avg generation throughput: 1672.8 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 27/30 [01:07<00:07,  2.46s/it]\nProfiling iterations:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e| 28/30 [01:09<00:04,  2.46s/it]INFO 01-21 00:01:50 metrics.py:460] Avg prompt throughput: 6547.7 tokens/s, Avg generation throughput: 1675.3 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.\n\nProfiling iterations:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 29/30 [01:11<00:02,  2.46s/it]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:14<00:00,  2.46s/it]\nProfiling iterations: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [01:14<00:00,  2.48s/it]\nAvg latency: 2.481586223166687 seconds\n10% percentile latency: 2.4434334706997562 seconds\n25% percentile latency: 2.4573513177502946 seconds\n50% percentile latency: 2.461691733999942 seconds\n75% percentile latency: 2.468662827250114 seconds\n90% percentile latency: 2.588686951600175 seconds\n99% percentile latency: 2.6104405884099924 seconds\n[rank0]:[W121 00:01:54.612594919 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
  "timestamp": "2026-01-21 08:02:03"
}