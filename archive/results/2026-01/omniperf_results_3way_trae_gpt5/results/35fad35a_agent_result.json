{
  "human_commit": "35fad35a",
  "human_commit_full": "35fad35a485eac9195c510731ba4a9d297dfd963",
  "parent_commit": "733e7c9e95f5b066ac420b00701eef7ea164a79e",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 33.05205202102661,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nvLLM uses mllama - checking transformers version...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file vllm/v1/sample/ops/topk_topp_sampler.py\nchecking file vllm/v1/sample/sampler.py\npatching file vllm/v1/sample/ops/topk_topp_sampler.py\npatching file vllm/v1/sample/sampler.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/transformers_utils/config.py\nSearching for Python with vLLM...\nINFO 01-14 20:29:11 [__init__.py:239] Automatically detected platform cuda.\nINFO 01-14 20:29:16 [__init__.py:239] Automatically detected platform cuda.\nWARNING: Could not find Python with vLLM, trying default python3\nINFO 01-14 20:29:21 [__init__.py:239] Automatically detected platform cuda.\nvLLM import failed\noutlines.fsm OK\n=== Starting vLLM server for AGENT benchmark ===\nINFO 01-14 20:29:30 [__init__.py:239] Automatically detected platform cuda.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 11, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 22, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 16, in <module>\n    from vllm.model_executor.layers.sampler import SamplerOutput\n  File \"/opt/vllm_baseline/vllm/model_executor/layers/sampler.py\", line 23, in <module>\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n  File \"/opt/vllm_baseline/vllm/spec_decode/metrics.py\", line 9, in <module>\n    from vllm.model_executor.layers.spec_decode_base_sampler import (\n  File \"/opt/vllm_baseline/vllm/model_executor/layers/spec_decode_base_sampler.py\", line 10, in <module>\n    from vllm.platforms import current_platform\n  File \"/opt/vllm_baseline/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/utils.py\", line 1905, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nImportError: /opt/vllm_baseline/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled\nSERVER_CRASHED\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 11, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 22, in <module>\n    from vllm.executor.executor_base import ExecutorBase\n  File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 16, in <module>\n    from vllm.model_executor.layers.sampler import SamplerOutput\n  File \"/opt/vllm_baseline/vllm/model_executor/layers/sampler.py\", line 23, in <module>\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n  File \"/opt/vllm_baseline/vllm/spec_decode/metrics.py\", line 9, in <module>\n    from vllm.model_executor.layers.spec_decode_base_sampler import (\n  File \"/opt/vllm_baseline/vllm/model_executor/layers/spec_decode_base_sampler.py\", line 10, in <module>\n    from vllm.platforms import current_platform\n  File \"/opt/vllm_baseline/vllm/platforms/__init__.py\", line 271, in __getattr__\n    _current_platform = resolve_obj_by_qualname(\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/utils.py\", line 1905, in resolve_obj_by_qualname\n    module = importlib.import_module(module_name)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/platforms/cuda.py\", line 15, in <module>\n    import vllm._C  # noqa\n    ^^^^^^^^^^^^^^\nImportError: /opt/vllm_baseline/vllm/_C.abi3.so: undefined symbol: cuTensorMapEncodeTiled\n",
  "timestamp": "2026-01-15 04:29:32"
}