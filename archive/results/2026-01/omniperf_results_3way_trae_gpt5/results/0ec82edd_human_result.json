{
  "human_commit": "0ec82edd",
  "human_commit_full": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
  "parent_commit": "005ae9be6c22dfa2c2c5580b50b41e67faee4a87",
  "model": "Qwen/Qwen3-30B-A3B",
  "status": "error",
  "error": "No metrics in output",
  "duration_s": 191.41403770446777,
  "metrics": {},
  "raw_output": "t/s]\n\nLoading safetensors checkpoint shards:  81% Completed | 13/16 [00:08<00:02,  1.43it/s]\n\nLoading safetensors checkpoint shards:  88% Completed | 14/16 [00:09<00:01,  1.44it/s]\n\nLoading safetensors checkpoint shards:  94% Completed | 15/16 [00:10<00:00,  1.44it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 16/16 [00:10<00:00,  1.46it/s]\n\nLoading safetensors checkpoint shards: 100% Completed | 16/16 [00:10<00:00,  1.49it/s]\n\nINFO 01-14 05:46:32 [default_loader.py:262] Loading weights took 10.84 seconds\nINFO 01-14 05:46:32 [model_runner.py:1207] Model loading took 56.8814 GiB and 89.355314 seconds\nWARNING 01-14 05:46:33 [fused_moe.py:695] Using default MoE config. Performance might be sub-optimal! Config file not found at /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_H100_PCIe.json\nINFO 01-14 05:46:34 [worker.py:296] Memory profiling takes 1.92 seconds\nINFO 01-14 05:46:34 [worker.py:296] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\nINFO 01-14 05:46:34 [worker.py:296] model weights take 56.88GiB; non_torch_memory takes 0.14GiB; PyTorch activation peak memory takes 1.42GiB; the rest of the memory reserved for KV Cache is 12.75GiB.\nINFO 01-14 05:46:35 [executor_base.py:115] # cuda blocks: 8702, # CPU blocks: 2730\nINFO 01-14 05:46:35 [executor_base.py:120] Maximum concurrency for 4096 tokens per request: 33.99x\nINFO 01-14 05:46:39 [model_runner.py:1518] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n\nCapturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\nCapturing CUDA graph shapes:   3%|\u258e         | 1/35 [00:00<00:19,  1.72it/s]\nCapturing CUDA graph shapes:   6%|\u258c         | 2/35 [00:01<00:17,  1.88it/s]\nCapturing CUDA graph shapes:   9%|\u258a         | 3/35 [00:01<00:16,  1.92it/s]\nCapturing CUDA graph shapes:  11%|\u2588\u258f        | 4/35 [00:02<00:16,  1.90it/s]\nCapturing CUDA graph shapes:  14%|\u2588\u258d        | 5/35 [00:02<00:15,  1.89it/s]\nCapturing CUDA graph shapes:  17%|\u2588\u258b        | 6/35 [00:03<00:15,  1.93it/s]\nCapturing CUDA graph shapes:  20%|\u2588\u2588        | 7/35 [00:03<00:14,  1.92it/s]\nCapturing CUDA graph shapes:  23%|\u2588\u2588\u258e       | 8/35 [00:04<00:13,  1.93it/s]\nCapturing CUDA graph shapes:  26%|\u2588\u2588\u258c       | 9/35 [00:04<00:13,  1.93it/s]\nCapturing CUDA graph shapes:  29%|\u2588\u2588\u258a       | 10/35 [00:05<00:12,  1.96it/s]\nCapturing CUDA graph shapes:  31%|\u2588\u2588\u2588\u258f      | 11/35 [00:05<00:12,  1.94it/s]\nCapturing CUDA graph shapes:  34%|\u2588\u2588\u2588\u258d      | 12/35 [00:06<00:11,  1.96it/s]\nCapturing CUDA graph shapes:  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:06<00:11,  1.98it/s]\nCapturing CUDA graph shapes:  40%|\u2588\u2588\u2588\u2588      | 14/35 [00:07<00:10,  1.99it/s]\nCapturing CUDA graph shapes:  43%|\u2588\u2588\u2588\u2588\u258e     | 15/35 [00:07<00:10,  2.00it/s]\nCapturing CUDA graph shapes:  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:08<00:09,  1.98it/s]\nCapturing CUDA graph shapes:  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:09<00:10,  1.64it/s]\nCapturing CUDA graph shapes:  51%|\u2588\u2588\u2588\u2588\u2588\u258f    | 18/35 [00:09<00:09,  1.74it/s]\nCapturing CUDA graph shapes:  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:10<00:08,  1.82it/s]\nCapturing CUDA graph shapes:  57%|\u2588\u2588\u2588\u2588\u2588\u258b    | 20/35 [00:10<00:07,  1.88it/s]\nCapturing CUDA graph shapes:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:11<00:07,  1.92it/s]\nCapturing CUDA graph shapes:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 22/35 [00:11<00:06,  1.87it/s]\nCapturing CUDA graph shapes:  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:12<00:06,  1.87it/s]\nCapturing CUDA graph shapes:  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 24/35 [00:12<00:05,  1.91it/s]\nCapturing CUDA graph shapes:  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:13<00:05,  1.95it/s]\nCapturing CUDA graph shapes:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 26/35 [00:13<00:04,  1.98it/s]\nCapturing CUDA graph shapes:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:14<00:04,  1.97it/s]\nCapturing CUDA graph shapes:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 28/35 [00:14<00:03,  2.00it/s]\nCapturing CUDA graph shapes:  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:02,  2.02it/s]\nCapturing CUDA graph shapes:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 30/35 [00:15<00:02,  2.03it/s]\nCapturing CUDA graph shapes:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:16<00:01,  2.03it/s]\nCapturing CUDA graph shapes:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 32/35 [00:16<00:01,  2.00it/s]\nCapturing CUDA graph shapes:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:17<00:00,  2.02it/s]\nCapturing CUDA graph shapes:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 34/35 [00:17<00:00,  2.04it/s]\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:18<00:00,  1.68it/s]\nCapturing CUDA graph shapes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:18<00:00,  1.90it/s]\nINFO 01-14 05:46:57 [model_runner.py:1677] Graph capturing finished in 18 secs, took 0.65 GiB\nINFO 01-14 05:46:57 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 25.11 seconds\nWARNING 01-14 05:46:58 [config.py:1517] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\nINFO 01-14 05:46:58 [serving_responses.py:89] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\nINFO 01-14 05:46:58 [serving_chat.py:122] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\nINFO 01-14 05:46:58 [serving_completion.py:77] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}\nINFO 01-14 05:46:58 [api_server.py:1819] Starting vLLM API server 0 on http://0.0.0.0:8000\nINFO 01-14 05:46:58 [launcher.py:29] Available routes are:\nINFO 01-14 05:46:58 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /docs, Methods: HEAD, GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /redoc, Methods: HEAD, GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /health, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /load, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /ping, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /ping, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /tokenize, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /detokenize, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/models, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /version, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/responses, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/responses/{response_id}, Methods: GET\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/responses/{response_id}/cancel, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/completions, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/embeddings, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /pooling, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /classify, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /score, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/score, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /rerank, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v1/rerank, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /v2/rerank, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /invocations, Methods: POST\nINFO 01-14 05:46:58 [launcher.py:37] Route: /metrics, Methods: GET\nINFO:     Started server process [1143]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     127.0.0.1:36754 - \"GET /v1/models HTTP/1.1\" 200 OK\nSERVER_READY_AFTER=136s\nINFO:     127.0.0.1:36764 - \"GET /v1/models HTTP/1.1\" 200 OK\n=== Running HUMAN benchmark ===\nRunning benchmark_serving.py for serving metrics...\nDEPRECATED: This script has been moved to the vLLM CLI.\n\nPlease use the following command instead:\n    vllm bench serve\n\nFor help with the new command, run:\n    vllm bench serve --help\n\nAlternatively, you can run the new command directly with:\n    python -m vllm.entrypoints.cli.main bench serve --help\n\n============ Serving Benchmark Result ============\n==================================================\nBENCHMARK_DONE\nUnable to find image 'anonymous/vllm-bench:0ec82edda59aaf5cf3b07aadf4ecce1aa1131add' locally\n0ec82edda59aaf5cf3b07aadf4ecce1aa1131add: Pulling from anonymous/vllm-bench\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\ndd8de35d23de: Already exists\n692d4ce61fb0: Already exists\n2f2f4d7c8e07: Already exists\n58c2cae18147: Already exists\n3679cb576252: Already exists\n41deed2d6a0a: Already exists\nb613bd2e6281: Already exists\n89cc636f556d: Already exists\n1b47dceffbdf: Already exists\n34cf01fc4cf1: Already exists\nDigest: sha256:e2a131a8c4825be0657dcacbffa25cab3e8113d5a79bd6a1585e45b8ea83b025\nStatus: Downloaded newer image for anonymous/vllm-bench:0ec82edda59aaf5cf3b07aadf4ecce1aa1131add\n",
  "timestamp": "2026-01-14 13:47:06"
}