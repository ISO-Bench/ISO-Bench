{
  "human_commit": "8aa1485f",
  "human_commit_full": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
  "parent_commit": "89ac266b262f08d25ebf25fc66122d1b2367ae64",
  "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
  "status": "error",
  "error": "Server crashed during startup",
  "duration_s": 160.30165457725525,
  "metrics": {},
  "raw_output": "model\n    self.model_runner.load_model()\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1086, in load_model\n    self.model = get_model(vllm_config=self.vllm_config)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 118, in get_model\n    return loader.load_model(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 44, in load_model\n    model = initialize_model(vllm_config=vllm_config,\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 63, in initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama4.py\", line 750, in __init__\n    self.language_model = initialize_model(\n                          ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 63, in initialize_model\n    return model_class(vllm_config=vllm_config, prefix=prefix)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 532, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 524, in __init__\n    self.model = self._init_model(vllm_config=vllm_config,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 540, in _init_model\n    return Llama4Model(vllm_config=vllm_config,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 183, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 332, in __init__\n    super().__init__(vllm_config=vllm_config,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 183, in __init__\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 346, in __init__\n    self.start_layer, self.end_layer, self.layers = make_layers(\n                                                    ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 640, in make_layers\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 348, in <lambda>\n    lambda prefix: layer_type(config=config,\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 264, in __init__\n    self.self_attn = Llama4Attention(\n                     ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama4.py\", line 188, in __init__\n    self.rotary_emb = get_rope(\n                      ^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 1863, in get_rope\n    rotary_emb = Llama3RotaryEmbedding(head_size, rotary_dim,\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 872, in __init__\n    super().__init__(head_size, rotary_dim, max_position_embeddings, base,\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 114, in __init__\n    cache = self._compute_cos_sin_cache()\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/rotary_embedding.py\", line 134, in _compute_cos_sin_cache\n    freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 410, in einsum\n    return handle_torch_function(einsum, operands, equation, *operands)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/overrides.py\", line 1721, in handle_torch_function\n    result = mode.__torch_function__(public_api, types, args, kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\", line 104, in __torch_function__\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/functional.py\", line 422, in einsum\n    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 79.11 GiB of which 1.65 GiB is free. Process 123930 has 73.14 GiB memory in use. Process 127552 has 4.31 GiB memory in use. Of the allocated memory 3.76 GiB is allocated by PyTorch, and 38.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W114 06:21:04.470122332 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1877, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1809, in run_server\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 1829, in run_server_worker\n    async with build_async_engine_client(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 165, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 302, in build_async_engine_client_from_engine_args\n    raise RuntimeError(\nRuntimeError: Engine process failed to start. See stack trace for the root cause.\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-bench:8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8' locally\n8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8: Pulling from anonymous/vllm-bench\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\n95409a464347: Pulling fs layer\n6d79a20b044f: Pulling fs layer\n6d558d032a5b: Pulling fs layer\n54d92e561396: Pulling fs layer\n2e4f14a10651: Pulling fs layer\nbc8f0216f8db: Pulling fs layer\n952a7038bb63: Pulling fs layer\na30b6a699649: Pulling fs layer\na79234dac43a: Pulling fs layer\n3eee07124181: Pulling fs layer\n3eee07124181: Download complete\n2e4f14a10651: Download complete\n6d79a20b044f: Download complete\n6d558d032a5b: Download complete\n54d92e561396: Download complete\n952a7038bb63: Download complete\nbc8f0216f8db: Download complete\na30b6a699649: Download complete\na79234dac43a: Download complete\n95409a464347: Verifying Checksum\n95409a464347: Download complete\n95409a464347: Pull complete\n6d79a20b044f: Pull complete\n6d558d032a5b: Pull complete\n54d92e561396: Pull complete\n2e4f14a10651: Pull complete\nbc8f0216f8db: Pull complete\n952a7038bb63: Pull complete\na30b6a699649: Pull complete\na79234dac43a: Pull complete\n3eee07124181: Pull complete\nDigest: sha256:0e55e8f5a17042dba43c657cc0fa8c64236fc516f81a7c6686e36b99dd7e05cc\nStatus: Downloaded newer image for anonymous/vllm-bench:8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8\n",
  "timestamp": "2026-01-14 14:21:09"
}