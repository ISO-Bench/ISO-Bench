{
  "human_commit": "61b8cea3",
  "human_commit_full": "61b8cea3b42feab021d506e9143551de18f9165c",
  "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b",
  "model": "meta-llama/Llama-3.2-3B-Instruct",
  "status": "error",
  "error": "No throughput metrics in agent output",
  "duration_s": 280.0064721107483,
  "metrics": {},
  "raw_output": "ts\":false,\"full_cuda_graph\":false,\"max_capture_size\":null,\"local_cache_dir\":null}, additional_config={}, disable_log_stats=False, enable_prompt_adapter=False, disable_log_requests=False)\nINFO 01-14 20:36:51 [config.py:1605] Using max model len 131072\nWARNING 01-14 20:36:51 [arg_utils.py:1484] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.\nINFO 01-14 20:36:52 [config.py:2416] Chunked prefill is enabled with max_num_batched_tokens=2048.\nINFO 01-14 20:36:52 [llm_engine.py:228] Initializing a V0 LLM engine (v0.1.dev1+g526078a96.d20260110) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=False, \nINFO 01-14 20:36:53 [cuda.py:398] Using Flash Attention backend.\nINFO 01-14 20:36:54 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 01-14 20:36:54 [model_runner.py:1083] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\nINFO 01-14 20:36:54 [weight_utils.py:296] Using model weights format ['*.safetensors']\n\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.33s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  3.94s/it]\n\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.30s/it]\n\nINFO 01-14 20:37:03 [default_loader.py:262] Loading weights took 8.67 seconds\nINFO 01-14 20:37:04 [model_runner.py:1115] Model loading took 6.0160 GiB and 9.358769 seconds\nINFO 01-14 20:37:05 [worker.py:295] Memory profiling takes 0.64 seconds\nINFO 01-14 20:37:05 [worker.py:295] the current vLLM instance can use total_gpu_memory (79.11GiB) x gpu_memory_utilization (0.90) = 71.20GiB\nINFO 01-14 20:37:05 [worker.py:295] model weights take 6.02GiB; non_torch_memory takes 69.43GiB; PyTorch activation peak memory takes 1.21GiB; the rest of the memory reserved for KV Cache is -5.46GiB.\nINFO 01-14 20:37:06 [executor_base.py:113] # cuda blocks: 0, # CPU blocks: 2340\nINFO 01-14 20:37:06 [executor_base.py:118] Maximum concurrency for 131072 tokens per request: 0.00x\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 736, in <module>\n[rank0]:     main(args)\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 408, in main\n[rank0]:     elapsed_time, request_outputs = run_vllm(\n[rank0]:                                     ^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/benchmarks/benchmark_throughput.py\", line 50, in run_vllm\n[rank0]:     llm = LLM(**dataclasses.asdict(engine_args))\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 273, in __init__\n[rank0]:     self.llm_engine = LLMEngine.from_engine_args(\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 497, in from_engine_args\n[rank0]:     return engine_cls.from_vllm_config(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 473, in from_vllm_config\n[rank0]:     return cls(\n[rank0]:            ^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 266, in __init__\n[rank0]:     self._initialize_kv_caches()\n[rank0]:   File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 422, in _initialize_kv_caches\n[rank0]:     self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/executor_base.py\", line 124, in initialize_cache\n[rank0]:     self.collective_rpc(\"initialize_cache\",\n[rank0]:   File \"/opt/vllm_baseline/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\n[rank0]:     answer = run_method(self.driver_worker, method, args, kwargs)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/utils/__init__.py\", line 2986, in run_method\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 319, in initialize_cache\n[rank0]:     raise_if_cache_size_invalid(\n[rank0]:   File \"/opt/vllm_baseline/vllm/worker/worker.py\", line 577, in raise_if_cache_size_invalid\n[rank0]:     raise ValueError(\"No available memory for the cache blocks. \"\n[rank0]: ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n[rank0]:[W114 20:37:06.921056713 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nUnable to find image 'anonymous/vllm-baseline:baseline-526078a96c52' locally\nbaseline-526078a96c52: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\n0b45fc65175e: Pulling fs layer\n21741161128c: Pulling fs layer\n9da69fd00e97: Pulling fs layer\nfff70fd02e66: Pulling fs layer\n6a6ca5ae8853: Pulling fs layer\na5da2d4e7468: Pulling fs layer\n96f2ec5191d2: Pulling fs layer\nbcb7d137089c: Pulling fs layer\n8497eafc3278: Pulling fs layer\n65a47be34c52: Pulling fs layer\nfff70fd02e66: Download complete\na0555c6d1173: Pulling fs layer\nc0096c57533f: Pulling fs layer\n96f2ec5191d2: Waiting\n8f7811bd0912: Pulling fs layer\n6a6ca5ae8853: Waiting\na5da2d4e7468: Waiting\n4b0b2601cf00: Pulling fs layer\n65a47be34c52: Waiting\nbcb7d137089c: Waiting\n203b2b7dd2ec: Pulling fs layer\n8497eafc3278: Waiting\n874294342ba1: Pulling fs layer\ne45c00c4d2c5: Pulling fs layer\n203b2b7dd2ec: Waiting\n6e784e87eb2c: Pulling fs layer\n4b0b2601cf00: Waiting\n991a4d5b08cc: Pulling fs layer\na0555c6d1173: Waiting\n8f7811bd0912: Waiting\n9da69fd00e97: Download complete\nc0096c57533f: Waiting\n991a4d5b08cc: Waiting\ne45c00c4d2c5: Waiting\n6e784e87eb2c: Waiting\n874294342ba1: Waiting\n6a6ca5ae8853: Verifying Checksum\n6a6ca5ae8853: Download complete\na5da2d4e7468: Verifying Checksum\na5da2d4e7468: Download complete\n96f2ec5191d2: Download complete\nbcb7d137089c: Verifying Checksum\nbcb7d137089c: Download complete\n8497eafc3278: Verifying Checksum\n8497eafc3278: Download complete\n65a47be34c52: Verifying Checksum\n65a47be34c52: Download complete\n21741161128c: Verifying Checksum\n21741161128c: Download complete\nc0096c57533f: Verifying Checksum\nc0096c57533f: Download complete\n8f7811bd0912: Verifying Checksum\n8f7811bd0912: Download complete\n4b0b2601cf00: Download complete\n203b2b7dd2ec: Verifying Checksum\n203b2b7dd2ec: Download complete\n0b45fc65175e: Retrying in 5 seconds\n0b45fc65175e: Retrying in 4 seconds\n0b45fc65175e: Retrying in 3 seconds\n0b45fc65175e: Retrying in 2 seconds\n0b45fc65175e: Retrying in 1 second\na0555c6d1173: Verifying Checksum\na0555c6d1173: Download complete\ne45c00c4d2c5: Download complete\n6e784e87eb2c: Verifying Checksum\n6e784e87eb2c: Download complete\n991a4d5b08cc: Verifying Checksum\n991a4d5b08cc: Download complete\n874294342ba1: Verifying Checksum\n874294342ba1: Download complete\n0b45fc65175e: Download complete\n0b45fc65175e: Pull complete\n21741161128c: Pull complete\n9da69fd00e97: Pull complete\nfff70fd02e66: Pull complete\n6a6ca5ae8853: Pull complete\na5da2d4e7468: Pull complete\n96f2ec5191d2: Pull complete\nbcb7d137089c: Pull complete\n8497eafc3278: Pull complete\n65a47be34c52: Pull complete\na0555c6d1173: Pull complete\nc0096c57533f: Pull complete\n8f7811bd0912: Pull complete\n4b0b2601cf00: Pull complete\n203b2b7dd2ec: Pull complete\n874294342ba1: Pull complete\ne45c00c4d2c5: Pull complete\n6e784e87eb2c: Pull complete\n991a4d5b08cc: Pull complete\nDigest: sha256:b578b87a18847931751d56fc86204b79046a536bbc6987ef54b1e0b13d9b61a9\nStatus: Image is up to date for anonymous/vllm-baseline:baseline-526078a96c52\n",
  "timestamp": "2026-01-15 04:37:08"
}