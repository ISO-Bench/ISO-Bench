{
  "human_commit": "83450458",
  "human_commit_full": "83450458339b07765b0e72a822e5fe93eeaf5258",
  "parent_commit": "5b8a1fde84224e24ec121e0dc149d775330d911b",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "No latency metrics in agent output",
  "duration_s": 22.311821699142456,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nApplying patch...\nchecking file vllm/spec_decode/ngram_worker.py\npatching file vllm/spec_decode/ngram_worker.py\nAGENT_PATCH_APPLIED\nFound vLLM at: /usr/bin/python3\n=== Running AGENT latency benchmark (offline) ===\nOriginal perf command: python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\nUsing baseline benchmark_latency.py: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\nFinal command: /usr/bin/python3 /opt/vllm_baseline/benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150\n`torch_dtype` is deprecated! Use `dtype` instead!\nNamespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model=\"'[ngram]'\", num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-14 07:47:05 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 1262, in maybe_create_spec_config\n    draft_model_config = ModelConfig(\n                         ^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 162, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 148, in get_config\n    if is_gguf or file_or_path_exists(model,\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 76, in file_or_path_exists\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n    validate_repo_id(arg_value)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n    raise HFValidationError(\nhuggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''[ngram]''.\nBENCHMARK_DONE\n`torch_dtype` is deprecated! Use `dtype` instead!\nNamespace(model='meta-llama/Llama-3.1-8B-Instruct', speculative_model=\"'[ngram]'\", num_speculative_tokens=5, speculative_draft_tensor_parallel_size=None, tokenizer=None, quantization=None, tensor_parallel_size=1, input_len=550, output_len=150, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, trust_remote_code=False, max_model_len=None, dtype='auto', enforce_eager=False, kv_cache_dtype='auto', quantization_param_path=None, profile=False, profile_result_dir=None, device='auto', block_size=16, enable_chunked_prefill=False, enable_prefix_caching=False, use_v2_block_manager=True, ray_workers_use_nsight=False, download_dir=None, output_json=None, gpu_memory_utilization=0.9, load_format='auto', distributed_executor_backend=None, otlp_traces_endpoint=None)\nWARNING 01-14 07:47:05 arg_utils.py:964] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 284, in <module>\n    main(args)\n  File \"/opt/vllm_baseline/benchmarks/benchmark_latency.py\", line 24, in main\n    llm = LLM(\n          ^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/llm.py\", line 177, in __init__\n    self.llm_engine = LLMEngine.from_engine_args(\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/llm_engine.py\", line 571, in from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 976, in create_engine_config\n    speculative_config = SpeculativeConfig.maybe_create_spec_config(\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 1262, in maybe_create_spec_config\n    draft_model_config = ModelConfig(\n                         ^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/config.py\", line 162, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 148, in get_config\n    if is_gguf or file_or_path_exists(model,\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 76, in file_or_path_exists\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n    validate_repo_id(arg_value)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n    raise HFValidationError(\nhuggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''[ngram]''.\n",
  "timestamp": "2026-01-14 15:47:06"
}