{
  "human_commit": "b690e348",
  "human_commit_full": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
  "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157",
  "model": "ibm-ai-platform/Bamba-9B-v2",
  "status": "error",
  "error": "No metrics in agent output",
  "duration_s": 399.8413152694702,
  "metrics": {},
  "raw_output": "lm_engine.py:421] init engine (profile, create kv cache, warmup model) took 57.26 seconds\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:29 [api_server.py:1597] Supported_tasks: ['generate']\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [api_server.py:1846] Starting vLLM API server 0 on http://0.0.0.0:8000\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:29] Available routes are:\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /docs, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /redoc, Methods: HEAD, GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /health, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /load, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /ping, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /ping, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /tokenize, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /detokenize, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/models, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /version, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/responses, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/responses/{response_id}, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/responses/{response_id}/cancel, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/completions, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /pooling, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /classify, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /score, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/score, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /rerank, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v1/rerank, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /v2/rerank, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /scale_elastic_ep, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /is_scaling_elastic_ep, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /invocations, Methods: POST\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO 01-14 08:43:30 [launcher.py:37] Route: /metrics, Methods: GET\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO:     Started server process [996]\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO:     Waiting for application startup.\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO:     Application startup complete.\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO:     127.0.0.1:33482 - \"GET /v1/models HTTP/1.1\" 200 OK\nSERVER_READY_AFTER=119s\n\u001b[1;36m(APIServer pid=996)\u001b[0;0m INFO:     127.0.0.1:33496 - \"GET /v1/models HTTP/1.1\" 200 OK\n=== Running AGENT benchmark ===\nRunning benchmark_serving.py for serving metrics...\nINFO 01-14 08:43:36 [__init__.py:241] Automatically detected platform cuda.\n/opt/vllm_baseline/benchmarks/benchmark_serving.py:1298: DeprecationWarning: benchmark_serving.py is deprecated and will be removed in a future version. Please use 'vllm bench serve' instead.\n  main(args)\nNamespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='sonnet', dataset_path='/opt/vllm_baseline/benchmarks/sonnet.txt', no_stream=False, max_concurrency=None, model='ibm-ai-platform/Bamba-9B-v2', tokenizer=None, use_beam_search=False, num_prompts=100, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=256, sonnet_output_len=64, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/benchmarks/benchmark_serving.py\", line 1298, in <module>\n    main(args)\n  File \"/usr/local/lib/python3.12/dist-packages/typing_extensions.py\", line 3004, in wrapper\n    return arg(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/benchmarks/benchmark_serving.py\", line 676, in main\n    assert tokenizer.chat_template or tokenizer.default_chat_template, (\n                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py\", line 1099, in __getattr__\n    raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\nAttributeError: CachedPreTrainedTokenizerFast has no attribute default_chat_template. Did you mean: 'get_chat_template'?\n============ Serving Benchmark Result ============\n==================================================\nBENCHMARK_DONE\nUnable to find image 'anonymous/vllm-baseline:baseline-25373b6c6cc2' locally\nbaseline-25373b6c6cc2: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\n06ba41083b8d: Pulling fs layer\n3a13c90b0c49: Pulling fs layer\n63db0879e352: Pulling fs layer\nce4b1c2c1a4b: Pulling fs layer\n5605d3eadacd: Pulling fs layer\n969f07e447c8: Pulling fs layer\nce4b1c2c1a4b: Waiting\nd1013c4b4f44: Pulling fs layer\n5605d3eadacd: Waiting\nd76f357a7fd3: Pulling fs layer\n969f07e447c8: Waiting\n371b8738953d: Pulling fs layer\nd1013c4b4f44: Waiting\nd76f357a7fd3: Waiting\n3cf45a90fadf: Pulling fs layer\n371b8738953d: Waiting\n9af66e82fea9: Pulling fs layer\n3cf45a90fadf: Waiting\ncb14bfd96623: Pulling fs layer\n9af66e82fea9: Waiting\n386ac1592b6a: Pulling fs layer\ncb14bfd96623: Waiting\n963fda8b9aa0: Pulling fs layer\n386ac1592b6a: Waiting\n27b005b3f2b3: Pulling fs layer\n963fda8b9aa0: Waiting\nd8c7cc086f6f: Pulling fs layer\n27b005b3f2b3: Waiting\n945afabce0bd: Pulling fs layer\nd8c7cc086f6f: Waiting\n310e3f2c7a2a: Pulling fs layer\n49124192b7fe: Pulling fs layer\n945afabce0bd: Waiting\n310e3f2c7a2a: Waiting\n63db0879e352: Verifying Checksum\n63db0879e352: Download complete\n3a13c90b0c49: Verifying Checksum\n3a13c90b0c49: Download complete\nce4b1c2c1a4b: Verifying Checksum\nce4b1c2c1a4b: Download complete\n969f07e447c8: Verifying Checksum\n969f07e447c8: Download complete\nd1013c4b4f44: Verifying Checksum\nd1013c4b4f44: Download complete\n5605d3eadacd: Verifying Checksum\n5605d3eadacd: Download complete\nd76f357a7fd3: Verifying Checksum\nd76f357a7fd3: Download complete\n371b8738953d: Verifying Checksum\n371b8738953d: Download complete\n3cf45a90fadf: Verifying Checksum\n3cf45a90fadf: Download complete\n9af66e82fea9: Verifying Checksum\n9af66e82fea9: Download complete\n386ac1592b6a: Verifying Checksum\n386ac1592b6a: Download complete\n963fda8b9aa0: Verifying Checksum\n963fda8b9aa0: Download complete\n27b005b3f2b3: Verifying Checksum\n27b005b3f2b3: Download complete\nd8c7cc086f6f: Verifying Checksum\nd8c7cc086f6f: Download complete\n945afabce0bd: Verifying Checksum\n945afabce0bd: Download complete\n310e3f2c7a2a: Download complete\n49124192b7fe: Verifying Checksum\n49124192b7fe: Download complete\ncb14bfd96623: Verifying Checksum\ncb14bfd96623: Download complete\n06ba41083b8d: Download complete\n06ba41083b8d: Pull complete\n3a13c90b0c49: Pull complete\n63db0879e352: Pull complete\nce4b1c2c1a4b: Pull complete\n5605d3eadacd: Pull complete\n969f07e447c8: Pull complete\nd1013c4b4f44: Pull complete\nd76f357a7fd3: Pull complete\n371b8738953d: Pull complete\n3cf45a90fadf: Pull complete\n9af66e82fea9: Pull complete\ncb14bfd96623: Pull complete\n386ac1592b6a: Pull complete\n963fda8b9aa0: Pull complete\n27b005b3f2b3: Pull complete\nd8c7cc086f6f: Pull complete\n945afabce0bd: Pull complete\n310e3f2c7a2a: Pull complete\n49124192b7fe: Pull complete\nDigest: sha256:208e2784bc3b289cb2d9f3fc7b61b9c271d0fbc0528d0584ac6027b98921f3c8\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-25373b6c6cc2\n",
  "timestamp": "2026-01-14 16:43:42"
}