{
  "human_commit": "0ec82edd",
  "human_commit_full": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
  "parent_commit": "005ae9be6c22dfa2c2c5580b50b41e67faee4a87",
  "model": "Qwen/Qwen3-30B-A3B",
  "status": "error",
  "error": "No throughput metrics in agent output",
  "duration_s": 209.95857977867126,
  "metrics": {},
  "raw_output": "rch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/qwen3_moe.py\", line 313, in forward\n[rank0]:     hidden_states = self.mlp(hidden_states)\n[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/models/qwen3_moe.py\", line 136, in forward\n[rank0]:     final_hidden_states = self.experts(hidden_states=hidden_states,\n[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n[rank0]:     return self._call_impl(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n[rank0]:     return forward_call(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/layer.py\", line 1388, in forward\n[rank0]:     return torch.ops.vllm.moe_forward(hidden_states, router_logits,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/layer.py\", line 1579, in moe_forward\n[rank0]:     return self.forward_impl(hidden_states, router_logits)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/layer.py\", line 1489, in forward_impl\n[rank0]:     final_hidden_states = self.quant_method.apply(\n[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/layer.py\", line 380, in apply\n[rank0]:     return self.forward(\n[rank0]:            ^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/custom_op.py\", line 44, in forward\n[rank0]:     return self._forward_method(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/layer.py\", line 454, in forward_cuda\n[rank0]:     return self.fused_experts(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1322, in fused_experts\n[rank0]:     return dispatch_fused_experts_func(inplace)(\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1238, in torch_vllm_inplace_fused_experts\n[rank0]:     torch.ops.vllm.inplace_fused_experts(**kwargs)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 1158, in __call__\n[rank0]:     return self._op(*args, **(kwargs or {}))\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1028, in inplace_fused_experts\n[rank0]:     fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1492, in fused_experts_impl\n[rank0]:     invoke_fused_moe_kernel(qcurr_hidden_states,\n[rank0]:   File \"/opt/vllm_baseline/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 611, in invoke_fused_moe_kernel\n[rank0]:     fused_moe_kernel[grid](\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 347, in <lambda>\n[rank0]:     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n[rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py\", line 569, in run\n[rank0]:     kernel = self.compile(src, target=target, options=options.__dict__)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 278, in compile\n[rank0]:     module = src.make_ir(options, codegen_fns, module_map, context)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py\", line 81, in make_ir\n[rank0]:     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: triton.compiler.errors.CompilationError: at 212:24:\n[rank0]:         accumulator = accumulator.to(compute_type)\n[rank0]:     # -----------------------------------------------------------\n[rank0]:     # Write back the block of the output\n[rank0]:     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n[rank0]:     c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n[rank0]:         None, :]\n[rank0]:     c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n[rank0]:     tl.store(c_ptrs, accumulator, mask=c_mask)\n\n[rank0]:     # Fill padded positions for this expert with sentinel (numel)\n[rank0]:     # Total tokens for this expert after counting is in the last row\n[rank0]:     total_cnt = tl.load(tokens_cnts_ptr + num_experts * num_experts + pid)\n[rank0]:                         ^\n[rank0]: NameError('tokens_cnts_ptr is not defined')\n[rank0]:[W114 11:47:19.971246518 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nUnable to find image 'anonymous/vllm-baseline:baseline-005ae9be6c22' locally\nbaseline-005ae9be6c22: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\n6a2306edc128: Already exists\nbb5ee2f41954: Already exists\nae1cc335b65b: Already exists\n2fb01f5ad376: Already exists\nb7dfd152a1fd: Already exists\n2987a32afa11: Already exists\ndd8de35d23de: Pulling fs layer\n692d4ce61fb0: Pulling fs layer\n2f2f4d7c8e07: Pulling fs layer\n58c2cae18147: Pulling fs layer\n3679cb576252: Pulling fs layer\n41deed2d6a0a: Pulling fs layer\nb613bd2e6281: Pulling fs layer\n89cc636f556d: Pulling fs layer\n1b47dceffbdf: Pulling fs layer\n34cf01fc4cf1: Pulling fs layer\nf728b5eba988: Pulling fs layer\n4f38f3d26bb5: Pulling fs layer\n46f8ead54639: Pulling fs layer\n41deed2d6a0a: Waiting\nb613bd2e6281: Waiting\n9cadba529a8c: Pulling fs layer\n89cc636f556d: Waiting\n1b47dceffbdf: Waiting\n591c10b10151: Pulling fs layer\n34cf01fc4cf1: Waiting\n3344fb800d42: Pulling fs layer\nf728b5eba988: Waiting\na6b754188d60: Pulling fs layer\n877556280f24: Pulling fs layer\n58c2cae18147: Waiting\n3344fb800d42: Waiting\n46f8ead54639: Waiting\na6b754188d60: Waiting\n877556280f24: Waiting\n9cadba529a8c: Waiting\n591c10b10151: Waiting\n3679cb576252: Waiting\n4f38f3d26bb5: Waiting\n2f2f4d7c8e07: Verifying Checksum\n2f2f4d7c8e07: Download complete\n58c2cae18147: Verifying Checksum\n58c2cae18147: Download complete\n3679cb576252: Verifying Checksum\n3679cb576252: Download complete\n41deed2d6a0a: Verifying Checksum\n41deed2d6a0a: Download complete\nb613bd2e6281: Verifying Checksum\nb613bd2e6281: Download complete\n89cc636f556d: Verifying Checksum\n89cc636f556d: Download complete\n1b47dceffbdf: Verifying Checksum\n1b47dceffbdf: Download complete\n34cf01fc4cf1: Verifying Checksum\n34cf01fc4cf1: Download complete\n692d4ce61fb0: Verifying Checksum\n692d4ce61fb0: Download complete\n4f38f3d26bb5: Verifying Checksum\n4f38f3d26bb5: Download complete\n46f8ead54639: Verifying Checksum\n46f8ead54639: Download complete\n9cadba529a8c: Verifying Checksum\n9cadba529a8c: Download complete\n591c10b10151: Verifying Checksum\n591c10b10151: Download complete\n3344fb800d42: Download complete\na6b754188d60: Verifying Checksum\na6b754188d60: Download complete\n877556280f24: Verifying Checksum\n877556280f24: Download complete\nf728b5eba988: Verifying Checksum\nf728b5eba988: Download complete\ndd8de35d23de: Download complete\ndd8de35d23de: Pull complete\n692d4ce61fb0: Pull complete\n2f2f4d7c8e07: Pull complete\n58c2cae18147: Pull complete\n3679cb576252: Pull complete\n41deed2d6a0a: Pull complete\nb613bd2e6281: Pull complete\n89cc636f556d: Pull complete\n1b47dceffbdf: Pull complete\n34cf01fc4cf1: Pull complete\nf728b5eba988: Pull complete\n4f38f3d26bb5: Pull complete\n46f8ead54639: Pull complete\n9cadba529a8c: Pull complete\n591c10b10151: Pull complete\n3344fb800d42: Pull complete\na6b754188d60: Pull complete\n877556280f24: Pull complete\nDigest: sha256:9cf2419dd18357cc4402ec9bd3637d0857d2b6d003a20c7d469d1581e14ece83\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-005ae9be6c22\n",
  "timestamp": "2026-01-14 19:47:20"
}