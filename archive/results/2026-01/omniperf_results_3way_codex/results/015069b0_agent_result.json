{
  "human_commit": "015069b0",
  "human_commit_full": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
  "parent_commit": "fbefc8a78d22b20eac042c586805c7dcbfc66b1c",
  "model": "Qwen/Qwen3-7B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 383.48974323272705,
  "metrics": {},
  "raw_output": "lse, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\nERROR 01-14 13:26:58 [config.py:108] Error retrieving file list: 404 Client Error. (Request ID: Root=1-69680a22-2ca934154c01eac042e16f67;bfc5c212-51da-4378-8873-fdf133725c9a)\nERROR 01-14 13:26:58 [config.py:108] \nERROR 01-14 13:26:58 [config.py:108] Repository Not Found for url: https://huggingface.co/api/models/Qwen/Qwen3-7B-Instruct/tree/main?recursive=True&expand=False.\nERROR 01-14 13:26:58 [config.py:108] Please make sure you specified the correct `repo_id` and `repo_type`.\nERROR 01-14 13:26:58 [config.py:108] If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication, retrying 1 of 2\nERROR 01-14 13:27:00 [config.py:106] Error retrieving file list: 404 Client Error. (Request ID: Root=1-69680a24-158c8e7a40601f1c33019b21;fbcf3a5c-d689-42da-bde4-4132422633dd)\nERROR 01-14 13:27:00 [config.py:106] \nERROR 01-14 13:27:00 [config.py:106] Repository Not Found for url: https://huggingface.co/api/models/Qwen/Qwen3-7B-Instruct/tree/main?recursive=True&expand=False.\nERROR 01-14 13:27:00 [config.py:106] Please make sure you specified the correct `repo_id` and `repo_type`.\nERROR 01-14 13:27:00 [config.py:106] If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.12/dist-packages/requests/models.py\", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/api/models/Qwen/Qwen3-7B-Instruct/tree/main?recursive=True&expand=False\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 280, in get_config\n    if is_gguf or file_or_path_exists(\n                  ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 185, in file_or_path_exists\n    return file_exists(str(model),\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 160, in file_exists\n    file_list = list_repo_files(repo_id,\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 149, in list_repo_files\n    return with_retry(lookup_files, \"Error retrieving file list\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 103, in with_retry\n    return func()\n           ^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 139, in lookup_files\n    return hf_list_repo_files(repo_id,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\", line 3042, in list_repo_files\n    for f in self.list_repo_tree(\n             ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\", line 3177, in list_repo_tree\n    for path_info in paginate(path=tree_url, headers=headers, params={\"recursive\": recursive, \"expand\": expand}):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_pagination.py\", line 37, in paginate\n    hf_raise_for_status(r)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 459, in hf_raise_for_status\n    raise _format(RepositoryNotFoundError, message, response) from e\nhuggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-69680a24-158c8e7a40601f1c33019b21;fbcf3a5c-d689-42da-bde4-4132422633dd)\n\nRepository Not Found for url: https://huggingface.co/api/models/Qwen/Qwen3-7B-Instruct/tree/main?recursive=True&expand=False.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1130, in <module>\n    uvloop.run(run_server(args))\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/entrypoints/openai/api_server.py\", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 941, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 830, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File \"<string>\", line 41, in __init__\n  File \"/opt/vllm_baseline/vllm/config.py\", line 487, in __post_init__\n    hf_config = get_config(self.hf_config_path or self.model,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/transformers_utils/config.py\", line 305, in get_config\n    raise ValueError(error_message) from e\nValueError: Invalid repository ID or local directory specified: 'Qwen/Qwen3-7B-Instruct'.\nPlease verify the following requirements:\n1. Provide a valid Hugging Face repository ID.\n2. Specify a local directory that contains a recognized configuration file.\n   - For Hugging Face models: ensure the presence of a 'config.json'.\n   - For Mistral models: ensure the presence of a 'params.json'.\n\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-baseline:baseline-fbefc8a78d22' locally\nbaseline-fbefc8a78d22: Pulling from anonymous/vllm-baseline\n9cb31e2e37ea: Already exists\nb95112eaf283: Already exists\n030ef8250936: Already exists\n72ac9ccfda38: Already exists\n73389fbd088f: Already exists\n0264850675f7: Already exists\nde1d03310308: Already exists\nc1d2af7fad0f: Already exists\n5601308b3ac6: Already exists\n6b2035e8b73e: Already exists\ned71f8f81b33: Already exists\na04181a7ff83: Already exists\nb89dfbaf9e2c: Already exists\nf221ee730b1b: Already exists\nab7bef329942: Already exists\n46bce9cdf2db: Already exists\n27f7c531d01c: Already exists\nd8bb65e3f5b5: Pulling fs layer\n3223ae3372cf: Pulling fs layer\nbd1777c8392b: Pulling fs layer\n324924804bf9: Pulling fs layer\n444f103aa742: Pulling fs layer\nda4fe45f4026: Pulling fs layer\n7566a439a21c: Pulling fs layer\n4a5b7095821f: Pulling fs layer\n1cc21fa2e007: Pulling fs layer\ne4e6ada9d900: Pulling fs layer\n6ae75f8c5ac2: Pulling fs layer\n324924804bf9: Waiting\n444f103aa742: Waiting\nda4fe45f4026: Waiting\ne4e6ada9d900: Waiting\n6ae75f8c5ac2: Waiting\n7566a439a21c: Waiting\n4a5b7095821f: Waiting\n1cc21fa2e007: Waiting\nbd1777c8392b: Verifying Checksum\nbd1777c8392b: Download complete\n324924804bf9: Verifying Checksum\n324924804bf9: Download complete\n444f103aa742: Verifying Checksum\n444f103aa742: Download complete\nda4fe45f4026: Verifying Checksum\nda4fe45f4026: Download complete\n7566a439a21c: Download complete\n4a5b7095821f: Verifying Checksum\n4a5b7095821f: Download complete\n1cc21fa2e007: Verifying Checksum\n1cc21fa2e007: Download complete\ne4e6ada9d900: Verifying Checksum\ne4e6ada9d900: Download complete\n3223ae3372cf: Verifying Checksum\n3223ae3372cf: Download complete\n6ae75f8c5ac2: Verifying Checksum\n6ae75f8c5ac2: Download complete\nd8bb65e3f5b5: Download complete\nd8bb65e3f5b5: Pull complete\n3223ae3372cf: Pull complete\nbd1777c8392b: Pull complete\n324924804bf9: Pull complete\n444f103aa742: Pull complete\nda4fe45f4026: Pull complete\n7566a439a21c: Pull complete\n4a5b7095821f: Pull complete\n1cc21fa2e007: Pull complete\ne4e6ada9d900: Pull complete\n6ae75f8c5ac2: Pull complete\nDigest: sha256:4d11a5a146cdc501319991a5ebcd09aaee0ed5fefe9f2203aeb0bf765b3f1142\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-fbefc8a78d22\n",
  "timestamp": "2026-01-14 21:27:02"
}