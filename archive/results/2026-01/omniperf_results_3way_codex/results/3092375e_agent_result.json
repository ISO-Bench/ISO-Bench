{
  "human_commit": "3092375e",
  "human_commit_full": "3092375e274e9e003961e600e10a6192d33ceaa0",
  "parent_commit": "3cd91dc9555e6f10e55f23d37782c65b0366f7cf",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "status": "error",
  "error": "Server crashed after applying patch",
  "duration_s": 199.62179970741272,
  "metrics": {},
  "raw_output": "=== Applying agent patch to baseline vLLM ===\nvLLM uses mllama - checking transformers version...\nvLLM found at: /opt/vllm_baseline\nApplying patch...\nchecking file tests/v1/test_serial_utils.py\nchecking file vllm/envs.py\nchecking file vllm/v1/serial_utils.py\npatching file tests/v1/test_serial_utils.py\npatching file vllm/envs.py\npatching file vllm/v1/serial_utils.py\nAGENT_PATCH_APPLIED\nFixing rope_scaling compatibility for Llama-3.1 models...\n  Patching: /opt/vllm_baseline/vllm/transformers_utils/config.py\nSearching for Python with vLLM...\nWARNING: Could not find Python with vLLM, trying default python3\nvLLM import failed\noutlines.fsm OK\n=== Starting vLLM server for AGENT benchmark ===\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 189, in _run_module_as_main\n  File \"<frozen runpy>\", line 112, in _get_module_details\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 19, in <module>\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\n  File \"/opt/vllm_baseline/vllm/config.py\", line 30, in <module>\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n  File \"/opt/vllm_baseline/vllm/model_executor/__init__.py\", line 3, in <module>\n    from vllm.model_executor.parameter import (BasevLLMParameter,\n  File \"/opt/vllm_baseline/vllm/model_executor/parameter.py\", line 9, in <module>\n    from vllm.distributed import get_tensor_model_parallel_rank\n  File \"/opt/vllm_baseline/vllm/distributed/__init__.py\", line 3, in <module>\n    from .communication_op import *\n  File \"/opt/vllm_baseline/vllm/distributed/communication_op.py\", line 8, in <module>\n    from .parallel_state import get_tp_group\n  File \"/opt/vllm_baseline/vllm/distributed/parallel_state.py\", line 40, in <module>\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 16, in <module>\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 6, in <module>\n    from .base import KVConnectorBase\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_connector/base.py\", line 15, in <module>\n    from vllm.sequence import IntermediateTensors\n  File \"/opt/vllm_baseline/vllm/sequence.py\", line 19, in <module>\n    from vllm.multimodal import MultiModalDataDict, MultiModalPlaceholderDict\n  File \"/opt/vllm_baseline/vllm/multimodal/__init__.py\", line 10, in <module>\n    MULTIMODAL_REGISTRY = MultiModalRegistry()\n                          ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/registry.py\", line 123, in __init__\n    self._processing_cache = ProcessingCache(VLLM_MM_INPUT_CACHE_GIB)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/processing.py\", line 933, in __init__\n    self._cache = self.get_lru_cache(\n                  ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/processing.py\", line 919, in get_lru_cache\n    return LRUCache(GiB_bytes * capacity_gb, getsizeof=get_item_size)\n                    ~~~~~~~~~~^~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\nSERVER_CRASHED\nUnable to find image 'anonymous/vllm-baseline:baseline-3cd91dc9555e' locally\nbaseline-3cd91dc9555e: Pulling from anonymous/vllm-baseline\n3c645031de29: Already exists\n0d6448aff889: Already exists\n0a7674e3e8fe: Already exists\nb71b637b97c5: Already exists\n56dc85502937: Already exists\nec6d5f6c9ed9: Already exists\n47b8539d532f: Already exists\nfd9cc1ad8dee: Already exists\n83525caeeb35: Already exists\n8e79813a7b9d: Already exists\n312a542960e3: Already exists\n949691b47390: Already exists\n7cbb09265719: Already exists\ncb7c80b8c4f1: Already exists\nf4d72f8f1249: Already exists\n5b7bacc7057e: Already exists\ne64afe835865: Already exists\n12910821d266: Pulling fs layer\nc2e156ac312f: Pulling fs layer\n9ba5614620d8: Pulling fs layer\n0e0b9afae2de: Pulling fs layer\n6e36bbc5664e: Pulling fs layer\nf54b6e948b66: Pulling fs layer\nca26e1d286eb: Pulling fs layer\n0e0b9afae2de: Waiting\n6e36bbc5664e: Waiting\n9bebb419ad34: Pulling fs layer\nf54b6e948b66: Waiting\n634f274e8a37: Pulling fs layer\n9bebb419ad34: Waiting\nca26e1d286eb: Waiting\nf76281b6dfd1: Pulling fs layer\n634f274e8a37: Waiting\nff9004fbcea8: Pulling fs layer\n7aa226acb8e6: Pulling fs layer\nf76281b6dfd1: Waiting\n0c298516babe: Pulling fs layer\nff9004fbcea8: Waiting\ne43966a39056: Pulling fs layer\nd555276d72e9: Pulling fs layer\n7aa226acb8e6: Waiting\naab4066e2724: Pulling fs layer\n0c298516babe: Waiting\ne43966a39056: Waiting\naab4066e2724: Waiting\nd555276d72e9: Waiting\n9ba5614620d8: Download complete\n0e0b9afae2de: Verifying Checksum\n0e0b9afae2de: Download complete\n6e36bbc5664e: Verifying Checksum\n6e36bbc5664e: Download complete\nf54b6e948b66: Verifying Checksum\nf54b6e948b66: Download complete\nca26e1d286eb: Verifying Checksum\nca26e1d286eb: Download complete\nc2e156ac312f: Verifying Checksum\nc2e156ac312f: Download complete\n634f274e8a37: Verifying Checksum\n634f274e8a37: Download complete\nf76281b6dfd1: Verifying Checksum\nf76281b6dfd1: Download complete\nff9004fbcea8: Verifying Checksum\nff9004fbcea8: Download complete\n7aa226acb8e6: Verifying Checksum\n7aa226acb8e6: Download complete\n0c298516babe: Verifying Checksum\n0c298516babe: Download complete\n9bebb419ad34: Verifying Checksum\n9bebb419ad34: Download complete\nd555276d72e9: Verifying Checksum\nd555276d72e9: Download complete\naab4066e2724: Download complete\ne43966a39056: Verifying Checksum\ne43966a39056: Download complete\n12910821d266: Verifying Checksum\n12910821d266: Download complete\n12910821d266: Pull complete\nc2e156ac312f: Pull complete\n9ba5614620d8: Pull complete\n0e0b9afae2de: Pull complete\n6e36bbc5664e: Pull complete\nf54b6e948b66: Pull complete\nca26e1d286eb: Pull complete\n9bebb419ad34: Pull complete\n634f274e8a37: Pull complete\nf76281b6dfd1: Pull complete\nff9004fbcea8: Pull complete\n7aa226acb8e6: Pull complete\n0c298516babe: Pull complete\ne43966a39056: Pull complete\nd555276d72e9: Pull complete\naab4066e2724: Pull complete\nDigest: sha256:2ee8ca546ee09e1274ad35cbe01e72c886b1f740d92dcd4d2512e53c52fb35e0\nStatus: Downloaded newer image for anonymous/vllm-baseline:baseline-3cd91dc9555e\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/opt/vllm_baseline/vllm/__init__.py\", line 12, in <module>\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\n  File \"/opt/vllm_baseline/vllm/engine/arg_utils.py\", line 19, in <module>\n    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\n  File \"/opt/vllm_baseline/vllm/config.py\", line 30, in <module>\n    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n  File \"/opt/vllm_baseline/vllm/model_executor/__init__.py\", line 3, in <module>\n    from vllm.model_executor.parameter import (BasevLLMParameter,\n  File \"/opt/vllm_baseline/vllm/model_executor/parameter.py\", line 9, in <module>\n    from vllm.distributed import get_tensor_model_parallel_rank\n  File \"/opt/vllm_baseline/vllm/distributed/__init__.py\", line 3, in <module>\n    from .communication_op import *\n  File \"/opt/vllm_baseline/vllm/distributed/communication_op.py\", line 8, in <module>\n    from .parallel_state import get_tp_group\n  File \"/opt/vllm_baseline/vllm/distributed/parallel_state.py\", line 40, in <module>\n    import vllm.distributed.kv_transfer.kv_transfer_agent as kv_transfer\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_transfer_agent.py\", line 16, in <module>\n    from vllm.distributed.kv_transfer.kv_connector.factory import (\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_connector/factory.py\", line 6, in <module>\n    from .base import KVConnectorBase\n  File \"/opt/vllm_baseline/vllm/distributed/kv_transfer/kv_connector/base.py\", line 15, in <module>\n    from vllm.sequence import IntermediateTensors\n  File \"/opt/vllm_baseline/vllm/sequence.py\", line 19, in <module>\n    from vllm.multimodal import MultiModalDataDict, MultiModalPlaceholderDict\n  File \"/opt/vllm_baseline/vllm/multimodal/__init__.py\", line 10, in <module>\n    MULTIMODAL_REGISTRY = MultiModalRegistry()\n                          ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/registry.py\", line 123, in __init__\n    self._processing_cache = ProcessingCache(VLLM_MM_INPUT_CACHE_GIB)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/processing.py\", line 933, in __init__\n    self._cache = self.get_lru_cache(\n                  ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/vllm_baseline/vllm/multimodal/processing.py\", line 919, in get_lru_cache\n    return LRUCache(GiB_bytes * capacity_gb, getsizeof=get_item_size)\n                    ~~~~~~~~~~^~~~~~~~~~~~~\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType'\n",
  "timestamp": "2026-01-14 22:35:07"
}