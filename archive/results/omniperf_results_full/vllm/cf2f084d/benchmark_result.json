{
  "instance": {
    "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)",
    "repo": "vllm",
    "perf_command": null,
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: cf2f084d56a1293cb08da2393984cdc7685ac019\nMessage: Dynamic scheduler delay to improve ITL performance  (#3279)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the diff, the main changes are in Scheduler._passed_delay\n        module_path = \"vllm.core.scheduler\"\n        symbol_name = \"Scheduler\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required classes for scheduler setup\n    try:\n        from vllm.config import SchedulerConfig, CacheConfig\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block.utils import SequenceGroup\n        from vllm.core.scheduler import SequenceGroupMetadata\n        from vllm.compilation.backends import Sequence\n        from vllm import SamplingParams\n        from vllm.block import LogicalTokenBlock\n    except ImportError as e:\n        print(json.dumps({\"target_resolved\": False, \"error\": f\"Import error: {e}\"}))\n        sys.exit(1)\n    \n    # Create scheduler configurations\n    block_size = 16\n    max_num_batched_tokens = 4096\n    max_num_seqs = 256\n    max_model_len = 2048\n    \n    # Test with delay factor (the optimization parameter)\n    delay_factor = float(os.getenv(\"DELAY_FACTOR\", \"0.5\"))\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=max_num_batched_tokens,\n        max_num_seqs=max_num_seqs,\n        max_model_len=max_model_len,\n        delay_factor=delay_factor\n    )\n    \n    cache_config = CacheConfig(\n        block_size=block_size,\n        gpu_memory_utilization=0.9,\n        swap_space_bytes=0,\n        cache_dtype=\"auto\"\n    )\n    cache_config.num_cpu_blocks = 512\n    cache_config.num_gpu_blocks = 1024\n    \n    # Create scheduler instance\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    \n    # Create simulated requests queue\n    num_requests = 100\n    requests = []\n    \n    for i in range(num_requests):\n        # Mix of different prompt lengths\n        prompt_length = np.random.choice([128, 256, 512, 1024])\n        request_id = f\"req_{i}\"\n        \n        # Create sequence and sequence group\n        prompt_tokens = list(range(prompt_length))\n        seq = Sequence(\n            seq_id=i,\n            inputs={\"prompt_token_ids\": prompt_tokens},\n            block_size=block_size\n        )\n        \n        sampling_params = SamplingParams(\n            temperature=0.7,\n            max_tokens=128\n        )\n        \n        # Create sequence group with arrival time\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time() + i * 0.01  # Stagger arrivals\n        )\n        \n        requests.append(seq_group)\n    \n    data = {\n        \"device\": hw_info[\"device\"],\n        \"dtype\": torch.float16,\n        \"hw_info\": hw_info,\n        \"scheduler\": scheduler,\n        \"requests\": requests,\n        \"delay_factor\": delay_factor,\n        \"scheduler_config\": scheduler_config,\n        \"cache_config\": cache_config\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    scheduler = data[\"scheduler\"]\n    requests = data[\"requests\"]\n    \n    # Reset scheduler state\n    scheduler.waiting = deque()\n    scheduler.running = []\n    scheduler.swapped = deque()\n    scheduler.prev_time = 0.0\n    scheduler.prev_prompt = False\n    scheduler.last_prompt_latency = 0.0\n    \n    # Simulate scheduling with delay factor\n    results = {\n        \"scheduled_prompts\": [],\n        \"schedule_times\": [],\n        \"waiting_times\": [],\n        \"batch_sizes\": []\n    }\n    \n    # Add requests progressively and schedule\n    request_idx = 0\n    total_scheduled = 0\n    \n    # Run scheduling iterations\n    for iteration in range(50):\n        # Add some new requests\n        while request_idx < len(requests) and request_idx < (iteration + 1) * 2:\n            scheduler.add_seq_group(requests[request_idx])\n            request_idx += 1\n        \n        # Record time before scheduling\n        start_time = time.perf_counter()\n        \n        # Call schedule method (the optimized function)\n        seq_group_meta, scheduler_outputs = scheduler.schedule()\n        \n        # Record scheduling time\n        schedule_time = time.perf_counter() - start_time\n        results[\"schedule_times\"].append(schedule_time * 1000)  # Convert to ms\n        \n        if scheduler_outputs.scheduled_seq_groups:\n            total_scheduled += len(scheduler_outputs.scheduled_seq_groups)\n            results[\"scheduled_prompts\"].append(len(scheduler_outputs.scheduled_seq_groups))\n            results[\"batch_sizes\"].append(scheduler_outputs.num_batched_tokens)\n            \n            # Simulate processing time for prompts\n            if scheduler_outputs.prompt_run:\n                # Simulate prompt processing latency\n                time.sleep(0.01)  # 10ms simulated processing\n        \n        # Record waiting queue size\n        results[\"waiting_times\"].append(len(scheduler.waiting))\n        \n        # Break if all requests scheduled\n        if total_scheduled >= len(requests):\n            break\n        \n        # Small delay between iterations\n        time.sleep(0.001)\n    \n    # Calculate metrics\n    results[\"total_scheduled\"] = total_scheduled\n    results[\"avg_schedule_time_ms\"] = np.mean(results[\"schedule_times\"]) if results[\"schedule_times\"] else 0\n    results[\"avg_batch_size\"] = np.mean(results[\"batch_sizes\"]) if results[\"batch_sizes\"] else 0\n    results[\"max_waiting_queue\"] = max(results[\"waiting_times\"]) if results[\"waiting_times\"] else 0\n    \n    return results\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    # For scheduler, check that key metrics are similar\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check that total scheduled is the same\n        assert current_result.get(\"total_scheduled\") == reference_result.get(\"total_scheduled\"), \\\n            f\"Total scheduled mismatch: {current_result.get('total_scheduled')} vs {reference_result.get('total_scheduled')}\"\n        \n        # Check that scheduling times are reasonable (within 2x)\n        curr_time = current_result.get(\"avg_schedule_time_ms\", 0)\n        ref_time = reference_result.get(\"avg_schedule_time_ms\", 0)\n        if ref_time > 0:\n            ratio = curr_time / ref_time\n            assert 0.5 <= ratio <= 2.0, f\"Schedule time ratio {ratio} out of bounds\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu_scheduler(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU scheduler operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing - scheduler is CPU-based\n    warmup = 3\n    iters = 10\n    \n    result, timing_stats = time_cpu_scheduler(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"cf2f084d56a1293cb08da2393984cdc7685ac019\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Scheduler runs on CPU\n        \"dtype\": \"torch.float16\",\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": True,\n        \"delay_factor\": data[\"delay_factor\"],\n        \"avg_schedule_time_ms\": result.get(\"avg_schedule_time_ms\", 0),\n        \"total_scheduled\": result.get(\"total_scheduled\", 0)\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "files_changed": [
      "tests/core/test_scheduler.py",
      "vllm/config.py",
      "vllm/core/scheduler.py",
      "vllm/engine/arg_utils.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/3279",
    "models": [
      "N/A"
    ]
  },
  "result": {
    "status": "no_perf_command",
    "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "benchmark_type": "unknown",
    "baseline_metrics": null,
    "patched_metrics": null,
    "improvement": null,
    "error_message": "No perf_command in dataset",
    "duration_s": 2.0265579223632812e-05,
    "perf_command": null,
    "patch_path": null
  }
}