=== BASELINE ===
Failed to start vLLM server
b'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status\n    response.raise_for_status()\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama/Llama-3-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files\n    hf_hub_download(\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error\n    raise head_call_error\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper\n    hf_raise_for_status(response)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 452, in hf_raise_for_status\n    raise _format(RepositoryNotFoundError, message, response) from e\nhuggingface_hub.errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-694e3c1f-1148bc6148f6838437e55e28;9a612e45-325b-41f2-9835-ac4202ce80b4)\n\nRepository Not Found for url: https://huggingface.co/meta-llama/Llama-3-8B-Instruct/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "<frozen runpy>", line 198, in _run_module_as_main\n  File "<frozen runpy>", line 88, in _run_code\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 476, in <module>\n    asyncio.run(run_server(args))\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 443, in run_server\n    async with build_async_engine_client(args) as async_engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 117, in build_async_engine_client\n    if (model_is_embedding(args.model, args.trust_remote_code,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 71, in model_is_embedding\n    return ModelConfig(model=model_name,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 172, in __init__\n    self.hf_config = get_config(self.model, trust_remote_code, revision,\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 66, in get_config\n    config = AutoConfig.from_pretrained(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1332, in from_pretrained\n    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n                           ^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 511, in cached_files\n    raise OSError(\nOSError: meta-llama/Llama-3-8B-Instruct is not a local folder and is not a valid model identifier listed on \'https://huggingface.co/models\'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n'