=== BASELINE ===
Failed to start vLLM server
b'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status\n    response.raise_for_status()\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files\n    hf_hub_download(\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error\n    raise head_call_error\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error\n    metadata = get_hf_file_metadata(\n               ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper\n    hf_raise_for_status(response)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-694e50bd-4f2662414912ed8b592231a2;8019e888-1647-4f3c-a2c8-2915a2160680)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo\'s authors.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "<frozen runpy>", line 198, in _run_module_as_main\n  File "<frozen runpy>", line 88, in _run_code\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1130, in <module>\n    uvloop.run(run_server(args))\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1078, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 166, in build_async_engine_client_from_engine_args\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1130, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1018, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 452, in __init__\n    hf_config = get_config(self.hf_config_path or self.model,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 305, in get_config\n    config_dict, _ = PretrainedConfig.get_config_dict(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict\n    resolved_config_file = cached_file(\n                           ^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py", line 543, in cached_files\n    raise OSError(\nOSError: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-694e50bd-4f2662414912ed8b592231a2;8019e888-1647-4f3c-a2c8-2915a2160680)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo\'s authors.\n'