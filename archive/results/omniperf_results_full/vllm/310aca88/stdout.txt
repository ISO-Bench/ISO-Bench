=== BASELINE ===
Namespace(input_len=32, output_len=128, batch_size=8, n=1, use_beam_search=False, num_iters_warmup=10, num_iters=30, profile=False, profile_result_dir=None, output_json=None, model='meta-llama/Meta-Llama-3-70B', task='auto', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='dummy', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='xgrammar', logits_processor_pattern=None, distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_quantization=None, num_speculative_tokens=None, speculative_disable_mqa_scorer=False, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', generation_config=None)
INFO 12-26 07:40:33 __init__.py:179] Automatically detected platform cuda.
ERROR 12-26 07:40:39 registry.py:294] Error in inspecting model architecture 'LlamaForCausalLM'
ERROR 12-26 07:40:39 registry.py:294] Traceback (most recent call last):
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 489, in _run_in_subprocess
ERROR 12-26 07:40:39 registry.py:294]     returned.check_returncode()
ERROR 12-26 07:40:39 registry.py:294]   File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/subprocess.py", line 502, in check_returncode
ERROR 12-26 07:40:39 registry.py:294]     raise CalledProcessError(self.returncode, self.args, self.stdout,
ERROR 12-26 07:40:39 registry.py:294] subprocess.CalledProcessError: Command '['/ephemeral/benchmark_venv/bin/python', '-m', 'vllm.model_executor.models.registry']' returned non-zero exit status 1.
ERROR 12-26 07:40:39 registry.py:294] 
ERROR 12-26 07:40:39 registry.py:294] The above exception was the direct cause of the following exception:
ERROR 12-26 07:40:39 registry.py:294] 
ERROR 12-26 07:40:39 registry.py:294] Traceback (most recent call last):
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 292, in _try_inspect_model_cls
ERROR 12-26 07:40:39 registry.py:294]     return model.inspect_model_cls()
ERROR 12-26 07:40:39 registry.py:294]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 263, in inspect_model_cls
ERROR 12-26 07:40:39 registry.py:294]     return _run_in_subprocess(
ERROR 12-26 07:40:39 registry.py:294]            ^^^^^^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 492, in _run_in_subprocess
ERROR 12-26 07:40:39 registry.py:294]     raise RuntimeError(f"Error raised in subprocess:\n"
ERROR 12-26 07:40:39 registry.py:294] RuntimeError: Error raised in subprocess:
ERROR 12-26 07:40:39 registry.py:294] /ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
ERROR 12-26 07:40:39 registry.py:294]   warnings.warn(
ERROR 12-26 07:40:39 registry.py:294] /ephemeral/benchmark_worktree/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
ERROR 12-26 07:40:39 registry.py:294] No module named 'vllm._version'
ERROR 12-26 07:40:39 registry.py:294]   from vllm.version import __version__ as VLLM_VERSION
ERROR 12-26 07:40:39 registry.py:294] <frozen runpy>:128: RuntimeWarning: 'vllm.model_executor.models.registry' found in sys.modules after import of package 'vllm.model_executor.models', but prior to execution of 'vllm.model_executor.models.registry'; this may result in unpredictable behaviour
ERROR 12-26 07:40:39 registry.py:294] Traceback (most recent call last):
ERROR 12-26 07:40:39 registry.py:294]   File "<frozen runpy>", line 198, in _run_module_as_main
ERROR 12-26 07:40:39 registry.py:294]   File "<frozen runpy>", line 88, in _run_code
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/model_executor/models/registry.py", line 513, in <module>
ERROR 12-26 07:40:39 registry.py:294]     _run()
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/model_executor/models/registry.py", line 502, in _run
ERROR 12-26 07:40:39 registry.py:294]     load_general_plugins()
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/plugins/__init__.py", line 63, in load_general_plugins
ERROR 12-26 07:40:39 registry.py:294]     from vllm.platforms import current_platform
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/platforms/__init__.py", line 211, in __getattr__
ERROR 12-26 07:40:39 registry.py:294]     _current_platform = resolve_obj_by_qualname(
ERROR 12-26 07:40:39 registry.py:294]                         ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/utils.py", line 1703, in resolve_obj_by_qualname
ERROR 12-26 07:40:39 registry.py:294]     module = importlib.import_module(module_name)
ERROR 12-26 07:40:39 registry.py:294]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294]   File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py", line 90, in import_module
ERROR 12-26 07:40:39 registry.py:294]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 12-26 07:40:39 registry.py:294]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294]   File "/ephemeral/benchmark_worktree/vllm/platforms/cuda.py", line 15, in <module>
ERROR 12-26 07:40:39 registry.py:294]     import vllm._C  # noqa
ERROR 12-26 07:40:39 registry.py:294]     ^^^^^^^^^^^^^^
ERROR 12-26 07:40:39 registry.py:294] ModuleNotFoundError: No module named 'vllm._C'
ERROR 12-26 07:40:39 registry.py:294] 
