{
  "instance": {
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 4fb56914c5f27ef062e10d44a0f79c6ceab382f9\nMessage: [perf] Add fused MLA QKV + strided layernorm (#21116)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata - targeting RMSNorm\n    if not (module_path and symbol_name):\n        module_path = \"vllm.model_executor.layers.layernorm\"\n        symbol_name = \"RMSNorm\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Setup for strided RMSNorm kernel test\n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Model sizes for 7B scale testing\n    batch_size = 4\n    seq_len = 2048\n    hidden_size = 4096\n    \n    # Create strided input by slicing a larger tensor\n    # This mimics the fused QKV scenario where we slice projections\n    last_dim = 2 * hidden_size  # Create extra space for striding\n    \n    # Create base tensor with extra width\n    x_base = torch.randn(batch_size * seq_len, last_dim, device=device, dtype=dtype)\n    # Scale down to prevent overflow\n    x_base *= (1.0 / math.sqrt(2 * hidden_size))\n    \n    # Create strided view by slicing\n    x_strided = x_base[..., :hidden_size]\n    \n    # Verify it's actually strided\n    assert not x_strided.is_contiguous()\n    assert x_strided.stride(-1) == 1  # Last dim stride is 1\n    assert x_strided.stride(-2) == last_dim  # Second-to-last stride is last_dim\n    \n    # Create RMSNorm layer\n    RMSNorm, _ = resolve_target()\n    layer = RMSNorm(hidden_size).to(device=device, dtype=dtype)\n    layer.weight.data.normal_(mean=1.0, std=0.1)\n    \n    # Also prepare residual for fused add variant\n    residual = torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype)\n    residual *= (1.0 / math.sqrt(2 * hidden_size))\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"x_base\": x_base,\n        \"x_strided\": x_strided,\n        \"layer\": layer,\n        \"residual\": residual,\n        \"hidden_size\": hidden_size,\n        \"batch_seq\": batch_size * seq_len,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    \n    # Test the strided RMSNorm kernel\n    x_strided = data[\"x_strided\"]\n    layer = data[\"layer\"]\n    residual = data[\"residual\"]\n    \n    # Clone inputs to avoid in-place modification\n    x_work = x_strided.clone()\n    residual_work = residual.clone() if residual is not None else None\n    \n    with torch.no_grad():\n        # Call the RMSNorm forward which internally uses the optimized kernel\n        # The optimization allows this to work efficiently with strided input\n        output = layer(x_work, residual_work)\n    \n    return output\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Clear cache\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Ensure we're testing the strided path\n    assert not data[\"x_strided\"].is_contiguous(), \"Input must be strided to test optimization\"\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            _ = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n        # Produce a result for reference handling\n        result = experiment(data)\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"4fb56914c5f27ef062e10d44a0f79c6ceab382f9\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "files_changed": [
      "csrc/layernorm_kernels.cu",
      "csrc/layernorm_quant_kernels.cu",
      "csrc/quantization/fp8/common.cu",
      "tests/kernels/core/test_layernorm.py",
      "vllm/model_executor/layers/linear.py",
      "vllm/model_executor/layers/quantization/fp8.py",
      "vllm/model_executor/models/deepseek_v2.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21116",
    "models": [
      "deepseek-ai/DeepSeek-V3-0324"
    ]
  },
  "result": {
    "status": "baseline_failed",
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "benchmark_type": "serving",
    "baseline_metrics": {
      "request_throughput": null,
      "token_throughput": null,
      "output_throughput": null,
      "ttft_mean": null,
      "ttft_p50": null,
      "ttft_p90": null,
      "ttft_p99": null,
      "tpot_mean": null,
      "tpot_p50": null,
      "tpot_p90": null,
      "tpot_p99": null,
      "itl_mean": null,
      "itl_p50": null,
      "itl_p90": null,
      "itl_p99": null,
      "e2e_latency_mean": null,
      "e2e_latency_p50": null,
      "e2e_latency_p90": null,
      "e2e_latency_p99": null,
      "latency_ms": null,
      "raw_output": "\nFailed to start vLLM server\nb'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\\n  warnings.warn(\\n`torch_dtype` is deprecated! Use `dtype` instead!\\n/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\\n  warnings.warn(\\nProcess EngineCore_0:\\nTraceback (most recent call last):\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\\n    self.run()\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\\n    self._target(*self._args, **self._kwargs)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 617, in run_engine_core\\n    raise e\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 604, in run_engine_core\\n    engine_core = EngineCoreProc(*args, **kwargs)\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 431, in __init__\\n    super().__init__(vllm_config, executor_class, log_stats,\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 77, in __init__\\n    self.model_executor = executor_class(vllm_config)\\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 55, in __init__\\n    self._init_executor()\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\\n    self.collective_rpc(\"load_model\")\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\\n    answer = run_method(self.driver_worker, method, args, kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 2990, in run_method\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 195, in load_model\\n    self.model_runner.load_model(eep_scale_up=eep_scale_up)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1827, in load_model\\n    self.model = model_loader.load_model(\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 44, in load_model\\n    model = initialize_model(vllm_config=vllm_config,\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py\", line 66, in initialize_model\\n    return model_class(vllm_config=vllm_config, prefix=prefix)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 723, in __init__\\n    self.model = DeepseekV2Model(vllm_config=vllm_config,\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 152, in __init__\\n    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 661, in __init__\\n    self.start_layer, self.end_layer, self.layers = make_layers(\\n                                                    ^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 640, in make_layers\\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 663, in <lambda>\\n    lambda prefix: DeepseekV2DecoderLayer(\\n                   ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 549, in __init__\\n    self.self_attn = attn_cls(\\n                     ^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 417, in __init__\\n    self.q_b_proj = ColumnParallelLinear(q_lora_rank,\\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 434, in __init__\\n    self.quant_method.create_weights(\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/fp8.py\", line 274, in create_weights\\n    weight = ModelWeightParameter(data=torch.empty(\\n                                       ^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 79.11 GiB of which 10.88 MiB is free. Process 321904 has 79.09 GiB memory in use. Of the allocated memory 78.56 GiB is allocated by PyTorch, and 13.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\\n[rank0]:[W1226 07:53:02.128338352 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\\nTraceback (most recent call last):\\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\\n  File \"<frozen runpy>\", line 88, in _run_code\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1857, in <module>\\n    uvloop.run(run_server(args))\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 96, in run\\n    return __asyncio.run(\\n           ^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 195, in run\\n    return runner.run(main)\\n           ^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 118, in run\\n    return self._loop.run_until_complete(task)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 48, in wrapper\\n    return await main\\n           ^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1792, in run_server\\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1812, in run_server_worker\\n    async with build_async_engine_client(args, client_config) as engine_client:\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n    return await anext(self.gen)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 158, in build_async_engine_client\\n    async with build_async_engine_client_from_engine_args(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n    return await anext(self.gen)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args\\n    async_llm = AsyncLLM.from_vllm_config(\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 161, in from_vllm_config\\n    return cls(\\n           ^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 115, in __init__\\n    self.engine_core = EngineCoreClient.make_async_mp_client(\\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 98, in make_async_mp_client\\n    return AsyncMPClient(*client_args)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 672, in __init__\\n    super().__init__(\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 408, in __init__\\n    with launch_core_engines(vllm_config, executor_class,\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 144, in __exit__\\n    next(self.gen)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 653, in launch_core_engines\\n    wait_for_engine_startup(\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 703, in wait_for_engine_startup\\n    raise RuntimeError(\"Engine core initialization failed. \"\\nRuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\\n'"
    },
    "patched_metrics": null,
    "improvement": null,
    "error_message": "Baseline produced no metrics",
    "duration_s": 42.05817890167236,
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0023/model_patch.diff"
  }
}