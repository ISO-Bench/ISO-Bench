{
  "instance": {
    "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "commit_subject": "[Perf] Disable chunked local attention by default with llama4 (#21761)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --trust-remote-code --max-model-len 16384",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: 8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8\nMessage: [Perf] Disable chunked local attention by default with llama4 (#21761)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.config\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"VllmConfig\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    try:\n        from vllm.config import (\n            ModelConfig, CacheConfig, ParallelConfig, \n            SchedulerConfig, DeviceConfig, LoadConfig,\n            LoRAConfig, PromptAdapterConfig, SpeculativeConfig,\n            TokenizerPoolConfig, ObservabilityConfig, DecodingConfig\n        )\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block.utils import SequenceGroup\n        from vllm.compilation.backends import Sequence\n        from vllm.core.block_manager import SequenceStatus\n        from vllm.block import LogicalTokenBlock\n        from vllm import SamplingParams\n    except ImportError as e:\n        print(json.dumps({\n            \"target_resolved\": False,\n            \"error\": f\"Failed to import vLLM components: {e}\"\n        }))\n        sys.exit(1)\n    \n    device = torch.device(hw_info[\"device\"] if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create model config with chunked local attention enabled (llama4-style)\n    model_config = ModelConfig(\n        model=\"meta-llama/Llama-3.2-3B\",  # Use a Llama model\n        tokenizer=\"meta-llama/Llama-3.2-3B\",\n        tokenizer_mode=\"auto\",\n        trust_remote_code=False,\n        dtype=dtype,\n        seed=42,\n        max_model_len=4096,\n        attention_chunk_size=1024,  # Enable chunked local attention\n        quantization=None,\n        enforce_eager=True,  # Disable CUDA graphs for testing\n        max_context_len_to_capture=None,\n        max_seq_len_to_capture=8192,\n        max_logprobs=20,\n        disable_sliding_window=False,\n        skip_tokenizer_init=True,\n        served_model_name=\"llama4-test\"\n    )\n    \n    # Create cache config\n    cache_config = CacheConfig(\n        block_size=16,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=dtype,\n        num_gpu_blocks_override=1024,  # Fixed number for testing\n        sliding_window=None,\n        enable_prefix_caching=False,\n        cpu_offload_gb=0\n    )\n    \n    # Create scheduler config\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=2048,\n        max_num_seqs=64,\n        max_model_len=4096,\n        enable_hybrid_kv_cache_manager=True,  # Will be toggled by environment\n        disable_hybrid_kv_cache_manager=False,  # Initial value\n        use_v2_block_manager=True,\n        num_lookahead_slots=0,\n        delay_factor=0.0,\n        enable_chunked_prefill=False,\n        is_multimodal_model=False,\n        send_delta_data=False,\n        policy=\"fcfs\",\n        use_async_output_proc=False,\n        multi_step_stream_outputs=False,\n        recompute_depth=0,\n        use_kv_compression=False\n    )\n    \n    # Create other configs\n    parallel_config = ParallelConfig(\n        pipeline_parallel_size=1,\n        tensor_parallel_size=1,\n        worker_use_ray=False,\n        max_parallel_loading_workers=None,\n        disable_custom_all_reduce=False,\n        tokenizer_pool_config=None,\n        ray_workers_use_nsight=False,\n        placement_group=None,\n        distributed_executor_backend=None\n    )\n    \n    device_config = DeviceConfig(device=\"cuda\" if hw_info[\"device\"] == \"cuda\" else \"cpu\")\n    \n    # Create sequences for scheduling test\n    num_sequences = 32\n    sequences = []\n    for i in range(num_sequences):\n        seq_id = i\n        prompt_tokens = list(range(100, 100 + 512))  # 512 prompt tokens\n        sampling_params = SamplingParams(\n            temperature=0.7,\n            top_p=0.95,\n            max_tokens=128\n        )\n        \n        seq = Sequence(\n            seq_id=seq_id,\n            inputs={\"prompt_token_ids\": prompt_tokens},\n            block_size=cache_config.block_size,\n            eos_token_id=2,\n            lora_request=None,\n            prompt_adapter_request=None\n        )\n        \n        seq_group = SequenceGroup(\n            request_id=f\"request_{i}\",\n            seqs=[seq],\n            arrival_time=time.time() - (num_sequences - i) * 0.1,\n            sampling_params=sampling_params,\n            lora_request=None,\n            trace_headers=None,\n            prompt_adapter_request=None,\n            encoder_seq=None,\n            priority=0\n        )\n        \n        sequences.append(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"model_config\": model_config,\n        \"cache_config\": cache_config,\n        \"scheduler_config\": scheduler_config,\n        \"parallel_config\": parallel_config,\n        \"device_config\": device_config,\n        \"sequences\": sequences,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    VllmConfig, _ = resolve_target()\n    \n    # Create VllmConfig with the configurations\n    # This will trigger the optimization logic in the commit\n    try:\n        vllm_config = VllmConfig(\n            model_config=data[\"model_config\"],\n            cache_config=data[\"cache_config\"],\n            parallel_config=data[\"parallel_config\"],\n            scheduler_config=data[\"scheduler_config\"],\n            device_config=data[\"device_config\"],\n            lora_config=None,\n            speculative_config=None,\n            load_config=LoadConfig(load_format=\"auto\"),\n            decoding_config=None,\n            observability_config=None,\n            prompt_adapter_config=None,\n            tokenizer_pool_config=None,\n            kv_transfer_config=None,\n            compilation_config=None\n        )\n        \n        # The optimization affects the scheduler configuration\n        # Check if hybrid KV cache manager was disabled due to chunked local attention\n        result = {\n            \"hybrid_kv_disabled\": vllm_config.scheduler_config.disable_hybrid_kv_cache_manager,\n            \"attention_chunk_size\": vllm_config.model_config.attention_chunk_size,\n            \"env_allow_chunked\": os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\") == \"1\"\n        }\n        \n        # Create scheduler to measure actual performance impact\n        from vllm.core.scheduler import Scheduler\n        from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n        \n        # Create block manager\n        block_manager = BlockSpaceManagerV2(\n            block_size=data[\"cache_config\"].block_size,\n            num_gpu_blocks=data[\"cache_config\"].num_gpu_blocks_override,\n            num_cpu_blocks=0,\n            watermark=0.01,\n            sliding_window=None,\n            enable_caching=False,\n            hybrid_enabled=not vllm_config.scheduler_config.disable_hybrid_kv_cache_manager\n        )\n        \n        # Create scheduler\n        scheduler = Scheduler(\n            scheduler_config=vllm_config.scheduler_config,\n            cache_config=data[\"cache_config\"],\n            lora_config=None,\n            parallel_config=data[\"parallel_config\"],\n            pipeline_parallel_size=1,\n            output_proc_callback=None\n        )\n        \n        # Add sequences to scheduler\n        for seq_group in data[\"sequences\"]:\n            scheduler.add_seq_group(seq_group)\n        \n        # Run scheduling iterations\n        schedule_times = []\n        for _ in range(10):\n            start = time.perf_counter()\n            scheduler_outputs = scheduler.schedule()\n            end = time.perf_counter()\n            schedule_times.append((end - start) * 1000)\n        \n        result[\"avg_schedule_ms\"] = sum(schedule_times) / len(schedule_times)\n        result[\"scheduler_outputs\"] = len(scheduler_outputs.scheduled_seq_groups) if scheduler_outputs else 0\n        \n    except Exception as e:\n        # If there's an error, return minimal result\n        result = {\n            \"hybrid_kv_disabled\": data[\"scheduler_config\"].disable_hybrid_kv_cache_manager,\n            \"attention_chunk_size\": data[\"model_config\"].attention_chunk_size,\n            \"env_allow_chunked\": os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\") == \"1\",\n            \"error\": str(e)\n        }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # For configuration results, check key fields\n        assert current_result.get(\"attention_chunk_size\") == reference_result.get(\"attention_chunk_size\")\n        # The hybrid_kv_disabled flag may differ between commits (that's the optimization)\n        # So we don't assert equality on that field\n    elif isinstance(current_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Always use CPU timing since this is a configuration/scheduling test\n    warmup = 3\n    iters = 10\n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"behavioral\"),\n        \"opt_path_hit\": result.get(\"hybrid_kv_disabled\", False) if isinstance(result, dict) else True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "files_changed": [
      "vllm/config.py",
      "vllm/envs.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21761",
    "models": [
      "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    ]
  },
  "result": {
    "status": "baseline_failed",
    "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "benchmark_type": "serving",
    "baseline_metrics": {
      "request_throughput": null,
      "token_throughput": null,
      "output_throughput": null,
      "ttft_mean": null,
      "ttft_p50": null,
      "ttft_p90": null,
      "ttft_p99": null,
      "tpot_mean": null,
      "tpot_p50": null,
      "tpot_p90": null,
      "tpot_p99": null,
      "itl_mean": null,
      "itl_p50": null,
      "itl_p90": null,
      "itl_p99": null,
      "e2e_latency_mean": null,
      "e2e_latency_p50": null,
      "e2e_latency_p90": null,
      "e2e_latency_p99": null,
      "latency_ms": null,
      "raw_output": "\nFailed to start vLLM server\nb'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\\n  warnings.warn(\\nTraceback (most recent call last):\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 402, in hf_raise_for_status\\n    response.raise_for_status()\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/requests/models.py\", line 1026, in raise_for_status\\n    raise HTTPError(http_error_msg, response=self)\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct/resolve/main/config.json\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py\", line 479, in cached_files\\n    hf_hub_download(\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\\n    return fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1007, in hf_hub_download\\n    return _hf_hub_download_to_cache_dir(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1114, in _hf_hub_download_to_cache_dir\\n    _raise_on_head_call_error(head_call_error, force_download, local_files_only)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1655, in _raise_on_head_call_error\\n    raise head_call_error\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1543, in _get_metadata_or_catch_error\\n    metadata = get_hf_file_metadata(\\n               ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\\n    return fn(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1460, in get_hf_file_metadata\\n    r = _request_wrapper(\\n        ^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 283, in _request_wrapper\\n    response = _request_wrapper(\\n               ^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 307, in _request_wrapper\\n    hf_raise_for_status(response)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py\", line 419, in hf_raise_for_status\\n    raise _format(GatedRepoError, message, response) from e\\nhuggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-694e3e56-055dffad1918a11f355fd384;192dd2db-3ed3-4423-be81-68497cb68959)\\n\\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct/resolve/main/config.json.\\nAccess to model meta-llama/Llama-4-Scout-17B-16E-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct to ask for access.\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\\n  File \"<frozen runpy>\", line 88, in _run_code\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1877, in <module>\\n    uvloop.run(run_server(args))\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 96, in run\\n    return __asyncio.run(\\n           ^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 195, in run\\n    return runner.run(main)\\n           ^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 118, in run\\n    return self._loop.run_until_complete(task)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 48, in wrapper\\n    return await main\\n           ^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1809, in run_server\\n    await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1829, in run_server_worker\\n    async with build_async_engine_client(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n    return await anext(self.gen)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 165, in build_async_engine_client\\n    async with build_async_engine_client_from_engine_args(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n    return await anext(self.gen)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 191, in build_async_engine_client_from_engine_args\\n    vllm_config = engine_args.create_engine_config(usage_context=usage_context)\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py\", line 1015, in create_engine_config\\n    model_config = self.create_model_config()\\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py\", line 881, in create_model_config\\n    return ModelConfig(\\n           ^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py\", line 121, in __init__\\n    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py\", line 601, in __post_init__\\n    hf_config = get_config(self.hf_config_path or self.model,\\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py\", line 355, in get_config\\n    config_dict, _ = PretrainedConfig.get_config_dict(\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 662, in get_config_dict\\n    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)\\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/configuration_utils.py\", line 721, in _get_config_dict\\n    resolved_config_file = cached_file(\\n                           ^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py\", line 322, in cached_file\\n    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py\", line 543, in cached_files\\n    raise OSError(\\nOSError: You are trying to access a gated repo.\\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct.\\n403 Client Error. (Request ID: Root=1-694e3e56-055dffad1918a11f355fd384;192dd2db-3ed3-4423-be81-68497cb68959)\\n\\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct/resolve/main/config.json.\\nAccess to model meta-llama/Llama-4-Scout-17B-16E-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct to ask for access.\\n'"
    },
    "patched_metrics": null,
    "improvement": null,
    "error_message": "Baseline produced no metrics",
    "duration_s": 24.155948877334595,
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --trust-remote-code --max-model-len 16384",
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0043/model_patch.diff"
  }
}