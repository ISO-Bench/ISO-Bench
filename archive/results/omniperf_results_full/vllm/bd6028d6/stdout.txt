=== BASELINE ===
INFO 12-26 10:11:45 [__init__.py:239] Automatically detected platform cuda.
Namespace(input_len=1000, output_len=1000, batch_size=1, n=1, use_beam_search=False, num_iters_warmup=5, num_iters=5, profile=False, profile_result_dir=None, output_json=None, disable_detokenize=False, model='RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config=None, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=8000, guided_decoding_backend='auto', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=2, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_chunked_mm_input=False)
ERROR 12-26 10:11:52 [registry.py:346] Error in inspecting model architecture 'Llama4ForConditionalGeneration'
ERROR 12-26 10:11:52 [registry.py:346] Traceback (most recent call last):
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 578, in _run_in_subprocess
ERROR 12-26 10:11:52 [registry.py:346]     returned.check_returncode()
ERROR 12-26 10:11:52 [registry.py:346]   File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/subprocess.py", line 502, in check_returncode
ERROR 12-26 10:11:52 [registry.py:346]     raise CalledProcessError(self.returncode, self.args, self.stdout,
ERROR 12-26 10:11:52 [registry.py:346] subprocess.CalledProcessError: Command '['/ephemeral/benchmark_venv/bin/python', '-m', 'vllm.model_executor.models.registry']' returned non-zero exit status 1.
ERROR 12-26 10:11:52 [registry.py:346] 
ERROR 12-26 10:11:52 [registry.py:346] The above exception was the direct cause of the following exception:
ERROR 12-26 10:11:52 [registry.py:346] 
ERROR 12-26 10:11:52 [registry.py:346] Traceback (most recent call last):
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 344, in _try_inspect_model_cls
ERROR 12-26 10:11:52 [registry.py:346]     return model.inspect_model_cls()
ERROR 12-26 10:11:52 [registry.py:346]            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 315, in inspect_model_cls
ERROR 12-26 10:11:52 [registry.py:346]     return _run_in_subprocess(
ERROR 12-26 10:11:52 [registry.py:346]            ^^^^^^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/registry.py", line 581, in _run_in_subprocess
ERROR 12-26 10:11:52 [registry.py:346]     raise RuntimeError(f"Error raised in subprocess:\n"
ERROR 12-26 10:11:52 [registry.py:346] RuntimeError: Error raised in subprocess:
ERROR 12-26 10:11:52 [registry.py:346] /ephemeral/benchmark_worktree/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:
ERROR 12-26 10:11:52 [registry.py:346] No module named 'vllm._version'
ERROR 12-26 10:11:52 [registry.py:346]   from .version import __version__, __version_tuple__  # isort:skip
ERROR 12-26 10:11:52 [registry.py:346] /ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
ERROR 12-26 10:11:52 [registry.py:346]   warnings.warn(
ERROR 12-26 10:11:52 [registry.py:346] /ephemeral/benchmark_worktree/vllm/utils.py:1236: SyntaxWarning: invalid escape sequence '\s'
ERROR 12-26 10:11:52 [registry.py:346]   single_newline = re.compile("(?<!\n)\n(?!\n)\s*")
ERROR 12-26 10:11:52 [registry.py:346] /ephemeral/benchmark_worktree/vllm/utils.py:1237: SyntaxWarning: invalid escape sequence '\s'
ERROR 12-26 10:11:52 [registry.py:346]   multiple_newlines = re.compile("\n{2,}\s*")
ERROR 12-26 10:11:52 [registry.py:346] Traceback (most recent call last):
ERROR 12-26 10:11:52 [registry.py:346]   File "<frozen runpy>", line 189, in _run_module_as_main
ERROR 12-26 10:11:52 [registry.py:346]   File "<frozen runpy>", line 112, in _get_module_details
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/__init__.py", line 12, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/engine/arg_utils.py", line 16, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/config.py", line 30, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/model_executor/__init__.py", line 3, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.model_executor.parameter import (BasevLLMParameter,
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/model_executor/parameter.py", line 9, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.distributed import get_tensor_model_parallel_rank
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/distributed/__init__.py", line 3, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from .communication_op import *
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/distributed/communication_op.py", line 8, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from .parallel_state import get_tp_group
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/distributed/parallel_state.py", line 122, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     from vllm.platforms import current_platform
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/platforms/__init__.py", line 271, in __getattr__
ERROR 12-26 10:11:52 [registry.py:346]     _current_platform = resolve_obj_by_qualname(
ERROR 12-26 10:11:52 [registry.py:346]                         ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/utils.py", line 2009, in resolve_obj_by_qualname
ERROR 12-26 10:11:52 [registry.py:346]     module = importlib.import_module(module_name)
ERROR 12-26 10:11:52 [registry.py:346]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346]   File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/importlib/__init__.py", line 90, in import_module
ERROR 12-26 10:11:52 [registry.py:346]     return _bootstrap._gcd_import(name[level:], package, level)
ERROR 12-26 10:11:52 [registry.py:346]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346]   File "/ephemeral/benchmark_worktree/vllm/platforms/cuda.py", line 15, in <module>
ERROR 12-26 10:11:52 [registry.py:346]     import vllm._C  # noqa
ERROR 12-26 10:11:52 [registry.py:346]     ^^^^^^^^^^^^^^
ERROR 12-26 10:11:52 [registry.py:346] ModuleNotFoundError: No module named 'vllm._C'
ERROR 12-26 10:11:52 [registry.py:346] 
