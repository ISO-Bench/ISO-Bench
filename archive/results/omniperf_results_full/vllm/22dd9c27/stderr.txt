=== BASELINE ===
Failed to start vLLM server
b'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File "<frozen runpy>", line 198, in _run_module_as_main\n  File "<frozen runpy>", line 88, in _run_code\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 39, in <module>\n    from vllm.config import VllmConfig\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 37, in <module>\n    from vllm.transformers_utils.config import (\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 33, in <module>\n    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/configs/__init__.py", line 26, in <module>\n    from vllm.transformers_utils.configs.ovis import OvisConfig\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/configs/ovis.py", line 76, in <module>\n    AutoConfig.register("aimv2", AIMv2Config)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1401, in register\n    CONFIG_MAPPING.register(model_type, config, exist_ok=exist_ok)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1081, in register\n    raise ValueError(f"\'{key}\' is already used by a Transformers config, pick another name.")\nValueError: \'aimv2\' is already used by a Transformers config, pick another name.\n'