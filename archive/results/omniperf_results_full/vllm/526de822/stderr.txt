=== BASELINE ===
/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: benchmark_latency.py [-h] [--input-len INPUT_LEN]
                            [--output-len OUTPUT_LEN]
                            [--batch-size BATCH_SIZE] [--n N]
                            [--use-beam-search]
                            [--num-iters-warmup NUM_ITERS_WARMUP]
                            [--num-iters NUM_ITERS] [--profile]
                            [--profile-result-dir PROFILE_RESULT_DIR]
                            [--output-json OUTPUT_JSON] [--model MODEL]
                            [--task {auto,generate,embedding,embed,classify,score,reward}]
                            [--tokenizer TOKENIZER] [--skip-tokenizer-init]
                            [--revision REVISION]
                            [--code-revision CODE_REVISION]
                            [--tokenizer-revision TOKENIZER_REVISION]
                            [--tokenizer-mode {auto,slow,mistral}]
                            [--trust-remote-code]
                            [--allowed-local-media-path ALLOWED_LOCAL_MEDIA_PATH]
                            [--download-dir DOWNLOAD_DIR]
                            [--load-format {auto,pt,safetensors,npcache,dummy,tensorizer,sharded_state,gguf,bitsandbytes,mistral,runai_streamer}]
                            [--config-format {auto,hf,mistral}]
                            [--dtype {auto,half,float16,bfloat16,float,float32}]
                            [--kv-cache-dtype {auto,fp8,fp8_e5m2,fp8_e4m3}]
                            [--quantization-param-path QUANTIZATION_PARAM_PATH]
                            [--max-model-len MAX_MODEL_LEN]
                            [--guided-decoding-backend {outlines,lm-format-enforcer,xgrammar}]
                            [--logits-processor-pattern LOGITS_PROCESSOR_PATTERN]
                            [--distributed-executor-backend {ray,mp}]
                            [--worker-use-ray]
                            [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                            [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                            [--max-parallel-loading-workers MAX_PARALLEL_LOADING_WORKERS]
                            [--ray-workers-use-nsight]
                            [--block-size {8,16,32,64,128}]
                            [--enable-prefix-caching | --no-enable-prefix-caching]
                            [--disable-sliding-window]
                            [--use-v2-block-manager]
                            [--num-lookahead-slots NUM_LOOKAHEAD_SLOTS]
                            [--seed SEED] [--swap-space SWAP_SPACE]
                            [--cpu-offload-gb CPU_OFFLOAD_GB]
                            [--gpu-memory-utilization GPU_MEMORY_UTILIZATION]
                            [--num-gpu-blocks-override NUM_GPU_BLOCKS_OVERRIDE]
                            [--max-num-batched-tokens MAX_NUM_BATCHED_TOKENS]
                            [--max-num-seqs MAX_NUM_SEQS]
                            [--max-logprobs MAX_LOGPROBS]
                            [--disable-log-stats]
                            [--quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,None}]
                            [--rope-scaling ROPE_SCALING]
                            [--rope-theta ROPE_THETA]
                            [--hf-overrides HF_OVERRIDES] [--enforce-eager]
                            [--max-seq-len-to-capture MAX_SEQ_LEN_TO_CAPTURE]
                            [--disable-custom-all-reduce]
                            [--tokenizer-pool-size TOKENIZER_POOL_SIZE]
                            [--tokenizer-pool-type TOKENIZER_POOL_TYPE]
                            [--tokenizer-pool-extra-config TOKENIZER_POOL_EXTRA_CONFIG]
                            [--limit-mm-per-prompt LIMIT_MM_PER_PROMPT]
                            [--mm-processor-kwargs MM_PROCESSOR_KWARGS]
                            [--disable-mm-preprocessor-cache] [--enable-lora]
                            [--enable-lora-bias] [--max-loras MAX_LORAS]
                            [--max-lora-rank MAX_LORA_RANK]
                            [--lora-extra-vocab-size LORA_EXTRA_VOCAB_SIZE]
                            [--lora-dtype {auto,float16,bfloat16}]
                            [--long-lora-scaling-factors LONG_LORA_SCALING_FACTORS]
                            [--max-cpu-loras MAX_CPU_LORAS]
                            [--fully-sharded-loras] [--enable-prompt-adapter]
                            [--max-prompt-adapters MAX_PROMPT_ADAPTERS]
                            [--max-prompt-adapter-token MAX_PROMPT_ADAPTER_TOKEN]
                            [--device {auto,cuda,neuron,cpu,openvino,tpu,xpu,hpu}]
                            [--num-scheduler-steps NUM_SCHEDULER_STEPS]
                            [--multi-step-stream-outputs [MULTI_STEP_STREAM_OUTPUTS]]
                            [--scheduler-delay-factor SCHEDULER_DELAY_FACTOR]
                            [--enable-chunked-prefill [ENABLE_CHUNKED_PREFILL]]
                            [--speculative-model SPECULATIVE_MODEL]
                            [--speculative-model-quantization {aqlm,awq,deepspeedfp,tpu_int8,fp8,fbgemm_fp8,modelopt,marlin,gguf,gptq_marlin_24,gptq_marlin,awq_marlin,gptq,compressed-tensors,bitsandbytes,qqq,hqq,experts_int8,neuron_quant,ipex,None}]
                            [--num-speculative-tokens NUM_SPECULATIVE_TOKENS]
                            [--speculative-disable-mqa-scorer]
                            [--speculative-draft-tensor-parallel-size SPECULATIVE_DRAFT_TENSOR_PARALLEL_SIZE]
                            [--speculative-max-model-len SPECULATIVE_MAX_MODEL_LEN]
                            [--speculative-disable-by-batch-size SPECULATIVE_DISABLE_BY_BATCH_SIZE]
                            [--ngram-prompt-lookup-max NGRAM_PROMPT_LOOKUP_MAX]
                            [--ngram-prompt-lookup-min NGRAM_PROMPT_LOOKUP_MIN]
                            [--spec-decoding-acceptance-method {rejection_sampler,typical_acceptance_sampler}]
                            [--typical-acceptance-sampler-posterior-threshold TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_THRESHOLD]
                            [--typical-acceptance-sampler-posterior-alpha TYPICAL_ACCEPTANCE_SAMPLER_POSTERIOR_ALPHA]
                            [--disable-logprobs-during-spec-decoding [DISABLE_LOGPROBS_DURING_SPEC_DECODING]]
                            [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                            [--ignore-patterns IGNORE_PATTERNS]
                            [--preemption-mode PREEMPTION_MODE]
                            [--served-model-name SERVED_MODEL_NAME [SERVED_MODEL_NAME ...]]
                            [--qlora-adapter-name-or-path QLORA_ADAPTER_NAME_OR_PATH]
                            [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                            [--collect-detailed-traces COLLECT_DETAILED_TRACES]
                            [--disable-async-output-proc]
                            [--scheduling-policy {fcfs,priority}]
                            [--override-neuron-config OVERRIDE_NEURON_CONFIG]
                            [--override-pooler-config OVERRIDE_POOLER_CONFIG]
                            [--compilation-config COMPILATION_CONFIG]
                            [--kv-transfer-config KV_TRANSFER_CONFIG]
                            [--worker-cls WORKER_CLS]
                            [--generation-config GENERATION_CONFIG]
benchmark_latency.py: error: argument --batch-size: invalid int value: 'BS'
