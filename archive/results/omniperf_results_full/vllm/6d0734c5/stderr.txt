=== BASELINE ===
/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/ephemeral/benchmark_worktree/benchmarks/benchmark_serving.py:728: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  tokenizer = get_tokenizer(
Traceback (most recent call last):
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_serving.py", line 1384, in <module>
    main(args)
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_serving.py", line 859, in main
    input_requests = dataset_mapping[args.dataset_name]()
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_serving.py", line 838, in <lambda>
    "sharegpt": lambda: ShareGPTDataset(
                        ^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_dataset.py", line 384, in __init__
    self.load_data()
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_dataset.py", line 388, in load_data
    raise ValueError("dataset_path must be provided for loading data.")
ValueError: dataset_path must be provided for loading data.
