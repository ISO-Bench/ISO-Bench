{
  "instance": {
    "commit_hash": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "commit_subject": "[Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)",
    "repo": "vllm",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2",
    "test_script": "#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ac45c44d98e77f30e47b8fb69134f4635183070d\nMessage: [Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)\n\nThis script measures the actual performance impact of the optimization.\nIt supports cross-commit comparison with functional equivalence checking.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport math\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        # Disable TF32 for reproducibility unless required\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"\")\n    \n    # Priority 2: Parse from commit metadata\n    if not (module_path and symbol_name):\n        # Based on the commit diff, target is DeepEPHTPrepareAndFinalize.prepare\n        module_path = \"vllm.model_executor.layers.fused_moe.deepep_ht_prepare_finalize\"\n        symbol_name = \"DeepEPHTPrepareAndFinalize\"\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = module\n        for attr in symbol_name.split(\".\"):\n            target = getattr(target, attr)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # MoE configuration for DeepSeek-like models\n    batch_size = 4\n    seq_len = 512  # Reduced for performance testing\n    hidden_size = 4096\n    num_experts = 8\n    top_k = 2\n    expert_intermediate_size = 14336\n    \n    # Create mock quantization config classes\n    class BlockQuantConfig:\n        def __init__(self, is_block=True):\n            self.is_block_quantized = is_block\n            self.per_act_token_quant = not is_block\n            self.quant_dtype = torch.int8\n            self.block_shape = (128, 128) if is_block else None\n    \n    # Create input tensors\n    hidden_states = torch.randn(batch_size * seq_len, hidden_size, device=device, dtype=dtype)\n    \n    # Router logits for expert selection\n    router_logits = torch.randn(batch_size * seq_len, num_experts, device=device, dtype=dtype)\n    \n    # Get top-k experts\n    topk_weights, topk_ids = torch.topk(router_logits, top_k, dim=-1)\n    topk_weights = torch.softmax(topk_weights, dim=-1)\n    \n    # Create expert weights (for MoE)\n    w1 = torch.randn(num_experts, hidden_size, expert_intermediate_size, device=device, dtype=dtype)\n    w2 = torch.randn(num_experts, expert_intermediate_size, hidden_size, device=device, dtype=dtype)\n    \n    # Mock scale parameters\n    a1_scale = None\n    a2_scale = None\n    \n    # Create mock config for block quantization (triggers the optimization)\n    quant_config_block = BlockQuantConfig(is_block=True)\n    # Create mock config for non-block (original path)\n    quant_config_orig = BlockQuantConfig(is_block=False)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"hidden_states\": hidden_states,\n        \"topk_weights\": topk_weights,\n        \"topk_ids\": topk_ids,\n        \"num_experts\": num_experts,\n        \"top_k\": top_k,\n        \"w1\": w1,\n        \"w2\": w2,\n        \"a1_scale\": a1_scale,\n        \"a2_scale\": a2_scale,\n        \"quant_config_block\": quant_config_block,\n        \"quant_config_orig\": quant_config_orig,\n        \"apply_router_weight_on_input\": True,\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized operation.\"\"\"\n    target_class, fq_name = resolve_target()\n    \n    # Instantiate the target class\n    try:\n        prepare_finalize = target_class()\n    except:\n        # Fallback: return mock result if class cannot be instantiated\n        return {\n            \"expert_x\": data[\"hidden_states\"].clone(),\n            \"expert_x_scale\": None,\n            \"opt_path_hit\": False\n        }\n    \n    # Use block quantization config to trigger the optimized path\n    quant_config = data[\"quant_config_block\"]\n    \n    with torch.no_grad():\n        try:\n            # Call the prepare method with the workload\n            result = prepare_finalize.prepare(\n                a1=data[\"hidden_states\"],\n                topk_ids=data[\"topk_ids\"],\n                topk_weights=data[\"topk_weights\"],\n                num_experts=data[\"num_experts\"],\n                a1_scale=data[\"a1_scale\"],\n                a2_scale=data[\"a2_scale\"],\n                quant_config=quant_config,\n                apply_router_weight_on_input=data[\"apply_router_weight_on_input\"]\n            )\n            \n            # Result is a tuple: (expert_x, expert_x_scale, ...)\n            if isinstance(result, tuple):\n                result = {\n                    \"expert_x\": result[0],\n                    \"expert_x_scale\": result[1] if len(result) > 1 else None,\n                    \"opt_path_hit\": True\n                }\n        except Exception as e:\n            # Fallback for missing dependencies\n            result = {\n                \"expert_x\": data[\"hidden_states\"].clone(),\n                \"expert_x_scale\": None,\n                \"opt_path_hit\": False,\n                \"error\": str(e)\n            }\n    \n    return result\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    if isinstance(result, dict):\n        save_dict = {}\n        for k, v in result.items():\n            if isinstance(v, torch.Tensor):\n                save_dict[k] = v.cpu()\n            else:\n                save_dict[k] = v\n        torch.save({\"type\": \"dict\", \"data\": save_dict}, filepath)\n    elif isinstance(result, torch.Tensor):\n        torch.save({\"type\": \"tensor\", \"data\": result.cpu()}, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    if isinstance(current_result, dict) and isinstance(reference_result, dict):\n        # Check dictionary keys match\n        assert set(current_result.keys()) == set(reference_result.keys()), \\\n            f\"Keys mismatch: {current_result.keys()} vs {reference_result.keys()}\"\n        \n        for key in current_result:\n            if key in [\"opt_path_hit\", \"error\"]:\n                continue  # Skip metadata fields\n            \n            curr_val = current_result[key]\n            ref_val = reference_result[key]\n            \n            if isinstance(curr_val, torch.Tensor) and isinstance(ref_val, torch.Tensor):\n                assert curr_val.shape == ref_val.shape, f\"Shape mismatch for {key}\"\n                assert curr_val.dtype == ref_val.dtype, f\"Dtype mismatch for {key}\"\n                \n                # Determine tolerances based on dtype\n                if curr_val.dtype in (torch.float16, torch.bfloat16):\n                    rtol, atol = 1e-3, 1e-4\n                else:\n                    rtol, atol = 1e-5, 1e-7\n                \n                torch.testing.assert_close(\n                    curr_val.cpu(),\n                    ref_val.cpu(),\n                    rtol=rtol, atol=atol\n                )\n    elif isinstance(current_result, torch.Tensor) and isinstance(reference_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape\n        assert current_result.dtype == reference_result.dtype\n        \n        # Determine tolerances based on dtype\n        if current_result.dtype in (torch.float16, torch.bfloat16):\n            rtol, atol = 1e-3, 1e-4\n        else:\n            rtol, atol = 1e-5, 1e-7\n        \n        torch.testing.assert_close(\n            current_result.cpu(),\n            reference_result.cpu(),\n            rtol=rtol, atol=atol\n        )\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_gpu(func, warmup=5, iterations=50) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time GPU operations with CUDA events.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n        torch.cuda.synchronize()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        torch.cuda.synchronize()\n        start.record()\n        result = func()\n        end.record()\n        torch.cuda.synchronize()\n        \n        times_ms.append(start.elapsed_time(end))\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95)],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99)],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms)\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Timing\n    if hw_info[\"device\"] == \"cuda\":\n        warmup = 5\n        iters = 50\n        result, timing_stats = time_gpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n        avg_ms = timing_stats[\"avg_ms\"]\n        p50_ms = timing_stats[\"p50_ms\"]\n        p95_ms = timing_stats[\"p95_ms\"]\n    else:\n        warmup = 3\n        iters = 10\n        # CPU warmup\n        for _ in range(warmup):\n            _ = experiment(data)\n        # CPU timing\n        times = []\n        for _ in range(iters):\n            start = time.perf_counter()\n            result = experiment(data)\n            times.append((time.perf_counter() - start) * 1000)\n        times.sort()\n        avg_ms = sum(times) / len(times)\n        p50_ms = times[len(times) // 2]\n        p95_ms = times[int(len(times) * 0.95) - 1] if len(times) > 1 else times[0]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ac45c44d98e77f30e47b8fb69134f4635183070d\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Check if optimization path was hit\n    opt_path_hit = True\n    if isinstance(result, dict) and \"opt_path_hit\" in result:\n        opt_path_hit = result[\"opt_path_hit\"]\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(hw_info[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": opt_path_hit\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)",
    "files_changed": [
      "vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py"
    ],
    "pr_url": "https://github.com/vllm-project/vllm/pull/21837",
    "models": [
      "deepseek-ai/DeepSeek-V2",
      "deepseek-ai/DeepSeek-V3"
    ]
  },
  "result": {
    "status": "baseline_failed",
    "commit_hash": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "benchmark_type": "serving",
    "baseline_metrics": {
      "request_throughput": null,
      "token_throughput": null,
      "output_throughput": null,
      "ttft_mean": null,
      "ttft_p50": null,
      "ttft_p90": null,
      "ttft_p99": null,
      "tpot_mean": null,
      "tpot_p50": null,
      "tpot_p90": null,
      "tpot_p99": null,
      "itl_mean": null,
      "itl_p50": null,
      "itl_p90": null,
      "itl_p99": null,
      "e2e_latency_mean": null,
      "e2e_latency_p50": null,
      "e2e_latency_p90": null,
      "e2e_latency_p99": null,
      "latency_ms": null,
      "raw_output": "\nFailed to start vLLM server\nb'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\\n  warnings.warn(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s factor field must be a float >= 1, got 40\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s beta_fast field must be a float, got 32\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s beta_slow field must be a float, got 1\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s factor field must be a float >= 1, got 40\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s beta_fast field must be a float, got 32\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m `rope_scaling`\\'s beta_slow field must be a float, got 1\\n/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\\n  warnings.warn(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m Process EngineCore_0:\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m Traceback (most recent call last):\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.run()\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py\", line 108, in run\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self._target(*self._args, **self._kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 671, in run_engine_core\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     raise e\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 658, in run_engine_core\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 472, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 77, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.model_executor = executor_class(vllm_config)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 53, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self._init_executor()\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 49, in _init_executor\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.collective_rpc(\"load_model\")\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 58, in collective_rpc\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 2954, in run_method\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     return func(*args, **kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 212, in load_model\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1962, in load_model\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.model = model_loader.load_model(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 44, in load_model\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     model = initialize_model(vllm_config=vllm_config,\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/model_loader/utils.py\", line 63, in initialize_model\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 736, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.model = DeepseekV2Model(vllm_config=vllm_config,\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 183, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 674, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                                                     ^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 640, in make_layers\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 676, in <lambda>\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     lambda prefix: DeepseekV2DecoderLayer(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 583, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.mlp = DeepseekV2MoE(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                ^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/models/deepseek_v2.py\", line 149, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.experts = FusedMoE(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                    ^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 788, in __init__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     self.quant_method.create_weights(layer=self, **moe_quant_params)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py\", line 283, in create_weights\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     w13_weight = torch.nn.Parameter(torch.empty(\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m                                     ^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m     return func(*args, **kwargs)\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(EngineCore_0 pid=158792)\\x1b[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.69 GiB. GPU 0 has a total capacity of 79.11 GiB of which 2.59 GiB is free. Process 320661 has 76.51 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 18.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\\n[rank0]:[W1226 07:50:23.839258791 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m Traceback (most recent call last):\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"<frozen runpy>\", line 198, in _run_module_as_main\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"<frozen runpy>\", line 88, in _run_code\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1881, in <module>\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     uvloop.run(run_server(args))\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 96, in run\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return __asyncio.run(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 195, in run\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return runner.run(main)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 118, in run\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return self._loop.run_until_complete(task)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py\", line 48, in wrapper\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return await main\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1813, in run_server\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 1833, in run_server_worker\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     async with build_async_engine_client(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return await anext(self.gen)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 165, in build_async_engine_client\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     async with build_async_engine_client_from_engine_args(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 210, in __aenter__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return await anext(self.gen)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py\", line 205, in build_async_engine_client_from_engine_args\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     async_llm = AsyncLLM.from_vllm_config(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/utils/__init__.py\", line 1519, in inner\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return fn(*args, **kwargs)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 170, in from_vllm_config\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return cls(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py\", line 118, in __init__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 100, in make_async_mp_client\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     return AsyncMPClient(*client_args)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 731, in __init__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     super().__init__(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py\", line 420, in __init__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     with launch_core_engines(vllm_config, executor_class,\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py\", line 144, in __exit__\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     next(self.gen)\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 697, in launch_core_engines\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     wait_for_engine_startup(\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m   File \"/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py\", line 750, in wait_for_engine_startup\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m     raise RuntimeError(\"Engine core initialization failed. \"\\n\\x1b[1;36m(APIServer pid=158670)\\x1b[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}\\n'"
    },
    "patched_metrics": null,
    "improvement": null,
    "error_message": "Baseline produced no metrics",
    "duration_s": 42.413618087768555,
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2",
    "patch_path": "/root/OmniPerf-Bench/perf-agents-bench/state/runs/vllm/claude_code/default/2025-12-23_04-37-32/vllm_core-0058/model_patch.diff"
  }
}