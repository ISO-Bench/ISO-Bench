=== BASELINE ===
Failed to start vLLM server
b'/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status\n    response.raise_for_status()\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "<frozen runpy>", line 198, in _run_module_as_main\n  File "<frozen runpy>", line 88, in _run_code\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 909, in <module>\n    uvloop.run(run_server(args))\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run\n    return __asyncio.run(\n           ^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper\n    return await main\n           ^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 873, in run_server\n    async with build_async_engine_client(args) as engine_client:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 134, in build_async_engine_client\n    async with build_async_engine_client_from_engine_args(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 215, in build_async_engine_client_from_engine_args\n    engine_config = engine_args.create_engine_config()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1059, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 983, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 286, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 183, in get_config\n    if is_gguf or file_or_path_exists(\n                  ^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 102, in file_or_path_exists\n    return file_exists(model,\n           ^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py", line 3018, in file_exists\n    get_hf_file_metadata(url, token=token)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper\n    hf_raise_for_status(response)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-694e3b9b-5b78702c0c5c7d72513c0d50;573b2f5c-1bfb-4d7c-b7ac-7e7cc39ac4a0)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo\'s authors.\n/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status\n    response.raise_for_status()\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap\n    self.run()\n  File "/root/.local/share/uv/python/cpython-3.12.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/process.py", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 389, in run_mp_engine\n    raise e\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 378, in run_mp_engine\n    engine = MQLLMEngine.from_engine_args(engine_args=engine_args,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/multiprocessing/engine.py", line 116, in from_engine_args\n    engine_config = engine_args.create_engine_config(usage_context)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1059, in create_engine_config\n    model_config = self.create_model_config()\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 983, in create_model_config\n    return ModelConfig(\n           ^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 286, in __init__\n    hf_config = get_config(self.model, trust_remote_code, revision,\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 183, in get_config\n    if is_gguf or file_or_path_exists(\n                  ^^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 102, in file_or_path_exists\n    return file_exists(model,\n           ^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py", line 3018, in file_exists\n    get_hf_file_metadata(url, token=token)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata\n    r = _request_wrapper(\n        ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper\n    response = _request_wrapper(\n               ^^^^^^^^^^^^^^^^^\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper\n    hf_raise_for_status(response)\n  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status\n    raise _format(GatedRepoError, message, response) from e\nhuggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-694e3b9f-156306df0774128b1e474304;6094e253-fe07-4894-b2fc-888423dbcd55)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\nYour request to access model meta-llama/Llama-3.1-8B-Instruct has been rejected by the repo\'s authors.\n'