=== BASELINE ===
/ephemeral/benchmark_venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Traceback (most recent call last):
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_latency.py", line 150, in <module>
    main(args)
  File "/ephemeral/benchmark_worktree/benchmarks/benchmark_latency.py", line 27, in main
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/utils.py", line 1039, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 239, in __init__
    self.llm_engine = self.engine_class.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 86, in from_engine_args
    vllm_config = engine_args.create_engine_config(usage_context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1045, in create_engine_config
    model_config = self.create_model_config()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 970, in create_model_config
    return ModelConfig(
           ^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/config.py", line 282, in __init__
    hf_config = get_config(self.model, trust_remote_code, revision,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 182, in get_config
    if is_gguf or file_or_path_exists(
                  ^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 91, in file_or_path_exists
    cached_filepath = try_to_load_from_cache(repo_id=model,
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/ephemeral/benchmark_venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/users/ktong/llama/llm_8b_oss'. Use `repo_type` argument if needed.
