=== BASELINE ===
INFO 12-25 09:13:00 [__init__.py:235] Automatically detected platform cuda.
Namespace(subparser='bench', bench_type='serve', dispatch_function=<function BenchmarkServingSubcommand.cmd at 0x72eb54b2f1a0>, seed=0, num_prompts=100, dataset_name='random', no_stream=False, dataset_path=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=700, random_output_len=1, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, endpoint_type='openai', label=None, backend='vllm', base_url=None, host='localhost', port=8000, endpoint='/v1/completions', max_concurrency=None, model='facebook/opt-125m', tokenizer=None, use_beam_search=False, logprobs=None, request_rate=200.0, burstiness=1.0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=True, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name='facebook/opt-125m', lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None)
INFO 12-25 09:13:06 [datasets.py:355] Sampling input_len from [699, 699] and output_len from [1, 1]
Starting initial single prompt test run...
