%%%%%%%% ICML 2026 ISO-Bench PAPER %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2026}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{listings}
\usepackage[most]{tcolorbox}

% Custom blue for GSO-style boxes
\definecolor{gsoboxblue}{HTML}{1f78b4}

% GSO-style box definition
\newtcolorbox{gsobox}[1][]{
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries,
  attach boxed title to top left={yshift=-2mm, xshift=0mm},
  boxed title style={
    colback=gsoboxblue,
    sharp corners,
    boxrule=0pt,
  },
  sharp corners,
  boxrule=0.5pt,
  left=6pt,
  right=6pt,
  top=4pt,
  bottom=4pt,
  breakable,
  #1
}

% Colors for diff syntax highlighting (using RGB for compatibility)
\definecolor{diffadd}{RGB}{34,134,58}      % green for added lines
\definecolor{diffrem}{RGB}{203,36,49}      % red for removed lines
\definecolor{diffhunk}{RGB}{111,66,193}    % purple for @@ lines
\definecolor{difffile}{RGB}{0,92,197}      % blue for file headers
\definecolor{codebg}{RGB}{246,248,250}     % light gray background

% Listings configuration for diff
\lstdefinelanguage{diff}{
  morecomment=[f][\color{difffile}]{diff\ },
  morecomment=[f][\color{difffile}]{index\ },
  morecomment=[f][\color{difffile}]{---\ },
  morecomment=[f][\color{difffile}]{+++\ },
  morecomment=[f][\color{diffhunk}]{@@},
  morecomment=[f][\color{diffadd}]{+},
  morecomment=[f][\color{diffrem}]{-},
  morecomment=[f][\color{black}]{\ },
}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development
\usepackage[textsize=tiny]{todonotes}

% Running title
\icmltitlerunning{ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?}

\begin{document}

\twocolumn[
  \icmltitle{ISO-Bench: Can Coding Agents Optimize Real-World Inference Workloads?}

  % Authors will be added for camera-ready version
  \icmlsetsymbol{equal}{*}

  \begin{icmlauthorlist}
    \icmlauthor{Anonymous Authors}{anon}
  \end{icmlauthorlist}

  \icmlaffiliation{anon}{Anonymous Institution}

  \icmlcorrespondingauthor{Anonymous}{anonymous@email.com}

  \icmlkeywords{AI Coding Agents, Performance Optimization, Benchmarking, Software Engineering, Code Generation}

  \vskip 0.3in
]


\begin{abstract}
We introduce ISO-Bench, a benchmark for coding agents to test their capabilities on real-world inference optimization tasks. These tasks were taken from vLLM and SGLang, two of the most popular LLM serving frameworks. Each task provides an agent with a codebase and bottleneck description, whereby the agent must produce an optimization patch evaluated against expert human solutions. We curated 50+ tasks from merged pull requests with measurable performance improvements. While existing benchmarks heavily use runtime-based metrics, such approaches can be gamed to pass tests without capturing the actual intent of the code changes. Therefore, we combine both hard (execution-based) and soft (LLM-based) metrics to show that both are necessary for complete evaluation. While evaluating both closed and open-source coding agents, we observe that closed-source alternatives outperform open-source agents. Furthermore, even agents with the same underlying model can differ substantially in performance, suggesting scaffolding is as important as the model.
\end{abstract}

% ISO-Bench Introduction - Final Version with Citations
% Compile with: pdflatex -> bibtex -> pdflatex -> pdflatex

\section{Introduction}
LLM inference engines have become essential for deploying large language models at scale. Production workloads in industry and research are done by systems like vLLM \citep{kwon2023vllm} and SGLang \citep{zheng2023sglang}, which achieve high throughput through systems-level optimisations. Techniques such as PagedAttention~\citep{kwon2023vllm} and FlashAttention~\citep{dao2022flashattention} required extensive work and deep expertise in memory management, kernel development, and scheduling. The need for optimisation is growing as more models are produced and model architectures change. 
LLM-based coding agents have become powerful tools for software engineering, capable of finding bugs and generating patches across codebases which raises the question of whether they can help with optimization work. Systems like SWE-Agent~\citep{yang2024sweagent} and OpenHands~\citep{wang2025openhands} perform well on benchmarks like SWE-bench~\citep{jimenez2024swebench}.
However, recent benchmarks suggest these agents still struggle with optimization tasks. KernelBench~\citep{ouyang2025kernelbench} finds that frontier models match GPU kernel baselines in under 20\% of cases, GSO~\citep{shetty2025gso} reports success rates below 5\% on repository-level tasks, and SWE-Perf~\citep{he2025sweperf} observes large gaps between agent and expert solutions.
These benchmarks measure whether agents succeed, but not why they fail. 
GSO takes a step further by combining execution metrics with semantic analysis, but a key question remains: when agents fail, do they misunderstand the problem, or do they understand it but struggle to implement the solution? 

In this work, We present ISO-Bench, a benchmark of 54 optimization tasks from vLLM and SGLang.
Beyond measuring throughput gains, we evaluate whether agents target the correct bottleneck and use appropriate strategies.
This dual evaluation separates true successes from lucky wins.

Our contributions are as follows:
\begin{enumerate}
    \item \textbf{Benchmarking}: 54 optimization tasks extracted from real commits in vLLM and SGLang. Each task includes a repository snapshot, throughput and latency benchmarks, and correctness tests.
    \item \textbf{Dual evaluation framework}: Hard and soft metrics are introduced that distinguish true successes from lucky wins, showing that traditional metrics might overestimate agent capabilities.
    \item \textbf{Behavioral insights}:  Identification of an understanding-execution gap as a primary failure mode and showing that agent performance varies substantially across codebases.
\end{enumerate}

% We introduce \textbf{ISO-Bench} (Inference System Optimization Benchmark), a benchmark for evaluating coding agents on optimization tasks from production ML inference engines.
% We extract performance-critical commits from vLLM and SGLang, filtering for measurable performance impact.
% After manual curation, we obtain 54 optimization tasks: 39 from vLLM and 15 from SGLang.
% Each task provides a repository snapshot, the target optimization from expert developers, automated performance tests, and functional validation through GSM8K~\citep{cobbe2021gsm8k} accuracy using the LM Evaluation Harness~\citep{eval-harness}.

% Beyond execution-based metrics, we introduce a dual evaluation framework combining hard metrics with soft metrics.
% Hard metrics measure whether performance improves.
% Soft metrics assess semantic alignment: does the patch target the correct bottleneck?
% This framework separates \textit{true successes}, where agents both improve performance and demonstrate correct understanding, from \textit{lucky wins}, where performance improves through unrelated changes.

% We evaluate four agents representing different architectures and scaffolding approaches: Claude Code, Codex CLI, and two configurations of TRAE-Agent~\citep{gao2025traeagent}.
% Our evaluation reveals that hard metrics alone overestimate agent capability by 10--20\% due to lucky wins.
% We also find that agents demonstrate high understanding of optimization targets but struggle to execute solutions, and that agent rankings reverse between vLLM and SGLang, suggesting optimization capabilities do not generalize uniformly across codebases.


% Recent advances in large language models have enabled autonomous coding agents capable of repository-level software engineering tasks. Systems like OpenHands~\citep{wang2025openhands} and SWE-Agent~\citep{yang2024sweagent} achieve up to  76.2\% success on SWE-bench Verified~\citep{jimenez2024swebench,chowdhury2024swebenchverified}   with Gemini 3 Pro, a benchmark of real-world GitHub issues requiring multi-file changes and repository understanding. However, these benchmarks emphasize functional correctness (i.e., whether code generates the correct output) rather than performance optimization, where the objective is to reduce execution time or memory consumption.

% This gap is significant for production machine learning systems and automated development using agents. Modern LLM inference engines require aggressive optimization to meet deployment cost and latency constraints. Techniques like PagedAttention~\citep{kwon2023vllm} and 
% FlashAttention~\citep{dao2022flashattention} have achieved multiple improvements in serving throughput and training speed, but required systems-level co-design across memory management, scheduling, and GPU kernel implementation. These are capabilities distinct from functional code generation.

% Recent work has begun addressing performance-aware code generation. KernelBench~\citep{ouyang2025kernelbench} evaluates LLMs on generating efficient GPU kernels across 250 PyTorch workloads, finding that frontier models match baseline performance in less than 20\% of cases. TritonBench~\citep{li2025tritonbench} focuses on Triton operator generation with 184 real-world operators, revealing substantial gaps in producing efficient low-level code. Moving to general software optimization, GSO~\citep{shetty2025gso} curates 102 tasks across 10 repositories using commit history mining, reporting less than 5\% agent success. SWE-Perf~\citep{he2025sweperf} provides 140 instances from performance-enhancing GitHub pull requests, evaluating file-level and repository-level approaches.

% While these benchmarks establish that agents struggle with optimization, they report primarily aggregate success rates. This leaves open critical questions: \textbf{Why do agents fail?} What behavioral patterns distinguish successful optimizations from catastrophic failures? Do agent capabilities generalize uniformly across codebases of different complexity?

% We introduce \textbf{ISO-Bench}, a benchmark of 50+ performance optimization tasks extracted from production ML inference engines: 39 from vLLM~\citep{kwon2023vllm} and 15 from SGLang~\citep{zheng2023sglang}. We have an automated pipeline that fetch performance-critical commits followed by manual curation and evaluating agents on reproducing expert-written optimizations. We evaluate agents using vLLM's standard benchmarks—serving (TTFT, throughput), offline throughput, and latency—comparing agent patches against both the unoptimized baseline and human-written optimizations. Beyond measuring success rates, we track behavioral metrics including commit counts, scope violations, and time-to-first-edit, enabling detailed analysis of failure modes.

% We evaluate three agent architectures representing different design points: Claude Code, Codex, Trae Agent.


% Our contributions are threefold. \textbf{(i) Benchmark:} 179 real-world optimization tasks from production ML inference engines, with automated test generation, git-based isolation, and comprehensive behavioral metrics beyond binary success/failure. \textbf{(ii) Behavioral insights:} We identify instant-edit pathology (time-to-first-edit $<1$s predicts 95\% failure), and commit explosion (median 234, max 7,755) that challenge assumptions about agent generalization. \textbf{(iii) Multi-agent comparison:} Evaluation of three architectures on identical tasks reveals cost-observability-success trade-offs critical for deployment decisions.

%\section{Related Work}
%\label{sec:related}

%We review prior work on evaluating and building LLM systems for programming, focusing on two aspects: (i)~benchmarks evolving from unit-test correctness on isolated functions to executable, repository-scale tasks; (ii)~benchmarks targeting \emph{efficiency} rather than correctness alone.

%\paragraph{Correctness-driven benchmarks:} Early code-generation benchmarks measure functional correctness at the function level. \textbf{HumanEval}~\citep{chen2021evaluating} introduced 164 hand-crafted Python problems with unit tests and popularized the \texttt{pass@$k$ metric}, which measures whether at least one of $k$ sampled solutions passes all tests.
%Repository-scale benchmarks are more realistic as agents must navigate the full codebase, find the relevant code, and edit multiple files. \textbf{SWE-bench}~\citep{jimenez2024swebench} constructs tasks from real GitHub issues in popular Python repositories, requiring agents to pass executable tests. \textbf{SWE-bench Verified}~\citep{chowdhury2024swebenchverified} filters for reproducible evaluation and has become a standard target for coding agents.
%\paragraph{Efficiency-driven benchmarks:}
%Optimizing for performance is a different problem: agents must find the bottleneck, and success is measured by actual speedup rather than just correctness.

%At the \emph{kernel level}, \textbf{KernelBench}~\citep{ouyang2025kernelbench} evaluates LLMs on generating efficient GPU kernels for 250 PyTorch ML workloads.
%It introduces the \texttt{fast\_p} metric, counting solutions that are both correct and achieve at least $p\times$ speedup over a PyTorch baseline; even strong models succeed on fewer than 20\% of tasks.
%\textbf{TritonBench}~\citep{li2025tritonbench} complements this with two evaluation testcases: \textit{TritonBench-G} (GitHub-sourced operators) and \textit{TritonBench-T} (PyTorch-aligned tasks), profiling Triton code against reference implementations and reporting both correctness and GPU efficiency.
%At the \emph{repository level}, several recent benchmarks ask whether agents can optimize real codebases. \textbf{SWE-Perf}~\citep{he2025sweperf} constructs tasks from performance-improving pull requests, checking that patches apply cleanly, pass tests, and yield measurable speedups. \textbf{GSO}~\citep{shetty2025gso} anchors evaluation to human expert commits rather than fixed thresholds. \textbf{SWE-fficiency}~\citep{ma2025swefficiencylanguagemodelsoptimize} scales this approach to more Python libraries, reporting how close agents get to expert-level improvements. Across all three, agents struggle to locate bottlenecks and reason about low-level performance.

\section{Related Work}
\label{sec:related}

We review prior work on evaluating and building LLM systems for programming, focusing on two aspects: (i)~benchmarks evolving from unit-test correctness on isolated functions to executable, repository-scale tasks  (ii)~benchmarks targeting \emph{efficiency}.

\paragraph{Correctness-driven benchmarks:}Early benchmarks for code generation focused on measuring functional correctness for standalone functions. \textbf{HumanEval}~\citep{chen2021evaluating} introduced 164 hand-crafted Python problems with unit tests and popularized the pass@$k$ metric, which measures whether at least one of $k$ sampled solutions passes all tests.However, such function-level benchmarks evaluate code generation in isolation, separate from the complexities of real software development. 
Repository-scale benchmarks are more realistic as agents must navigate the full codebase, find the relevant code, and edit multiple files. \textbf{SWE-bench}~\citep{jimenez2024swebench} constructs tasks from real GitHub issues in popular Python repositories, requiring agents to pass executable tests. \textbf{SWE-bench Verified}~\citep{chowdhury2024swebenchverified} filters for reproducible evaluation and has become a standard target for coding agents.
\paragraph{Efficiency-driven benchmarks:}
Optimizing for performance is a different problem: agents must find the bottleneck, and success is measured by actual speedup rather than just correctness.
At the kernel level, KernelBench \citep{ouyang2025kernelbench} evaluates LLMs on generating efficient GPU kernels for 250 PyTorch ML workloads, introducing the \texttt{fast\_p} metric to count solutions that are both correct and achieve at least $p\times p$
 speedup over a PyTorch baseline; even strong models succeed on fewer than 20\% of tasks. Complementing this,
TritonBench \citep{li2025tritonbench} offers two evaluation suites: \textit{TritonBench-G} (GitHub-sourced operators) and \textit{TritonBench-T} (PyTorch-aligned tasks) ,profiling Triton code against reference implementations and reporting both correctness and GPU efficiency.
\\\\
Moving to the repository level, several recent benchmarks examine whether agents can optimize real-world codebases. SWE-Perf \citep{he2025sweperf} constructs tasks from performance-improving pull requests, verifying that patches apply cleanly, pass tests, and yield measurable speedups. GSO \citep{shetty2025gso} takes a different approach by anchoring evaluation to human expert commits rather than fixed thresholds, while SWE-fficiency \citep{ma2025swefficiencylanguagemodelsoptimize} scales this methodology across a broader set of Python libraries, reporting how close agents come to expert-level improvements. Across all three repository-level benchmarks, agents consistently struggle to locate bottlenecks and reason about low-level performance characteristics.



% \paragraph{Agents and scaffolds for repository-scale coding.}
% Strong results on repository benchmarks depend as much on the agent loop as on the base model: systems must search code, form hypotheses, execute tests or benchmarks, and iteratively refine patches.
% \textbf{SWE-Agent}~\citep{yang2024sweagent} demonstrates that the agent--computer interface (ACI)—how the agent reads files, edits code, and observes execution results—has a first-order impact on task success.
% \textbf{OpenHands}~\citep{wang2024openhands} provides an open-source framework for end-to-end repository interaction, facilitating comparison of prompts, tools, and planning strategies.
% Complementary approaches (e.g., staged or ``agentless'' pipelines) decompose the problem into localization, patch synthesis, and validation, trading interactive exploration for structured inference.


% \section{Related Work}
% \label{sec:related}

% This section reviews prior work on evaluating and building LLM systems for programming, with emphasis on (i) benchmarks that move from unit-test correctness on isolated functions to executable, repository-scale tasks, (ii) benchmarks where the target is \emph{efficiency} rather than only correctness, and (iii) agent scaffolds for end-to-end coding (search, edit, test, and iterate).

% \paragraph{Correctness-driven benchmarks: from functions to repositories.}
% Early code-generation benchmarks primarily measure functional correctness at the function level. \textbf{HumanEval}~\citep{chen2021evaluating} introduced 164 hand-crafted Python problems paired with unit tests and popularized pass@k, which evaluates whether at least one of $k$ sampled solutions passes the tests.

% Repository-scale benchmarks increase realism by requiring systems to understand project structure, localize relevant code, and make integrated changes. \textbf{SWE-bench}~\citep{jimenez2024swebench} constructs tasks from real GitHub issues across popular Python repositories: models must diagnose the issue, edit the codebase (often across files), and pass an executable evaluation (tests). Variants such as \emph{Verified} further emphasize evaluation reliability by filtering for instances with stable, reproducible tests, and have become a standard target for coding agents.

% \paragraph{Efficiency-driven benchmarks: performance as objective.}
% Optimizing for performance changes the problem qualitatively: progress depends on profiling and bottleneck localization, and improvements must be validated by measured speedups rather than semantic equivalence alone.

% \textbf{KernelBench}~\citep{ouyang2025kernelbench} evaluates LLMs on generating efficient GPU kernels for \textbf{250} PyTorch ML workloads and introduces the \texttt{fast\_p} metric, which counts solutions that are both correct and achieve at least a $p\times$ speedup over a PyTorch baseline; their results show that even strong models match the baseline in fewer than \textbf{20\%} of tasks.


% \textbf{TritonBench}~\citep{li2025tritonbench} benchmarks Triton operator generation in a \textbf{dual-channel} setup (\emph{TritonBench-G} from real GitHub operators and \emph{TritonBench-T} with PyTorch-aligned tasks), emphasizing performance profiling against reference implementations on NVIDIA GPUs and reporting both correctness and efficiency metrics (e.g., execution accuracy, speedup, GPU efficiency).


% \textbf{Repository-level optimization.}
% Several recent benchmarks aim to capture real-world software optimization where the agent must operate inside an existing repository and improve end-to-end runtime.

% \textbf{SWE-Perf}~\citep{he2025sweperf} builds optimization instances mined from performance-improving pull requests.
% Each instance pairs a repository snapshot with an executable evaluation harness that checks (i) patch applicability, (ii) correctness via tests, and (iii) measured performance improvement.
% Importantly, it supports evaluation in both an oracle setting (where the system is given strong context such as the relevant files) and a realistic setting where the system must discover the bottleneck within the repo.

% \textbf{GSO}~\citep{shetty2025gso} curates optimization tasks by mining performance-related commits and constructing accompanying performance tests.
% A distinguishing feature is \emph{expert-anchored} evaluation: tasks come with human optimization commits, enabling comparisons against expert performance rather than a fixed speed threshold.
% Their analysis shows that current methods often fail to find the bottleneck and to reason about low-level performance effects.

% \textbf{SWE-fficiency}~\citep{ma2025swefficiencylanguagemodelsoptimize} scales commit/PR mining to a larger set of optimization tasks across widely used Python libraries and measures model improvements relative to expert patches (via a speedup ratio).
% The benchmark is end-to-end: systems must run a real workload, locate the slow code, change the implementation, and then verify both correctness and the measured speedup under the same evaluation setup.


% \paragraph{Agents and scaffolds for repository-scale coding.}
% Achieving strong results on repository benchmarks often depends as much on the agent loop as on the base model: systems must search code, form hypotheses, run tests/benchmarks, and iteratively refine patches.

% \textbf{SWE-Agent}~\citep{yang2024sweagent} argues that the agent--computer interface (ACI)—how the agent reads files, edits code, executes commands, and observes results—has a first-order impact on success in realistic software tasks.
% \textbf{OpenHands}~\citep{wang2024openhands} provides an open-source agent framework designed for end-to-end repository interaction, making it easier to compare prompts, tools, and planning strategies under a common interface.
% Complementary approaches (e.g., staged or “agentless” pipelines) decompose the problem into localization, patch synthesis, and validation, trading interactive exploration for structured inference.

\section{ISO-Bench}
\label{sec:iso-bench}
Existing benchmarks evaluate either standalone kernel generation (KernelBench, TritonBench) or general repository optimization (GSO, SWE-Perf, SWE-fficiency), but none target the specific challenges of GPU-based inference serving systems.
\\\\
ISO-Bench fills this gap by focusing on execution-based metrics for real-world, GPU-based inference optimization workloads.
In this section, We describe how tasks are formulated, how the benchmark was constructed, and how we evaluate agent performance. 
\subsection{Task Formulation}
\label{sec:task-formulation}

Each ISO-Bench task presents an agent with two inputs: (i)~the repository at a pre-optimization commit state, (ii)~a task description explaining which performance bottleneck to address without revealing the solution. The agent must produce an optimization patch that improves performance on the specified benchmark. We evaluate agent patches against human expert solutions from the original pull requests.

\subsection{Benchmark Construction}
\label{sec:benchmark-construction}

We collect optimization tasks from two production ML inference engines: \textbf{vLLM}~\citep{kwon2023vllm} and \textbf{SGLang}~\citep{zheng2023sglang}. We selected vLLM and SGLang primarily because they are widely used inference engines. This enables a realistic evaluation of agent performance on production-grade inference optimization tasks.

\paragraph{Commit Extraction.} We use a GSO-inspired pipeline to identify performance-related commits through keyword filtering (terms like \texttt{optim}, \texttt{speed}, \texttt{latency}, \texttt{memory}) followed by LLM-based classification to only keep commits focused on GPU-based inference optimization.

\paragraph{Manual Curation.} We manually review each candidate commit to verify that (i)~the optimization is reproducible, (ii)~the commit represents a genuine optimization rather than a refactoring or bug fix. This curation step filters out false positives from the automated pipeline.

\paragraph{PR Analysis.} For each retained commit, we fetch the associated pull request to extract the benchmarking model, evaluation commands, and performance claims from the PR discussion. This metadata serves two purposes: it provides the benchmark commands we use during evaluation, and it aids manual curation by helping us verify whether a commit represents a legitimate optimization.

This pipeline produces \textbf{54 benchmark instances}: 39 from vLLM and 15 from SGLang, each with verified performance benchmarks, model specifications, and evaluation commands.

% Figure 1: Pipeline Diagram (spans both columns)
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/main_fig.png}
\caption{\textbf{ISO-Bench evaluation pipeline.} Given a codebase and task description, a coding agent produces an optimization patch. We compare this 
  patch against the human commit using hard metrics (TTFT, throughput) and soft metrics (bottleneck targeting, implementation approach). Hard metrics    
  measure performance improvement; soft metrics assess whether the agent targeted the correct code.} \label{fig:pipeline}
\end{figure*}


\subsection{Evaluation Metrics}
\label{sec:evaluation-metrics}
We evaluate agent performance using two types of metrics. Hard metrics measure execution performance using each project's own benchmarking tools. Soft metrics assess whether agents correctly identify the optimization target by comparing their approach to human solutions. Combining both allows us to distinguish genuine optimization capability from accidental improvements.

\subsubsection{Hard Metrics}
\label{sec:hard-metrics}
We measure agent optimizations using the same benchmarks that developers used in the original pull requests. We track Time to First Token (TTFT) and throughput, comparing agent performance against the human baseline and classifying results according to Table~\ref{tab:hard-metric-categories}:
\begin{equation}
\Delta_{\text{TTFT}} = \frac{\text{TTFT}_h - \text{TTFT}_a}{\text{TTFT}_h} \times 100
\label{eq:ttft}
\end{equation}
\begin{equation}
\Delta_{\text{throughput}} = \frac{\text{Throughput}_a - \text{Throughput}_h}{\text{Throughput}_h} \times 100
\label{eq:throughput}
\end{equation}

\begin{table}[t]
\caption{Hard metric classification based on performance delta.}
\label{tab:hard-metric-categories}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Category} & \textbf{Criteria} \\
\midrule
Beats & Agent improves on human by $>5\%$ \\
Similar & Agent within $\pm 5\%$ of human \\
Worse & Agent degrades by $>5\%$ \\
Failed & Patch causes benchmarking error \\
\bottomrule
\end{tabular}
\end{table}

The 5\% threshold accounts for measurement noise. For serving-based benchmarks, we measure latency using $\Delta_{\text{TTFT}}$. When TTFT is not produced, we use $\Delta_{\text{throughput}}$ for standalone benchmarks.

\subsubsection{Soft Metrics}
\label{sec:soft-metrics}
Hard metrics alone cannot distinguish genuine optimization capability from accidental improvements. Agents may achieve performance gains through changes unrelated to the actual optimization target. To address this, we introduce soft metrics, which use automated semantic analysis to compare agent patches against human solutions using an LLM as an evaluator (Gemini-3-Flash-Preview)~\citep{deepmind2025gemini3flash_modelcard}. We assess two dimensions as shown in Table~\ref{tab:soft-metric-categories}.


\begin{table}[t]
\caption{Soft metric categories for semantic analysis.}
\label{tab:soft-metric-categories}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\multicolumn{2}{l}{\textbf{Bottleneck Targeting}} \\
\midrule
Same target & Identical code locations as human \\
Related target & Same module or subsystem \\
Different target & Unrelated code areas \\
No optimization & No performance-relevant changes \\
\midrule
\multicolumn{2}{l}{\textbf{Implementation Approach}} \\
\midrule
Similar approach & Same technique as human \\
Valid alternative & Different but sound \\
Partial solution & Subset of required changes \\
Ineffective & Fails to address bottleneck \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Quadrant Framework}
\label{sec:quadrant-framework}
We combine hard and soft metrics to classify each optimization attempt into one of four quadrants, as shown in Figure~\ref{fig:quadrant-framework}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/quadrant_framework.png}
\caption{Quadrant framework for evaluating optimization attempts.}
\label{fig:quadrant-framework}
\end{figure}

Q3 (Lucky Win) cases are particularly interesting: agents achieve good hard metrics despite targeting the wrong code. Without soft metrics, these would be classified as genuine success.

This framework gives two measurements:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=3pt]
    \item \textbf{True Success} = Q1 (based on hard and soft metrics)
    \item \textbf{Hard Success} = Q1 + Q3 (based on hard metric only)
    % \item \textbf{Intent Success} = Q1 + Q2 (identified correct bottleneck)
\end{itemize}
\label{def:metrics}

Only True Success measures actual optimization ability of an agent for a particular task.

\subsubsection{Functional Correctness}
\label{sec:functional-correctness}
The quadrant framework identifies Hard Success cases (Q1 + Q3) where agents achieve performance improvements. However, speedup alone does not guarantee correctness. An agent might achieve faster execution by changing model behavior in ways that produce incorrect outputs.

We validate functional correctness for all Hard Success cases using the LM Evaluation Harness~\citep{eval-harness}. For each task, we use the evaluation benchmarks specified in the original PR. We measure accuracy across versions: the unoptimized baseline, and the agent patch. If accuracy remains consistent, the optimization preserves correctness. If the agent patch degrades accuracy, the optimization introduced functional errors despite achieving speedup.

This validation is especially important for Q3 (Lucky Win) cases. These agents achieve speedup without targeting the correct bottleneck, so their improvements may come from changes that alter model behavior rather than genuine optimization.

\section{Experimental Setup}
\label{sec:experimental-setup}

\subsection{Agents Under Evaluation}
\label{sec:agents}

We evaluate four coding agents representing different architectures and scaffolding approaches:
\begin{itemize}
  \item \textbf{Claude Code:} Anthropic's coding assistant designed for autonomous software engineering tasks using Claude Sonnet 4.5.
  \item \textbf{Codex CLI:} OpenAI's command-line coding interface, supporting iterative code editing, execution, and debugging from the terminal using GPT-5.
    \item \textbf{TRAE-Agent:} Modified version of ByteDance's open-source TRAE-Agent framework, evaluated with two underlying models (Claude Sonnet 4.5 and GPT-5). We refer to these as TRAE (Sonnet) and TRAE (GPT-5) respectively.

\end{itemize}
\begin{table}[h]
\centering
\caption{True Success by Project}
\label{tab:true-success}
\begin{tabular}{lcc}
\toprule
\textbf{Agent} & \textbf{vLLM} & \textbf{SGLang} \\
\midrule
Claude Code & 46.2\% & 26.7\% \\
TRAE (Sonnet) & 28.2\% & 80.0\% \\
TRAE (GPT-5) & 17.9\% & 86.7\% \\
Codex CLI & 20.5\% & 80.0\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Execution Environment}
\label{sec:execution-environment}

Each agent operates on an isolated git worktree where it can freely explore the codebase, modify files, and commit changes. Agents have 120 minutes per task to work on their solution. During this time, they can run tests, check performance, and refine their code based on the results. All runs happen inside Docker containers to keep measurements consistent across experiments. We save all agent edits as git commits so we can analyze them later.

\subsection{Evaluation Protocol}
\label{sec:evaluation-protocol}

After all agent runs complete, we evaluate patches in two stages. For hard metrics, we execute the benchmark commands on NVIDIA H100 GPUs, measuring TTFT and throughput against both the unoptimized baseline and the human solution. For soft metrics, we perform LLM-based semantic analysis comparing agent patches to human patches, assessing bottleneck targeting and implementation approach.

\section{Results}
We evaluate four coding agents on ISO-Bench across 54 optimization tasks (39 from vLLM, 15 from SGLang). The remainder of this section provides a detailed discussion of how coding agents approach real-world GPU optimization challenges and why traditional metrics alone fail to fully reflect their performance.

\begin{figure*}[t]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/png_figure/section_3_3_good_intent_bad_execution.png}
        \caption{Good Intent vs Bad Execution on vLLM (39 tasks). Light bars show correct target identification
        (Q1+Q2). Dark bars show True Success (Q1). The gap represents Q2 failures.}
        \label{fig:good-intent-vllm}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/png_figure/section_3_3_good_intent_bad_execution_sglang.png}
        \caption{Good Intent vs Bad Execution on SGLang (15 tasks). Three agents show minimal Q2 gaps. Claude Code
        shows the opposite pattern.}
        \label{fig:good-intent-sglang}
    \end{minipage}
\end{figure*}

\subsection{Can Agents Optimize GPU Inference Code?}
The percentage of tasks where agents both identified the correct bottleneck and achieved measurable performance improvements is defined as the True Success rate, presented in Table~\ref{tab:true-success}.


The percentage of tasks where agents both identified the correct bottleneck and achieved measurable performance improvements is defined as the True Success rate, presented in Table~\ref{tab:true-success}.

In our experiments, we study the two inference engines vLLM and SGLang separately. On vLLM, Claude Code gets 46.2\%, while the other agents get lower True Success rates. However, on SGLang, Claude Code gets 26.7\%, while the other agents get higher True Success rates, with TRAE (GPT-5) at 86.7\%. This difference in agent performance for these projects is further studied in Section~\ref{sec:generalize}.

\begin{table*}[t]
  \centering
  \small
  \caption{\textbf{Hard Success vs.\ True Success rates.} The Gap column quantifies cases where agents achieved performance improvements without targeting the correct bottleneck. A gap of zero indicates all hard successes genuinely addressed the specified optimization target.}
  \label{tab:hard-vs-true}
  \begin{tabular}{@{}llccc@{}}
  \toprule
  \textbf{Agent} & \textbf{Project} & \textbf{Hard Success (\%)} & \textbf{True Success (\%)} & \textbf{Gap (\%)} \\
  \midrule
  Claude Code & SGLang & 46.7 & 26.7 & 20.0 \\
  Codex CLI & vLLM & 33.3 & 20.5 & 12.8 \\
  Claude Code & vLLM & 56.4 & 46.2 & 10.2 \\
  TRAE (Sonnet) & vLLM & 33.3 & 28.2 & 5.1 \\
  TRAE (GPT-5) & vLLM & 20.5 & 17.9 & 2.6 \\
  \midrule
  TRAE (GPT-5) & SGLang & 86.7 & 86.7 & 0.0 \\
  TRAE (Sonnet) & SGLang & 80.0 & 80.0 & 0.0 \\
  Codex CLI & SGLang & 80.0 & 80.0 & 0.0 \\
  \bottomrule
  \end{tabular}
\end{table*}

\subsection{Do Hard Metrics Tell the Full Story?}

Each task in ISO-Bench specifies a particular bottleneck that agents must optimize. However, agents sometimes achieve measurable speedups by modifying code unrelated to the specified bottleneck. These are Lucky Wins (Q3) from Section~\ref{sec:quadrant-framework}, and the gap between Hard Success and True Success measures how often this occurs.

Table~\ref{tab:hard-vs-true} presents the complete breakdown across all agent-project combinations. Claude Code on SGLang achieves a Hard Success rate of 46.7\%, but under True Success this drops to 26.7\%, a gap of 20\%. Similar patterns emerge across other configurations, with gaps ranging from 2.6\% to 12.8\% on vLLM. These results show that hard metrics alone can overestimate agent capabilities. Soft metrics address this by checking whether agents actually modify the code regions specified in each task, making it possible to separate understanding from accidental improvements.
% \subsection{Do Hard Metrics Tell the Full Story?}
% Each task in ISO-Bench describes a specific bottleneck to optimize. However, agents sometimes improve performance by modifying unrelated code, achieving speedups without addressing the bottleneck described in the task.

% Claude Code on SGLang illustrates this. It achieves 46.7\% hard success, but only 26.7\% True Success. The difference: 3 of its 7 successful runs (42.9\%) are lucky wins where performance improved despite targeting different code than what the task specified. Codex CLI on vLLM shows a similar pattern 38.5\% of its hard successes come from changes unrelated to the task's bottleneck.

% \begin{table*}[t]
%   \centering
%   \small
%   \caption{Hard Success vs True Success rates. The gap column shows the difference, representing cases where agents achieved performance improvements without targeting the correct bottleneck.}
%   \label{tab:hard-vs-true}
%   \begin{tabular}{@{}llccc@{}}
%   \toprule
%   \textbf{Agent} & \textbf{Project} & \textbf{Hard Success (\%)} & \textbf{True Success (\%)} & \textbf{Gap (\%)} \\
%   \midrule
%   Claude Code & SGLang & 46.7 & 26.7 & 20.0 \\
%   Codex CLI & vLLM & 33.3 & 20.5 & 12.8 \\
%   Claude Code & vLLM & 56.4 & 46.2 & 10.2 \\
%   TRAE (Sonnet) & vLLM & 33.3 & 28.2 & 5.1 \\
%   TRAE (GPT-5) & vLLM & 20.5 & 17.9 & 2.6 \\
%   TRAE (GPT-5) & SGLang & 86.7 & 86.7 & 0.0 \\
%   TRAE (Sonnet) & SGLang & 80.0 & 80.0 & 0.0 \\
%   Codex CLI & SGLang & 80.0 & 80.0 & 0.0 \\
%   \bottomrule
%   \end{tabular}
% \end{table*}

% The gap column shows how much hard metrics overestimate True Success. On vLLM, all agents show a gap, hard success rates are inflated by 2.6 to 12.8 percentage points. On SGLang, only Claude Code shows a gap (20pp). The other three agents have zero lucky wins, every hard success targets the correct bottleneck. Soft metrics reveal whether performance improvements come from addressing the specified bottleneck or from unrelated changes.
\begin{table*}[t]
    \centering
    \caption{Quadrant Distribution.}
    \label{tab:quadrant-distribution}
    \begin{tabular}{@{}llcccc@{}}
    \toprule
    \textbf{Project} & \textbf{Agent} & \textbf{Q1} & \textbf{Q2} & \textbf{Q3} & \textbf{Q4} \\
    \midrule
    \multirow{4}{*}{vLLM}
      & Claude Code   & 18 & 15 & 4 & 2 \\
      & Codex CLI     &  8 & 20 & 5 & 6 \\
      & TRAE (Sonnet) & 11 & 20 & 2 & 6 \\
      & TRAE (GPT-5)  &  7 & 27 & 1 & 4 \\
    \midrule
    \multirow{4}{*}{SGLang}
      & Claude Code   &  4 &  8 & 3 & 0 \\
      & Codex CLI     & 12 &  3 & 0 & 0 \\
      & TRAE (Sonnet) & 12 &  3 & 0 & 0 \\
      & TRAE (GPT-5)  & 13 &  2 & 0 & 0 \\
    \bottomrule
    \end{tabular}
\end{table*}


\subsection{What Limits Agent Performance?}
\label{sec:what-limits}
From Table~\ref{tab:quadrant-distribution}, on vLLM, three of four agents have their highest count in Q2: TRAE (GPT-5) (27), TRAE (Sonnet) (20), and Codex CLI (20). Q2 represents cases where agents identify the correct optimization target but fail to implement a working solution - Good Intent, Bad Execution. Only Claude Code has more Q1 (18) than Q2 (15).


Figure~\ref{fig:good-intent-vllm} highlights this gap. TRAE (GPT-5) pinpoints the correct bottleneck in 34 of 39 tasks, but translates only 7 into True Success (21\%). Claude Code identifies the correct target in 33 tasks and converts 18 (55\%).


Figure~\ref{fig:good-intent-sglang} reports results on the SGLang optimization tasks. TRAE (GPT-5), Codex CLI, and TRAE (Sonnet) identify the correct optimization target in all 15 tasks and achieve True Success on 12 -- 13 tasks, leaving only 2 -- 3 tasks in Q2 for each agent. In contrast, Claude Code underperforms on SGLang: it identifies the correct target in 12 of 15 tasks but reaches True Success in only 4, with 8 tasks remaining in Q2.

Prior benchmarks (e.g., GSO \cite{shetty2025gso}, KernelBench\cite{ouyang2025kernelbench}) often pointed to limited understanding as the primary bottleneck. In our experiments, agents usually identify the right target, but  execution remains a limiting factor, and failures differ across different projects. 



\subsection{Does Performance Generalize Across Codebases?}
\label{sec:generalize}
\begin{figure*}[t]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/png_figure/section_3_4_approach_sglang.png}
        \caption{Approach distribution on SGLang (15 tasks). Three agents concentrate on Similar approach; Claude Code has zero.}
        \label{fig:approach-sglang}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/png_figure/section_3_4_approach_vllm.png}
        \caption{Approach distribution on vLLM (39 tasks). No single approach dominates.}
        \label{fig:approach-vllm}
    \end{minipage}
\end{figure*}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/png_figure/section_3_4_approach_sglang.png}
%     \caption{Approach distribution on SGLang (15 tasks). Three agents concentrate on Similar approach.}
%     \label{fig:approach-sglang}
% \end{figure}

Figure~\ref{fig:approach-sglang} clearly shows a split in how agents behave on SGLang. TRAE (GPT-5),
TRAE (Sonnet), and Codex CLI each use a \textbf{Similar approach} in \textbf{8/15} tasks, while Claude Code
uses a Similar approach in \textbf{0/15} tasks. Claude Code instead produces \textbf{Partial solutions (8/15)}
and \textbf{Valid alternatives (5/15)} (categories defined in Table~\ref{tab:soft-metric-categories}). This
difference aligns with the outcomes on SGLang: TRAE (GPT-5) reaches \textbf{86.7\% True Success}, while Claude
Code reaches \textbf{26.7\%}.

In our SGLang task set, many human fixes reuse the same types of changes (e.g., memory pooling, buffer reuse,
lazy initialization). When an agent follows the same overall plan as the human fix, it more often reaches a
working implementation. When it chooses an alternative plan, it more often ends in Q2: the correct target is
identified, but the final patch does not work.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/png_figure/section_3_4_approach_vllm.png}
%     \caption{Approach distribution on vLLM (39 tasks). No single approach dominates.}
%     \label{fig:approach-vllm}
% \end{figure}

vLLM shows a different picture (Figure~\ref{fig:approach-vllm}). Approach counts are spread across categories,
and no single approach dominates. In our vLLM task set, many fixes touch multiple connected parts of the
system (e.g., attention code, KV cache, memory management), so small implementation differences can break
cross-component behavior. TRAE (GPT-5) matches the human approach in \textbf{17/39} tasks, but still shows a
high Q2 rate on vLLM (\textbf{69\%}), consistent with failures that appear only after the change interacts
with other subsystems. Claude Code produces more \textbf{Partial solutions} on vLLM (\textbf{14/39}), which
reduces the change scope and can avoid some cross-system breakage; this aligns with its True Success on vLLM
(\textbf{46.2\%}).

Across both codebases, we do not see agents changing their approach based on the project. TRAE (GPT-5) leans
toward matching the reference approach, which works well on SGLang but is less reliable on vLLM. Claude Code
leans toward smaller-scope changes, which helps on vLLM but performs poorly on SGLang where the human-style
pattern often works. This suggests single-codebase results can overstate how well an agent will do on a new
repository, and supports evaluating agents across codebases with different task structures.


\subsection{What Role Does Agent Design Play?}
  \label{sec:scaffolding}

  TRAE (Sonnet) and Claude Code both use the same underlying model: Claude Sonnet 4.5. On vLLM, Claude Code
  achieves 46.2\% True Success while TRAE (Sonnet) reaches 28.2\%, a 64\% relative difference. On SGLang,
  TRAE (Sonnet) achieves 80.0\% while Claude Code reaches 26.7\%, a 67\% relative difference in the opposite
  direction.

  \begin{table}[h]
  \centering
  \caption{Same Model, Different Outcomes}
  \label{tab:scaffolding}
  \begin{tabular}{lccc}
  \toprule
  \textbf{Project} & \textbf{Claude Code} & \textbf{TRAE (Sonnet)} & \textbf{Difference} \\
  \midrule
  vLLM & 46.2\% & 28.2\% & +64\% \\
  SGLang & 26.7\% & 80.0\% & $-$67\% \\
  \bottomrule
  \end{tabular}
  \end{table}

  The model is identical, but the agents differ in how they explore the codebase, decompose tasks, and decide
  when to stop iterating. These scaffolding choices produce the approach distributions in
  Figures~\ref{fig:approach-vllm} and \ref{fig:approach-sglang}: Claude Code favors partial solutions,
  TRAE (Sonnet) favors matching the human approach.

  This suggests model capability alone does not determine agent performance on optimization tasks. Two agents
  built on the same model show 2 -- 3$\times$ performance differences depending on scaffolding design. Benchmarks
  that evaluate only the underlying model may not predict how an agent performs on real-world tasks.


\subsection{Do Optimizations Preserve Correctness?}
  \label{sec:func-validation}
We check functional correctness with LM Evaluation Harness on GSM8K by comparing baseline accuracy with the agent-updated code.

Q1 is the best case. The agent makes a real optimization and keeps outputs correct. In our Q1 runs across agents and projects, accuracy stayed within the specified tolerance. These cases reflect the agent’s ability to complete the optimization task end to end, closing the gap to human changes, and sometimes beating them.

Q3 shows why every agent change still needs validation with both hard and soft metrics. Agents can over-optimize for the easiest target to measure, here that target was speed. One patch sped up Bamba-9B, so hard metrics looked good. Soft metrics flagged that the agent likely changed the wrong code. GSM8K confirmed the break: accuracy dropped from 32\% to 0\%. The patch hardcoded tensor sizes in the Mamba mixer layer, which produced wrong outputs.


% \subsection{Old -- To Be Removed -- Need To Put This in Appendix}
% % TO BE REMOVED. However, this details need to go in Appendix (Shikhar) 
% \noindent A separate LLM (distinct from the agent under evaluation) analyzes the complete trajectory---tool calls, file reads, edits, and test outputs---to assign scores. We additionally compute \textbf{patch similarity} via Jaccard index over modified files and lines compared to the human expert solution, capturing whether agents discover similar optimization strategies or arrive at functionally equivalent alternatives through different code paths.

% % TO BE REMOVED
% \paragraph{Human Anchoring.} Each task includes the human expert's optimization as ground truth. The ratio $S_{\text{human}} = \text{GM}(\mathbf{t}_{\text{base}}) / \text{GM}(\mathbf{t}_{\text{head}})$ provides an upper bound, contextualizing agent achievements within the space of known-possible improvements.




% \section{Experimental Results}
% \label{sec:results}

% \subsection{Overall Performance}

% \subsection{Bimodal Distribution}

% \section{Analysis and Discussion}
% \label{sec:analysis}

% \subsection{Instant-Edit Pathology}

% \subsection{Commit Explosion}

% \subsection{Codebase Characteristics}

% \subsection{Multi-Agent Trade-offs}

% \section{Conclusion}
% \label{sec:conclusion}

% Bibliography
\bibliography{references}
\bibliographystyle{icml2026}

\appendix
\begingroup
\small
\sloppy
\setlength{\emergencystretch}{1em}

%% ====================================================
\section{Task Collection}
\label{app:task-collection}

ISO-Bench tasks are derived from merged pull requests in vLLM and SGLang that demonstrate measurable performance improvements. We extract optimization commits through a multi-stage filtering pipeline, then manually curate each candidate to ensure task quality. This section describes the collection methodology following the structure established by GSO~\citep{shetty2025gso}.

\subsection{Commit Filtering Pipeline}

We identify performance-related commits through a three-stage automated pipeline. First, we scan the complete commit history of each repository, filtering commits by keyword matching on messages and diffs for performance-related terms (\texttt{optim}, \texttt{perf}, \texttt{speed}, \texttt{latency}, \texttt{throughput}, \texttt{memory}, \texttt{cache}, \texttt{kernel}, \texttt{fusion}, \texttt{batch}). We selected these 10 keywords based on manual analysis of 100 sample commits from each repository, achieving 92\% precision against expert annotation.

Second, we apply scope filtering to retain only commits modifying fewer than 10 files. Analysis of 100 commits shows that changes affecting more than 10 files typically mix optimization with refactoring, making them unsuitable as isolated optimization tasks.

Third, for remaining candidates, GPT-4o-mini classifies whether each commit represents a genuine GPU inference optimization versus a bug fix, refactoring, or documentation change. The model receives the commit message and truncated diff (up to 20,000 characters) and outputs a binary classification with confidence score.

Table~\ref{tab:filtering-stats} summarizes the filtering pipeline results across both repositories.

\begin{table}[h]
\centering
\caption{Commit filtering statistics for ISO-Bench.}
\label{tab:filtering-stats}
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Stage} & \textbf{vLLM} & \textbf{SGLang} \\
\midrule
Total commits & 15,234 & 8,421 \\
Keyword matches & 892 & 547 \\
Scope filter ($<$10 files) & 341 & 312 \\
LLM classification & 186 & 201 \\
Manual curation & \textbf{39} & \textbf{15} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Manual Curation}

After automated filtering, we manually review each candidate by examining the associated pull request discussion. This curation step serves four purposes: (1)~verify that the commit represents a genuine performance optimization rather than a bug fix or refactoring; (2)~extract the benchmarking configuration used by the original author, including model name, batch size, and request rate; (3)~identify performance claims from the PR discussion to establish expected improvement targets; and (4)~write a natural language task description explaining the bottleneck without revealing the solution approach.

The task description is critical for fair evaluation---it must provide enough context for agents to understand the optimization target while avoiding hints that would trivialize the task. We follow GSO's approach of describing the symptom (e.g., ``high latency during prefill phase'') rather than the solution (e.g., ``implement chunked prefill''). This curation step reduces 186 vLLM candidates to 39 final tasks, and 201 SGLang candidates to 15 final tasks.

%% ====================================================
\section{Agent Evaluation Pipeline}
\label{app:agent-pipeline}

This section describes the infrastructure for running agents on ISO-Bench tasks and collecting their outputs. Each agent receives identical inputs and operates under the same constraints to ensure fair comparison.

\subsection{Task Specification}

Each task provides structured metadata defining the optimization challenge. The specification includes repository information (URL, human commit hash, parent commit for baseline), runner requirements (GPU type, CUDA version, Python version), and an optimization contract describing target files and constraints.

\begin{gsobox}[title={Task Specification Schema}]
{\small\ttfamily
id: "vllm\_attention\_opt"\\
name: "FlashAttention H100 optimization"\\
description: "Optimize attention kernel for..."\\[0.3em]
repo:\\
\hspace*{1em}url: "github.com/vllm-project/vllm"\\
\hspace*{1em}human\_commit: "f092153f..."\\
\hspace*{1em}pre\_commit: null \# defaults to parent\\[0.3em]
runner:\\
\hspace*{1em}requires\_gpu: true\\
\hspace*{1em}cuda\_version: "12.4"\\
\hspace*{1em}python\_version: "3.12"\\[0.3em]
optimization\_contract:\\
\hspace*{1em}target\_files: ["vllm/attention/*.py"]\\
\hspace*{1em}constraints: ["No public API breakage"]
}
\end{gsobox}

\subsection{Execution Environment}

Each agent operates in an isolated environment to ensure reproducibility and prevent cross-task interference. We use Docker containers built at the specific commit being evaluated, with commit-specific dependencies installed. Each container receives access to a single NVIDIA H100 GPU with 80GB VRAM.

Agents work on isolated git worktrees, allowing them to freely explore the codebase, modify files, run tests, and commit changes without affecting other runs. All agent edits are saved as git commits for post-hoc analysis. The time budget is 120 minutes per task, after which the agent process is terminated and the final worktree state is captured.

Resource limits are set to 8 CPUs and 64GB RAM per container. Network access is permitted for downloading model weights and dependencies but is logged for reproducibility verification.

\subsection{Agent Configurations}

We evaluate four agent configurations representing different architectures and scaffolding approaches. Table~\ref{tab:agent-configs} summarizes the configurations.

\begin{table}[h]
\centering
\caption{Agent configurations for ISO-Bench evaluation.}
\label{tab:agent-configs}
\small
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Agent} & \textbf{Model} & \textbf{Framework} & \textbf{Budget} \\
\midrule
Claude Code & Claude Sonnet 4.5 & Native CLI & 120 min \\
Codex CLI & GPT-5 & OpenAI CLI & 120 min \\
TRAE (Sonnet) & Claude Sonnet 4.5 & TRAE-Agent & 120 min \\
TRAE (GPT-5) & GPT-5 & TRAE-Agent & 120 min \\
\bottomrule
\end{tabular}
\end{table}

Claude Code and Codex CLI are proprietary agent systems with opaque scaffolding. TRAE-Agent is a modified version of ByteDance's open-source framework, providing full trajectory visibility. By evaluating TRAE with both Claude Sonnet 4.5 and GPT-5, we can isolate the effect of scaffolding from the underlying model.

\subsection{Task Prompt}

Following GSO~\citep{shetty2025gso}, we provide agents with a structured prompt describing the optimization task without revealing the solution. The prompt includes the repository location, a performance benchmark demonstrating the bottleneck, and guidelines for making changes. Figure~\ref{fig:task-prompt} shows the complete prompt template.

\begin{figure*}[t]
\centering
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries,
  title={Performance Optimization Task Prompt},
  sharp corners,
  boxrule=0.5pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
  width=0.95\textwidth
]
I've uploaded a python code repository in the directory \texttt{/workspace}. The repository contains a performance benchmark that measures inference latency and throughput. Your task is to optimize the codebase to improve performance on this benchmark.

\vspace{0.3em}
\textbf{Basic guidelines:}
\begin{enumerate}[leftmargin=1.5em,itemsep=1pt,topsep=2pt]
\item Your task is to make changes to non-test files in the \texttt{/workspace} directory to improve performance.
\item Make changes while ensuring the repository is functionally equivalent to the original---outputs must remain identical.
\item Do not overoptimize for specific inputs. Make general performance improvements that benefit diverse workloads.
\item You may need to rebuild the repository for your changes to take effect before testing.
\end{enumerate}

\vspace{0.3em}
\textbf{Recommended workflow:}
\begin{enumerate}[leftmargin=1.5em,itemsep=1pt,topsep=2pt]
\item Explore the repository structure to understand the codebase architecture.
\item Run the benchmark script to establish baseline performance and identify bottlenecks.
\item Analyze the bottleneck code paths using profiling or code inspection.
\item Edit the source code to address the identified performance bottleneck.
\item Rebuild and rerun the benchmark to confirm improvement.
\end{enumerate}

\vspace{0.3em}
\textbf{Bottleneck description:} \texttt{[[ TASK-SPECIFIC DESCRIPTION OF THE PERFORMANCE ISSUE ]]}
\end{tcolorbox}
\caption{Task prompt template provided to agents. The bottleneck description is customized per task based on PR analysis.}
\label{fig:task-prompt}
\end{figure*}

%% ====================================================
\section{Evaluation Methodology}
\label{app:evaluation}

This section details the procedures for computing hard metrics, soft metrics, and functional correctness validation.

\subsection{Hard Metrics}

We execute performance benchmarks on NVIDIA H100 GPUs using the benchmark commands specified in each task's original pull request. This ensures we measure performance on the same workloads that motivated the original optimization.

Each benchmark follows a standard protocol: 3 warmup iterations to stabilize GPU state, followed by 10 timed trials for statistical significance. We measure Time-To-First-Token (TTFT) in milliseconds for serving benchmarks and throughput in tokens per second for offline benchmarks, computing the geometric mean across trials.

\begin{gsobox}[title={Example Benchmark Command}]
{\small\ttfamily
python benchmarks/benchmark\_serving.py \textbackslash\\
\hspace*{1em}--model meta-llama/Llama-3.1-8B-Instruct \textbackslash\\
\hspace*{1em}--num-prompts 1000 \textbackslash\\
\hspace*{1em}--request-rate 10 \textbackslash\\
\hspace*{1em}--backend vllm
}
\end{gsobox}

Performance deltas are computed relative to the human solution using Equations~\ref{eq:ttft} and~\ref{eq:throughput} from Section~\ref{sec:hard-metrics}. The 5\% threshold for classification (Table~\ref{tab:hard-metric-categories}) accounts for measurement noise inherent in GPU benchmarking.

\subsection{Soft Metrics}

We use Gemini-2.0-Flash via OpenRouter to perform semantic analysis comparing agent patches to human reference patches. The analysis evaluates two primary dimensions used in the main paper (bottleneck targeting and implementation approach), plus additional quality scores for detailed trajectory analysis.

\paragraph{Primary Dimensions.} For each agent patch, we assess:
\begin{itemize}[leftmargin=1.5em,itemsep=1pt,topsep=2pt]
\item \textbf{Bottleneck Targeting}: Whether the agent modifies the same code region as the human patch. Categories: Same target, Related target (same module/subsystem), Different target, No optimization.
\item \textbf{Implementation Approach}: Whether the agent uses a similar optimization technique. Categories: Similar approach, Valid alternative (different but sound), Partial solution (subset of required changes), Ineffective.
\end{itemize}

\begin{gsobox}[title={Soft Metric Analysis Prompt (Abridged)}]
{\small
Analyze both patches and return JSON with fields:
\begin{itemize}[leftmargin=1.5em,itemsep=0pt,topsep=2pt]
\item \texttt{bottleneck\_target.category}: \texttt{same|related|different|none}
\item \texttt{approach\_comparison.category}: \texttt{similar|partial|alternative|ineffective}
\item \texttt{speedup\_likelihood}: \texttt{likely|partial|ineffective}
\item \texttt{failure\_mode}: \texttt{localization|implementation|approach|none}
\end{itemize}
}
\end{gsobox}

\paragraph{Quality Scores.} For trajectory-level analysis, we additionally compute four quality dimensions on a 0--10 scale with calibrated scoring guidelines:
\begin{itemize}[leftmargin=1.5em,itemsep=1pt,topsep=2pt]
\item \textbf{Code Understanding} (0--10): Repository navigation, function identification, dependency awareness.
\item \textbf{Task Alignment} (0--10): Goal comprehension, constraint adherence, output relevance.
\item \textbf{Approach Quality} (0--10): Strategy coherence, exploration efficiency, decision quality.
\item \textbf{Execution Quality} (0--10): Edit correctness, test verification, error recovery.
\end{itemize}

The calibration scale reserves 9--10 for exceptional performance, 7--8 for good results with minor issues, 5--6 for average partial success, 3--4 for poor results with significant gaps, and 0--2 for complete failures.

\paragraph{Patch Comparison.} We quantify patch similarity using Jaccard overlap at two levels: (1)~file-level overlap $= |F_{\text{common}}| / |F_{\text{union}}|$, measuring whether agent and human modify the same files; and (2)~line-level overlap computed over added/removed lines, with precision ($|L_{\text{match}}| / |L_{\text{agent}}|$) and recall ($|L_{\text{match}}| / |L_{\text{human}}|$) metrics.

\paragraph{Validation.} We validated the automated analysis against human annotators on 50 randomly sampled judgments, finding 92\% agreement on bottleneck targeting and 88\% agreement on implementation approach. Disagreements primarily occurred on boundary cases between ``Related target'' and ``Different target.''

\subsection{Functional Correctness}

We validate that optimization patches preserve model correctness using a 3-way comparison with EleutherAI's LM Evaluation Harness (lm-eval). For each task, we measure GSM8K accuracy on three versions:

\begin{enumerate}[leftmargin=1.5em,itemsep=2pt,topsep=4pt]
\item \textbf{Baseline}: Parent commit (pre-optimization state)
\item \textbf{Human}: Merged commit with human-written optimization
\item \textbf{Agent}: Baseline with AI-generated patch applied
\end{enumerate}

\paragraph{Docker Image Versioning.} Each version runs in an isolated Docker container built at the corresponding commit. We maintain separate image registries with distinct tagging conventions to prevent confusion: baseline images use 12-character truncated parent hashes (e.g., \texttt{baseline-bc8a8ce5ec37}), while human images use the full 40-character commit hash (e.g., \texttt{22d33baca2c0c639cfd...}). Agent evaluation applies the generated patch to the baseline image using standard \texttt{git apply}.

\paragraph{LM-Eval Configuration.} All 39 commits are evaluated using GSM8K with 100 samples per phase. The evaluation command is standardized across commits:

\vspace{0.3em}
\begin{quote}
\footnotesize\ttfamily\raggedright\hyphenpenalty=10000\exhyphenpenalty=10000
lm\_eval --model vllm --model\_args pretrained=<model>,dtype=auto --tasks gsm8k --batch\_size auto --limit 100
\end{quote}
\vspace{-0.5em}

We handle compatibility issues across vLLM versions through targeted Dockerfile patches: (1)~dataset path remapping (\texttt{gsm8k}$\rightarrow$\texttt{openai/gsm8k}) for HuggingFace namespace changes, (2)~lm-eval version pinning (0.4.2 for older commits), and (3)~\texttt{rope\_scaling} KeyError patching for deprecated configuration formats.

\paragraph{Accuracy Validation.} Accuracy serves as a proxy for functional equivalence. If accuracy matches across all three versions (within $\pm$1\% tolerance on GSM8K exact-match), the optimization preserves functional correctness. If the agent patch degrades accuracy while baseline and human match, the optimization introduced functional errors despite achieving speedup. This validation is especially important for Q3 (Lucky Win) cases, where agents achieve speedup without targeting the correct bottleneck.

\subsection{Case Study: FlashAttention True Success}
\label{app:flash-attention-case-study}

Figure~\ref{fig:flash-attention-patch} shows a Q1 (True Success) case where TRAE (Sonnet) correctly identified and optimized the FlashAttention CPU overhead bottleneck. On commit \texttt{98f47f2a}, the task required minimizing CPU overhead in the FlashAttention custom op for CUDA graph compatibility. The agent's patch achieved +21.09\% throughput improvement by moving tensor reshape operations outside the custom op---precisely matching the human optimization strategy.

This represents genuine optimization capability: the agent understood that tensor reshaping inside the custom op creates unnecessary CPU overhead during non-CUDA-graph regions, and applied the same solution as the human engineer.

\begin{figure*}[t]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: FlashAttention CPU Overhead Optimization (Q1 True Success)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE (Sonnet 4.5)

\textbf{Result:} {\color{diffadd}\textbf{PASS}} (+21.09\% Throughput)

\textbf{Groundtruth Commit:} \url{https://github.com/vllm-project/vllm/commit/98f47f2a}

\textbf{Specification Summary:} Optimize FlashAttention custom op to minimize CPU overhead for CUDA graph compatibility.

\textbf{Target:} Move tensor reshape operations outside the custom op to reduce non-CUDA-graph CPU overhead.

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Agent Generated Patch}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
diff --git a/vllm/v1/attention/backends/flash\_attn.py b/...\\
index 5f8535eaa..b4f7f6529 100644\\
\textcolor{diffrem}{--- a/vllm/v1/attention/backends/flash\_attn.py}\\
\textcolor{diffadd}{+++ b/vllm/v1/attention/backends/flash\_attn.py}\\
\textcolor{diffrem}{@@ -135,6 +135,13 @@ class FlashAttentionImpl(AttentionImpl):}\\
\mbox{}\hspace{1em}assert k\_scale == 1.0 and v\_scale == 1.0, (\\
\mbox{}\hspace{2em}"key/v\_scale is not supported in FlashAttention.")\\[2pt]
\textcolor{diffadd}{+\hspace{1em}\# Reshape the query, key, and value tensors.}\\
\textcolor{diffadd}{+\hspace{1em}\# NOTE(woosuk): We do this outside the custom op to minimize}\\
\textcolor{diffadd}{+\hspace{1em}\# the CPU overheads from the non-CUDA-graph regions.}\\
\textcolor{diffadd}{+\hspace{1em}query = query.view(-1, self.num\_heads, self.head\_size)}\\
\textcolor{diffadd}{+\hspace{1em}key = key.view(-1, self.num\_kv\_heads, self.head\_size)}\\
\textcolor{diffadd}{+\hspace{1em}value = value.view(-1, self.num\_kv\_heads, self.head\_size)}\\[2pt]
\mbox{}\hspace{1em}output = torch.empty\_like(query)\\[4pt]
\textcolor{diffrem}{@@ -184,10 +191,8 @@ def unified\_v1\_flash\_attention(...):}\\
\textcolor{diffrem}{-\hspace{1em}\# Reshape the query, key, and value tensors.}\\
\textcolor{diffrem}{-\hspace{1em}query = query.view(-1, num\_heads, head\_size)}\\
\textcolor{diffrem}{-\hspace{1em}key = key.view(-1, num\_kv\_heads, head\_size)}\\
\textcolor{diffrem}{-\hspace{1em}value = value.view(-1, num\_kv\_heads, head\_size)}\\
\textcolor{diffadd}{+\hspace{1em}\# NOTE: Tensors are already reshaped in forward method}\\
\textcolor{diffadd}{+\hspace{1em}\# to minimize CPU overheads.}
\endgroup

\vspace{6pt}
\textbf{Analysis:} The agent correctly identified that tensor reshaping inside the custom op creates CPU overhead in the non-CUDA-graph Python regions; moving it to the capture-time path reduces per-step overhead.

\end{tcolorbox}
\caption{Patch from TRAE (Sonnet 4.5) on commit \texttt{98f47f2a}, demonstrating Q1 (True Success). The agent correctly identified the bottleneck and achieved +21\% throughput matching the human approach.}
\label{fig:flash-attention-patch}
\end{figure*}

\subsection{Case Study: Qwen3 Parser Good Intent}
\label{app:qwen3-case-study}

Figure~\ref{fig:qwen3-patch} shows a Q2 (Good Intent, Bad Execution) case where TRAE (Sonnet) correctly identified the optimization target but failed to produce working code. On commit \texttt{015069b0}, the task required replacing regex with string operations in the Qwen3 reasoning parser. The agent's patch targeted the same file (\texttt{qwen3\_reasoning\_parser.py}) and used the same approach (replacing regex with \texttt{str.find()}/\texttt{str.partition()}), but the generated code failed to execute properly during benchmarking.

This pattern is common: the agent demonstrates semantic understanding of the optimization goal but introduces bugs during implementation---suggesting a gap between comprehension and execution capability.

\begin{figure*}[t]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: Qwen3 Reasoning Parser Optimization (Q2 Good Intent, Bad Exec)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE (Sonnet 4.5)

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (Agent Code Did Not Execute)

\textbf{Groundtruth Commit:} \url{https://github.com/vllm-project/vllm/commit/015069b0}

\textbf{Specification Summary:} Optimize \texttt{extract\_reasoning\_content} in Qwen3ReasoningParser by replacing regex with string operations.

\textbf{Target:} Replace \texttt{re.compile()} and \texttt{findall()} with \texttt{str.partition()} for faster text extraction.

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Agent Generated Patch}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
diff --git a/vllm/reasoning/qwen3\_reasoning\_parser.py b/...\\
index f588f4016..7a2786bcd 100644\\
\textcolor{diffrem}{--- a/vllm/reasoning/qwen3\_reasoning\_parser.py}\\
\textcolor{diffadd}{+++ b/vllm/reasoning/qwen3\_reasoning\_parser.py}\\
\textcolor{diffrem}{@@ -1,6 +1,5 @@}\\
\mbox{}\hspace{1em}\# SPDX-License-Identifier: Apache-2.0\\
\textcolor{diffrem}{-import re}\\
\mbox{}\hspace{1em}from collections.abc import Sequence\\[2pt]
\textcolor{diffrem}{@@ -31,9 +30,6 @@ class Qwen3ReasoningParser(ReasoningParser):}\\
\textcolor{diffrem}{-\hspace{2em}self.reasoning\_regex = re.compile(}\\
\textcolor{diffrem}{-\hspace{4em}rf"\{self.think\_start\_token\}(.*?)\{self.think\_end\_token\}", re.DOTALL)}\\[2pt]
\textcolor{diffrem}{@@ -121,29 +117,31 @@ def extract\_reasoning\_content(...):}\\
\textcolor{diffadd}{+\hspace{1em}\# Find start and end positions using str.find()}\\
\textcolor{diffadd}{+\hspace{1em}start\_index = model\_output.find(self.think\_start\_token)}\\
\textcolor{diffadd}{+\hspace{1em}if start\_index == -1:}\\
\textcolor{diffadd}{+\hspace{2em}return None, model\_output}\\[2pt]
\textcolor{diffadd}{+\hspace{1em}end\_index = model\_output.find(self.think\_end\_token, start\_index)}\\
\textcolor{diffadd}{+\hspace{1em}if end\_index == -1:}\\
\textcolor{diffadd}{+\hspace{2em}return None, model\_output}\\[2pt]
\textcolor{diffadd}{+\hspace{1em}\# Extract reasoning content between the tokens}\\
\textcolor{diffadd}{+\hspace{1em}reasoning\_start = start\_index + len(self.think\_start\_token)}\\
\textcolor{diffadd}{+\hspace{1em}reasoning\_content = model\_output[reasoning\_start:end\_index]}
\endgroup

\vspace{6pt}
\textbf{Analysis:} The agent chose the correct strategy (replace regex with string-based parsing), but its implementation failed due to basic Python correctness issues (e.g., \texttt{none} vs \texttt{None}, incorrect symbol casing), whereas the ground-truth uses \texttt{str.partition()}.

\end{tcolorbox}
\caption{Patch from TRAE (Sonnet 4.5) on commit \texttt{015069b0}, demonstrating Q2 (Good Intent, Bad Execution). The agent targeted the correct bottleneck but produced non-working code.}
\label{fig:qwen3-patch}
\end{figure*}

\subsection{Case Study: Bamba-9B Accuracy Regression}
\label{app:bamba-case-study}

Figure~\ref{fig:bamba-patch} shows a Q3 (Lucky Win) case where TRAE (Sonnet) achieved speedup on Bamba-9B but broke model correctness. On commit \texttt{fe66b347}, the task required optimizing Mamba-architecture inference. The agent produced a patch that achieved speedup matching the human reference on hard metrics, but soft metrics flagged that the agent modified code unrelated to the specified bottleneck. Functional testing confirmed: GSM8K exact-match accuracy dropped from 32\% to 0\%.

The critical error was replacing dynamic dimension preservation (\texttt{-1} in \texttt{expand()}) with hardcoded \texttt{num\_heads\_per\_rank}. This causes shape mismatches when actual tensor dimensions differ, corrupting the Mamba state-space model calculations. The patch achieved speedup by computing incorrect values faster---demonstrating why functional correctness validation is essential for Q3 cases.

\begin{figure*}[t]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: Bamba-9B Mamba Mixer Optimization (Q3 Lucky Win)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE (Sonnet 4.5)

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (Accuracy Regression)

\textbf{Groundtruth Commit:} \url{https://github.com/vllm-project/vllm/commit/fe66b347}

\textbf{Specification Summary:} Optimize \texttt{mamba\_mixer2.py} for Mamba-architecture models.

\textbf{Target:} Reduce tensor operation overhead while preserving functional correctness.

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Agent Generated Patch}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
diff --git a/vllm/model\_executor/layers/mamba/mamba\_mixer2.py b/...\\
index b53a540..6956750 100644\\
\textcolor{diffrem}{--- a/vllm/model\_executor/layers/mamba/mamba\_mixer2.py}\\
\textcolor{diffadd}{+++ b/vllm/model\_executor/layers/mamba/mamba\_mixer2.py}\\
\textcolor{diffrem}{@@ -503,15 +503,29 @@ class MambaMixer2(CustomOp):}\\
\mbox{}\hspace{1em}else:\\[1pt]
\mbox{}\hspace{2em}n\_groups = self.n\_groups // self.tp\_size\\
\textcolor{diffrem}{-\hspace{2em}A = self.A[:, None, ...][:, :, None].expand(}\\
\textcolor{diffrem}{-\hspace{4em}-1, self.head\_dim, self.ssm\_state\_size).to(dtype=torch.float32)}\\
\textcolor{diffrem}{-\hspace{2em}dt = dt[:, :, None].expand(-1, -1, self.head\_dim)}\\
\textcolor{diffrem}{-\hspace{2em}dt\_bias = self.dt\_bias[:, None, ...].expand(-1, self.head\_dim)}\\
\textcolor{diffrem}{-\hspace{2em}D = self.D[:, None, ...].expand(-1, self.head\_dim)}\\
\textcolor{diffrem}{-\hspace{2em}B = B.view(-1, n\_groups, B.shape[1] // n\_groups)}\\
\textcolor{diffrem}{-\hspace{2em}C = C.view(-1, n\_groups, C.shape[1] // n\_groups)}\\
\textcolor{diffadd}{+\hspace{2em}num\_heads\_per\_rank = self.num\_heads // self.tp\_size}\\[2pt]
\textcolor{diffadd}{+\hspace{2em}\# Optimize tensor preparations to reduce overhead}\\
\textcolor{diffadd}{+\hspace{2em}\# Use unsqueeze instead of indexing with None for clarity}\\
\textcolor{diffadd}{+\hspace{2em}A = self.A.unsqueeze(1).unsqueeze(2).expand(}\\
\textcolor{diffadd}{+\hspace{4em}num\_heads\_per\_rank, self.head\_dim, self.ssm\_state\_size).to(...)}\\[2pt]
\textcolor{diffadd}{+\hspace{2em}\# Optimize dt expansion - use unsqueeze for better performance}\\
\textcolor{diffadd}{+\hspace{2em}dt = dt.unsqueeze(2).expand(-1, -1, self.head\_dim)}\\[2pt]
\textcolor{diffadd}{+\hspace{2em}\# Optimize dt\_bias and D expansions}\\
\textcolor{diffadd}{+\hspace{2em}dt\_bias = self.dt\_bias.unsqueeze(1).expand(num\_heads\_per\_rank, self.head\_dim)}\\
\textcolor{diffadd}{+\hspace{2em}D = self.D.unsqueeze(1).expand(num\_heads\_per\_rank, self.head\_dim)}\\[2pt]
\textcolor{diffadd}{+\hspace{2em}\# Pre-compute B and C dimensions}\\
\textcolor{diffadd}{+\hspace{2em}b\_c\_dim = B.shape[1] // n\_groups}\\
\textcolor{diffadd}{+\hspace{2em}B = B.view(-1, n\_groups, b\_c\_dim)}\\
\textcolor{diffadd}{+\hspace{2em}C = C.view(-1, n\_groups, b\_c\_dim)}\\[2pt]
\mbox{}\hspace{2em}hidden\_states\_reshaped = hidden\_states.view(\\
\textcolor{diffrem}{-\hspace{4em}-1, self.num\_heads // self.tp\_size, self.head\_dim)}\\
\textcolor{diffadd}{+\hspace{4em}-1, num\_heads\_per\_rank, self.head\_dim)}\\[4pt]
\textcolor{diffrem}{@@ -535,7 +549,7 @@ class MambaMixer2(CustomOp):}\\
\mbox{}\hspace{2em}hidden\_states = hidden\_states.view(\\
\textcolor{diffrem}{-\hspace{4em}-1, (self.num\_heads // self.tp\_size) * self.head\_dim)}\\
\textcolor{diffadd}{+\hspace{4em}-1, num\_heads\_per\_rank * self.head\_dim)}
\endgroup

\vspace{6pt}
\textbf{Root Cause:} The original code uses \texttt{-1} in \texttt{expand()} to preserve each tensor's original first dimension dynamically. The agent replaced this with \texttt{num\_heads\_per\_rank}, a hardcoded value computed from model configuration. When actual tensor dimensions differ from this value (e.g., during certain batch configurations), shape mismatches corrupt the Mamba state-space model calculations, producing garbage outputs.

\end{tcolorbox}
\caption{Full patch from TRAE (Sonnet 4.5) on commit \texttt{fe66b347}, demonstrating a Q3 (Lucky Win) failure. The agent achieved speedup by computing incorrect values faster, causing 100\% accuracy regression on Bamba-9B.}
\label{fig:bamba-patch}
\end{figure*}

\subsection{Case Study: Prefix Caching Complete Failure}
\label{app:prefix-caching-case-study}

Figure~\ref{fig:prefix-caching-patch} shows a Q4 (Complete Failure) case where TRAE (Sonnet) completely missed the optimization target. On commit \texttt{2deb029d}, the task required fixing the prefix caching warmup performance in BlockManagerV2 by marking cache hit blocks as computed after scheduling. The human's elegant fix added a simple \texttt{\_touched\_blocks} set to track blocks and mark them computed via \texttt{mark\_blocks\_as\_computed()}.

The agent instead produced a sprawling patch that modified test files and reimplemented portions of the block allocator with unnecessary complexity---achieving \texttt{no\_optimization} classification because it never addressed the actual bottleneck: blocks not being marked as computed until freed.

\begin{figure*}[t]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=gsoboxblue,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: Prefix Caching Block Manager (Q4 Complete Failure)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE (Sonnet 4.5)

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (Agent Code Did Not Execute)

\textbf{Groundtruth Commit:} \url{https://github.com/vllm-project/vllm/commit/2deb029d}

\textbf{Specification Summary:} Fix BlockManagerV2 prefix caching warmup by marking cache hit blocks as computed after scheduling.

\textbf{Target:} Add \texttt{\_touched\_blocks} tracking to mark blocks computed after batch scheduling, matching BlockManagerV1 behavior.

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Agent Generated Patch} (excerpt from 153 lines)
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
diff --git a/tests/core/block/test\_prefix\_caching\_block.py b/...\\
\textcolor{diffadd}{+\hspace{1em}@staticmethod}\\
\textcolor{diffadd}{+\hspace{1em}def test\_mark\_blocks\_as\_computed():}\\
\textcolor{diffadd}{+\hspace{2em}block\_size = 16}\\
\textcolor{diffadd}{+\hspace{2em}common\_blocks = 4}\\
\textcolor{diffadd}{+\hspace{2em}allocator = PrefixCachingBlockAllocator(num\_blocks=8, ...)}\\
\textcolor{diffadd}{+\hspace{2em}...}\\
\textcolor{diffadd}{+\hspace{2em}allocator.mark\_blocks\_as\_computed(block\_ids)}\\[2pt]
diff --git a/vllm/core/block/prefix\_caching\_block.py b/...\\
\textcolor{diffrem}{-def \_incr\_refcount\_cached\_block(self, block: Block) -> None:}\\
\textcolor{diffrem}{-\hspace{2em}block.computed = True}\\
\textcolor{diffadd}{+def \_incr\_refcount\_cached\_block(self, block: Block,}\\
\textcolor{diffadd}{+\hspace{10em}computed: bool = False) -> None:}\\
\textcolor{diffadd}{+\hspace{2em}block.computed = computed}\\[2pt]
\textcolor{diffrem}{-def mark\_blocks\_as\_computed(self, block\_ids: List[int]) -> None:}\\
\textcolor{diffrem}{-\hspace{2em}raise NotImplementedError("Marking as computed is incremental")}\\
\textcolor{diffadd}{+def mark\_blocks\_as\_computed(self, block\_ids: List[int]) -> None:}\\
\textcolor{diffadd}{+\hspace{2em}for block\_id in block\_ids:}\\
\textcolor{diffadd}{+\hspace{3em}if block\_id in self.\_block\_tracker and ...}\\
\textcolor{diffadd}{+\hspace{4em}self.\_block\_tracker[block\_id].computed = True}
\endgroup

\vspace{6pt}
\textbf{Analysis:} The agent modified test files and reimplemented \texttt{mark\_blocks\_as\_computed()} but missed the core insight: blocks need to be tracked when allocated (via \texttt{\_touched\_blocks}) and marked computed after the entire batch is scheduled. The agent's approach of marking individual blocks by ID doesn't solve the warmup problem because the scheduler never passes the right block IDs. This demonstrates failure to understand the actual performance bottleneck.

\end{tcolorbox}
\caption{Patch from TRAE (Sonnet 4.5) on commit \texttt{2deb029d}, demonstrating Q4 (Complete Failure). The agent produced complex code that completely missed the actual optimization target.}
\label{fig:prefix-caching-patch}
\end{figure*}

\subsection{Case Study: MiniMax M2.1 Planning Without Execution}
\label{app:minimax-case-study}

Figure~\ref{fig:minimax-failure} shows an open-source model failure where MiniMax M2.1 verbalized optimization plans for 75 steps but never executed a single tool call. Despite generating 81,782 output tokens over 477 seconds, the model produced zero tool invocations---a fundamental failure to translate intent into action.

\subsection{Case Study: GPT-OSS-120B Environment Confusion}
\label{app:gpt-oss-case-study}

Figure~\ref{fig:gpt-oss-failure} shows an open-source model failure where GPT-OSS-120B fundamentally misunderstood the task environment. Instead of optimizing vLLM code, the model attempted to create mock implementations of PyTorch, Triton, and Transformers libraries inside the project directory---treating external dependencies as code to be written rather than APIs to be used.

\subsection{Case Study: GLM 4.7 Task Completion Failure}
\label{app:glm47-case-study}

Figure~\ref{fig:glm47-failure} shows an open-source model that made valid code edits but failed to complete the task workflow. GLM 4.7 successfully executed 386 tool calls including 59 successful \texttt{str\_replace} edits to the target files, but hit the 400-step limit without ever calling the \texttt{finish} command---cycling through git operations after encountering a confusing error message.

\subsection{Case Study: Qwen3-Coder Environment Errors}
\label{app:qwen3-case-study}

Figure~\ref{fig:qwen3-failure} shows an open-source model that actively engaged with the task but was terminated while attempting to resolve environment dependencies. Qwen3-Coder made 92 tool calls, created test scripts, and attempted to run performance benchmarks---but encountered \texttt{ModuleNotFoundError} when vLLM tried to import PyTorch, which was not available in the execution environment.

%% OSS Case Study Figures (one per page) %%

\begin{figure*}[p]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=diffrem,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: MiniMax M2.1 Infinite Loop (OSS Failure)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE-MiniMax M2.1

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (0 Tool Calls / 75 Steps)

\textbf{Task:} Scheduler performance optimization (same task as closed-source agents)

\textbf{Tool Calls:} \textbf{0}

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Execution Log (Steps 70--75)}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
Step 73: Completed\\
LLM Response: The user keeps saying I haven't completed\\
\mbox{}\hspace{2em}the task. I need to explore the repository and\\
\mbox{}\hspace{2em}make changes. Let me start by using the\\
\mbox{}\hspace{2em}str\_replace\_based\_edit\_tool tool.\\[4pt]
\mbox{}\hspace{2em}I need to actually use the tools now.\\
\mbox{}\hspace{2em}I need to actually use the tools now.\\
\mbox{}\hspace{2em}I need to actually use the tools now.\\
\mbox{}\hspace{2em}I need to actually use the tools now.\\
\mbox{}\hspace{2em}I need to actually use the tools now.\\
\mbox{}\hspace{2em}{\color{diffrem}[...repeated 2,412 times total...]}\\[4pt]
Step 74: Completed\\
LLM Response: I need to actually use the tools now...
\endgroup

\vspace{6pt}
\textbf{Analysis:} The model exhibits a \textit{planning-action gap}: it can verbalize the correct strategy (``use str\_replace\_based\_edit\_tool'') but cannot translate this into an actual tool call. The phrase ``I need to actually use the tools'' appears 2,412 times without any tool invocation, suggesting the model lacks the capability to bridge reasoning and execution.

\end{tcolorbox}
\caption{Execution log from MiniMax M2.1 showing infinite verbalization loop. Despite 75 reasoning steps, the model made zero tool calls---a fundamental failure to translate planning into action.}
\label{fig:minimax-failure}
\end{figure*}

\begin{figure*}[p]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=diffrem,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: GPT-OSS-120B Environment Confusion (OSS Failure)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE-GPT-OSS-120B

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (Created Mock Dependencies Instead of Optimizing)

\textbf{Task:} Scheduler performance optimization (same task as closed-source agents)

\textbf{Tool Calls:} $\sim$84 (all file creation attempts)

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Files the Model Attempted to Create}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
\textcolor{diffadd}{+ vllm\_core-0006/torch/\_\_init\_\_.py}\\
\textcolor{diffadd}{+ vllm\_core-0006/torch/nn/\_\_init\_\_.py}\\
\textcolor{diffadd}{+ vllm\_core-0006/torch/cuda/\_\_init\_\_.py}\\
\textcolor{diffadd}{+ vllm\_core-0006/triton/\_\_init\_\_.py}\\
\textcolor{diffadd}{+ vllm\_core-0006/transformers/\_\_init\_\_.py}\\[4pt]
\textrm{Contents of attempted} torch/\_\_init\_\_.py:\\[2pt]
\mbox{}\hspace{1em}class dtype:\\
\mbox{}\hspace{2em}pass\\[2pt]
\mbox{}\hspace{1em}float16 = 'float16'\\
\mbox{}\hspace{1em}float32 = 'float32'\\
\mbox{}\hspace{1em}\_\_version\_\_ = '0.0.0'
\endgroup

\vspace{6pt}
\textbf{Analysis:} The model exhibits \textit{environment confusion}: rather than understanding that PyTorch is an external dependency to import, it attempted to recreate these libraries from scratch. This suggests a fundamental failure to reason about software architecture---the model cannot distinguish between ``code I should optimize'' and ``libraries I should use.''

\end{tcolorbox}
\caption{File creation attempts from GPT-OSS-120B showing environment confusion. The model tried to implement mock PyTorch/Triton libraries instead of optimizing the actual vLLM scheduler code.}
\label{fig:gpt-oss-failure}
\end{figure*}

\begin{figure*}[p]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=diffrem,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: GLM 4.7 Task Completion Failure (OSS Failure)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE-GLM 4.7

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (Valid Edits, Never Called Finish)

\textbf{Task:} Scheduler performance optimization (same task as closed-source agents)

\textbf{Tool Calls:} 386 (327 bash, 59 str\_replace)

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Successful Edits Made (Step 196)}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
git commit output: 2 files changed, 11 insertions(+), 9 deletions(-)\\[4pt]
\textrm{Sample optimization in} scheduler.py:\\[2pt]
\textcolor{diffrem}{-\hspace{1em}self.\_num\_batched\_tokens += num\_batched\_tokens}\\
\textcolor{diffadd}{+\hspace{1em}self.\_num\_batched\_tokens = self.\_num\_batched\_tokens + num\_batched\_tokens}\\[4pt]
\textrm{Then at Step 198, attempted to verify:}\\[2pt]
\mbox{}\hspace{1em}{\color{diffrem}error: patch failed: tests/core/test\_scheduler.py:214}\\
\mbox{}\hspace{1em}{\color{diffrem}error: tests/core/test\_scheduler.py: patch does not apply}\\[4pt]
\textrm{Model response: "Let me try a different approach..."}\\[2pt]
{\color{diffrem}[...cycled through git operations for 200+ more steps...]}\\[4pt]
\textrm{Final status:} max\_steps\_exceeded (400 steps)
\endgroup

\vspace{6pt}
\textbf{Analysis:} The model exhibits \textit{task completion failure}: it successfully made valid edits and committed them, but when it tried to re-apply the already-applied patch for verification, git returned ``patch does not apply.'' Unable to interpret this as success, the model cycled through alternative approaches for 200+ steps without ever calling \texttt{finish}. This demonstrates a gap between code generation and workflow navigation.

\end{tcolorbox}
\caption{Execution trace from GLM 4.7 showing valid edits followed by task completion failure. The model made successful optimizations but could not recognize when to finalize the task.}
\label{fig:glm47-failure}
\end{figure*}

\begin{figure*}[p]
\begin{tcolorbox}[
  enhanced,
  colback=white,
  colframe=diffrem,
  coltitle=white,
  fonttitle=\bfseries\large,
  title={Case Study: Qwen3-Coder Environment Errors (OSS Failure)},
  sharp corners,
  boxrule=1pt,
  left=8pt,
  right=8pt,
  top=6pt,
  bottom=6pt,
]

\textbf{Model:} TRAE-Qwen3-Coder

\textbf{Result:} {\color{diffrem}\textbf{FAIL}} (SIGKILL During Dependency Resolution)

\textbf{Task:} Scheduler performance optimization (same task as closed-source agents)

\textbf{Tool Calls:} 92

\vspace{4pt}
{\color{gray}\hrule}
\vspace{6pt}

\textbf{Execution Trace}
\vspace{4pt}

\begingroup
\ttfamily\small\raggedright
Step 89: Viewed target files (scheduler.py, test\_scheduler.py)\\
Step 90: Removed old test file\\
Step 91: Created .bench\_scratch/test\_scheduler\_perf.py\\
Step 92: Ran test script...\\[4pt]
\mbox{}\hspace{2em}{\color{diffrem}ModuleNotFoundError: No module named 'torch'}\\
\mbox{}\hspace{2em}{\color{diffrem}\hspace{1em}File ".../vllm/config.py", line 7}\\
\mbox{}\hspace{2em}{\color{diffrem}\hspace{2em}import torch}\\[4pt]
Step 93: Diagnosed import error\\
Step 94: Attempted pip install torch...\\[2pt]
\mbox{}\hspace{2em}{\color{diffrem}[SIGKILL -9]}
\endgroup

\vspace{6pt}
\textbf{Analysis:} The model exhibits \textit{environment navigation failure}: unlike GPT-OSS-120B which tried to mock dependencies, Qwen3-Coder correctly identified that PyTorch was missing and attempted to install it. However, the execution environment did not permit package installation, and the model was terminated. This represents the closest any open-source model came to productive work---blocked by infrastructure rather than capability.

\end{tcolorbox}
\caption{Execution trace from Qwen3-Coder showing active engagement followed by environment errors. The model was terminated while attempting to install missing dependencies.}
\label{fig:qwen3-failure}
\end{figure*}

% \subsection{Quadrant Classification}
%
% We combine hard metrics (performance) with soft metrics (understanding) using the Four Quadrants Framework:
%
% \begin{center}
% \small
% \begin{tabular}{@{}l|cc@{}}
% & \textbf{Beats/Similar} & \textbf{Worse} \\
% \midrule
% \textbf{Same/Related target} & Q1: TRUE SUCCESS & Q2: Good Intent \\
% \textbf{Other target} & Q3: Lucky Win & Q4: Failure \\
% \end{tabular}
% \end{center}
%
% \begin{itemize}[leftmargin=1.5em,itemsep=1pt,topsep=2pt]
% \item \textbf{Q1 TRUE SUCCESS}: Agent identified the correct bottleneck AND achieved competitive performance. This represents genuine optimization capability.
% \item \textbf{Q2 Good Intent, Bad Execution}: Agent understood the optimization target but failed to implement it effectively. May indicate scaffolding or execution issues.
% \item \textbf{Q3 Lucky Win}: Agent achieved speedup through unrelated changes. Performance improvement is coincidental and may not generalize.
% \item \textbf{Q4 Complete Failure}: Agent neither identified the bottleneck nor achieved performance improvement.
% \end{itemize}
%
% The TRUE SUCCESS rate (Q1) is our primary aggregate metric, as it captures both semantic understanding and practical effectiveness. Table~\ref{tab:agent-configs} in the main paper reports these quadrant distributions for each agent.

\subsection{Reproducibility}

To ensure reproducibility of our results, we implement several controls. All agent runs use fixed random seeds where applicable. Docker images are pinned to specific CUDA 12.4 and PyTorch 2.4 versions. Benchmark commands execute with GPU isolation via \texttt{CUDA\_VISIBLE\_DEVICES} to prevent interference from other processes. We report mean $\pm$ standard deviation across 10 trials for all performance measurements.

The complete evaluation infrastructure, including Docker configurations, benchmark scripts, and analysis code, is available in the supplementary materials.

\endgroup

\end{document}
% We extract performance-related commits through a four-stage filtering pipeline:

% \textbf{Stage 1: Keyword filtering.} Identify commits where message or diff contains performance-related terms: ``optim'', ``perf'', ``speed'', ``latency'', ``throughput'', ``memory'', ``cache'', ``kernel'', ``fusion'', ``batch''. We select these 10 keywords based on manual analysis of 100 sample commits from each repository, achieving 92\% precision against expert annotation.

% \textbf{Stage 2: Scope filtering.} Retain commits modifying $<10$ files. Analysis of 100 commits shows that changes affecting $>10$ files typically mix performance optimization with refactoring.

% \textbf{Stage 3: Test coverage filtering.} Retain commits that include or reference existing performance tests.

% % \textbf{Stage 4: Measurable impact filtering.} Retain commits where changes affect runtime or memory usage, requiring $\geq5\%$ measurable difference.

% Table~\ref{tab:filtering} shows filtering statistics. From 64 vLLM commits and 80 SGLang commits, we expand to 99 and 80 tasks respectively.

% \begin{table}[t]
% \caption{Filtering Statistics}
% \label{tab:filtering}
% \centering
% \small
% \begin{tabular}{lcc}
% \toprule
% \textbf{Stage} & \textbf{vLLM} & \textbf{SGLang} \\
% \midrule
% Total commits & 15,234 & 8,421 \\
% Keyword matches & 892 & 547 \\
% $<10$ files & 341 & 312 \\
% Test coverage & 186 & 201 \\
% \textbf{Final tasks} & \textbf{99} & \textbf{80} \\
% \bottomrule
% \end{tabular}
% \end{table}

% \subsection{Automated Test Generation}

% For each commit, we generate an executable performance test using LLM-based analysis.

% \subsubsection{Test Synthesis}

% The LLM generates a Python test script with four required components: \textbf{(1) Minimal reproduction}, smallest input exercising the optimized code path; \textbf{(2) Performance measurement}, \texttt{time.perf\_counter()} with GPU synchronization; \textbf{(3) Standardized output}, timing in fixed format; \textbf{(4) Correctness validation}, output matches reference within \texttt{atol=1e-5}.

% \subsubsection{Validation Pipeline}

% We execute generated tests on three commits: base (parent), head (optimization), and main (current). Tests must satisfy: (i) syntactic correctness, (ii) execution success on all three commits, (iii) measurable speedup ($\text{base} / \text{head} \geq 1.05$), and (iv) reproducibility (CV $<10\%$).

% \subsection{Evaluation Metrics}

% Beyond binary success/failure, we measure \textbf{behavioral patterns}.

% \paragraph{Success Metrics.}
% \textbf{Clean Success:} 1 commit, 0 scope violations, tests pass, performance $\geq 0.95 \times$ human speedup.

% \paragraph{Behavioral Metrics.}
% \textbf{Commit Count:} Clean: 1. Pathological: 100-7,755.
% \textbf{Scope Violations:} Clean: 0. Pathological: 87-2,970.
% \textbf{Time-to-First-Edit (TTFE):} Clean: 60-180s. Pathological: $<1$s. \textbf{Key finding:} TTFE $<1$s predicts 95\% failure.

% \section{Experimental Results}
% \label{sec:results}

% We evaluate three agent architectures, Codex, TRAE, and OpenHands, on 179 tasks.

% \subsection{Overall Performance}

% Table~\ref{tab:agent-performance} summarizes agent performance. Codex achieves 100\% completion but success varies dramatically: 96.2\% clean success on SGLang vs 38.4\% on vLLM, a \textbf{58-point gap}.

% \begin{table}[t]
% \caption{Agent Performance Summary}
% \label{tab:agent-performance}
% \centering
% \small
% \begin{tabular}{llccc}
% \toprule
% \textbf{Agent} & \textbf{Repo} & \textbf{Tasks} & \textbf{Clean} & \textbf{Cost} \\
% \midrule
% Codex & vLLM & 99 & 38.4\% & $<$\$XX \\
% Codex & SGLang & 80 & 96.2\% & $<$\$XX\\
% TRAE & vLLM & 100 & $\sim$50\% & $\sim$\$XXX \\
% TRAE & SGLang & 80 & $\sim$36\% & \$XXX \\
% \bottomrule
% \end{tabular}
% \end{table}

% TRAE costs 16$\times$ more than Codex but achieves only 50\% success vs 38\%. Token usage: 1.6M tokens/task (vLLM) vs 712K (SGLang), 55\% reduction.

% \subsection{Bimodal Distribution}

% Performance exhibits bimodal distribution, tasks either succeed cleanly or fail catastrophically.

% \textbf{Mode 1: Clean Success} (38\% vLLM, 96\% SGLang): 1 commit, 0 violations, 47 lines (median), 1-2 files.

% \textbf{Mode 2: Pathological} (60\% vLLM, 0\% SGLang): 234 commits (median), max 7,755; 87 violations (median), max 2,970; 12,450 lines; 87 files.

% \section{Analysis and Discussion}
% \label{sec:analysis}

% \subsection{Instant-Edit Pathology}

% Agents that modify code within 1 second fail 95\% of the time. Tasks partition into:

% \textbf{Analysis-first} (TTFE $> 60$s): 70\% success, median 94s.

% \textbf{Instant-edit} (TTFE $< 1$s): 5\% success, median 0.8s.

% \textbf{Implication.} Mandatory analysis phases before code modification could prevent 95\% of failures.

% \subsection{Commit Explosion}

% Pathological failures exhibit commit explosion: median 234, max 7,755. Bimodal distribution with almost no tasks at 2-99 commits suggests critical transition, once incorrect initial edit, recovery is rare.

% \subsection{Codebase Characteristics}

% The 58-point gap motivates analyzing optimization-friendly codebases.

% \begin{table}[t]
% \caption{Codebase Characteristics}
% \label{tab:codebase}
% \centering
% \small
% \begin{tabular}{lcc}
% \toprule
% \textbf{Characteristic} & \textbf{vLLM} & \textbf{SGLang} \\
% \midrule
% Lines of code & $\sim$50K & $\sim$20K \\
% Module coupling & High & Low \\
% Test coverage & 60\% & 75\% \\
% Function length & 45 LOC & 25 LOC \\
% \bottomrule
% \end{tabular}
% \end{table}

% Spearman correlations across 179 tasks: File count ($\rho = -0.42$, $p < 0.001$), test coverage ($\rho = 0.38$, $p < 0.001$), function length ($\rho = -0.31$, $p < 0.01$), coupling ($\rho = -0.45$, $p < 0.001$). Well-structured codebases enable success.

% \subsection{Multi-Agent Trade-offs}

% % No agent dominates. \textbf{Codex:} 100\% completion, cheap ($<$\$1), opaque. \textbf{TRAE:} Full observability, expensive (\$16), 50\% success. Researchers use TRAE for understanding failures. Production uses Codex on well-tested codebases.

% \section{Conclusion}
% \label{sec:conclusion}

% We introduced ISO-Bench, 179 performance optimization tasks from production ML inference engines. Key findings:

% \textbf{Codebase characteristics dominate.} 58-point variance suggests code structure matters as much as agent architecture.

% \textbf{Behavioral patterns predict failure.} Instant-edit ($<1$s) predicts 95\% failure. Commit explosion (7,755 max) and scope violations (2,970 max) follow failure spirals.

% \textbf{Architectural trade-offs are fundamental.} No dominant choice between cost, observability, and success.

% By providing behavioral analysis beyond aggregate metrics, ISO-Bench enables understanding \textbf{why} agents fail and \textbf{how} to improve them.


% Bibliography
\bibliography{references}
\bibliographystyle{icml2026}

\end{document}

