[
  {
    "commit_hash": "70b808fe1a63",
    "full_commit": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "instance_id": "vllm_core-0035",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14377",
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
    "hardware": "H100",
    "perf_change": "+0.9%",
    "target_type": "same_target",
    "approach_type": "similar_approach",
    "correct_domain": "yes",
    "corrected_perf_command": null,
    "invalid": null,
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "ed25054577f7",
    "full_commit": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "instance_id": "vllm_core-0087",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21222",
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21222)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "hardware": "H100",
    "perf_change": "-1.1%",
    "target_type": "no_optimization",
    "approach_type": "other",
    "correct_domain": "no",
    "corrected_perf_command": "python benchmarks/kv_cache/benchmark_block_pool.py",
    "invalid": "might be too small",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "8a4e5c5f3c1d",
    "full_commit": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "instance_id": "vllm_core-0042",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20906",
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "hardware": "H100",
    "perf_change": "+1.7%",
    "target_type": "related_target",
    "approach_type": "partial_solution",
    "correct_domain": null,
    "corrected_perf_command": null,
    "invalid": "Disaggregated serving is not supported",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "f26c4aeecba4",
    "full_commit": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
    "instance_id": "vllm_core-0090",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11275",
    "commit_subject": "[Misc] Optimize ray worker initialization time (#11275)",
    "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
    "hardware": "H100-TP4",
    "perf_change": "-0.0%",
    "target_type": "no_optimization",
    "approach_type": "ineffective",
    "correct_domain": "no",
    "corrected_perf_command": null,
    "invalid": "needs 4 H100s",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "fa63e710c7fb",
    "full_commit": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "instance_id": "vllm_core-0091",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12094",
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)",
    "perf_command": "VLLM_USE_V1=1 python benchmarks/benchmark_latency.py --model meta-llama/Llama-3-8B --tensor-parallel-size 1 --input-len 1000 --batch-size 32",
    "hardware": "H100",
    "perf_change": "-0.5%",
    "target_type": "different_target",
    "approach_type": "ineffective",
    "correct_domain": "yes",
    "corrected_perf_command": null,
    "invalid": "might be too small re runns",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "6d0734c562e7",
    "full_commit": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "instance_id": "vllm_core-0031",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20645",
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
    "hardware": "H100",
    "perf_change": "-1.3%",
    "target_type": "different_target",
    "approach_type": "ineffective",
    "correct_domain": "no",
    "corrected_perf_command": "export VLLM_WORKER_MULTIPROC_METHOD=\"spawn\" && export VLLM_USE_V1=\"1\" && export VLLM_USE_STANDALONE_COMPILE=\"0\" && export VLLM_USE_FLASHINFER_MOE_FP8=\"0\" && python benchmarks/benchmark_latency.py --model=<model_dir> --output-len=1024 --tensor-parallel-size=8 --enable-expert-parallel --input-len=128 --trust_remote_code --max-model-len=2048 --batch-size=1",
    "invalid": "needs H100 for R1",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "61b8cea3b42f",
    "full_commit": "61b8cea3b42feab021d506e9143551de18f9165c",
    "instance_id": "vllm_core-0026",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21137",
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)",
    "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.2-3B-Instruct --dataset-name random --input-len 256 --output-len 128 --num-prompts 100",
    "hardware": "H100",
    "perf_change": "+0.0%",
    "target_type": "no_optimization",
    "approach_type": "ineffective",
    "correct_domain": "no",
    "corrected_perf_command": null,
    "invalid": "NVIDIA B200 GPU test",
    "has_baseline": true,
    "benchmark_status": "all_three"
  },
  {
    "commit_hash": "cf2f084d56a1",
    "full_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "instance_id": "vllm_core-0075",
    "pr_url": "https://github.com/vllm-project/vllm/pull/3279",
    "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "hardware": "H100",
    "perf_change": "+0.4%",
    "target_type": "different_target",
    "approach_type": "harmful",
    "correct_domain": "no",
    "corrected_perf_command": null,
    "invalid": "trade off of ITL and TTFT focus on those properly and seperately",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "80aa7e91fcd5",
    "full_commit": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
    "instance_id": "vllm_core-0038",
    "pr_url": "https://github.com/vllm-project/vllm/pull/4971",
    "commit_subject": "[Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "hardware": "Intel-CPU",
    "perf_change": "-0.5%",
    "target_type": "no_optimization",
    "approach_type": "ineffective",
    "correct_domain": "no",
    "corrected_perf_command": null,
    "invalid": "this is for intel CPU",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "99abb8b650c6",
    "full_commit": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "instance_id": "vllm_core-0051",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14930",
    "commit_subject": "[V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]'--ngram_prompt_lookup_min 5 --ngram-prompt-lookup-max 5 --num_speculative_tokens 3 --dataset-name sharegpt",
    "hardware": "H100",
    "perf_change": "-0.5%",
    "target_type": "same_target",
    "approach_type": "ineffective",
    "correct_domain": "yes",
    "corrected_perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --ngram_prompt_lookup_min 5 --ngram-prompt-lookup-max 5 --num_speculative_tokens 3 --dataset-name sharegpt",
    "invalid": "command issues",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "6ce01f30667b",
    "full_commit": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "instance_id": "vllm_core-0030",
    "pr_url": "https://github.com/vllm-project/vllm/pull/7051",
    "commit_subject": "[Performance] Optimize `get_seqs` (#7051)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --backend vllm --num-prompts 100",
    "hardware": "H100",
    "perf_change": "-0.8%",
    "target_type": "different_target",
    "approach_type": "ineffective",
    "correct_domain": null,
    "corrected_perf_command": "python3 benchmarks/benchmark_throughput.py --backend vllm --model meta-llama/Meta-Llama-3-8B-Instruct --dataset-name random --input-len 1024 --output-len 256 --num-prompts 1000",
    "invalid": "fixed the command",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "ca7a2d5f28ea",
    "full_commit": "ca7a2d5f28eac9621474563cdda0e08596222755",
    "instance_id": "vllm_core-0072",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14471",
    "commit_subject": "Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14384)\" (#14471)",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --dataset-name sharegpt --num-prompts 100",
    "hardware": "H100-TP2",
    "perf_change": "-1.0%",
    "target_type": "no_optimization",
    "approach_type": "ineffective",
    "correct_domain": null,
    "corrected_perf_command": "VLLM_USE_V1=1 vllm bench serve --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --tensor_parallel_size=2 --port 8192 --trust-remote-code",
    "invalid": "does this not need H100:2",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "8bc68e198c4c",
    "full_commit": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
    "instance_id": "vllm_core-0044",
    "pr_url": "https://github.com/vllm-project/vllm/pull/4208",
    "commit_subject": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "hardware": "H100",
    "perf_change": "-1.2%",
    "target_type": "related_target",
    "approach_type": "partial_solution",
    "correct_domain": null,
    "corrected_perf_command": null,
    "invalid": "this is some https://docs.vllm.ai/en/stable/examples/others/tensorize_vllm_model/ stuff",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  },
  {
    "commit_hash": "3476ed0809ec",
    "full_commit": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "instance_id": "vllm_core-0017",
    "pr_url": "https://github.com/vllm-project/vllm/pull/5602",
    "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)",
    "perf_command": "python benchmarks/benchmark_latency.py --model facebook/opt-125m --input-len 1536 --output-len 50 --batch-size 8",
    "hardware": "H100",
    "perf_change": "-1.6%",
    "target_type": "related_target",
    "approach_type": "partial_solution",
    "correct_domain": null,
    "corrected_baseline_perf_command": "python benchmarks/benchmark_latency.py --model facebook/opt-125m --input-len 1536 --output-len 50 --batch-size 8",
    "corrected_perf_command": "python benchmarks/benchmark_latency.py --model facebook/opt-125m --input-len 1536 --output-len 50 --batch-size 8 --use-v2-block-manager",
    "invalid": "different commands for baseline and perf main",
    "has_baseline": false,
    "benchmark_status": "no_baseline"
  }
]