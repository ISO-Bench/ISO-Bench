graph TD
    %% === INPUT SOURCES ===
    A[Configuration Files<br/>experiments.yaml<br/>commit_config.yaml] --> B1
    A --> B2
    A --> B3
    
    REPO[Target Repository<br/>Git History] --> B1
    EXTRACT[Commit Extractions<br/>JSON Files] --> B3
    
    %% === MAIN WORKFLOWS ===
    subgraph COLLECTION ["üîç Collection Framework Pipeline"]
        B1[Stage 1: Commit Analysis<br/>src/collect/analysis/commits.py]
        B1 --> B1a[PerfCommitAnalyzer<br/>- Git log filtering<br/>- LLM commit analysis<br/>- Performance relevance scoring]
        B1a --> B1b[Performance Commits<br/>JSON Output]
        
        B1b --> B2[Stage 2: API Identification<br/>src/collect/analysis/apis.py]
        B2 --> B2a[APIAnalyzer + Retriever<br/>- RAG-based API mapping<br/>- Code structure analysis<br/>- API-commit relationships]
        B2a --> B2b[API-Commit Map<br/>JSON Output]
        
        B2b --> B4[Stage 3: Test Generation<br/>src/collect/generate/generate.py]
        B4 --> B4a[PerfExpGenerator<br/>- LLM test generation<br/>- Problem creation<br/>- Test validation]
        B4a --> B4b[Test Problems<br/>problems.json]
        
        B4b --> B5[Stage 4: Test Execution<br/>src/collect/execute/execute.py]
        B5 --> B5a[ExecutionManager<br/>- SkyPilot orchestration<br/>- Async test execution<br/>- Performance measurement]
        B5a --> B5b[Results<br/>results.json]
        
        B5b --> B6[Results Evaluation<br/>src/collect/execute/evaluate.py]
        B6 --> B6a[Performance Analysis<br/>- Speedup calculations<br/>- Statistical summaries<br/>- Report generation]
    end
    
    %% === DATASET BUILDING ===
    subgraph DATASET ["üìä Dataset Building Pipeline"]
        B5b --> C1[Build Dataset<br/>src/collect/build_dataset.py]
        C1 --> C1a[Dataset Builder<br/>- Filter valid problems<br/>- Create instances<br/>- Apply thresholds]
        C1a --> C1b[GSO Dataset<br/>JSONL Format]
        
        C1b --> C2[Augment Dataset<br/>src/collect/scripts/augment_dataset.py]
        C2 --> C2a[Dataset Augmentation<br/>- Instance creation<br/>- Metadata enhancement]
        C2a --> C2b[Enhanced Dataset]
    end
    
    %% === COMMIT-TO-DATASET PIPELINE ===
    subgraph COMMIT2DS ["‚ö° Commit-to-Dataset Pipeline"]
        B3[commit_to_dataset.py<br/>Batch Processing]
        B3 --> B3a[Commit Processing<br/>- Scan extraction files<br/>- Extract commit metadata<br/>- Build unified diffs]
        
        B3a --> B3b[Test Generation<br/>- Find/generate test scripts<br/>- LLM-based generation<br/>- Syntax validation]
        
        B3b --> B3c[Performance Testing<br/>- Run on base/head commits<br/>- Parse execution times<br/>- Docker/local execution]
        
        B3c --> B3d[Dataset Records<br/>- Canonical format<br/>- Environment metadata<br/>- Performance measurements]
        
        B3d --> B3e[Export & Push<br/>- JSONL/Parquet output<br/>- SWE-Perf compatibility<br/>- HuggingFace upload]
    end
    
    %% === EVALUATION HARNESS ===
    subgraph HARNESS ["üéØ Evaluation Harness"]
        D1[Model Predictions<br/>JSONL Format]
        D2[Docker Images<br/>src/harness/prepare_images.py]
        
        D1 --> D3[Opt@K Evaluation<br/>src/harness/opt_at_k.py]
        D2 --> D3
        
        D3 --> D3a[Run Evaluation<br/>src/harness/run_evaluation.py<br/>- Load predictions<br/>- Filter dataset<br/>- Parallel execution]
        
        D3a --> D3b[Grade Instances<br/>src/harness/grading/grade.py<br/>- Container execution<br/>- Patch application<br/>- Performance testing]
        
        D3b --> D3c[Generate Reports<br/>src/harness/grading/metrics.py<br/>- Test result parsing<br/>- Performance metrics<br/>- Pass/fail evaluation]
        
        D3c --> D3d[Evaluation Reports<br/>JSON Output]
    end
    
    %% === TEST GENERATION SUBSYSTEM ===
    subgraph TESTGEN ["üß™ Test Generation Subsystem"]
        E1[Commit Extractions<br/>JSON Files]
        E1 --> E2[generate_test_generators.py<br/>src/test_scripts/]
        E2 --> E2a[LLM Client<br/>- Multi-provider support<br/>- Template processing<br/>- Code validation]
        E2a --> E2b[Test Generators<br/>Python Scripts]
        
        E2b --> E3[Performance Test Execution<br/>- Setup/experiment functions<br/>- Timing measurement<br/>- Result validation]
        
        E3 --> E4[Test Results<br/>Execution Times]
    end
    
    %% === DATA FLOWS ===
    C1b --> D3
    B3e --> D3
    E2b --> B3b
    E4 --> B3c
    
    %% === EXTERNAL SYSTEMS ===
    F1[SkyPilot<br/>Cloud Infrastructure] --> B5a
    F2[Docker<br/>Containerization] --> D3b
    F3[HuggingFace<br/>Dataset Hub] --> B3e
    F4[LLM APIs<br/>OpenAI/Anthropic] --> B1a
    F4 --> B2a
    F4 --> B4a
    F4 --> E2a
    
    %% === UTILITY COMPONENTS ===
    G1[Constants<br/>src/constants.py]
    G2[Data Models<br/>src/data/]
    G3[Utils<br/>src/utils/]
    
    G1 --> B1
    G1 --> B2
    G1 --> B4
    G1 --> B5
    G1 --> C1
    G1 --> D3
    
    G2 --> B1
    G2 --> B2
    G2 --> B4
    G2 --> B5
    G2 --> C1
    G2 --> D3
    
    G3 --> B1
    G3 --> B2
    G3 --> B4
    G3 --> B5
    G3 --> C1
    G3 --> D3
    
    %% === STYLING ===
    classDef collectionStyle fill:#e1f5fe,stroke:#01579b,color:#000
    classDef datasetStyle fill:#f3e5f5,stroke:#4a148c,color:#000
    classDef commitStyle fill:#fff3e0,stroke:#e65100,color:#000
    classDef harnessStyle fill:#e8f5e8,stroke:#1b5e20,color:#000
    classDef testgenStyle fill:#fce4ec,stroke:#880e4f,color:#000
    classDef externalStyle fill:#f5f5f5,stroke:#424242,color:#000
    classDef utilStyle fill:#fff8e1,stroke:#f57f17,color:#000
    
    class B1,B1a,B1b,B2,B2a,B2b,B4,B4a,B4b,B5,B5a,B5b,B6,B6a collectionStyle
    class C1,C1a,C1b,C2,C2a,C2b datasetStyle
    class B3,B3a,B3b,B3c,B3d,B3e commitStyle
    class D1,D2,D3,D3a,D3b,D3c,D3d harnessStyle
    class E1,E2,E2a,E2b,E3,E4 testgenStyle
    class F1,F2,F3,F4 externalStyle
    class G1,G2,G3 utilStyle