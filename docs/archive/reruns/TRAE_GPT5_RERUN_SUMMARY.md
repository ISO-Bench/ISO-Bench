# TRAE GPT-5 Rerun Summary

**Date:** 2025-12-25
**Status:** Ready to Execute

---

## Overview

This document summarizes the setup for rerunning unsuccessful TRAE + GPT-5 commits from the evaluation analysis in `ISO-Bench/eval_results_v2/`.

## Background

From the evaluation analysis:

- **Total TRAE + GPT-5 runs:** 434 commits
- **Tests passed:** 55 (12.7%)
- **No patch produced:** 223 (51.4% failure rate)
- **Errors:** 75

### Key Finding: OpenAI Tool Output Bug

According to `eval_results_v2/AGENT_FAILURE_ROOT_CAUSES.md`, the critical "No tool output found" bug affected **142 GPT-5 runs** (32.7% of all runs).

This bug was the OpenAI equivalent of Anthropic's tool_results bug:
- **Error:** `No tool output found for function call call_xxx`
- **Root Cause:** TRAE not sending tool results back after function calls
- **Affected:** gpt-5 (137 runs), gpt-4o (5 runs)

**Fix Applied:** Commit `a1c6d41` (2025-12-25) in `third-party/trae-agent/trae_agent/agent/base_agent.py`

With this fix, **we expect many previously failed commits to now succeed**.

---

## Extracted Failed Commits

### Total: 104 unique failed commits

#### By Repository
- **vLLM:** 53 commits
- **SGLang:** 51 commits

#### By Failure Reason
| Reason | Count | Notes |
|--------|------:|-------|
| OpenAI Tool Output Missing | 142 | TRAE not sending tool results (NOW FIXED) |
| OpenAI Quota Exceeded | 71 | Billing limit reached |
| Invalid API Key | 33 | Malformed or expired key |
| JSON Parse Errors | 19 | Model output truncated/malformed |
| AWS SSO Token Expiration | 1 | N/A for OpenAI (uses direct API key) |
| Other | 22 | Various infrastructure issues |

**Note:** Similar to Claude Sonnet 4.5 reruns, the tool call bug fix should address the majority of failures.

---

## Rerun Setup

### Files Created

1. **Commit Lists**
   - `ISO-Bench/TRAE_GPT5_VLLM_FAILED.txt` (53 commits)
   - `ISO-Bench/TRAE_GPT5_SGLANG_FAILED.txt` (51 commits)
   - `ISO-Bench/TRAE_GPT5_ALL_FAILED.txt` (104 commits)

2. **Rerun Plans** (JSON format for bench.cli)
   - `ISO-Bench/state/plan_trae_gpt5_vllm_rerun.json` (53 commits)
   - `ISO-Bench/state/plan_trae_gpt5_sglang_rerun.json` (51 commits)

3. **Execution Scripts**
   - `scripts/reruns/rerun_trae_gpt5_vllm.sh` - Run vLLM rerun only
   - `scripts/reruns/rerun_trae_gpt5_sglang.sh` - Run SGLang rerun only
   - `scripts/reruns/create_gpt5_rerun_plans.py` - Generate rerun plans
   - `scripts/reruns/analyze_gpt5_rerun_results.py` - Analyze results

---

## How to Run

### Prerequisites

1. **Set OpenAI API Key:**
   ```bash
   export OPENAI_API_KEY="your_openai_api_key"
   ```

2. **Verify API Key and Quota:**
   ```bash
   # Test API access
   curl https://api.openai.com/v1/models \
     -H "Authorization: Bearer $OPENAI_API_KEY" | head -20

   # Check key length
   echo $OPENAI_API_KEY | wc -c  # Should be > 10
   ```

3. **Create Rerun Plans:**
   ```bash
   cd /home/ubuntu/ISO-Bench
   python3 scripts/reruns/create_gpt5_rerun_plans.py
   ```

### Option 1: Run vLLM Rerun Only (53 commits)

```bash
cd /home/ubuntu/ISO-Bench
export TRAE_PYTHON=/home/ubuntu/ISO-Bench/bench-env/bin/python
export TRAE_CONFIG=/home/ubuntu/ISO-Bench/third-party/trae-agent/trae_config.yaml
export OPENAI_API_KEY="your_key"

./scripts/reruns/rerun_trae_gpt5_vllm.sh
```

**Expected duration:** ~6-8 hours (at ~7-10 min/commit)

### Option 2: Run SGLang Rerun Only (51 commits)

```bash
cd /home/ubuntu/ISO-Bench
export TRAE_PYTHON=/home/ubuntu/ISO-Bench/bench-env/bin/python
export TRAE_CONFIG=/home/ubuntu/ISO-Bench/third-party/trae-agent/trae_config.yaml
export OPENAI_API_KEY="your_key"

./scripts/reruns/rerun_trae_gpt5_sglang.sh
```

**Expected duration:** ~5-7 hours (at ~6-8 min/commit)

### Option 3: Run Complete Rerun (104 commits)

Run both scripts sequentially:
```bash
cd /home/ubuntu/ISO-Bench
export TRAE_PYTHON=/home/ubuntu/ISO-Bench/bench-env/bin/python
export TRAE_CONFIG=/home/ubuntu/ISO-Bench/third-party/trae-agent/trae_config.yaml
export OPENAI_API_KEY="your_key"

./scripts/reruns/rerun_trae_gpt5_vllm.sh && ./scripts/reruns/rerun_trae_gpt5_sglang.sh
```

**Expected duration:** ~12-15 hours total

### Manual Invocation

If you prefer to run manually:

```bash
cd /home/ubuntu/ISO-Bench/ISO-Bench

# vLLM rerun
../bench-env/bin/python -m bench.cli prepare tasks/vllm.yaml \
  --from-plan state/plan_trae_gpt5_vllm_rerun.json \
  --bench-cfg bench.yaml \
  --max-workers 1 \
  --resume

# SGLang rerun
../bench-env/bin/python -m bench.cli prepare tasks/sglang.yaml \
  --from-plan state/plan_trae_gpt5_sglang_rerun.json \
  --bench-cfg bench.yaml \
  --max-workers 1 \
  --resume
```

---

## What to Expect

### Expected Improvements

With the "No tool output found" bug fix, we expect:

1. **Significant reduction in tool output errors**
   - Previously: 142 runs failed due to this
   - Expected: 0 (bug is fixed)

2. **Higher success rate overall**
   - Previously: 12.7% success rate (55/434)
   - Expected: 27-32% success rate after rerun
   - Improvement: +14-19 percentage points

### Known Limitations

Some failures will persist due to:
- **OpenAI Quota Exceeded** (71 commits): Need sufficient API quota
- **Invalid API Key** (33 commits): Need valid, non-expired key
- **JSON Parse Errors** (19 commits): Model output issues (may be improved with latest model version)

These require addressing API credentials and quota, not the agent.

---

## Monitoring Progress

### View Results

```bash
cd /home/ubuntu/ISO-Bench/ISO-Bench

# Check success/error counts
grep -c "status determined as: success" pipeline_run_*.log
grep -c "status determined as: error" pipeline_run_*.log

# Monitor live (if running)
tail -f pipeline_run_*.log | grep -E "(Starting task|status determined|token usage)"
```

### Analyze Results

```bash
cd /home/ubuntu/ISO-Bench
python3 scripts/reruns/analyze_gpt5_rerun_results.py
```

### Check Individual Run

```bash
cd /home/ubuntu/ISO-Bench/ISO-Bench

# Find latest run
LATEST_VLLM=$(ls -t state/runs/vllm/trae/gpt-5* | head -n1)
LATEST_SGLANG=$(ls -t state/runs/sglang/trae/gpt-5* | head -n1)

# View journal for a specific commit
cat "$LATEST_VLLM"/vllm_gpt5_rerun_*/journal.json

# View patch (if generated)
cat "$LATEST_VLLM"/vllm_gpt5_rerun_*/model_patch.diff
```

---

## Cost Estimation

### API Costs

Based on GPT-5 pricing:

- **Average tokens per commit:** ~150K tokens
- **Input tokens:** ~120K @ $15/1M = $1.80/commit
- **Output tokens:** ~30K @ $60/1M = $1.80/commit
- **Total per commit:** ~$3.60

**Total for 104 commits:** $374
**Buffer for retries (+20%):** $449

### Compute Costs

- **EC2 instance:** g5.2xlarge or similar
- **Duration:** ~12-15 hours for complete rerun
- **Estimated cost:** $40-60

**Total estimated cost:** $489-509

---

## Success Criteria

### Minimum Success (Acceptable)
- **vLLM:** ≥25 commits successful (47% of 53 reruns)
- **SGLang:** ≥35 commits successful (69% of 51 reruns)
- **Combined:** ≥60 commits successful (58% of 104 reruns)

### Target Success (Expected)
- **vLLM:** ≥35 commits successful (66% of 53 reruns)
- **SGLang:** ≥40 commits successful (78% of 51 reruns)
- **Combined:** ≥75 commits successful (72% of 104 reruns)

### Exceptional Success (Best Case)
- **vLLM:** ≥40 commits successful (75% of 53 reruns)
- **SGLang:** ≥45 commits successful (88% of 51 reruns)
- **Combined:** ≥85 commits successful (82% of 104 reruns)

---

## Next Steps After Rerun

1. **Compare results with original run**
   - How many previously failed commits now succeed?
   - What's the new success rate?

2. **Analyze remaining failures**
   - Which failure categories persist?
   - Are there new issues to address?

3. **Update evaluation analysis**
   - Regenerate evaluation reports
   - Update success rate metrics

4. **Document findings**
   - Create detailed results document
   - Compare with Claude Sonnet 4.5 rerun performance

---

## Files Reference

### Analysis Files
- `ISO-Bench/eval_results_v2/AGENT_FAILURE_ROOT_CAUSES.md`
- `ISO-Bench/eval_results_v2/DEEP_ANALYSIS.md`
- `ISO-Bench/eval_results_v2/evaluation_report.json`

### Generated Files
- `scripts/reruns/create_gpt5_rerun_plans.py` - Plan creation script
- `scripts/reruns/rerun_trae_gpt5_vllm.sh` - vLLM execution script
- `scripts/reruns/rerun_trae_gpt5_sglang.sh` - SGLang execution script
- `scripts/reruns/analyze_gpt5_rerun_results.py` - Results analysis
- `ISO-Bench/TRAE_GPT5_*.txt` - Commit lists
- `ISO-Bench/state/plan_trae_gpt5_*.json` - Rerun plans

---

## Troubleshooting

### If OpenAI quota is exceeded
```bash
# Check quota status in OpenAI dashboard
# Consider using a different API key or increasing quota
```

### If run gets interrupted
The scripts use `--resume` flag, so you can simply restart:
```bash
./scripts/reruns/rerun_trae_gpt5_vllm.sh  # Will resume from where it left off
```

### If you want to skip certain commits
Edit the plan JSON file and remove the entries you want to skip, then restart the run.

---

## Contact

For questions or issues, refer to:
- Main README: `/home/ubuntu/ISO-Bench/README.md`
- TRAE documentation: `third-party/trae-agent/`
- Benchmark documentation: `ISO-Bench/README.md`
- Claude Sonnet 4.5 rerun summary: `docs/reruns/TRAE_SONNET45_RERUN_SUMMARY.md`
