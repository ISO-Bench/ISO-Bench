id: "moe_align_opt"
name: "MoE Align Sum Kernels Optimization"
description: "Optimize MoE align sum kernels and Python wrapper to reduce unnecessary initialization and speed up cumsum"
repo:
  url: "https://github.com/vllm-project/vllm.git"
  human_commit: "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add"
runner:
  requires_gpu: false
  allow_network_during_prepare: true
env_build:
  allowed_strategies: ["dockerfile", "requirements"]
  params:
    dockerfile_path: null
    requirements_file: null
optimization_contract:
  strict_targets: false
  target_files:
    - "vllm/model_executor/layers/fused_moe/moe_align_block_size.py"
    - "csrc/moe/moe_align_sum_kernels.cu"
    - "benchmarks/kernels/benchmark_moe_align_block_size.py"
  constraints:
    - "No public API breakage"
    - "Maintain functional equivalence"
    - "Focus on performance optimization"
testpack:
  entrypoint: "../vlm-bench-generic"
metrics: []
scoring:
  primary: "functional_equivalence"
  tie_breaker: "functional_equivalence"
id: "moe_align_opt"
name: "MoE Align Sum Kernels Optimization"
description: "Optimize MoE align sum kernels for improved performance"

repo:
  url: "https://github.com/vllm-project/vllm.git"
  human_commit: "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add"
  # pre_commit omitted - will resolve to parent

runner:
  requires_gpu: false
  cuda_version: null
  python_version: null
  platform: "linux/amd64"
  allow_network_during_prepare: true
  allowed_env: []

env_build:
  allowed_strategies:
    - "dockerfile"
    - "requirements"
  params:
    dockerfile_path: null
    requirements_file: null

optimization_contract:
  strict_targets: false
  target_files:
    - "benchmarks/kernels/benchmark_moe_align_block_size.py"
    - "csrc/moe/moe_align_sum_kernels.cu"
    - "vllm/model_executor/layers/fused_moe/moe_align_block_size.py"
  constraints:
    - "No public API breakage"
    - "Maintain functional equivalence"
    - "Focus on performance optimization"

testpack:
  entrypoint: "../vlm-bench-generic"
  version_constraint: ">=0.1.0"

metrics:
  - name: "kernel_performance"
    using: "runtime:throughput"
    args:
      cmd: "python -c \"print(1.0)\""
      warmups: 1
      trials: 3
      duration_s: 1
  - name: "functional_equivalence"
    using: "quality:hash_match"
    args:
      ref: "human.json"
      candidate: "{candidate}.json"

scoring:
  primary: "kernel_performance"
  tie_breaker: "functional_equivalence"
