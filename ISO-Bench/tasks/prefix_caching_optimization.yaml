id: "prefix_caching_opt"
name: "Prefix Caching Block Optimization"
description: "Optimize prefix caching block management for improved performance"

repo:
  url: "https://github.com/vllm-project/vllm.git"
  human_commit: "2deb029d115dadd012ce5ea70487a207cb025493"
  # pre_commit omitted - will resolve to parent

runner:
  requires_gpu: false
  cuda_version: null
  python_version: null
  platform: "linux/amd64"
  allow_network_during_prepare: true
  allowed_env: []

env_build:
  allowed_strategies:
    - "dockerfile"
    - "requirements"
  params:
    dockerfile_path: null
    requirements_file: null

optimization_contract:
  strict_targets: false
  target_files:
    - "tests/core/block/test_prefix_caching_block.py"
    - "vllm/core/block/prefix_caching_block.py"
    - "vllm/core/block_manager_v2.py"
  constraints:
    - "No public API breakage"
    - "Maintain functional equivalence"
    - "Focus on performance optimization"

testpack:
  entrypoint: "../vlm-bench-generic"
  version_constraint: ">=0.1.0"

metrics:
  - name: "block_performance"
    using: "runtime:throughput"
    args:
      cmd: "python -c \"print(1.0)\""
      warmups: 1
      trials: 3
      duration_s: 1
  - name: "functional_equivalence"
    using: "quality:hash_match"
    args:
      ref: "human.json"
      candidate: "{candidate}.json"

scoring:
  primary: "block_performance"
  tie_breaker: "functional_equivalence"
