{"instance_id": "/root/OmniPerf-Bench/vllm__repo-d3ea501", "model_patch": "diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py\nindex 5f0eb00..7b12ba8 100644\n--- a/vllm/worker/neuron_worker.py\n+++ b/vllm/worker/neuron_worker.py\n@@ -1,17 +1,15 @@\n+from __future__ import annotations\n+\n # SPDX-License-Identifier: Apache-2.0\n \"\"\"A Neuron worker class.\"\"\"\n from typing import List, Optional, Tuple\n \n-import torch\n-import torch.distributed\n-\n from vllm.config import VllmConfig\n from vllm.distributed import (ensure_model_parallel_initialized,\n                               init_distributed_environment)\n from vllm.model_executor import set_random_seed\n from vllm.model_executor.layers.sampler import SamplerOutput\n from vllm.sequence import ExecuteModelRequest\n-from vllm.worker.neuron_model_runner import NeuronModelRunner\n from vllm.worker.worker_base import (LocalOrDistributedWorkerBase,\n                                      LoraNotSupportedWorkerBase, WorkerBase,\n                                      WorkerInput)\n@@ -38,6 +36,9 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n             from vllm.utils import init_cached_hf_modules\n             init_cached_hf_modules()\n \n+        # Lazily import the heavy runner to avoid importing torch and\n+        # backend-specific modules during module import time.\n+        from vllm.worker.neuron_model_runner import NeuronModelRunner\n         self.model_runner: NeuronModelRunner = NeuronModelRunner(\n             vllm_config=vllm_config)\n         self.is_driver_worker = is_driver_worker\n@@ -100,10 +101,9 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         return False\n \n     @property\n-    def kv_cache(self) -> Optional[List[List[torch.Tensor]]]:\n+    def kv_cache(self) -> Optional[List[List[object]]]:\n         return None\n \n-    @torch.inference_mode()\n     def prepare_worker_input(\n             self, execute_model_req: ExecuteModelRequest) -> WorkerInput:\n         return WorkerInput(num_seq_groups=len(\n", "model_name_or_path": "gpt-5-2025-08-07"}
