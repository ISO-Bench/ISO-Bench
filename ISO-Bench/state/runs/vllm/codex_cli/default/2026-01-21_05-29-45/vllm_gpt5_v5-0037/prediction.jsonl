{"instance_id": "/root/OmniPerf-Bench/vllm__repo-1d35662", "model_patch": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 329b03a..675cdc1 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -273,6 +273,9 @@ class XGrammarLogitsProcessor:\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n+    # Cached device/cpu buffers to reduce allocations and copies.\n+    _device_token_bitmask: torch.Tensor | None = None\n+    _pinned_cpu_bitmask: torch.Tensor | None = None\n \n     def __getstate__(self) -> dict[str, Any]:\n         return {'config': self.config}\n@@ -285,6 +288,8 @@ class XGrammarLogitsProcessor:\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n         self.prefilled = False\n+        self._device_token_bitmask = None\n+        self._pinned_cpu_bitmask = None\n \n     def _ensure_ctx(self):\n         \"\"\"Lazily initialize the processor in the worker process\"\"\"\n@@ -311,40 +316,73 @@ class XGrammarLogitsProcessor:\n             ]\n             self.token_bitmask = xgr.allocate_token_bitmask(\n                 self.batch_size, self.config.vocab_size)\n-\n-        if not self.prefilled:\n-            # Have not sampled a token yet\n-            self.prefilled = True\n-        else:\n-            for i, matcher in enumerate(self.matchers):\n-                if not matcher.is_terminated():\n-                    sampled_token = input_ids[-1]\n-                    assert self.matchers[i].accept_token(sampled_token)\n-\n+            # Reset cached buffers as shape is now known.\n+            self._device_token_bitmask = None\n+            self._pinned_cpu_bitmask = None\n+\n+        # Combine accept_token (when applicable) and bitmask fill in a single\n+        # pass over matchers to reduce Python overhead.\n+        do_accept = self.prefilled\n+        if do_accept:\n+            sampled_token = input_ids[-1]\n         for i, matcher in enumerate(self.matchers):\n-            if not matcher.is_terminated():\n-                # @ubospica: ideally, fill_next_token_bitmask should be\n-                # parallelized with model decoding\n-                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n-                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+            if matcher.is_terminated():\n+                continue\n+            if do_accept:\n+                assert matcher.accept_token(sampled_token)\n+            # Ideally, this would be parallelized with model decoding.\n+            matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+        # Mark that we have now prefilled at least once.\n+        self.prefilled = True\n \n         # token_bitmask is a CPU tensor for use with accept_token and\n         # fill_next_token_bitmask so we move it to the device of scores\n-        device_type = scores.device.type\n+        device = scores.device\n         dtype = scores.dtype\n-        if device_type != \"cuda\":\n-            # xgrammar on cpu only supports float32 scores\n-            # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            scores = scores.to(\"cpu\").float().unsqueeze(0)\n-\n-        # Note: In this method, if the tensors have different dimensions\n-        # on CPU device fails, but on GPU it runs without error. Hence the\n-        # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(\n-            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n-        if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n-\n+        if device.type != \"cuda\":\n+            # xgrammar on CPU only supports float32 scores; ensure shape match.\n+            need_unsqueeze = scores.dim() == 1\n+            cpu_scores = scores\n+            if need_unsqueeze:\n+                cpu_scores = cpu_scores.unsqueeze(0)\n+            if cpu_scores.device.type != \"cpu\" or cpu_scores.dtype != torch.float32:\n+                cpu_scores = cpu_scores.to(device=\"cpu\", dtype=torch.float32)\n+\n+            xgr.apply_token_bitmask_inplace(cpu_scores, self.token_bitmask)\n+\n+            # Restore original dtype/device and dimensionality in a single hop.\n+            if cpu_scores.device != device or cpu_scores.dtype != dtype:\n+                cpu_scores = cpu_scores.to(device=device, dtype=dtype)\n+            if need_unsqueeze:\n+                cpu_scores = cpu_scores.squeeze(0)\n+            return cpu_scores\n+\n+        # GPU path: avoid reallocations by caching device-side bitmask and\n+        # using pinned host memory for faster HtoD transfers.\n+        if self._pinned_cpu_bitmask is None or \\\n+                self._pinned_cpu_bitmask.data_ptr() != self.token_bitmask.data_ptr():\n+            try:\n+                self._pinned_cpu_bitmask = self.token_bitmask.pin_memory()\n+            except Exception:\n+                # Fall back to the regular tensor if pinning is unavailable.\n+                self._pinned_cpu_bitmask = self.token_bitmask\n+\n+        if (self._device_token_bitmask is None or\n+                self._device_token_bitmask.device != device or\n+                self._device_token_bitmask.shape != self.token_bitmask.shape or\n+                self._device_token_bitmask.dtype != self.token_bitmask.dtype):\n+            self._device_token_bitmask = torch.empty_like(self.token_bitmask,\n+                                                          device=device)\n+\n+        # Non-blocking copy when possible (pinned host memory -> device).\n+        src = self._pinned_cpu_bitmask\n+        try:\n+            self._device_token_bitmask.copy_(src, non_blocking=True)\n+        except Exception:\n+            # If non_blocking copy is unsupported, fall back to a blocking copy.\n+            self._device_token_bitmask.copy_(src, non_blocking=False)\n+\n+        xgr.apply_token_bitmask_inplace(scores, self._device_token_bitmask)\n         return scores\n \n     def clone(self) -> XGrammarLogitsProcessor:\n", "model_name_or_path": "gpt-5-2025-08-07"}
