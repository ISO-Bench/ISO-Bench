diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index 329b03a..675cdc1 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -273,6 +273,9 @@ class XGrammarLogitsProcessor:
     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)
     batch_size: int = field(default=1)
     prefilled: bool = field(default=False)
+    # Cached device/cpu buffers to reduce allocations and copies.
+    _device_token_bitmask: torch.Tensor | None = None
+    _pinned_cpu_bitmask: torch.Tensor | None = None
 
     def __getstate__(self) -> dict[str, Any]:
         return {'config': self.config}
@@ -285,6 +288,8 @@ class XGrammarLogitsProcessor:
         self.batch_size = 1
         self.token_bitmask = None  # type: ignore[assignment]
         self.prefilled = False
+        self._device_token_bitmask = None
+        self._pinned_cpu_bitmask = None
 
     def _ensure_ctx(self):
         """Lazily initialize the processor in the worker process"""
@@ -311,40 +316,73 @@ class XGrammarLogitsProcessor:
             ]
             self.token_bitmask = xgr.allocate_token_bitmask(
                 self.batch_size, self.config.vocab_size)
-
-        if not self.prefilled:
-            # Have not sampled a token yet
-            self.prefilled = True
-        else:
-            for i, matcher in enumerate(self.matchers):
-                if not matcher.is_terminated():
-                    sampled_token = input_ids[-1]
-                    assert self.matchers[i].accept_token(sampled_token)
-
+            # Reset cached buffers as shape is now known.
+            self._device_token_bitmask = None
+            self._pinned_cpu_bitmask = None
+
+        # Combine accept_token (when applicable) and bitmask fill in a single
+        # pass over matchers to reduce Python overhead.
+        do_accept = self.prefilled
+        if do_accept:
+            sampled_token = input_ids[-1]
         for i, matcher in enumerate(self.matchers):
-            if not matcher.is_terminated():
-                # @ubospica: ideally, fill_next_token_bitmask should be
-                # parallelized with model decoding
-                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303
-                matcher.fill_next_token_bitmask(self.token_bitmask, i)
+            if matcher.is_terminated():
+                continue
+            if do_accept:
+                assert matcher.accept_token(sampled_token)
+            # Ideally, this would be parallelized with model decoding.
+            matcher.fill_next_token_bitmask(self.token_bitmask, i)
+        # Mark that we have now prefilled at least once.
+        self.prefilled = True
 
         # token_bitmask is a CPU tensor for use with accept_token and
         # fill_next_token_bitmask so we move it to the device of scores
-        device_type = scores.device.type
+        device = scores.device
         dtype = scores.dtype
-        if device_type != "cuda":
-            # xgrammar on cpu only supports float32 scores
-            # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22
-            scores = scores.to("cpu").float().unsqueeze(0)
-
-        # Note: In this method, if the tensors have different dimensions
-        # on CPU device fails, but on GPU it runs without error. Hence the
-        # unsqueeze above for scores, to match the token bitmask shape
-        xgr.apply_token_bitmask_inplace(
-            scores, self.token_bitmask.to(scores.device, non_blocking=True))
-        if device_type != "cuda":
-            scores = scores.to(dtype).to(device_type).squeeze()
-
+        if device.type != "cuda":
+            # xgrammar on CPU only supports float32 scores; ensure shape match.
+            need_unsqueeze = scores.dim() == 1
+            cpu_scores = scores
+            if need_unsqueeze:
+                cpu_scores = cpu_scores.unsqueeze(0)
+            if cpu_scores.device.type != "cpu" or cpu_scores.dtype != torch.float32:
+                cpu_scores = cpu_scores.to(device="cpu", dtype=torch.float32)
+
+            xgr.apply_token_bitmask_inplace(cpu_scores, self.token_bitmask)
+
+            # Restore original dtype/device and dimensionality in a single hop.
+            if cpu_scores.device != device or cpu_scores.dtype != dtype:
+                cpu_scores = cpu_scores.to(device=device, dtype=dtype)
+            if need_unsqueeze:
+                cpu_scores = cpu_scores.squeeze(0)
+            return cpu_scores
+
+        # GPU path: avoid reallocations by caching device-side bitmask and
+        # using pinned host memory for faster HtoD transfers.
+        if self._pinned_cpu_bitmask is None or \
+                self._pinned_cpu_bitmask.data_ptr() != self.token_bitmask.data_ptr():
+            try:
+                self._pinned_cpu_bitmask = self.token_bitmask.pin_memory()
+            except Exception:
+                # Fall back to the regular tensor if pinning is unavailable.
+                self._pinned_cpu_bitmask = self.token_bitmask
+
+        if (self._device_token_bitmask is None or
+                self._device_token_bitmask.device != device or
+                self._device_token_bitmask.shape != self.token_bitmask.shape or
+                self._device_token_bitmask.dtype != self.token_bitmask.dtype):
+            self._device_token_bitmask = torch.empty_like(self.token_bitmask,
+                                                          device=device)
+
+        # Non-blocking copy when possible (pinned host memory -> device).
+        src = self._pinned_cpu_bitmask
+        try:
+            self._device_token_bitmask.copy_(src, non_blocking=True)
+        except Exception:
+            # If non_blocking copy is unsupported, fall back to a blocking copy.
+            self._device_token_bitmask.copy_(src, non_blocking=False)
+
+        xgr.apply_token_bitmask_inplace(scores, self._device_token_bitmask)
         return scores
 
     def clone(self) -> XGrammarLogitsProcessor:
