OpenAI Codex v0.87.0 (research preview)
--------
workdir: /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
session id: 019bdf25-97a4-7301-9579-e5e1d72356e1
--------
user
I've uploaded a python code repository in the directory /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038.
Consider the following test script showing an example usage of the repository:

<test_script>
# This is a performance optimization task
# The specific operations to optimize are in the files listed below
# Focus on performance improvements in the target functions

</test_script>

Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?

Basic guidelines:
1. Your task is to make changes to non-test files in the /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 directory to improve the performance of the <test_script>.
2. Make changes while ensuring the repository is functionally equivalent to the original.
3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.
4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.

Follow these steps to improve performance:
1. As a first step, explore the repository structure and understand the target files.
2. Create a script ONLY inside /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/.bench_scratch (e.g., /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/.bench_scratch/test_opt.py) to reproduce and time the example, then execute it with python <filename.py> from the repo root.
3. Profile and identify performance bottlenecks in the target files.
4. Edit the source code of the repository to improve performance.
5. Rebuild and rerun your script to confirm that performance has improved.

Common optimization strategies to consider:
- Reduce unnecessary memory allocations
- Avoid redundant computations
- Optimize data structures and algorithms
- Improve cache locality
- Reduce synchronization overhead
- Use more efficient library calls where available

Target files to optimize:
- vllm/model_executor/models/qwen2_5_vl.py
- vllm/model_executor/models/qwen2_vl.py

IMPORTANT: You MUST make actual code changes to at least one file.
The task will fail if no files are modified.

## Constraints
- No public API breakage
- All TestPack checks must pass

## Target Files (ONLY modify these)
- `vllm/model_executor/models/qwen2_5_vl.py`
- `vllm/model_executor/models/qwen2_vl.py`

## Files Modified (statistics)
The following files were changed in the reference optimization:
```
vllm/model_executor/models/qwen2_5_vl.py | 45 +++++++++++++++++++++--------
 vllm/model_executor/models/qwen2_vl.py   | 49 ++++++++++++++++++++++++--------
 2 files changed, 70 insertions(+), 24 deletions(-)
```

## TASK COMPLETION COMMAND:
When you have made optimizations:
```bash
git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' > /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/model_patch.diff || true
finish
```

START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.

mcp startup: no servers



thinking
**Assessing code optimization approach**

I need to optimize a Python code repository based on the path provided. I've reviewed Codex CLI guidelines about preambles before shell commands and updating plans when using the plan tool. Since we only have access to local functions like `functions.shell` and `functions.update_plan`, web options aren't available. The user mentioned skills in AGENTS.md, but they don't seem relevant for this specific performance optimization task. I should clarify that we are not using skills here.
thinking
**Planning code optimization process**

I need to explore the repository to identify relevant files and create a script under `.bench_scratch` for timing and profiling. The goal is to modify two target files, rebuild if needed, and ensure performance improvements before committing and producing a patch file. I won’t ask for approval since the policy is to run the shell commands proactively. Before tool calls, I’ll provide concise preambles. First, I'll list the directory contents and inspect the two target files for any bottlenecks related to the "MoE align sum kernels" optimization.
thinking
**Exploring target files for optimization**

I can't just create content; I need to check the actual files to see what changes are necessary. The target files are `qwen2_5_vl.py` and `qwen2_vl.py`, located in `vllm/model_executor/models`. These files pertain to the Qwen visual-language model backends. I need to add optimizations while keeping the functionality the same. Even though the test script is a placeholder, I'll create my bench script under `.bench_scratch` for timing some functions. Let’s start by exploring the files with ripgrep to locate and open them.
codex
I’m going to scan the repo to find and inspect the two target model files, then create a small timing script under `.bench_scratch` to reproduce usage before making focused optimizations.
exec
bash -lc 'ls -la' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:
total 296
drwxr-xr-x 14 root root  4096 Jan 21 06:02 .
drwxr-xr-x  7 root root   155 Jan 21 06:02 ..
drwxr-xr-x  4 root root  4096 Jan 21 06:02 .buildkite
-rw-r--r--  1 root root   641 Jan 21 06:02 .clang-format
-rw-r--r--  1 root root   345 Jan 21 06:02 .dockerignore
drwxr-xr-x  8 root root  4096 Jan 21 06:02 .git
drwxr-xr-x  5 root root  4096 Jan 21 06:02 .github
-rw-r--r--  1 root root  3602 Jan 21 06:02 .gitignore
-rw-r--r--  1 root root  4236 Jan 21 06:02 .pre-commit-config.yaml
-rw-r--r--  1 root root   476 Jan 21 06:02 .readthedocs.yaml
-rw-r--r--  1 root root   496 Jan 21 06:02 .shellcheckrc
-rw-r--r--  1 root root    15 Jan 21 06:02 .yapfignore
-rw-r--r--  1 root root 24053 Jan 21 06:02 CMakeLists.txt
-rw-r--r--  1 root root  5318 Jan 21 06:02 CODE_OF_CONDUCT.md
-rw-r--r--  1 root root   154 Jan 21 06:02 CONTRIBUTING.md
-rw-r--r--  1 root root  1366 Jan 21 06:02 DCO
-rw-r--r--  1 root root 12597 Jan 21 06:02 Dockerfile
-rw-r--r--  1 root root  2395 Jan 21 06:02 Dockerfile.arm
-rw-r--r--  1 root root  2739 Jan 21 06:02 Dockerfile.cpu

thinking
**Finding target files**

I'm recognizing that the next step involves locating the target files. I need to ensure I'm clear on what specific files I'm looking for and their expected locations. I should maybe consider how I'm searching for them—through specific folders or using search functions. It's all about being organized and focused in this task, which is crucial for efficiency. Let's get started on this search!
exec
bash -lc "rg --line-number \"qwen2.*_vl\\.py"'$" -n vllm -S || true' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 19ms:
vllm/model_executor/models/qwen2_vl.py:4:# https://github.com/huggingface/transformers/blob/19e6e80e10118f855137b90740936c0b11ac397f/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
vllm/model_executor/models/qwen2_5_vl.py:4:# https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py
exec
bash -lc "sed -n '1,200p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:
# SPDX-License-Identifier: Apache-2.0

# Adapted from
# https://github.com/huggingface/transformers/blob/19e6e80e10118f855137b90740936c0b11ac397f/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py
# Copyright 2024 The Qwen team.
# Copyright 2023 The vLLM team.
# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
exec
bash -lc "sed -n '200,400p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 9ms:
    else:
        x1, x2 = x[..., ::2], x[..., 1::2]
        return rearrange(torch.stack((-x2, x1), dim=-1),
                         "... d two -> ... (d two)",
                         two=2)


def apply_rotary_emb_torch(x: torch.Tensor,
                           cos: torch.Tensor,
                           sin: torch.Tensor,
                           interleaved: bool = False) -> torch.Tensor:
    """
    x: (batch_size, seqlen, nheads, headdim)
    cos, sin: (seqlen, rotary_dim / 2) or (batch_size, seqlen, rotary_dim / 2)
    """
    ro_dim = cos.shape[-1] * 2
    assert ro_dim <= x.shape[-1]
    cos = repeat(
        cos,
        "... d -> ... 1 (2 d)" if not interleaved else "... d -> ... 1 (d 2)")
exec
bash -lc "sed -n '400,800p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:

        self.attn = Qwen2VisionAttention(embed_dim=dim,
                                         num_heads=num_heads,
                                         projection_size=dim,
                                         quant_config=quant_config,
                                         prefix=f"{prefix}.attn")
        self.mlp = Qwen2VisionMLP(dim,
                                  mlp_hidden_dim,
                                  act_layer=act_layer,
                                  quant_config=quant_config,
                                  prefix=f"{prefix}.mlp")

    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,
                rotary_pos_emb: torch.Tensor) -> torch.Tensor:
        x = x + self.attn(self.norm1(x),
                          cu_seqlens=cu_seqlens,
                          rotary_pos_emb=rotary_pos_emb)
        x = x + self.mlp(self.norm2(x))
        return x


thinking
**Optimizing performance issues**

This large file contains quite a bit. The goal is to optimize, but I need to pinpoint potential performance issues. Could it involve "align sum kernels" related to the mixture-of-experts approach? These are part of the vision modules, specifically in Qwen2-VL that uses vision transformer blocks. But the commit message about optimizing MoE performance might not relate directly to this file. I might need to check the other file for further insights.
exec
bash -lc "sed -n '1,240p' vllm/model_executor/models/qwen2_5_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 5ms:
# SPDX-License-Identifier: Apache-2.0

# Adapted from
# https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py
# Copyright 2025 The vLLM team.
# Copyright 2025 The Qwen Team.
# Copyright 2025 The HuggingFace Inc. team.
# All rights reserved.
#
# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
# and OPT implementations in this library. It has been modified from its
# original forms to accommodate minor architectural differences compared
# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
exec
bash -lc "sed -n '240,560p' vllm/model_executor/models/qwen2_5_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 5ms:
        # [s, b, 3 * head * head_dim] -> 3 * [s, b, head * head_dim]
        q, k, v = qkv.chunk(3, dim=2)

        # 3 * [s, b, head * head_dim]
        if self.tp_size > 1:
            splitter = partial(dist_utils.split_tensor_along_last_dim,
                               num_partitions=self.tp_size)
            q = splitter(q)[self.tp_rank]
            k = splitter(k)[self.tp_rank]
            v = splitter(v)[self.tp_rank]

        # 3 * [s, b, head * head_dim] -> 3 * [s, b, head, head_dim]
        new_shape = (seq_len, bs, self.num_attention_heads_per_partition,
                     self.hidden_size_per_attention_head)
        q, k, v = (x.view(*new_shape) for x in (q, k, v))
        return q, k, v

    def forward(
        self,
        x: torch.Tensor,
exec
bash -lc "sed -n '560,1120p' vllm/model_executor/models/qwen2_5_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:
        max_grid_size = grid_thw[:, 1:].max()
        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
        return rotary_pos_emb

    def get_window_index(self, grid_thw):
        window_index: list = []
        cu_window_seqlens: list = [0]
        window_index_id = 0
        vit_merger_window_size = (self.window_size //
                                  self.spatial_merge_size // self.patch_size)

        for grid_t, grid_h, grid_w in grid_thw:
            llm_grid_h = grid_h // self.spatial_merge_size
            llm_grid_w = grid_w // self.spatial_merge_size
            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(
                grid_t, llm_grid_h, llm_grid_w)
            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size
            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size
            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size

thinking
**Optimizing complex files**

Both files are intricate, and I’m thinking about how to optimize them. The commit message, "Optimize MoE align sum kernels performance," stands out as odd for this context. Maybe they’re looking for patterns like "repeat_interleave," "reshape," and "permute," to improve performance by reducing unnecessary copies and using in-place operations. The mention of "MoE align sum kernels" hints at some mixture-of-experts technique, possibly for vision transformers. I’ll search for terms like "align," "moe," and "expert" in both files to clarify.
exec
bash -lc 'rg -n "moe|expert|align|sum kernel|align_sum|einsum|repeat_interleave|cumsum" vllm/model_executor/models/qwen2_vl.py vllm/model_executor/models/qwen2_5_vl.py -S' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 12ms:
vllm/model_executor/models/qwen2_vl.py:620:        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
vllm/model_executor/models/qwen2_vl.py:621:                                             grid_thw[:, 0]).cumsum(
vllm/model_executor/models/qwen2_5_vl.py:593:            cu_seqlens_tmp = seqlens.cumsum(
vllm/model_executor/models/qwen2_5_vl.py:629:        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
vllm/model_executor/models/qwen2_5_vl.py:630:                                             grid_thw[:, 0]).cumsum(


thinking
**Evaluating optimization strategies**

I'm noticing that heavy torch operations may not be optimized well, particularly when CPU operations create large arrays that get moved to a device. There’s room for improvement here. For example, using "smart_resize" isn’t relevant in some parts. I’m considering replacing `torch.cat(outputs, dim=1)` with preallocated output to reduce memory allocation. The hint about optimizing “MoE align sum kernels performance” makes me want to implement “align_sum” using torch.add. It’s in the utils merge function, though not currently in the target files.
thinking
**Considering necessary modifications**

It looks like the modifications need to be made only in two specific files. I'm not entirely sure what the test harness measures, but it often imports the model and runs functions like "get_window_index" or "rot_pos_emb." I need to ensure that performance improvements are measured properly in their script. The reference optimization changed certain lines and may have replaced naive Python loops with vectorized operations. In `rot_pos_emb`, the loop over grid_thw builds position IDs and could be a bottleneck with many images. Similar issues may arise in `get_window_index`, so I'll search for analogous functions in Qwen2VLVisionTransformer.
exec
bash -lc "rg -n \"rot_pos_emb\\(|get_window_index\\(|patch_embed|VisionTransformer\" vllm/model_executor/models/qwen2_vl.py -n -S" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 7ms:
521:class Qwen2VisionTransformer(nn.Module):
546:        self.patch_embed = Qwen2VisionPatchEmbed(
576:        return self.patch_embed.proj.weight.dtype
580:        return self.patch_embed.proj.weight.device
582:    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
614:        x = self.patch_embed(x)
617:        rotary_pos_emb = self.rot_pos_emb(grid_thw)
1068:        self.visual = Qwen2VisionTransformer(
exec
bash -lc "sed -n '520,700p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:

class Qwen2VisionTransformer(nn.Module):

    def __init__(
        self,
        vision_config: Qwen2VLVisionConfig,
        norm_eps: float = 1e-6,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()

        patch_size = vision_config.patch_size
        temporal_patch_size = vision_config.temporal_patch_size
        spatial_merge_size = vision_config.spatial_merge_size
        in_channels = vision_config.in_channels
        hidden_size = vision_config.hidden_size
        embed_dim = vision_config.embed_dim
        depth = vision_config.depth
        num_heads = vision_config.num_heads





thinking
**Implementing library changes**

I need to import the 'flash_attn' library, as it might not be installed. The previous versions are already in the FlashAttn path. I noticed that the `apply_rotary_pos_emb_vision` function only uses 'flash_attn.layers.rotary' if `use_flash_attn` is true, and we'll call it with `use_flash_attn` set to true.

Next, I’m replacing 'einops.rearrange' for simpler transposes with `.transpose()` or `.permute()` to save on overhead, rewriting the relevant sections accordingly. For the FlashAttn path, I'll use 'reshape' to maintain safety when dealing with dimensions.
thinking
**Reshaping tensor dimensions**

I’m working with the shape of `q`, where B and S are `q.shape[0]` and `q.shape[1]`, respectively. I'll create a new shape `(B*S,) + q.shape[2:]` and reshape `q` accordingly. This sim applies for `k` and `v` as well.

When reassembling, I’ll rearrange `output` into `context_layer` using `reshape`, which is more straightforward than `view`. I’ll also be replacing einops rearrangements with `.transpose()` or `.permute()` for clarity and efficiency throughout.
thinking
**Optimizing tensor operations**

I'm replacing einops with built-in functions like `transpose`, `permute`, and `reshape` to reduce overhead. For the TORCH_SDPA path, I need to preallocate `context_layer` as an empty tensor with specified dimensions using the data type and device from `q`.

Next, I noticed in `Qwen2VisionRotaryEmbedding.update_freqs_cache`, `self.inv_freq` is reassigned to a new tensor every time, but it's only recalculated when `seqlen` increases, which is reasonable. Lastly, I want to vectorize the `rot_pos_emb` loops to improve efficiency without losing readability, although that adds some complexity.
thinking
**Implementing positional ID optimizations**

To improve speed, I want to compute positional IDs using arithmetic without stacking operations in a loop. I noticed changes made in the 'reference optimization' that align with rearranging and preallocating, along with caching cos and sin. I’ll implement these updates across both files.

First, in `qwen2_vl.apply_rotary_pos_emb_vision`, I’ll check if `freqs` is a tuple or list to adjust cosine and sine calculations accordingly. In `qwen2_vl.Qwen2VisionAttention.forward`, after computing and transposing `q`, `k`, and `v`, I need to check the attention backend, using `use_flash_attn` with the appropriate function calls for FLASH_ATTN, which we’ll modify as necessary.
thinking
**Implementing optimizations in VisionTransformer**

To implement changes in `VisionTransformer.forward`, I'll first interpret `rotary_pos_emb` into cosine and sine components, then reindex them accordingly. While applying attention, I need to ensure `rotary_pos_emb` is passed as a tuple.

I’ll maintain consistency in types, making sure calculations are done on `self.device`.

Next, I’ll create a testing script, `.bench_scratch/test_opt.py`, to benchmark `VisionAttention.forward` using minimal tensors and different configurations, avoiding heavy libraries. The focus will be on comparing performance between the old and new methods, while also ensuring we don’t disrupt the public API.
codex
Next, I’ll implement targeted optimizations in the two model files and add a small timing script under `.bench_scratch` to exercise the modified paths. Then I’ll run the script to validate runtime.

thinking
**Correcting the tool usage**

I think I made a mistake by using the wrong tool. The correct way to access the apply_patch tool is through functions.shell with the command ["apply_patch", patch]. I’m realizing I need to run it properly this time. It’s sometimes tricky to keep all these processes straight! Let's make sure to get it right on the next attempt. Alright, I’ll proceed to re-run it correctly.
exec
bash -lc "nl -ba vllm/model_executor/models/qwen2_vl.py | sed -n '150,240p'" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 7ms:
   150	    Tensor shape: `(num_image_features, hidden_size)`
   151	    - `num_image_features` varies based on
   152	        the number and resolution of the videos.
   153	    - `hidden_size` must match the hidden size of language model backbone.
   154	    """
   155
   156	    video_grid_thw: torch.Tensor
   157	    """Shape: `(num_videos, 3)`
   158	    This should be in `(grid_t, grid_h, grid_w)` format.
   159	    """
   160
   161
   162	Qwen2VLVideoInputs = Union[Qwen2VLVideoPixelInputs,
   163	                           Qwen2VLVideoEmbeddingInputs]
   164
   165	# === Vision Encoder === #
   166
   167
   168	class Qwen2VisionMLP(nn.Module):
   169
exec
bash -lc "sed -n '240,320p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:
        from flash_attn.layers.rotary import apply_rotary_emb
    output = apply_rotary_emb(t_, cos, sin).type_as(t)
    return output


class Qwen2VisionAttention(nn.Module):

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        projection_size: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
    ) -> None:
        super().__init__()
        # Per attention head and per partition values.
        world_size = parallel_state.get_tensor_model_parallel_world_size()
        self.tp_size = world_size
        self.tp_rank = parallel_state.get_tensor_model_parallel_rank()
exec
bash -lc "sed -n '320,520p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 6ms:
                   for x in (q, k, v))
        if rotary_pos_emb is not None:
            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)

        if self.attn_backend == _Backend.FLASH_ATTN:
            # from vllm_flash_attn.flash_attn_interface import (
            #   flash_attn_varlen_func)
            from flash_attn import flash_attn_varlen_func

            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])

            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
            output = flash_attn_varlen_func(q,
                                            k,
                                            v,
                                            cu_seqlens_q=cu_seqlens,
                                            cu_seqlens_k=cu_seqlens,
                                            max_seqlen_q=max_seqlen,
                                            max_seqlen_k=max_seqlen,

thinking
**Updating patch changes**

I’m focusing on crafting the correct patch changes to replace the 'apply_rotary_pos_emb_vision' function. I want to make sure to thoroughly modify the attention forward and rot_pos_emb functions step by step. First, I’ll update 'apply_rotary_pos_emb_vision' to accept a tuple and the use_flash_attn parameter. I'll ensure to adjust the 'forward' method types for consistency. Lastly, I'll confirm the return type for the rot_pos_emb method and check the necessary imports at the top. Let's finalize this patch!
file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_vl.py
@@ -231,8 +231,18 @@

-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
apply_patch(auto_approved=true) exited 0 in 5ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_vl.py
file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..c66b1f03297b8990b4d1ba2fd1700aee8ddc7377
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb

file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..c66b1f03297b8990b4d1ba2fd1700aee8ddc7377
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb





thinking
**Optimizing CPU Usage**

I need to ensure we're using a CPU environment that doesn’t require heavy GPU processing, but I wonder if vllm might still need that. Since the files are imported, maybe I can instantiate minimal components and call specific functions like rot_pos_emb to measure performance. My goal is to optimize those operations. I think a script that runs VisionTransformer and computes patch embedding quickly on CPU would help. On the other hand, vLLM's multi-dependencies might be heavy, so I'll have to be strategic about isolating functions.
thinking
**Designing a Timing Script**

The guideline suggests I create a script to reproduce and time example usage, but there isn't one provided. I think I can design a script to call Qwen2VisionTransformer.forward using random data. However, that requires the Qwen2VLVisionConfig from transformers. To avoid heavy installations, I could create a test that only runs internal functions like apply_rotary_pos_emb_vision and rot_pos_emb. To show speed improvement, I'll measure computations and compare them, focusing on changes we made to include cosine and sine functions, but I realize I struggled with modifying rot_pos_emb successfully.
thinking
**Updating rot_pos_emb for Performance**

I need to update rot_pos_emb in qwen2_vl to precompute cosine and sine values and return a tuple instead. I'll also modify the forward function and document types accordingly, making sure to update its usage in Qwen2VisionAttention.forward with the use_flash_attn parameter. Without these changes, the code could break since apply_rotary_pos_emb_vision can now accept tuples. The earlier patch focused on just that function. I want to ensure our updates don't create API breaks, especially since changing to a tuple could affect Qwen2VisionAttention.forward. Let's go ahead and implement these modifications.
thinking
**Optimizing the TORCH_SDPA Block**

I need to optimize the "TORCH_SDPA" block, where we previously collected outputs in a list and concatenated them. To simplify this, I’ll preallocate and assign segments to avoid overhead. It seems that my earlier attempt to patch this didn’t work because the function starts around line 334. I’ll focus on replacing that specific section. I also want to modify how we handle shapes to avoid creating multiple temporary variables by using permute instead of rearrange.

Additionally, I’ll optimize rot_pos_emb by changing it to use self.device for CPU range and converting to int while maintaining the default dtype. I’ll include cosine and sine precomputation as well. Let's patch qwen2_vl!
codex
I’ll update the Qwen2-VL model to precompute rotary cos/sin once per forward, avoid repeated reallocation in SDPA by preallocating the output tensor, and minimize tensor reshapes. Then I’ll apply similar optimizations to Qwen2.5-VL and add a timing script.
file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_vl.py
@@ -318,3 +318,3 @@
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:
@@ -328,7 +328,10 @@

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

@@ -339,3 +342,7 @@

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

@@ -352,8 +359,12 @@

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
@@ -364,4 +375,6 @@
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
@@ -370,5 +383,5 @@
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
@@ -384,4 +397,5 @@
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_vl.py
file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..0845361c7ee64e3a78f90bd2b6dfefe696a0035f
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output

file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..0845361c7ee64e3a78f90bd2b6dfefe696a0035f
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output

file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_vl.py
@@ -605,19 +605,25 @@

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
@@ -625,6 +631,6 @@
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

@@ -639,3 +645,3 @@

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)
apply_patch(auto_approved=true) exited 0 in 6ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_vl.py
file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_5_vl.py
@@ -273,2 +273,3 @@
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
@@ -285,3 +286,6 @@

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

@@ -298,8 +302,12 @@

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
@@ -310,4 +318,5 @@
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
@@ -316,5 +325,4 @@
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
@@ -330,4 +338,4 @@
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_5_vl.py
file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..64c7de21d0aea786784ba445834d8c248418351d
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..64c7de21d0aea786784ba445834d8c248418351d
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_5_vl.py
@@ -547,7 +547,14 @@

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
@@ -557,3 +564,3 @@
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
@@ -563,3 +570,3 @@
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
@@ -567,6 +574,6 @@
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_5_vl.py
file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

exec
bash -lc "mkdir -p .bench_scratch && python - << 'PY'
import torch
from vllm.model_executor.models.qwen2_vl import Qwen2VisionTransformer
from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLVisionConfig
import time

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vision_config = Qwen2VLVisionConfig(
    patch_size=14,
    temporal_patch_size=2,
    spatial_merge_size=2,
    in_channels=3,
    hidden_size=1024,
    embed_dim=1152,
    depth=2,
    num_heads=16,
    mlp_ratio=4.0,
)
model = Qwen2VisionTransformer(vision_config).to(device)
# create a fake grid: 2 images with t=1,h=28,w=28 patches before merge
grid = torch.tensor([[1,28,28],[1,28,28]], device=device)
# input patches: L = sum(t*h*w) * (temporal_patch_size*patch_size*patch_size*in_channels)
L = int((grid[:,0]*grid[:,1]*grid[:,2]).sum().item())
C = 3 * vision_config.patch_size * vision_config.patch_size * vision_config.temporal_patch_size
x = torch.randn(L, C, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)

# warmup
with torch.no_grad():
    for _ in range(2):
        model(x, grid)

# time
start = time.time()
with torch.no_grad():
    for _ in range(5):
        model(x, grid)
end = time.time()
print(\"Qwen2VisionTransformer avg iter ms:\", (end-start)/5*1000)
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 exited 1 in 2.97s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from .version import __version__, __version_tuple__  # isort:skip
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/__init__.py", line 11, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/engine/arg_utils.py", line 14, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/config.py", line 33, in <module>
    from vllm.transformers_utils.config import (
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/configs/__init__.py", line 17, in <module>
    from vllm.transformers_utils.configs.mllama import MllamaConfig
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/configs/mllama.py", line 3, in <module>
    from transformers.models.mllama import configuration_mllama as mllama_hf_config
ModuleNotFoundError: No module named 'transformers.models.mllama'
file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens






thinking
**Optimizing functions**

I need to optimize functions from qwen2_vl.py and qwen2_5_vl.py, specifically the pure functions I modified: apply_rotary_pos_emb_vision, rot_pos_emb, and the SDPA loops in Qwen2VisionAttention. To do this, I need to import the module properly; however, I can read and exec the file with an alias import path to avoid heavy dependencies. It looks like direct loading will fail due to numerous imports. I should create a micro-benchmark script that replicates the essential operations I optimized while keeping performance improvements in focus.
thinking
**Creating a testing script**

It seems I can't run our script directly, but the instruction specifies it must execute with python <filename.py> to reproduce the results. Even though the environment is complex, I could craft a script that doesn't import vllm while timing a baseline versus the optimized code path. However, that doesn’t validate the repository changes. The test pack likely runs unit tests that require vllm, but I can't execute them now. I need to make .bench_scratch/test_opt.py and find a way around the import issue, possibly by setting 'importlib' to bypass loading vllm.__init__.
thinking
**Developing a loading script**

I think I could create a script that uses importlib to load 'qwen2_vl_module' from the specified file path while defining stub modules to satisfy the imports like 'from vllm...'. This would involve creating a fake 'vllm' package with just enough attributes to meet the requirements. Although I could provide stub versions for things like VllmConfig and parallel_state, it might be too complex because of the numerous classes being used. An alternative would be to write a benchmark script that synthesizes data and employs the updated algorithm with precomputed cos/sin rather than running everything from the original module.
thinking
**Addressing execution requirements**

It looks like I need to keep in mind that the requirement is to "execute it with python <filename.py> from the repo root." They do want me to time the example usage, so my script can perform an independent micro-benchmark to demonstrate performance. The second step mentions reproducing and timing the example, which refers back to an earlier test script that was empty. I can create a micro-benchmark while also applying similar changes to qwen2_5_vl, matching the patch's line numbers. I'll also ensure that the Union type is imported at the top of the file.
thinking
**Finalizing benchmark setup**

Good, I've changed the function rot_pos_emb to return a tuple, and we're now correctly passing it in Qwen2VisionAttention.forward. However, Qwen2VisionBlock.forward still expects a 'torch.Tensor' type that we didn't modify, but that's just Python type hints, not enforced. In qwen2_5_vl, similar changes were made for attention and rot_pos_emb. Now, we need to create a timing script in .bench_scratch, to avoid importing vllm, using synthetic attention with replicated functions. While we can read functions from our local code, actual module testing is not the main goal—rather focusing on optimization. I'll implement test_opt.py for benchmarking both baseline and optimized functions, as well as simulate SDPA paths. We may need to install the correct transformers version since the ModuleNotFoundError indicates it’s outdated. There are options to install quickly or check the environment setup. Let's see what works best!
exec
bash -lc "sed -n '1,200p' python_only_dev.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 5ms:
# SPDX-License-Identifier: Apache-2.0

msg = """Old style python only build (without compilation) is deprecated, please check https://docs.vllm.ai/en/latest/getting_started/installation.html#python-only-build-without-compilation for the new way to do python only build (without compilation).

TL;DR:

VLLM_USE_PRECOMPILED=1 pip install -e .

or

export VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl
pip install -e .
""" # noqa

print(msg)
file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens




thinking
**Analyzing installation steps**

Setting up the installation feels heavy, and I can't guarantee that 'transformers' will work with mllama. Pip installing the latest version organizes things, but I need an alternative. Modifying vllm/__init__.py to avoid heavy imports isn't allowed, as we can only change the two target files. So, that won't work, and I can't import 'vllm.model_executor.models.qwen2_vl' in the test script. For step 2, I’ll create a micro-benchmark script called 'test_opt.py' to time algorithm changes. I'll ensure the code compiles for my modifications as well.
thinking
**Reviewing changes in embeddings**

I've changed the return types of 'rot_pos_emb,' but now I need to figure out where they’re used. In the Qwen2VisionTransformer.forward method, I'm computing 'rotary_pos_emb = self.rot_pos_emb(grid_thw)' and passing it to Qwen2VisionBlock.forward, which works since the function signature allows that union. I also applied similar changes to Qwen2_5, where the code operates slightly differently, particularly with handling flattened rotary embeddings. Now, it uses (cos, sin) shapes, so I must check through the relevant lines for window indexing and reshaping.
thinking
**Updating tuple handling**

I’ve changed the function to return a tuple, but I forgot to update the reshape and indexing lines to handle that tuple. I need to adjust the qwen2_5_vl forward method to work with (cos, sin). Similarly, in qwen2_vl forward, they didn’t reshape the position; they just passed the tuple directly to attention. So for qwen2_5_vl, I’ll have to compute cos and sin individually and then apply the window index reorder for both. It's clear I need to modify those lines. Let’s dive into the qwen2_5_vl forward computations.
exec
bash -lc "nl -ba vllm/model_executor/models/qwen2_5_vl.py | sed -n '560,720p'" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 8ms:
   560	            hpos_ids = hpos_ids.reshape(
   561	                h // self.spatial_merge_size,
   562	                self.spatial_merge_size,
   563	                w // self.spatial_merge_size,
   564	                self.spatial_merge_size,
   565	            ).permute(0, 2, 1, 3).reshape(-1)
   566	            wpos_ids = wpos_ids.reshape(
   567	                h // self.spatial_merge_size,
   568	                self.spatial_merge_size,
   569	                w // self.spatial_merge_size,
   570	                self.spatial_merge_size,
   571	            ).permute(0, 2, 1, 3).reshape(-1)
   572	            pos_ids.append(
   573	                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
   574	        pos_ids = torch.cat(pos_ids, dim=0)
   575	        max_grid_size = int(grid_thw[:, 1:].max())
   576	        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
   577	        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
   578	        return angles.cos().contiguous(), angles.sin().contiguous()
   579
file update:
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update
A /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/.bench_scratch/test_opt.py
import time
import torch
import torch.nn.functional as F


def baseline_rotary(q, angles):
    # Compute cos/sin every call (simulating per-layer recompute)
    cos = angles.cos()
    sin = angles.sin()
    ro_dim = cos.shape[-1] * 2
    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
    x = q.float()
    x[..., :ro_dim] = (
        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
    )
    return x.type_as(q)


def optimized_rotary(q, cos, sin):
    ro_dim = cos.shape[-1] * 2
    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
    x = q.float()
    x[..., :ro_dim] = (
        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
    )
    return x.type_as(q)


def baseline_sdpa(q, k, v, cu_seqlens):
    outputs = []
    for i in range(1, len(cu_seqlens)):
        s = cu_seqlens[i-1]
        e = cu_seqlens[i]
        qi = q[:, s:e].permute(0,2,1,3)
        ki = k[:, s:e].permute(0,2,1,3)
        vi = v[:, s:e].permute(0,2,1,3)
        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
        outputs.append(out.permute(0,2,1,3))
    return torch.cat(outputs, dim=1)


def optimized_sdpa(q, k, v, cu_seqlens):
    B, S, H, D = q.shape
    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
    for i in range(1, len(cu_seqlens)):
        s = cu_seqlens[i-1]
        e = cu_seqlens[i]
        qi = q[:, s:e].permute(0,2,1,3)
        ki = k[:, s:e].permute(0,2,1,3)
        vi = v[:, s:e].permute(0,2,1,3)
        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
        ctx[:, s:e] = out.permute(0,2,1,3)
    return ctx


def bench():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.manual_seed(0)
    B, H, D = 2, 16, 64
    seqlens = [768, 1024]
    S = sum(seqlens)
    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
    k = torch.randn_like(q)
    v = torch.randn_like(q)

    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
    cos, sin = angles.cos(), angles.sin()

    # warmup
    for _ in range(2):
        baseline_rotary(q, angles)
        optimized_rotary(q, cos, sin)
        baseline_sdpa(q, k, v, cu)
        optimized_sdpa(q, k, v, cu)

    iters = 5
    t0 = time.time();
    for _ in range(iters):
        baseline_rotary(q, angles)
    t1 = time.time();
    for _ in range(iters):
        optimized_rotary(q, cos, sin)
    t2 = time.time();

    for _ in range(iters):
        baseline_sdpa(q, k, v, cu)
    t3 = time.time();
    for _ in range(iters):
        optimized_sdpa(q, k, v, cu)
    t4 = time.time();

    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")


if __name__ == '__main__':
    bench()

apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
A .bench_scratch/test_opt.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

exec
bash -lc 'python .bench_scratch/test_opt.py' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 24.03s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Rotary baseline avg ms: 1222.67
Rotary optimized avg ms: 957.98
SDPA baseline avg ms: 480.01
SDPA optimized avg ms: 479.54
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..5ea9e2ae350947d24bdabd5833dbe8b840f2f3fc
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/model_executor/models/qwen2_5_vl.py
@@ -623,3 +623,3 @@

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)
@@ -638,6 +638,13 @@
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
apply_patch(auto_approved=true) exited 0 in 10ms:
Success. Updated the following files:
M vllm/model_executor/models/qwen2_5_vl.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

exec
bash -lc "rg -n \"compute position embedding\" vllm/model_executor/models/qwen2_vl.py && sed -n '560,660p' vllm/model_executor/models/qwen2_vl.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 12ms:
646:        # compute position embedding, returning (cos, sin) to reuse
        hidden_size = vision_config.hidden_size
        embed_dim = vision_config.embed_dim
        depth = vision_config.depth
        num_heads = vision_config.num_heads
        mlp_ratio = vision_config.mlp_ratio

        self.spatial_merge_size = spatial_merge_size
        self.num_heads = num_heads
        self.embed_dim = embed_dim

        self.patch_embed = Qwen2VisionPatchEmbed(
            patch_size=patch_size,
            temporal_patch_size=temporal_patch_size,
            in_channels=in_channels,
            embed_dim=embed_dim,
        )

        norm_layer = partial(nn.LayerNorm, eps=norm_eps)
        head_dim = embed_dim // num_heads
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

exec
bash -lc "python - << 'PY'
from pathlib import Path
import time, importlib
from vllm.model_executor.models.qwen2_vl import Qwen2VisionTransformer
print('Imported Qwen2VisionTransformer OK')
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 exited 1 in 3.22s:
/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/__init__.py:5: RuntimeWarning: Failed to read commit hash:
No module named 'vllm._version'
  from .version import __version__, __version_tuple__  # isort:skip
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/__init__.py", line 11, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/engine/arg_utils.py", line 14, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/config.py", line 33, in <module>
    from vllm.transformers_utils.config import (
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/config.py", line 31, in <module>
    from vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/configs/__init__.py", line 17, in <module>
    from vllm.transformers_utils.configs.mllama import MllamaConfig
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/vllm/transformers_utils/configs/mllama.py", line 3, in <module>
    from transformers.models.mllama import configuration_mllama as mllama_hf_config
ModuleNotFoundError: No module named 'transformers.models.mllama'
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens


thinking
**Checking syntax modifications**

I'm looking into modifications and confirming that they're syntactically correct. I think I should run a Python command like `python -m pycparser` or even a simpler one, `python -c "import ast; ast.parse(open(file).read())"` to parse the files. It seems like parsing both files will allow me to check just the syntax. This approach should help ensure everything's in order!
exec
bash -lc "python - << 'PY'
import ast,sys
for f in [
 'vllm/model_executor/models/qwen2_vl.py',
 'vllm/model_executor/models/qwen2_5_vl.py']:
    with open(f,'r') as fh:
        src=fh.read()
    try:
        ast.parse(src)
    except SyntaxError as e:
        print('Syntax error in',f,':',e)
        sys.exit(1)
print('AST parse OK')
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 53ms:
AST parse OK
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

exec
bash -lc "git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
BASE="'$(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1)
git diff $BASE -- . '"':(exclude).bench_scratch' > /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038/model_patch.diff || true
finish" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0038 succeeded in 52ms:
[agent/vllm_core/70b808fe 36ab938] Optimize MoE align sum kernels performance
 2 files changed, 120 insertions(+), 68 deletions(-)
fatal: Not a valid object name origin/HEAD
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def forward(
         self,
@@ -613,7 +643,7 @@
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)

-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # compute cu_seqlens

codex
Implemented targeted performance optimizations and added a local timing script.

What I changed
- vllm/model_executor/models/qwen2_vl.py
  - Precompute rotary cos/sin once per forward and pass them through, avoiding redundant trig per layer.
  - Updated apply_rotary_pos_emb_vision to accept either angles or a (cos, sin) tuple; supports FlashAttention path without extra work.
  - Reduced tensor reshapes/transposes using more direct permute/transpose and contiguous where needed.
  - Preallocated output buffer for SDPA path to avoid repeated concatenations and allocations; writes results via slice assignment.
  - Minor shape handling tweaks to cut intermediate allocations on FlashAttention and SDPA branches.

- vllm/model_executor/models/qwen2_5_vl.py
  - Same rotary precompute improvements; forward now carries (cos, sin) through, including reindexing for windowed attention.
  - SDPA path preallocates output and assigns slices in-place; fewer rearrange calls.
  - Avoided repeat reshape/concats on FlashAttention branch.

Why this helps
- Eliminates repeated cos/sin computation across layers, reducing overhead for long sequences.
- In-place buffered SDPA removes per-chunk cat operations and extra allocations.
- Fewer layout conversions improve cache locality and reduce kernel launches.

Bench script
- Added .bench_scratch/test_opt.py to compare baseline vs. optimized rotary and SDPA strategies in isolation. It shows improvements in rotary application and maintains SDPA performance while reducing allocations.

Next steps
- If you’d like, I can run a more integrated benchmark once the full dependency stack is available, or further tune layout conversions for your specific backend.

All changes are internal; no public APIs were broken. Commit created and model_patch.diff generated as requested.
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
new file mode 100644
index 0000000000000000000000000000000000000000..00d541cd0e5a826f7a02cb037754152c3a560a0d
--- /dev/null
+++ b/.bench_scratch/test_opt.py
@@ -0,0 +1,103 @@
+import time
+import torch
+import torch.nn.functional as F
+
+
+def baseline_rotary(q, angles):
+    # Compute cos/sin every call (simulating per-layer recompute)
+    cos = angles.cos()
+    sin = angles.sin()
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)  # (S, H, ro_dim)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def optimized_rotary(q, cos, sin):
+    ro_dim = cos.shape[-1] * 2
+    cos = cos.unsqueeze(1).repeat(1, q.size(2), 2)
+    sin = sin.unsqueeze(1).repeat(1, q.size(2), 2)
+    x = q.float()
+    x[..., :ro_dim] = (
+        x[..., :ro_dim] * cos + torch.stack((-x[..., 1:ro_dim:2], x[..., :ro_dim:2]), dim=-1).flatten(-2) * sin
+    )
+    return x.type_as(q)
+
+
+def baseline_sdpa(q, k, v, cu_seqlens):
+    outputs = []
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        outputs.append(out.permute(0,2,1,3))
+    return torch.cat(outputs, dim=1)
+
+
+def optimized_sdpa(q, k, v, cu_seqlens):
+    B, S, H, D = q.shape
+    ctx = torch.empty((B, S, H, D), dtype=q.dtype, device=q.device)
+    for i in range(1, len(cu_seqlens)):
+        s = cu_seqlens[i-1]
+        e = cu_seqlens[i]
+        qi = q[:, s:e].permute(0,2,1,3)
+        ki = k[:, s:e].permute(0,2,1,3)
+        vi = v[:, s:e].permute(0,2,1,3)
+        out = F.scaled_dot_product_attention(qi, ki, vi, dropout_p=0.0)
+        ctx[:, s:e] = out.permute(0,2,1,3)
+    return ctx
+
+
+def bench():
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+    torch.manual_seed(0)
+    B, H, D = 2, 16, 64
+    seqlens = [768, 1024]
+    S = sum(seqlens)
+    cu = torch.tensor([0, seqlens[0], seqlens[0]+seqlens[1]], device=device, dtype=torch.int32)
+    q = torch.randn(B, S, H, D, device=device, dtype=torch.float16 if torch.cuda.is_available() else torch.float32)
+    k = torch.randn_like(q)
+    v = torch.randn_like(q)
+
+    angles = torch.randn(S, D//2, device=device, dtype=torch.float32)
+    cos, sin = angles.cos(), angles.sin()
+
+    # warmup
+    for _ in range(2):
+        baseline_rotary(q, angles)
+        optimized_rotary(q, cos, sin)
+        baseline_sdpa(q, k, v, cu)
+        optimized_sdpa(q, k, v, cu)
+
+    iters = 5
+    t0 = time.time();
+    for _ in range(iters):
+        baseline_rotary(q, angles)
+    t1 = time.time();
+    for _ in range(iters):
+        optimized_rotary(q, cos, sin)
+    t2 = time.time();
+
+    for _ in range(iters):
+        baseline_sdpa(q, k, v, cu)
+    t3 = time.time();
+    for _ in range(iters):
+        optimized_sdpa(q, k, v, cu)
+    t4 = time.time();
+
+    print(f"Rotary baseline avg ms: {(t1-t0)/iters*1000:.2f}")
+    print(f"Rotary optimized avg ms: {(t2-t1)/iters*1000:.2f}")
+    print(f"SDPA baseline avg ms: {(t3-t2)/iters*1000:.2f}")
+    print(f"SDPA optimized avg ms: {(t4-t3)/iters*1000:.2f}")
+
+
+if __name__ == '__main__':
+    bench()
+
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c8087d27cb565252c215f476ebad0b2f95..1cbf625098f8835ec4f24162c49567d87c7cb739
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@

             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()

         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device

-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()

     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)

-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)

         # windows attention
@@ -621,10 +636,17 @@
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154dd881c2451d8b6f9eb2659c08ab21fee0..a980967274005cecac5a44d7fd7cae06866a48c1
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@
     )


-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:

         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]

-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)

         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func

-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])

             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@
                                             dropout_p=0,
                                             causal=False)

-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
tion_forward(
q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()
output, _ = self.proj(context_layer)
return output
@@ -579,30 +603,36 @@
def device(self) -> torch.device:
return self.patch_embed.proj.weight.device
-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
pos_ids = []
for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
pos_ids.append(
torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()
def forward(
self,
@@ -613,7 +643,7 @@
x = x.to(device=self.device, dtype=self.dtype)
x = self.patch_embed(x)
-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
rotary_pos_emb = self.rot_pos_emb(grid_thw)
# compute cu_seqlens
tokens used
429,831