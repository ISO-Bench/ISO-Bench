{"instance_id": "/root/OmniPerf-Bench/vllm__repo-63d635d", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ef3d28c..1cbf625 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -271,6 +271,7 @@ class Qwen2_5_VisionAttention(nn.Module):\n                    for x in (q, k, v))\n         if rotary_pos_emb is not None:\n             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN\n+            # Allow passing precomputed (cos, sin) to avoid redundant trig\n             q = apply_rotary_pos_emb_vision(q,\n                                             rotary_pos_emb,\n                                             use_flash_attn=use_flash_attn)\n@@ -283,7 +284,10 @@ class Qwen2_5_VisionAttention(nn.Module):\n             #   flash_attn_varlen_func)\n             from flash_attn import flash_attn_varlen_func\n \n-            q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n+            B, S = q.shape[:2]\n+            q = q.reshape(B * S, *q.shape[2:])\n+            k = k.reshape(B * S, *k.shape[2:])\n+            v = v.reshape(B * S, *v.shape[2:])\n \n             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n@@ -296,27 +300,31 @@ class Qwen2_5_VisionAttention(nn.Module):\n                                             dropout_p=0,\n                                             causal=False)\n \n-            context_layer = rearrange(output,\n-                                      \"(b s) ... -> b s ...\",\n-                                      b=batch_size)\n+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])\n         elif self.attn_backend == _Backend.TORCH_SDPA:\n             # Execute attention entry by entry for speed & less VRAM.\n-            outputs = []\n+            total_seqlen = q.shape[1]\n+            nheads = q.shape[2]\n+            headdim = q.shape[3]\n+            context_layer = torch.empty((batch_size, total_seqlen, nheads,\n+                                         headdim),\n+                                        dtype=q.dtype,\n+                                        device=q.device)\n             for i in range(1, len(cu_seqlens)):\n                 start_idx = cu_seqlens[i - 1]\n                 end_idx = cu_seqlens[i]\n                 q_i = q[:, start_idx:end_idx]\n                 k_i = k[:, start_idx:end_idx]\n                 v_i = v[:, start_idx:end_idx]\n-                q_i, k_i, v_i = (rearrange(x, \"b s h d -> b h s d\")\n-                                 for x in [q_i, k_i, v_i])\n+                q_i = q_i.permute(0, 2, 1, 3)\n+                k_i = k_i.permute(0, 2, 1, 3)\n+                v_i = v_i.permute(0, 2, 1, 3)\n                 output_i = F.scaled_dot_product_attention(q_i,\n                                                           k_i,\n                                                           v_i,\n                                                           dropout_p=0.0)\n-                output_i = rearrange(output_i, \"b h s d -> b s h d \")\n-                outputs.append(output_i)\n-            context_layer = torch.cat(outputs, dim=1)\n+                context_layer[:, start_idx:end_idx] = output_i.permute(\n+                    0, 2, 1, 3)\n         elif self.attn_backend == _Backend.XFORMERS:\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n@@ -328,8 +336,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n \n             context_layer = xops.memory_efficient_attention_forward(\n                 q, k, v, attn_bias=attn_bias, p=0, scale=None)\n-        context_layer = rearrange(context_layer,\n-                                  \"b s h d -> s b (h d)\").contiguous()\n+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(\n+            context_layer.shape[1], batch_size, -1).contiguous()\n \n         output, _ = self.proj(context_layer)\n         return output\n@@ -537,30 +545,37 @@ class Qwen2_5_VisionTransformer(nn.Module):\n     def device(self) -> torch.device:\n         return self.patch_embed.proj.weight.device\n \n-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,\n+                                                           torch.Tensor]:\n+        device = self.device\n         pos_ids = []\n         for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+            h = int(h)\n+            w = int(w)\n+            t = int(t)\n+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(\n+                -1, w)\n+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(\n+                h, -1)\n             hpos_ids = hpos_ids.reshape(\n                 h // self.spatial_merge_size,\n                 self.spatial_merge_size,\n                 w // self.spatial_merge_size,\n                 self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n+            ).permute(0, 2, 1, 3).reshape(-1)\n             wpos_ids = wpos_ids.reshape(\n                 h // self.spatial_merge_size,\n                 self.spatial_merge_size,\n                 w // self.spatial_merge_size,\n                 self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n+            ).permute(0, 2, 1, 3).reshape(-1)\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n-        return rotary_pos_emb\n+        max_grid_size = int(grid_thw[:, 1:].max())\n+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)\n+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)\n+        return angles.cos().contiguous(), angles.sin().contiguous()\n \n     def get_window_index(self, grid_thw):\n         window_index: list = []\n@@ -606,7 +621,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n         hidden_states = x.to(device=self.device, dtype=self.dtype)\n         hidden_states = self.patch_embed(hidden_states)\n \n-        # compute position embedding\n+        # compute position embedding; returns (cos, sin)\n         rotary_pos_emb = self.rot_pos_emb(grid_thw)\n \n         # windows attention\n@@ -621,10 +636,17 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n         hidden_states = hidden_states[window_index, :, :]\n         hidden_states = hidden_states.reshape(seq_len, -1)\n-        rotary_pos_emb = rotary_pos_emb.reshape(\n-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n+        # Reorder rotary embeddings to match windowed hidden_states\n+        cos, sin = rotary_pos_emb\n+        cos = cos.reshape(seq_len // self.spatial_merge_unit,\n+                          self.spatial_merge_unit, -1)\n+        cos = cos[window_index, :, :]\n+        cos = cos.reshape(seq_len, -1)\n+        sin = sin.reshape(seq_len // self.spatial_merge_unit,\n+                          self.spatial_merge_unit, -1)\n+        sin = sin[window_index, :, :]\n+        sin = sin.reshape(seq_len, -1)\n+        rotary_pos_emb = (cos, sin)\n         # compute cu_seqlens\n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],\n                                              grid_thw[:, 0]).cumsum(\ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex ac3d154..a980967 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -229,12 +229,22 @@ def apply_rotary_emb_torch(x: torch.Tensor,\n     )\n \n \n-def apply_rotary_pos_emb_vision(t: torch.Tensor,\n-                                freqs: torch.Tensor,\n-                                use_flash_attn=False) -> torch.Tensor:\n+def apply_rotary_pos_emb_vision(\n+        t: torch.Tensor,\n+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],\n+        use_flash_attn: bool = False) -> torch.Tensor:\n+    \"\"\"Apply rotary embeddings, accepting either angles or (cos, sin).\n+\n+    This avoids recomputing cos/sin in every layer when a precomputed\n+    (cos, sin) tuple is provided.\n+    \"\"\"\n     t_ = t.float()\n-    cos = freqs.cos()\n-    sin = freqs.sin()\n+    if isinstance(freqs, (tuple, list)):\n+        cos, sin = freqs\n+    else:\n+        cos = freqs.cos()\n+        sin = freqs.sin()\n+\n     apply_rotary_emb = apply_rotary_emb_torch\n     if use_flash_attn:\n         from flash_attn.layers.rotary import apply_rotary_emb\n@@ -306,7 +316,7 @@ class Qwen2VisionAttention(nn.Module):\n         self,\n         x: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor,\n+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],\n     ) -> torch.Tensor:\n \n         # [s, b, c] --> [s, b, 3 * head * head_dim]\n@@ -316,18 +326,25 @@ class Qwen2VisionAttention(nn.Module):\n         q, k, v = self.split_qkv(x)\n         batch_size = q.shape[1]\n \n-        q, k, v = (rearrange(x, \"s b ... -> b s ...\").contiguous()\n-                   for x in (q, k, v))\n+        # [s, b, h, d] -> [b, s, h, d]\n+        q = q.transpose(0, 1).contiguous()\n+        k = k.transpose(0, 1).contiguous()\n+        v = v.transpose(0, 1).contiguous()\n         if rotary_pos_emb is not None:\n-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)\n-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)\n+            use_fa = self.attn_backend == _Backend.FLASH_ATTN\n+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)\n+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)\n \n         if self.attn_backend == _Backend.FLASH_ATTN:\n             # from vllm_flash_attn.flash_attn_interface import (\n             #   flash_attn_varlen_func)\n             from flash_attn import flash_attn_varlen_func\n \n-            q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n+            # Flatten batch and sequence for FlashAttention varlen kernel\n+            B, S = q.shape[:2]\n+            q = q.reshape(B * S, *q.shape[2:])\n+            k = k.reshape(B * S, *k.shape[2:])\n+            v = v.reshape(B * S, *v.shape[2:])\n \n             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n@@ -340,27 +357,33 @@ class Qwen2VisionAttention(nn.Module):\n                                             dropout_p=0,\n                                             causal=False)\n \n-            context_layer = rearrange(output,\n-                                      \"(b s) ... -> b s ...\",\n-                                      b=batch_size)\n+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])\n         elif self.attn_backend == _Backend.TORCH_SDPA:\n             # Execute attention entry by entry for speed & less VRAM.\n-            outputs = []\n+            total_seqlen = q.shape[1]\n+            nheads = q.shape[2]\n+            headdim = q.shape[3]\n+            context_layer = torch.empty((batch_size, total_seqlen, nheads,\n+                                         headdim),\n+                                        dtype=q.dtype,\n+                                        device=q.device)\n             for i in range(1, len(cu_seqlens)):\n                 start_idx = cu_seqlens[i - 1]\n                 end_idx = cu_seqlens[i]\n                 q_i = q[:, start_idx:end_idx]\n                 k_i = k[:, start_idx:end_idx]\n                 v_i = v[:, start_idx:end_idx]\n-                q_i, k_i, v_i = (rearrange(x, \"b s h d -> b h s d\")\n-                                 for x in [q_i, k_i, v_i])\n+                # b s h d -> b h s d without extra copies\n+                q_i = q_i.permute(0, 2, 1, 3)\n+                k_i = k_i.permute(0, 2, 1, 3)\n+                v_i = v_i.permute(0, 2, 1, 3)\n                 output_i = F.scaled_dot_product_attention(q_i,\n                                                           k_i,\n                                                           v_i,\n                                                           dropout_p=0.0)\n-                output_i = rearrange(output_i, \"b h s d -> b s h d \")\n-                outputs.append(output_i)\n-            context_layer = torch.cat(outputs, dim=1)\n+                # b h s d -> b s h d and place into output buffer\n+                context_layer[:, start_idx:end_idx] = output_i.permute(\n+                    0, 2, 1, 3)\n         elif self.attn_backend == _Backend.XFORMERS:\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n@@ -372,8 +395,9 @@ class Qwen2VisionAttention(nn.Module):\n \n             context_layer = xops.memory_efficient_attention_forward(\n                 q, k, v, attn_bias=attn_bias, p=0, scale=None)\n-        context_layer = rearrange(context_layer,\n-                                  \"b s h d -> s b (h d)\").contiguous()\n+        # [b, s, h, d] -> [s, b, h*d]\n+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(\n+            context_layer.shape[1], batch_size, -1).contiguous()\n \n         output, _ = self.proj(context_layer)\n         return output\n@@ -579,30 +603,36 @@ class Qwen2VisionTransformer(nn.Module):\n     def device(self) -> torch.device:\n         return self.patch_embed.proj.weight.device\n \n-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,\n+                                                           torch.Tensor]:\n+        # Construct on correct device and precompute cos/sin once.\n+        device = self.device\n         pos_ids = []\n         for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n-            hpos_ids = hpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            wpos_ids = wpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n+            h = int(h)\n+            w = int(w)\n+            t = int(t)\n+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(\n+                -1, w)\n+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(\n+                h, -1)\n+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,\n+                                        self.spatial_merge_size,\n+                                        w // self.spatial_merge_size,\n+                                        self.spatial_merge_size)\n+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)\n+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,\n+                                        self.spatial_merge_size,\n+                                        w // self.spatial_merge_size,\n+                                        self.spatial_merge_size)\n+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n-        return rotary_pos_emb\n+        max_grid_size = int(grid_thw[:, 1:].max())\n+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)\n+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)\n+        return angles.cos().contiguous(), angles.sin().contiguous()\n \n     def forward(\n         self,\n@@ -613,7 +643,7 @@ class Qwen2VisionTransformer(nn.Module):\n         x = x.to(device=self.device, dtype=self.dtype)\n         x = self.patch_embed(x)\n \n-        # compute position embedding\n+        # compute position embedding, returning (cos, sin) to reuse\n         rotary_pos_emb = self.rot_pos_emb(grid_thw)\n \n         # compute cu_seqlens\n", "model_name_or_path": "gpt-5-2025-08-07"}
