diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ef3d28c..1cbf625 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -271,6 +271,7 @@ class Qwen2_5_VisionAttention(nn.Module):
                    for x in (q, k, v))
         if rotary_pos_emb is not None:
             use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN
+            # Allow passing precomputed (cos, sin) to avoid redundant trig
             q = apply_rotary_pos_emb_vision(q,
                                             rotary_pos_emb,
                                             use_flash_attn=use_flash_attn)
@@ -283,7 +284,10 @@ class Qwen2_5_VisionAttention(nn.Module):
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func
 
-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])
 
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -296,27 +300,31 @@ class Qwen2_5_VisionAttention(nn.Module):
                                             dropout_p=0,
                                             causal=False)
 
-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -328,8 +336,8 @@ class Qwen2_5_VisionAttention(nn.Module):
 
             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()
 
         output, _ = self.proj(context_layer)
         return output
@@ -537,30 +545,37 @@ class Qwen2_5_VisionTransformer(nn.Module):
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device
 
-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
             hpos_ids = hpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             wpos_ids = wpos_ids.reshape(
                 h // self.spatial_merge_size,
                 self.spatial_merge_size,
                 w // self.spatial_merge_size,
                 self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            ).permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()
 
     def get_window_index(self, grid_thw):
         window_index: list = []
@@ -606,7 +621,7 @@ class Qwen2_5_VisionTransformer(nn.Module):
         hidden_states = x.to(device=self.device, dtype=self.dtype)
         hidden_states = self.patch_embed(hidden_states)
 
-        # compute position embedding
+        # compute position embedding; returns (cos, sin)
         rotary_pos_emb = self.rot_pos_emb(grid_thw)
 
         # windows attention
@@ -621,10 +636,17 @@ class Qwen2_5_VisionTransformer(nn.Module):
             seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
         hidden_states = hidden_states[window_index, :, :]
         hidden_states = hidden_states.reshape(seq_len, -1)
-        rotary_pos_emb = rotary_pos_emb.reshape(
-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)
-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]
-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)
+        # Reorder rotary embeddings to match windowed hidden_states
+        cos, sin = rotary_pos_emb
+        cos = cos.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        cos = cos[window_index, :, :]
+        cos = cos.reshape(seq_len, -1)
+        sin = sin.reshape(seq_len // self.spatial_merge_unit,
+                          self.spatial_merge_unit, -1)
+        sin = sin[window_index, :, :]
+        sin = sin.reshape(seq_len, -1)
+        rotary_pos_emb = (cos, sin)
         # compute cu_seqlens
         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],
                                              grid_thw[:, 0]).cumsum(
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index ac3d154..a980967 100644
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -229,12 +229,22 @@ def apply_rotary_emb_torch(x: torch.Tensor,
     )
 
 
-def apply_rotary_pos_emb_vision(t: torch.Tensor,
-                                freqs: torch.Tensor,
-                                use_flash_attn=False) -> torch.Tensor:
+def apply_rotary_pos_emb_vision(
+        t: torch.Tensor,
+        freqs: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
+        use_flash_attn: bool = False) -> torch.Tensor:
+    """Apply rotary embeddings, accepting either angles or (cos, sin).
+
+    This avoids recomputing cos/sin in every layer when a precomputed
+    (cos, sin) tuple is provided.
+    """
     t_ = t.float()
-    cos = freqs.cos()
-    sin = freqs.sin()
+    if isinstance(freqs, (tuple, list)):
+        cos, sin = freqs
+    else:
+        cos = freqs.cos()
+        sin = freqs.sin()
+
     apply_rotary_emb = apply_rotary_emb_torch
     if use_flash_attn:
         from flash_attn.layers.rotary import apply_rotary_emb
@@ -306,7 +316,7 @@ class Qwen2VisionAttention(nn.Module):
         self,
         x: torch.Tensor,
         cu_seqlens: torch.Tensor,
-        rotary_pos_emb: torch.Tensor,
+        rotary_pos_emb: Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]],
     ) -> torch.Tensor:
 
         # [s, b, c] --> [s, b, 3 * head * head_dim]
@@ -316,18 +326,25 @@ class Qwen2VisionAttention(nn.Module):
         q, k, v = self.split_qkv(x)
         batch_size = q.shape[1]
 
-        q, k, v = (rearrange(x, "s b ... -> b s ...").contiguous()
-                   for x in (q, k, v))
+        # [s, b, h, d] -> [b, s, h, d]
+        q = q.transpose(0, 1).contiguous()
+        k = k.transpose(0, 1).contiguous()
+        v = v.transpose(0, 1).contiguous()
         if rotary_pos_emb is not None:
-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
+            use_fa = self.attn_backend == _Backend.FLASH_ATTN
+            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb, use_flash_attn=use_fa)
+            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb, use_flash_attn=use_fa)
 
         if self.attn_backend == _Backend.FLASH_ATTN:
             # from vllm_flash_attn.flash_attn_interface import (
             #   flash_attn_varlen_func)
             from flash_attn import flash_attn_varlen_func
 
-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
+            # Flatten batch and sequence for FlashAttention varlen kernel
+            B, S = q.shape[:2]
+            q = q.reshape(B * S, *q.shape[2:])
+            k = k.reshape(B * S, *k.shape[2:])
+            v = v.reshape(B * S, *v.shape[2:])
 
             max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()
             output = flash_attn_varlen_func(q,
@@ -340,27 +357,33 @@ class Qwen2VisionAttention(nn.Module):
                                             dropout_p=0,
                                             causal=False)
 
-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
+            context_layer = output.reshape(batch_size, -1, *output.shape[1:])
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
+            total_seqlen = q.shape[1]
+            nheads = q.shape[2]
+            headdim = q.shape[3]
+            context_layer = torch.empty((batch_size, total_seqlen, nheads,
+                                         headdim),
+                                        dtype=q.dtype,
+                                        device=q.device)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
                 q_i = q[:, start_idx:end_idx]
                 k_i = k[:, start_idx:end_idx]
                 v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
+                # b s h d -> b h s d without extra copies
+                q_i = q_i.permute(0, 2, 1, 3)
+                k_i = k_i.permute(0, 2, 1, 3)
+                v_i = v_i.permute(0, 2, 1, 3)
                 output_i = F.scaled_dot_product_attention(q_i,
                                                           k_i,
                                                           v_i,
                                                           dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
+                # b h s d -> b s h d and place into output buffer
+                context_layer[:, start_idx:end_idx] = output_i.permute(
+                    0, 2, 1, 3)
         elif self.attn_backend == _Backend.XFORMERS:
             from xformers import ops as xops
             from xformers.ops.fmha.attn_bias import BlockDiagonalMask
@@ -372,8 +395,9 @@ class Qwen2VisionAttention(nn.Module):
 
             context_layer = xops.memory_efficient_attention_forward(
                 q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
+        # [b, s, h, d] -> [s, b, h*d]
+        context_layer = context_layer.permute(1, 0, 2, 3).reshape(
+            context_layer.shape[1], batch_size, -1).contiguous()
 
         output, _ = self.proj(context_layer)
         return output
@@ -579,30 +603,36 @@ class Qwen2VisionTransformer(nn.Module):
     def device(self) -> torch.device:
         return self.patch_embed.proj.weight.device
 
-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:
+    def rot_pos_emb(self, grid_thw: torch.Tensor) -> tuple[torch.Tensor,
+                                                           torch.Tensor]:
+        # Construct on correct device and precompute cos/sin once.
+        device = self.device
         pos_ids = []
         for t, h, w in grid_thw:
-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)
-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)
-            hpos_ids = hpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
-            wpos_ids = wpos_ids.reshape(
-                h // self.spatial_merge_size,
-                self.spatial_merge_size,
-                w // self.spatial_merge_size,
-                self.spatial_merge_size,
-            ).permute(0, 2, 1, 3).flatten()
+            h = int(h)
+            w = int(w)
+            t = int(t)
+            hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(
+                -1, w)
+            wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(
+                h, -1)
+            hpos_ids = hpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            hpos_ids = hpos_ids.permute(0, 2, 1, 3).reshape(-1)
+            wpos_ids = wpos_ids.reshape(h // self.spatial_merge_size,
+                                        self.spatial_merge_size,
+                                        w // self.spatial_merge_size,
+                                        self.spatial_merge_size)
+            wpos_ids = wpos_ids.permute(0, 2, 1, 3).reshape(-1)
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
-        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
-        return rotary_pos_emb
+        max_grid_size = int(grid_thw[:, 1:].max())
+        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size).to(device)
+        angles = rotary_pos_emb_full[pos_ids].reshape(pos_ids.shape[0], -1)
+        return angles.cos().contiguous(), angles.sin().contiguous()
 
     def forward(
         self,
@@ -613,7 +643,7 @@ class Qwen2VisionTransformer(nn.Module):
         x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)
 
-        # compute position embedding
+        # compute position embedding, returning (cos, sin) to reuse
         rotary_pos_emb = self.rot_pos_emb(grid_thw)
 
         # compute cu_seqlens
