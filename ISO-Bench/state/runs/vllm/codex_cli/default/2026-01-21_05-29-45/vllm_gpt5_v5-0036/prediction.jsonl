{"instance_id": "/root/OmniPerf-Bench/vllm__repo-0032903", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex fec6d61..fdce5f8 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -4,6 +4,7 @@ from typing import List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n+import torch.nn.functional as F\n \n from vllm.attention.backends.abstract import AttentionMetadata\n from vllm.attention.backends.flash_attn import FlashAttentionMetadata\n@@ -68,19 +69,22 @@ class Mixer2RMSNormGated(CustomOp):\n         #   3. The general case can be pretty complicated so we AllGather\n         #      the input and then redundantly compute the RMSNorm.\n         input_dtype = x.dtype\n-        x = x * nn.functional.silu(gate.to(torch.float32))\n+        # Compute gate in fp32 for stability; do it in-place to reduce temps.\n+        gate32 = gate.to(torch.float32)\n+        F.silu(gate32, inplace=True)\n+        x = x * gate32\n \n         if self.n_groups == 1:\n             if self.tp_size > 1:\n                 # Compute local sum and then reduce to obtain global sum\n-                local_sums = x.pow(2).sum(dim=-1, keepdim=True)\n+                local_sums = x.square().sum(dim=-1, keepdim=True)\n                 global_sums = tensor_model_parallel_all_reduce(local_sums)\n                 # Calculate the variance\n                 count = self.tp_size * x.shape[-1]\n                 variance = (global_sums / count)\n \n             else:\n-                variance = x.pow(2).mean(-1, keepdim=True)\n+                variance = x.square().mean(-1, keepdim=True)\n             x = x * torch.rsqrt(variance + self.variance_epsilon)\n         else:\n             redundant_tp: bool = self.n_groups % self.tp_size != 0\n@@ -91,9 +95,10 @@ class Mixer2RMSNormGated(CustomOp):\n             *prefix_dims, hidden_dim = x.shape\n             group_count = hidden_dim // self.group_size\n             x_grouped = x.view(*prefix_dims, group_count, self.group_size)\n-            variance = x_grouped.pow(2).mean(-1, keepdim=True)\n-            x_grouped = x_grouped * torch.rsqrt(variance +\n-                                                self.variance_epsilon)\n+            # Use square() instead of pow(2) and apply in-place scale to reduce\n+            # temporaries and memory bandwidth.\n+            variance = x_grouped.square().mean(-1, keepdim=True)\n+            x_grouped = x_grouped * torch.rsqrt(variance + self.variance_epsilon)\n             x = x_grouped.view(*prefix_dims, hidden_dim)\n \n             if redundant_tp:\n@@ -114,9 +119,11 @@ class Mixer2RMSNormGated(CustomOp):\n \n         from vllm import _custom_ops as ops\n \n-        # cast x and gate to float32 before silu\n+        # cast gate to float32 and apply silu in-place before multiply\n         out = torch.empty_like(x)\n-        y = x * nn.functional.silu(gate.to(torch.float32))\n+        gate32 = gate.to(torch.float32)\n+        F.silu(gate32, inplace=True)\n+        y = x * gate32\n         ops.rms_norm(\n             out,\n             y.to(x.dtype),\n", "model_name_or_path": "gpt-5-2025-08-07"}
