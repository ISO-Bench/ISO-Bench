{"instance_id": "/root/OmniPerf-Bench/vllm__repo-e642ec9", "model_patch": "diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py\nindex 8184b07..c34e2f8 100644\n--- a/vllm/attention/backends/mla/common.py\n+++ b/vllm/attention/backends/mla/common.py\n@@ -296,7 +296,8 @@ class MLACommonBackend(AttentionBackend):\n \n     @staticmethod\n     def get_supported_head_sizes() -> List[int]:\n-        return [576]\n+        # Use an immutable tuple to avoid per-call list allocations.\n+        return (576,)\n \n \n T = TypeVar(\"T\", bound=\"MLACommonMetadata\")\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex c98262e..66f0618 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -267,7 +267,8 @@ class MLACommonBackend(AttentionBackend):\n \n     @staticmethod\n     def get_supported_head_sizes() -> list[int]:\n-        return [576]\n+        # Return an immutable tuple to avoid per-call list allocations.\n+        return (576,)\n \n     @staticmethod\n     def use_cascade_attention(*args, **kwargs) -> bool:\n", "model_name_or_path": "gpt-5-2025-08-07"}
