{"instance_id": "/root/OmniPerf-Bench/vllm__repo-f728ab8", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex a9ef973..15aa5cd 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -10,15 +10,25 @@ def get_token_bin_counts_and_mask(\n     vocab_size: int,\n     num_seqs: int,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    # Compute the bin counts for the tokens.\n-    # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n-                             dtype=torch.long,\n-                             device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n-    bin_counts = bin_counts[:, :vocab_size]\n-    mask = bin_counts > 0\n+    \"\"\"Return per-sequence token counts and presence mask.\n \n+    Optimized using a single vectorized ``bincount`` over offset indices,\n+    which is typically faster than ``scatter_add_`` and avoids allocating\n+    a large temporary ``ones_like(tokens)`` tensor.\n+    \"\"\"\n+    # Offsets ensure each row counts into its own range [row*(V+1), (row+1)*(V+1)).\n+    # ``+1`` is for the padding token (``vocab_size``) which we drop afterwards.\n+    tokens = tokens.to(dtype=torch.long)\n+    offsets = torch.arange(num_seqs,\n+                           device=tokens.device,\n+                           dtype=tokens.dtype).unsqueeze(1)\n+    offsets = offsets * (vocab_size + 1)\n+    flat_idx = (tokens + offsets).reshape(-1)\n+    # NOTE: torch.bincount works on both CPU and CUDA; offsets keep rows disjoint.\n+    counts = torch.bincount(flat_idx,\n+                            minlength=num_seqs * (vocab_size + 1))\n+    bin_counts = counts.view(num_seqs, vocab_size + 1)[:, :vocab_size]\n+    mask = bin_counts > 0\n     return bin_counts, mask\n \n \n@@ -45,14 +55,21 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                                    vocab_size, num_seqs)\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n-        1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n+    # Apply repetition penalty with broadcasting to avoid materializing a\n+    # (num_seqs, vocab_size) repeated tensor and avoid double boolean indexing.\n+    penalty_mask = prompt_mask | output_mask\n+    rep = repetition_penalties.unsqueeze(1).to(dtype=logits.dtype)\n+    inv_rep = 1.0 / rep\n+    # Choose factor per element depending on sign of the logit.\n+    factor = torch.where(logits > 0, inv_rep, rep)\n+    # Apply only where penalty is active; elsewhere multiply by 1.\n+    logits.mul_(torch.where(penalty_mask, factor, torch.ones(1,\n+                                                            device=logits.device,\n+                                                            dtype=logits.dtype)))\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n-    logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts\n-    logits -= presence_penalties.unsqueeze(dim=1) * output_mask\n+    fpen = frequency_penalties.to(dtype=logits.dtype).unsqueeze(1)\n+    ppen = presence_penalties.to(dtype=logits.dtype).unsqueeze(1)\n+    logits -= fpen * output_bin_counts.to(dtype=logits.dtype)\n+    logits -= ppen * output_mask.to(dtype=logits.dtype)\n     return logits\n", "model_name_or_path": "gpt-5-2025-08-07"}
