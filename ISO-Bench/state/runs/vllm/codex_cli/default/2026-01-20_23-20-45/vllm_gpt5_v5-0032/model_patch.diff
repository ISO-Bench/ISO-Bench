diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py
index a9ef973..15aa5cd 100644
--- a/vllm/model_executor/layers/utils.py
+++ b/vllm/model_executor/layers/utils.py
@@ -10,15 +10,25 @@ def get_token_bin_counts_and_mask(
     vocab_size: int,
     num_seqs: int,
 ) -> Tuple[torch.Tensor, torch.Tensor]:
-    # Compute the bin counts for the tokens.
-    # vocab_size + 1 for padding.
-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),
-                             dtype=torch.long,
-                             device=tokens.device)
-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))
-    bin_counts = bin_counts[:, :vocab_size]
-    mask = bin_counts > 0
+    """Return per-sequence token counts and presence mask.
 
+    Optimized using a single vectorized ``bincount`` over offset indices,
+    which is typically faster than ``scatter_add_`` and avoids allocating
+    a large temporary ``ones_like(tokens)`` tensor.
+    """
+    # Offsets ensure each row counts into its own range [row*(V+1), (row+1)*(V+1)).
+    # ``+1`` is for the padding token (``vocab_size``) which we drop afterwards.
+    tokens = tokens.to(dtype=torch.long)
+    offsets = torch.arange(num_seqs,
+                           device=tokens.device,
+                           dtype=tokens.dtype).unsqueeze(1)
+    offsets = offsets * (vocab_size + 1)
+    flat_idx = (tokens + offsets).reshape(-1)
+    # NOTE: torch.bincount works on both CPU and CUDA; offsets keep rows disjoint.
+    counts = torch.bincount(flat_idx,
+                            minlength=num_seqs * (vocab_size + 1))
+    bin_counts = counts.view(num_seqs, vocab_size + 1)[:, :vocab_size]
+    mask = bin_counts > 0
     return bin_counts, mask
 
 
@@ -45,14 +55,21 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,
                                                    vocab_size, num_seqs)
     output_bin_counts, output_mask = get_token_bin_counts_and_mask(
         output_tokens_tensor, vocab_size, num_seqs)
-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(
-        1, vocab_size)
-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,
-                                      repetition_penalties, 1.0)[logits > 0]
-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,
-                                       repetition_penalties, 1.0)[logits <= 0]
+    # Apply repetition penalty with broadcasting to avoid materializing a
+    # (num_seqs, vocab_size) repeated tensor and avoid double boolean indexing.
+    penalty_mask = prompt_mask | output_mask
+    rep = repetition_penalties.unsqueeze(1).to(dtype=logits.dtype)
+    inv_rep = 1.0 / rep
+    # Choose factor per element depending on sign of the logit.
+    factor = torch.where(logits > 0, inv_rep, rep)
+    # Apply only where penalty is active; elsewhere multiply by 1.
+    logits.mul_(torch.where(penalty_mask, factor, torch.ones(1,
+                                                            device=logits.device,
+                                                            dtype=logits.dtype)))
     # We follow the definition in OpenAI API.
     # Refer to https://platform.openai.com/docs/api-reference/parameter-details
-    logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts
-    logits -= presence_penalties.unsqueeze(dim=1) * output_mask
+    fpen = frequency_penalties.to(dtype=logits.dtype).unsqueeze(1)
+    ppen = presence_penalties.to(dtype=logits.dtype).unsqueeze(1)
+    logits -= fpen * output_bin_counts.to(dtype=logits.dtype)
+    logits -= ppen * output_mask.to(dtype=logits.dtype)
     return logits
