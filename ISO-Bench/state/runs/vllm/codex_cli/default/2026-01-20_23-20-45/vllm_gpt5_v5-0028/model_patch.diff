diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 5f8535e..04a52a4 100644
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -8,7 +8,8 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionMetadata, AttentionType)
 from vllm.forward_context import get_forward_context
 from vllm.utils import direct_register_custom_op
-from vllm.vllm_flash_attn import flash_attn_varlen_func
+# NOTE: Import flash_attn kernels lazily inside the hot path function to
+# avoid importing the heavy top-level vllm package at module import time.
 
 
 class FlashAttentionBackend(AttentionBackend):
@@ -185,6 +186,8 @@ def unified_v1_flash_attention(
     num_actual_tokens = attn_metadata.num_actual_tokens
 
     # Reshape the query, key, and value tensors.
+    # Reshape once and keep local references to reduce repeated
+    # attribute lookups and shape computations in the hot path.
     query = query.view(-1, num_heads, head_size)
     key = key.view(-1, num_kv_heads, head_size)
     value = value.view(-1, num_kv_heads, head_size)
@@ -192,9 +195,11 @@ def unified_v1_flash_attention(
     # Reshape the input keys and values and store them in the cache.
     key_cache = kv_cache[0]
     value_cache = kv_cache[1]
+    # Use narrow() instead of Python slicing to avoid creating
+    # temporary slice objects and improve clarity of intent.
     torch.ops._C_cache_ops.reshape_and_cache_flash(
-        key[:num_actual_tokens],
-        value[:num_actual_tokens],
+        key.narrow(0, 0, num_actual_tokens),
+        value.narrow(0, 0, num_actual_tokens),
         key_cache,
         value_cache,
         attn_metadata.slot_mapping,
@@ -203,24 +208,35 @@ def unified_v1_flash_attention(
         v_scale,
     )
 
+    # Cache frequently accessed metadata into locals to reduce repeated
+    # attribute lookups in this hot path.
+    q_start = attn_metadata.query_start_loc
+    max_q = attn_metadata.max_query_len
+    k_start = attn_metadata.seq_start_loc
+    max_k = attn_metadata.max_seq_len
+    block_tbl = attn_metadata.block_table
+
     attn_output = flash_attn_varlen_func(
-        q=query[:num_actual_tokens],
+        q=query.narrow(0, 0, num_actual_tokens),
         k=key_cache,
         v=value_cache,
-        cu_seqlens_q=attn_metadata.query_start_loc,
-        max_seqlen_q=attn_metadata.max_query_len,
-        cu_seqlens_k=attn_metadata.seq_start_loc,
-        max_seqlen_k=attn_metadata.max_seq_len,
+        cu_seqlens_q=q_start,
+        max_seqlen_q=max_q,
+        cu_seqlens_k=k_start,
+        max_seqlen_k=max_k,
         softmax_scale=softmax_scale,
         causal=True,
         alibi_slopes=alibi_slopes,
         window_size=window_size,
-        block_table=attn_metadata.block_table,
+        block_table=block_tbl,
         softcap=logits_soft_cap,
     )
-    attn_output = attn_output.view(num_actual_tokens, -1)
-    # TODO(woosuk): Optimize this.
-    output[:num_actual_tokens].copy_(attn_output)
+    # Flatten heads into the last dim and copy non-blocking to let
+    # the runtime schedule the copy asynchronously when possible
+    # (e.g. on CUDA). This reduces synchronizations on the critical path.
+    # NOTE: copy_ is still required since flash_attn returns a new tensor.
+    flat = attn_output.reshape(num_actual_tokens, -1)
+    output.narrow(0, 0, num_actual_tokens).copy_(flat, non_blocking=True)
 
 
 def unified_v1_flash_attention_fake(
