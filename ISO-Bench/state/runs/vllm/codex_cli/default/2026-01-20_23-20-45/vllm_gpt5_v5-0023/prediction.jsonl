{"instance_id": "/root/OmniPerf-Bench/vllm__repo-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6..ac6c6d9 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -821,12 +821,21 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         \"\"\"\n         # Combine and flatten intermediate data.\n         input_tokens = []\n-        token_types = []\n+        # Many models do not provide token type ids; avoid flattening\n+        # potentially large lists of empties by first checking if any\n+        # token type data exists across all groups.\n+        has_token_types = False\n         for inter_data in self.inter_data_list:\n             for cur_input_tokens in inter_data.input_tokens:\n                 input_tokens.extend(cur_input_tokens)\n-            for cur_token_types in inter_data.token_types:\n-                token_types.extend(cur_token_types)\n+            # cheap existence check without materializing a flattened list\n+            if not has_token_types and any(cur for cur in inter_data.token_types):\n+                has_token_types = True\n+        token_types = [] if has_token_types else None\n+        if has_token_types:\n+            for inter_data in self.inter_data_list:\n+                for cur_token_types in inter_data.token_types:\n+                    token_types.extend(cur_token_types)\n \n         if not input_tokens:\n             # This may happen when all prefill requests hit\n", "model_name_or_path": "gpt-5-2025-08-07"}
