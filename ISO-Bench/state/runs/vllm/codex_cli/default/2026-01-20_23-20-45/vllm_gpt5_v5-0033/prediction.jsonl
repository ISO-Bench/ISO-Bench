{"instance_id": "/root/OmniPerf-Bench/vllm__repo-beebf47", "model_patch": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed..7f641d1 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -1383,12 +1383,15 @@ class LLM:\n                             # Calculate tokens only for RequestOutput\n                             assert output.prompt_token_ids is not None\n                             total_in_toks += len(output.prompt_token_ids)\n-                            in_spd = total_in_toks / pbar.format_dict[\"elapsed\"]\n+                            # Access elapsed once to avoid repeated dict lookups\n+                            elapsed = pbar.format_dict.get(\"elapsed\", 0.0) or 0.0\n+                            in_spd = total_in_toks / elapsed if elapsed else 0.0\n                             total_out_toks += sum(\n                                 len(stp.token_ids) for stp in output.outputs)\n-                            out_spd = (total_out_toks /\n-                                       pbar.format_dict[\"elapsed\"])\n-                            pbar.postfix = (\n+                            out_spd = (total_out_toks / elapsed\n+                                       if elapsed else 0.0)\n+                            # set_postfix_str is marginally cheaper than assigning .postfix\n+                            pbar.set_postfix_str(\n                                 f\"est. speed input: {in_spd:.2f} toks/s, \"\n                                 f\"output: {out_spd:.2f} toks/s\")\n                         pbar.update(1)\n@@ -1398,4 +1401,5 @@ class LLM:\n         # Sort the outputs by request ID.\n         # This is necessary because some requests may be finished earlier than\n         # its previous requests.\n-        return sorted(outputs, key=lambda x: int(x.request_id))\n+        outputs.sort(key=lambda x: int(x.request_id))\n+        return outputs\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0..2b00690 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -315,8 +315,9 @@ class OpenAIServingChat(OpenAIServing):\n         # all_previous_token_ids will not be used twice in the same iteration.\n         if tool_choice_auto or should_stream_with_reasoning_parsing:\n             # These are only required in \"auto\" tool choice case\n-            previous_texts = [\"\"] * num_choices\n-            all_previous_token_ids = [[]] * num_choices\n+            # Avoid list aliasing which can cause unintended shared mutation\n+            previous_texts = [\"\" for _ in range(num_choices)]\n+            all_previous_token_ids = [[] for _ in range(num_choices)]\n         else:\n             previous_texts, all_previous_token_ids = None, None\n \ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af8..67b851b 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -246,10 +246,12 @@ class OpenAIServingCompletion(OpenAIServing):\n         request_metadata: RequestResponseMetadata,\n     ) -> AsyncGenerator[str, None]:\n         num_choices = 1 if request.n is None else request.n\n-        previous_text_lens = [0] * num_choices * num_prompts\n-        previous_num_tokens = [0] * num_choices * num_prompts\n-        has_echoed = [False] * num_choices * num_prompts\n-        num_prompt_tokens = [0] * num_prompts\n+        # Avoid list aliasing and pre-size lists without nested references\n+        total_slots = num_choices * num_prompts\n+        previous_text_lens = [0 for _ in range(total_slots)]\n+        previous_num_tokens = [0 for _ in range(total_slots)]\n+        has_echoed = [False for _ in range(total_slots)]\n+        num_prompt_tokens = [0 for _ in range(num_prompts)]\n \n         stream_options = request.stream_options\n         if stream_options:\ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a..bd6bf38 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -284,11 +284,9 @@ class OpenAIServingTranscription(OpenAIServing):\n                 lora_request=None,\n                 prompt_adapter_request=None)\n \n-            result_generator = self.engine_client.generate(\n-                prompt,\n-                sampling_params,\n-                request_id,\n-            )\n+            result_generator = self.engine_client.generate(prompt,\n+                                                           sampling_params,\n+                                                           request_id)\n         except ValueError as e:\n             # TODO: Use a vllm-specific Validation Error\n             return self.create_error_response(str(e))\n", "model_name_or_path": "gpt-5-2025-08-07"}
