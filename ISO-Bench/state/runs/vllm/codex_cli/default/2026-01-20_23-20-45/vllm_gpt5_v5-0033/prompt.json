{
  "task": "vLLM core performance",
  "description": "Run vLLM performance checks with Dockerfile-based env",
  "constraints": [
    "No public API breakage",
    "All TestPack checks must pass"
  ],
  "target_files": [
    "vllm/entrypoints/llm.py",
    "vllm/entrypoints/openai/serving_chat.py",
    "vllm/entrypoints/openai/serving_completion.py",
    "vllm/entrypoints/openai/serving_transcription.py"
  ],
  "success": {
    "primary_metric": "functional_match",
    "rules": [
      "Do not modify tests or metrics harness",
      "Preserve external behavior; optimize internals only"
    ]
  },
  "commits": {
    "pre": "beebf4742af80296d3c3a657c66d512615c550c1",
    "human": "9badee53decb3d432dc805336abfb0eb81dfb48f"
  }
}