diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index 65f336f..95cb2c7 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -302,7 +302,6 @@ class XGrammarLogitsProcessor:
         if device_type != "cuda":
             # xgrammar on cpu only supports float32 scores
             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22
-            # Use a single cast/move to reduce allocations and dispatches.
             scores = scores.to("cpu", dtype=torch.float32).unsqueeze(0)
 
         # Note: In this method, if the tensors have different dimensions
@@ -311,8 +310,6 @@ class XGrammarLogitsProcessor:
         xgr.apply_token_bitmask_inplace(scores,
                                         self.token_bitmask.to(scores.device))
         if device_type != "cuda":
-            # Combine dtype+device conversion in a single call to avoid
-            # redundant copies and kernel dispatches.
             scores = scores.to(device_type, dtype=dtype).squeeze()
 
         return scores
