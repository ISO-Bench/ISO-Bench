{"instance_id": "/root/OmniPerf-Bench/vllm__repo-eb5741a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 65f336f..95cb2c7 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -302,7 +302,6 @@ class XGrammarLogitsProcessor:\n         if device_type != \"cuda\":\n             # xgrammar on cpu only supports float32 scores\n             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            # Use a single cast/move to reduce allocations and dispatches.\n             scores = scores.to(\"cpu\", dtype=torch.float32).unsqueeze(0)\n \n         # Note: In this method, if the tensors have different dimensions\n@@ -311,8 +310,6 @@ class XGrammarLogitsProcessor:\n         xgr.apply_token_bitmask_inplace(scores,\n                                         self.token_bitmask.to(scores.device))\n         if device_type != \"cuda\":\n-            # Combine dtype+device conversion in a single call to avoid\n-            # redundant copies and kernel dispatches.\n             scores = scores.to(device_type, dtype=dtype).squeeze()\n \n         return scores\n", "model_name_or_path": "gpt-5-2025-08-07"}
