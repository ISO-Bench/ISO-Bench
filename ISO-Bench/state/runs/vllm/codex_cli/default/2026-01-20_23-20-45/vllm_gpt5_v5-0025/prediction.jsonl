{"instance_id": "/root/OmniPerf-Bench/vllm__repo-f67e9e9", "model_patch": "diff --git a/tests/core/block/e2e/test_correctness.py b/tests/core/block/e2e/test_correctness.py\nindex e9b537e..6eb5a7d 100644\n--- a/tests/core/block/e2e/test_correctness.py\n+++ b/tests/core/block/e2e/test_correctness.py\n@@ -166,11 +166,11 @@ def test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids without lookahead scheduling')\n+    # Avoid extra I/O in tests for speed.\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids with lookahead scheduling')\n+    # Run with lookahead scheduling\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -244,11 +244,11 @@ def test_chunked_prefill_block_manager(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids with BlockManager')\n+    # Baseline run\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids with BlockManager, with lookahead slots.')\n+    # Run with lookahead slots\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -325,11 +325,11 @@ def test_block_manager_prefix_caching_enabled_with_preemption(\n         temperature=temperature,\n     )\n \n-    print('Getting token ids from block manager')\n+    # Baseline run\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids from block manager, with preemption')\n+    # Run with preemption\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -399,11 +399,11 @@ def test_auto_prefix_caching_with_preemption(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids with APC disabled')\n+    # Baseline run (APC disabled)\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids with APC enabled')\n+    # Run with APC enabled\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \ndiff --git a/vllm/attention/ops/prefix_prefill.py b/vllm/attention/ops/prefix_prefill.py\nindex e0478c2..d03bed1 100644\n--- a/vllm/attention/ops/prefix_prefill.py\n+++ b/vllm/attention/ops/prefix_prefill.py\n@@ -99,9 +99,9 @@ if triton.__version__ >= \"2.1.0\":\n             (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n             cur_head * stride_qh + offs_d[None, :] * stride_qd)\n \n-        dim_mask = tl.where(\n-            tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1,\n-            0).to(tl.int1)  # [D]\n+        # Use a lightweight boolean/int1 mask derived from existing `offs_d`.\n+        # This avoids creating an extra arange + cast on every program.\n+        dim_mask = (offs_d < BLOCK_DMODEL)  # [D]\n \n         q = tl.load(Q + off_q,\n                     mask=dim_mask[None, :] &\n", "model_name_or_path": "gpt-5-2025-08-07"}
