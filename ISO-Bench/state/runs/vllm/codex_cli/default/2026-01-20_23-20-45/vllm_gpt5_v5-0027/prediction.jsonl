{"instance_id": "/root/OmniPerf-Bench/vllm__repo-20478c4", "model_patch": "diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nindex 8b089a5..ade63a4 100644\n--- a/vllm/core/block_manager.py\n+++ b/vllm/core/block_manager.py\n@@ -193,15 +193,54 @@ class BlockSpaceManager:\n         # Allocate new physical token blocks that will store the prompt tokens.\n         num_prompt_blocks = len(seq.logical_token_blocks)\n \n+        # Optimization: computing \"hash of block\" via Sequence.hash_of_block\n+        # repeatedly is O(L^2). Instead, compute all prefix-hashes once using\n+        # a rolling hash over the prompt tokens, which reduces to O(L) work.\n+        def _fnv1a64_update(h: int, x: int) -> int:\n+            # 64-bit FNV-1a\n+            h ^= (x & 0xFFFFFFFFFFFFFFFF)\n+            return (h * 1099511628211) & 0xFFFFFFFFFFFFFFFF\n+\n+        def _compute_prompt_hashes_and_counts(s: Sequence) -> Tuple[List[int],\n+                                                                     List[int]]:\n+            tokens = s.data.get_token_ids()\n+            bsz = s.block_size\n+            total = len(tokens)\n+            # Start from FNV offset and fold in LoRA id so that prompts from\n+            # different LoRAs do not collide.\n+            h = 1469598103934665603\n+            h = _fnv1a64_update(h, s.lora_int_id)\n+            hashes: List[int] = []\n+            counts: List[int] = []\n+            for i, t in enumerate(tokens, start=1):\n+                h = _fnv1a64_update(h, int(t))\n+                if i % bsz == 0:\n+                    hashes.append(h)\n+                    counts.append(i)  # exact token count at block boundary\n+            if total % bsz != 0:\n+                # Last partial block: hash should be over the entire token\n+                # list (same behavior as Sequence.hash_of_block), while the\n+                # \"num_hashed_tokens\" follows the logical block size.\n+                hashes.append(h)\n+                # Match existing behavior: number of hashed tokens for block k\n+                # is (k+1)*block_size, even if the last block is partial.\n+                last_idx = total // bsz  # zero-based index of last block\n+                counts.append((last_idx + 1) * bsz)\n+            # Ensure we match the number of logical blocks.\n+            assert len(hashes) == num_prompt_blocks\n+            return hashes, counts\n+\n+        pre_hashes, pre_counts = _compute_prompt_hashes_and_counts(seq)\n+\n         block_table: BlockTable = []\n         for logical_idx in range(num_prompt_blocks):\n             if (self.block_sliding_window is not None\n                     and logical_idx >= self.block_sliding_window):\n+                # reuse a previously allocated block in the window\n                 block = block_table[logical_idx % self.block_sliding_window]\n             else:\n-                block = self.gpu_allocator.allocate(\n-                    seq.hash_of_block(logical_idx),\n-                    seq.num_hashed_tokens_of_block(logical_idx))\n+                block = self.gpu_allocator.allocate(pre_hashes[logical_idx],\n+                                                    pre_counts[logical_idx])\n             block_table.append(block)\n \n         # Assign the block table for each sequence.\n@@ -221,8 +260,20 @@ class BlockSpaceManager:\n         last_block: PhysicalTokenBlock,\n     ) -> PhysicalTokenBlock:\n         # Compute a new hash for the block so that it can be shared by\n-        # other Sequences\n-        new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n+        # other Sequences. Use the same rolling-hash scheme as allocate()\n+        # to avoid quadratic behavior and ensure consistency.\n+        def _fnv1a64_update(h: int, x: int) -> int:\n+            h ^= (x & 0xFFFFFFFFFFFFFFFF)\n+            return (h * 1099511628211) & 0xFFFFFFFFFFFFFFFF\n+\n+        logical_idx = len(seq.logical_token_blocks) - 1\n+        tokens = seq.data.get_token_ids()\n+        end = min(len(tokens), (logical_idx + 1) * seq.block_size)\n+        h = 1469598103934665603\n+        h = _fnv1a64_update(h, seq.lora_int_id)\n+        for t in tokens[:end]:\n+            h = _fnv1a64_update(h, int(t))\n+        new_hash = h\n \n         # if new_hash is already in the cached table, then free last_block\n         # and return the cached version\n@@ -256,7 +307,23 @@ class BlockSpaceManager:\n     ) -> PhysicalTokenBlock:\n         block_hash: Optional[int] = None\n         if (self._is_last_block_full(seq)):\n-            block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n+            # Avoid the quadratic Sequence.hash_of_block by recomputing the\n+            # prefix hash once with a rolling hash.\n+            logical_idx = len(seq.logical_token_blocks) - 1\n+\n+            def _fnv1a64_update(h: int, x: int) -> int:\n+                h ^= (x & 0xFFFFFFFFFFFFFFFF)\n+                return (h * 1099511628211) & 0xFFFFFFFFFFFFFFFF\n+\n+            # Compute prefix hash up to the end of this logical block.\n+            tokens = seq.data.get_token_ids()\n+            end = min(len(tokens), (logical_idx + 1) * seq.block_size)\n+            h = 1469598103934665603\n+            h = _fnv1a64_update(h, seq.lora_int_id)\n+            for t in tokens[:end]:\n+                h = _fnv1a64_update(h, int(t))\n+            block_hash = h\n+\n         num_hashed_tokens = seq.num_hashed_tokens_of_block(\n             len(seq.logical_token_blocks) - 1)\n         new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\ndiff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex 1d81f5a..69d414d 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,5 +1,5 @@\n import enum\n-from typing import Dict, List, Optional\n+from typing import Dict, List, Optional, Tuple\n from abc import ABC, abstractmethod, abstractproperty\n \n from vllm.block import PhysicalTokenBlock\n@@ -51,76 +51,79 @@ class Evictor(ABC):\n \n \n class LRUEvictor(Evictor):\n-    \"\"\"Evicts in a least-recently-used order using the last_accessed timestamp\n-    that's recorded in the PhysicalTokenBlock. If there are multiple blocks with\n-    the same last_accessed time, then the one with the largest num_hashed_tokens\n-    will be evicted. If two blocks each have the lowest last_accessed time and\n-    highest num_hashed_tokens value, then one will be chose arbitrarily\n+    \"\"\"LRU evictor with a heap-backed priority queue.\n+\n+    - Primary key: lowest ``last_accessed`` wins (LRU)\n+    - Tiebreaker: larger ``num_hashed_tokens`` wins (evict those first)\n+\n+    We keep a dict for O(1) membership and a heap for O(log N) eviction.\n+    Removal is handled lazily with a tombstone set.\n     \"\"\"\n \n     def __init__(self):\n-        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n+        # Active blocks by hash\n+        self._table: Dict[int, PhysicalTokenBlock] = {}\n+        # Min-heap of (last_accessed, -num_hashed_tokens, seq, block_hash)\n+        self._heap: List[Tuple[float, int, int, int]] = []\n+        # Tombstones for entries removed from table but still present in heap\n+        self._removed: set[int] = set()\n+        self._seq: int = 0\n \n     def __contains__(self, block_hash: int) -> bool:\n-        return block_hash in self.free_table\n+        return block_hash in self._table\n+\n+    def _push(self, block: PhysicalTokenBlock):\n+        self._seq += 1\n+        key = (block.last_accessed, -int(block.num_hashed_tokens), self._seq,\n+               block.block_hash)\n+        # Store in table for O(1) membership and retrieval\n+        self._table[block.block_hash] = block\n+        # Push to heap for O(log N) best-candidate eviction\n+        import heapq\n+        heapq.heappush(self._heap, key)\n+\n+    def _pop_valid(self) -> PhysicalTokenBlock:\n+        import heapq\n+        while self._heap:\n+            _, _, _, h = heapq.heappop(self._heap)\n+            if h in self._removed:\n+                self._removed.remove(h)\n+                continue\n+            # Should be in table\n+            block = self._table.pop(h, None)\n+            if block is None:\n+                # Already taken via remove(); skip\n+                continue\n+            return block\n+        raise ValueError(\"No usable cache memory left\")\n \n-    # TODO: The performance of this evict function can be optimized further.\n     def evict(self) -> PhysicalTokenBlock:\n-        free_blocks: List[PhysicalTokenBlock] = list(self.free_table.values())\n-        if len(free_blocks) == 0:\n-            raise ValueError(\"No usable cache memory left\")\n-\n-        # Find lowest timestamp\n-        lowest_timestamp = free_blocks[0].last_accessed\n-        for block in free_blocks:\n-            if block.last_accessed < lowest_timestamp:\n-                lowest_timestamp = block.last_accessed\n-\n-        # Find all blocks with the lowest timestamp\n-        least_recent: List[PhysicalTokenBlock] = []\n-        for block in free_blocks:\n-            if block.last_accessed == lowest_timestamp:\n-                least_recent.append(block)\n-\n-        # Find highest prefix count per block\n-        highest_num_hashed_tokens = 0\n-        for block in least_recent:\n-            if block.num_hashed_tokens > highest_num_hashed_tokens:\n-                highest_num_hashed_tokens = block.num_hashed_tokens\n-\n-        evicted_block: Optional[PhysicalTokenBlock] = None\n-\n-        # Find the first block with the lowest timestamp\n-        for block in least_recent:\n-            if block.num_hashed_tokens == highest_num_hashed_tokens:\n-                evicted_block = block\n-                break\n-\n-        assert evicted_block is not None\n-\n-        del self.free_table[evicted_block.block_hash]\n-\n-        evicted_block.computed = False\n-        return evicted_block\n+        block = self._pop_valid()\n+        block.computed = False\n+        return block\n \n     def add(self, block: PhysicalTokenBlock):\n-        self.free_table[block.block_hash] = block\n+        self._push(block)\n \n     def remove(self, block_hash: int) -> PhysicalTokenBlock:\n-        if block_hash not in self.free_table:\n+        if block_hash not in self._table:\n             raise ValueError(\n                 \"Attempting to remove block that's not in the evictor\")\n-        block: PhysicalTokenBlock = self.free_table[block_hash]\n-        del self.free_table[block_hash]\n+        # Mark tombstone and pop from table\n+        block = self._table.pop(block_hash)\n+        self._removed.add(block_hash)\n         return block\n \n     @property\n     def num_blocks(self) -> int:\n-        return len(self.free_table)\n+        return len(self._table)\n \n \n class RandomEvictor(Evictor):\n-    \"\"\"Evicts in a first-in-first-out order\"\"\"\n+    \"\"\"Simple FIFO-style evictor using dict insertion order.\n+\n+    Used when caching is disabled to keep overhead minimal.\n+    \"\"\"\n \n     def __init__(self):\n         self.free_table: Dict[int, PhysicalTokenBlock] = {}\n", "model_name_or_path": "gpt-5-2025-08-07"}
