{"instance_id": "/root/OmniPerf-Bench/vllm__repo-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex cde959d..6049b91 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -152,7 +152,7 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     cached_tokenizer.__class__ = CachedTokenizer\n-    return tokenizer\n+    return cached_tokenizer\n \n \n def patch_padding_side(tokenizer: PreTrainedTokenizer) -> None:\n@@ -301,7 +301,7 @@ def get_tokenizer(\n                 \"slowdown. Consider using a fast tokenizer instead.\")\n         tokenizer = get_cached_tokenizer(tokenizer)\n \n-    return cached_tokenizer\n+    return tokenizer\n \n \n cached_get_tokenizer = lru_cache(get_tokenizer)\n", "model_name_or_path": "gpt-5-2025-08-07"}
