diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py
index cde959d..6049b91 100644
--- a/vllm/transformers_utils/tokenizer.py
+++ b/vllm/transformers_utils/tokenizer.py
@@ -152,7 +152,7 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:
     CachedTokenizer.__name__ = f"Cached{tokenizer.__class__.__name__}"
 
     cached_tokenizer.__class__ = CachedTokenizer
-    return tokenizer
+    return cached_tokenizer
 
 
 def patch_padding_side(tokenizer: PreTrainedTokenizer) -> None:
@@ -301,7 +301,7 @@ def get_tokenizer(
                 "slowdown. Consider using a fast tokenizer instead.")
         tokenizer = get_cached_tokenizer(tokenizer)
 
-    return cached_tokenizer
+    return tokenizer
 
 
 cached_get_tokenizer = lru_cache(get_tokenizer)
