{"instance_id": "/root/OmniPerf-Bench/vllm__repo-edc4fa3", "model_patch": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 1be806b..8b41c41 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -51,12 +51,19 @@ void swap_blocks(torch::Tensor& src, torch::Tensor& dst,\n       src_device.is_cuda() ? src_device : dst_device);\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   // NOTE(woosuk): This can be slow if the number of blocks is large.\n+  // Avoid per-element Tensor indexing and .item() synchronizations by\n+  // reading the mapping through a raw pointer on CPU.\n   const int64_t num_blocks = block_mapping.size(0);\n-  for (size_t i = 0; i < num_blocks; i++) {\n-    int64_t src_block_number = block_mapping[i][0].item<int64_t>();\n-    int64_t dst_block_number = block_mapping[i][1].item<int64_t>();\n-    int64_t src_offset = src_block_number * block_size_in_bytes;\n-    int64_t dst_offset = dst_block_number * block_size_in_bytes;\n+  if (num_blocks == 0) {\n+    return;\n+  }\n+  const int64_t* __restrict__ map_ptr =\n+      block_mapping.data_ptr<int64_t>();  // shape = [num_blocks, 2]\n+  for (int64_t i = 0; i < num_blocks; ++i) {\n+    const int64_t src_block_number = map_ptr[2 * i + 0];\n+    const int64_t dst_block_number = map_ptr[2 * i + 1];\n+    const int64_t src_offset = src_block_number * block_size_in_bytes;\n+    const int64_t dst_offset = dst_block_number * block_size_in_bytes;\n     cudaMemcpyAsync(dst_ptr + dst_offset, src_ptr + src_offset,\n                     block_size_in_bytes, memcpy_type, stream);\n   }\ndiff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex d379890..9761226 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -101,6 +101,10 @@ class FlashAttentionImpl(AttentionImpl):\n                 f\"Head size {head_size} is not supported by FlashAttention. \"\n                 f\"Supported head sizes are: {support_head_sizes}.\")\n \n+        # Cache device-specific copies of constant tensors to avoid\n+        # redundant device transfers on every forward.\n+        self._alibi_device_cache: Dict[torch.device, torch.Tensor] = {}\n+\n     def forward(\n         self,\n         query: torch.Tensor,\n@@ -135,10 +139,12 @@ class FlashAttentionImpl(AttentionImpl):\n             \"key/v_scale is not supported in FlashAttention.\")\n \n         if attn_metadata is None:\n-            # Profiling run.\n+            # Profiling run or dry-run where metadata is not provided.\n             return output\n \n         num_actual_tokens = attn_metadata.num_actual_tokens\n+        if num_actual_tokens == 0:\n+            return output\n \n         # Reshape the input keys and values and store them in the cache.\n         key_cache = kv_cache[0]\n@@ -155,6 +161,16 @@ class FlashAttentionImpl(AttentionImpl):\n         )\n \n         # Compute attention and update output up to `num_actual_tokens`.\n+        # Ensure alibi slopes are on the same device without repeated H2D.\n+        alibi = None\n+        if self.alibi_slopes is not None:\n+            qdev = query.device\n+            cached = self._alibi_device_cache.get(qdev)\n+            if cached is None or cached.device != qdev:\n+                cached = self.alibi_slopes.to(device=qdev, non_blocking=True)\n+                self._alibi_device_cache[qdev] = cached\n+            alibi = cached\n+\n         flash_attn_varlen_func(\n             q=query[:num_actual_tokens],\n             k=key_cache,\n@@ -166,7 +182,7 @@ class FlashAttentionImpl(AttentionImpl):\n             max_seqlen_k=attn_metadata.max_seq_len,\n             softmax_scale=self.scale,\n             causal=True,\n-            alibi_slopes=self.alibi_slopes,\n+            alibi_slopes=alibi,\n             window_size=self.sliding_window,\n             block_table=attn_metadata.block_table,\n             softcap=self.logits_soft_cap,\n", "model_name_or_path": "gpt-5-2025-08-07"}
