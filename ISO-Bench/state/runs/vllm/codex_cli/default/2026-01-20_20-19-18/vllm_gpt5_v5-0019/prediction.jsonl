{"instance_id": "/root/OmniPerf-Bench/vllm__repo-3a1e648", "model_patch": "diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62..bc1bdab 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -84,13 +84,21 @@ class RejectionSampler(nn.Module):\n         # NOTE: The following input preparationg can be moved\n         # to the model runner with a persistent manner for better\n         # performance.\n-        # Convert draft token IDs to a tensor, split by sample_lens, then pad.\n-        draft_token_ids = [\n-            torch.tensor(x, dtype=int, device='cpu') for x in draft_token_ids\n-        ]\n-        draft_token_ids_tensor = pad_sequence(draft_token_ids,\n-                                              batch_first=True,\n-                                              padding_value=INVALID_TOKEN_ID)\n+        # Convert draft token IDs to a dense 2D tensor with minimal overhead.\n+        # Avoids per-row tensor constructions and pad_sequence overhead.\n+        max_spec_len = max((len(x) for x in draft_token_ids), default=0)\n+        batch_size = len(draft_token_ids)\n+        draft_token_ids_tensor = torch.full(\n+            (batch_size, max_spec_len),\n+            fill_value=INVALID_TOKEN_ID,\n+            dtype=torch.int64,\n+            device=\"cpu\",\n+        )\n+        if max_spec_len > 0:\n+            for i, seq in enumerate(draft_token_ids):\n+                if seq:\n+                    draft_token_ids_tensor[i, :len(seq)] = torch.tensor(\n+                        seq, dtype=torch.int64)\n \n         # NOTE: CPU <-> GPU synchronization happens here.\n         draft_token_ids_tensor = draft_token_ids_tensor.to(target_probs.device)\n@@ -103,6 +111,34 @@ class RejectionSampler(nn.Module):\n                                                      vocab_size,\n                                                      target_probs.device)\n         sample_lens = [len(x) + 1 for x in draft_token_ids]\n+\n+        # Fast path for greedy: avoid expanding to [B, T, V] by computing\n+        # argmax token ids per request directly.\n+        if sampling_metadata.all_greedy:\n+            batch = len(sample_lens)\n+            max_len = max(sample_lens) if sample_lens else 0\n+            # [B, max_spec_len + 1] int tensor of target token ids\n+            target_token_ids_tensor = torch.full(\n+                (batch, max_len),\n+                fill_value=INVALID_TOKEN_ID,\n+                dtype=torch.int64,\n+                device=target_probs.device,\n+            )\n+            offset = 0\n+            for i, l in enumerate(sample_lens):\n+                if l > 0:\n+                    # argmax over vocab for this request's tokens\n+                    target_token_ids_tensor[i, :l] = target_probs[offset:offset\n+                                                                  + l].argmax(\n+                                                                      dim=-1)\n+                    offset += l\n+            # Directly execute greedy native path using the token ids.\n+            return self.forward_native(draft_token_ids_tensor, draft_probs,\n+                                       bonus_token_ids_tensor,\n+                                       target_token_ids_tensor,\n+                                       sampling_metadata)\n+\n+        # Non-greedy path needs full probability tensors.\n         target_probs = _convert_2d_probs(target_probs, sample_lens)\n \n         return self.forward_native(draft_token_ids_tensor, draft_probs,\n@@ -122,29 +158,24 @@ class RejectionSampler(nn.Module):\n     ) -> torch.Tensor:\n         # Add 1 to include the 'bonus' token.\n         if sampling_metadata.all_greedy:\n-            # Produce a mask that remains 1 (True) until the first\n-            # mismatch (cumprod turns 0 after a mismatch).\n-            target_token_ids_tensor = target_probs.argmax(dim=-1)\n+            # Fast path: compute prefix length of accepted tokens per row\n+            # and construct the output via a single broadcasted mask.\n+            # Here, `target_probs` is actually the token id matrix\n+            # when all_greedy=True (constructed in forward()).\n+            target_token_ids_tensor = target_probs\n             accept_mask = (target_token_ids_tensor[:, :-1] ==\n                            draft_token_ids_tensor).cumprod(dim=1)\n-\n-            # Identify valid positions (non-padding).\n-            valid_mask = target_token_ids_tensor != INVALID_TOKEN_ID\n-            # Generate mask with bonus token.\n-            generate_mask = torch.cat([\n-                accept_mask,\n-                torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)\n-            ],\n-                                      dim=1).to(torch.bool) & valid_mask\n-            zeros_mask = (generate_mask == 0)\n-            first_zero_idx = zeros_mask.float().argmax(dim=1)\n-            # Figure out which rows actually contain at least one zero.\n-            rows_with_zero = zeros_mask.any(dim=1)\n-            # Use indexing to set the first zero in each of those rows to 1.\n-            generate_mask[rows_with_zero, first_zero_idx[rows_with_zero]] = 1\n-\n-            output_token_ids = target_token_ids_tensor\n-            output_token_ids[~generate_mask] = INVALID_TOKEN_ID\n+            # Number of accepted tokens before the first mismatch.\n+            prefix_len = accept_mask.sum(dim=1)\n+            # Build a mask that includes the first mismatch (or bonus).\n+            arange_idx = torch.arange(target_token_ids_tensor.size(1),\n+                                      device=target_token_ids_tensor.device)\n+            generate_mask = arange_idx.unsqueeze(0) <= prefix_len.unsqueeze(1)\n+            output_token_ids = torch.where(generate_mask,\n+                                           target_token_ids_tensor,\n+                                           torch.full_like(\n+                                               target_token_ids_tensor,\n+                                               INVALID_TOKEN_ID))\n         else:\n             # Reference: https://arxiv.org/pdf/2211.17192\n             # 1. Extract the probabilities of the draft tokens.\n@@ -176,23 +207,25 @@ class RejectionSampler(nn.Module):\n                                            draft_token_probs,\n                                            torch.finfo(torch.float32).tiny)\n             accepted = uniform_samples <= target_token_probs / safe_draft_probs\n+            # Use cumulative product over 0/1 to keep prefix of accepts.\n             accept_mask = accepted.cumprod(dim=1)\n             # Set the token ids to the draft token ids if accepted, otherwise\n             # set them to INVALID_TOKEN_ID.\n-            accepted_token_ids = (draft_token_ids_tensor * accept_mask +\n-                                  INVALID_TOKEN_ID * (1 - accept_mask))\n+            accepted_token_ids = torch.where(\n+                accept_mask.to(torch.bool), draft_token_ids_tensor,\n+                torch.full_like(draft_token_ids_tensor, INVALID_TOKEN_ID))\n \n             # 4. Adjust the distribution for the recovered tokens.\n             # Clamp the bonus probabilities to the smallest positive normal\n             # value representable by float32.\n-            bonus_prob = torch.clamp(target_probs[:, :-1, :] - draft_probs,\n-                                     min=torch.finfo(torch.float32).tiny)\n-            normalized_bonus_prob = bonus_prob / bonus_prob.sum(dim=-1,\n-                                                                keepdim=True)\n+            bonus_prob = target_probs[:, :-1, :].sub(draft_probs)\n+            bonus_prob.clamp_(min=torch.finfo(torch.float32).tiny)\n+            denom = bonus_prob.sum(dim=-1, keepdim=True)\n+            bonus_prob.div_(denom)\n \n             # 5. Sample recovered token ids.\n             recovered_token_ids = random_sample(\n-                normalized_bonus_prob,\n+                bonus_prob,\n                 sampling_metadata.generators).reshape(batch_size, max_spec_len)\n \n             # 6. Get the final output token ids.\n@@ -202,23 +235,18 @@ class RejectionSampler(nn.Module):\n             recovered_bonus_token_ids = torch.cat(\n                 [recovered_token_ids, bonus_token_ids_tensor], dim=1)\n             # Generate mask with bonus tokens.\n-            generate_mask = torch.cat([\n-                accept_mask,\n-                torch.zeros(batch_size, 1, device=accept_mask.device)\n-            ],\n-                                      dim=1).to(torch.bool)\n-            zeros_mask = (generate_mask == 0)\n-            first_zero_idx = zeros_mask.float().argmax(dim=1)\n-            output_token_ids = torch.cat([\n-                accepted_token_ids,\n-                torch.full((batch_size, 1),\n-                           fill_value=INVALID_TOKEN_ID,\n-                           device=accept_mask.device)\n-            ],\n-                                         dim=1)\n-            output_token_ids[torch.arange(batch_size),\n-                             first_zero_idx] = recovered_bonus_token_ids[\n-                                 torch.arange(batch_size), first_zero_idx]\n+            # Index of the first mismatch per row equals number of accepts.\n+            first_zero_idx = accept_mask.sum(dim=1)\n+            output_token_ids = torch.full(\n+                (batch_size, max_spec_len + 1),\n+                fill_value=INVALID_TOKEN_ID,\n+                dtype=draft_token_ids_tensor.dtype,\n+                device=draft_token_ids_tensor.device,\n+            )\n+            output_token_ids[:, :max_spec_len] = accepted_token_ids\n+            rows = torch.arange(batch_size, device=draft_token_ids_tensor.device)\n+            output_token_ids[rows, first_zero_idx] = recovered_bonus_token_ids[\n+                rows, first_zero_idx]\n \n         return output_token_ids\n \n@@ -288,21 +316,22 @@ def _convert_2d_probs(\n         probs: torch.Tensor,  # [num_total_tokens, vocab_size]\n         sample_lens: list[int]) -> torch.Tensor:\n     \"\"\"\n-        Converts a 2D tensor of probabilities to a 3D tensor with padding.\n-        [num_total_tokens, vocab_size] -> \n-            [batch_size, max_spec_len + 1, vocab_size]\n+    Convert a 2D tensor of per-token probabilities to a padded 3D tensor.\n+    [num_total_tokens, vocab_size] -> [batch_size, max_spec_len + 1, vocab]\n+    This implementation avoids pad_sequence and redundant tensor splits.\n     \"\"\"\n-    cumulative_lens = torch.cumsum(torch.tensor(sample_lens,\n-                                                device=probs.device),\n-                                   dim=0)\n-    split_indices = cumulative_lens[:-1].tolist()  # Exclude last index\n-\n-    # Split into chunks without loops\n-    chunks = torch.tensor_split(probs, split_indices, dim=0)\n-\n-    # Pad all sequences to maximum length\n-    padded_probs = pad_sequence(chunks, batch_first=True, padding_value=0.0)\n-    return padded_probs\n+    if not sample_lens:\n+        return probs.new_zeros((0, 0, probs.size(-1)))\n+    batch_size = len(sample_lens)\n+    max_len = max(sample_lens)\n+    vocab_size = probs.size(-1)\n+    out = probs.new_zeros((batch_size, max_len, vocab_size))\n+    offset = 0\n+    for i, l in enumerate(sample_lens):\n+        if l > 0:\n+            out[i, :l].copy_(probs[offset:offset + l])\n+            offset += l\n+    return out\n \n \n def _create_uniform_samples(seeded_seqs: dict[int, torch.Generator],\n", "model_name_or_path": "gpt-5-2025-08-07"}
