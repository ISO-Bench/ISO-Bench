{"instance_id": "/root/OmniPerf-Bench/vllm__repo-67abdbb", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca6..616f00f 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -487,8 +487,13 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         # When forking, we must make sure that each block's `ref_count`\n         # is only incremented by one, so we deduplicate them by wrapping\n         # them in a set.\n-        for block in set(src_block_table):\n-            block.ref_count += 1\n+        # Avoid set() overhead when sliding window is disabled\n+        if self.block_sliding_window is None:\n+            for block in src_block_table:\n+                block.ref_count += 1\n+        else:\n+            for block in set(src_block_table):\n+                block.ref_count += 1\n \n     def _get_physical_blocks(\n             self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n@@ -607,7 +612,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         blocks_to_free = (block_table[-self.block_sliding_window:]\n                           if self.block_sliding_window is not None else\n                           block_table)\n-        for block in set(blocks_to_free):\n+        # Avoid creating a set when no sliding window is used\n+        unique_blocks = (set(blocks_to_free)\n+                         if self.block_sliding_window is not None else\n+                         blocks_to_free)\n+        for block in unique_blocks:\n             if block.device == Device.GPU:\n                 self.gpu_allocator.free(block)\n             else:\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477ef..32dc8fe 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -388,7 +388,8 @@ class Sequence:\n         # Compute the number of tokens in the sequence\n         # TODO: The current hashing function is O(L^2). We should optimize\n         # this in the future.\n-        num_tokens = self.num_hashed_tokens_of_block(logical_idx)\n+        num_tokens = min(self.get_len(),\n+                         self.num_hashed_tokens_of_block(logical_idx))\n         hashed_tokens = self.data.get_prefix_token_ids(num_tokens)\n         return hash((hashed_tokens, self.lora_int_id))\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
