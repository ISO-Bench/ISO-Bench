{"instance_id": "/root/OmniPerf-Bench/vllm__repo-bc8a8ce", "model_patch": "diff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py\nindex 3a4c259..64bc9da 100644\n--- a/vllm/v1/sample/logits_processor.py\n+++ b/vllm/v1/sample/logits_processor.py\n@@ -246,6 +246,9 @@ class MinPLogitsProcessor(LogitsProcessor):\n             self.min_p_device = self.min_p_cpu_tensor\n         # Current slice of the device tensor\n         self.min_p: torch.Tensor = self.min_p_device[:0]\n+        # Log of min_p; computed alongside self.min_p to avoid recompute in\n+        # apply(). Shape matches self.min_p ([num_reqs, 1])\n+        self._log_min_p: torch.Tensor = self.min_p_device[:0]\n \n     def is_argmax_invariant(self) -> bool:\n         \"\"\"Min-p never impacts greedy sampling\"\"\"\n@@ -293,24 +296,22 @@ class MinPLogitsProcessor(LogitsProcessor):\n             if self.use_double_tensor:\n                 self.min_p.copy_(self.min_p_cpu_tensor[:size],\n                                  non_blocking=True)\n+            # Expand to [num_reqs, 1] for broadcasting with logits rows\n             self.min_p.unsqueeze_(1)\n+            # Cache log(min_p) to avoid softmax in apply(). When min_p == 0,\n+            # log(0) -> -inf which yields a threshold of -inf and thus no mask.\n+            self._log_min_p = torch.log(self.min_p)\n \n     def apply(self, logits: torch.Tensor) -> torch.Tensor:\n         if not self.min_p_count:\n             return logits\n \n-        # Convert logits to probability distribution\n-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n-        # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Adjust min_p\n-        adjusted_min_p = max_probabilities.mul_(self.min_p)\n-        # Identify valid tokens using threshold comparison\n-        invalid_token_mask = probability_values < adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[invalid_token_mask] = -float('inf')\n+        # Typical sampling: p_i >= min_p * p_max. Since softmax is monotonic,\n+        # compare logits directly: logit_i >= max_logit + log(min_p).\n+        # This avoids expensive softmax over the vocabulary.\n+        max_logits = torch.amax(logits, dim=-1, keepdim=True)\n+        threshold = max_logits + self._log_min_p\n+        logits.masked_fill_(logits < threshold, float(\"-inf\"))\n         return logits\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
