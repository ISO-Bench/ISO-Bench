diff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py
index 3a4c259..64bc9da 100644
--- a/vllm/v1/sample/logits_processor.py
+++ b/vllm/v1/sample/logits_processor.py
@@ -246,6 +246,9 @@ class MinPLogitsProcessor(LogitsProcessor):
             self.min_p_device = self.min_p_cpu_tensor
         # Current slice of the device tensor
         self.min_p: torch.Tensor = self.min_p_device[:0]
+        # Log of min_p; computed alongside self.min_p to avoid recompute in
+        # apply(). Shape matches self.min_p ([num_reqs, 1])
+        self._log_min_p: torch.Tensor = self.min_p_device[:0]
 
     def is_argmax_invariant(self) -> bool:
         """Min-p never impacts greedy sampling"""
@@ -293,24 +296,22 @@ class MinPLogitsProcessor(LogitsProcessor):
             if self.use_double_tensor:
                 self.min_p.copy_(self.min_p_cpu_tensor[:size],
                                  non_blocking=True)
+            # Expand to [num_reqs, 1] for broadcasting with logits rows
             self.min_p.unsqueeze_(1)
+            # Cache log(min_p) to avoid softmax in apply(). When min_p == 0,
+            # log(0) -> -inf which yields a threshold of -inf and thus no mask.
+            self._log_min_p = torch.log(self.min_p)
 
     def apply(self, logits: torch.Tensor) -> torch.Tensor:
         if not self.min_p_count:
             return logits
 
-        # Convert logits to probability distribution
-        probability_values = torch.nn.functional.softmax(logits, dim=-1)
-        # Calculate maximum probabilities per sequence
-        max_probabilities = torch.amax(probability_values,
-                                       dim=-1,
-                                       keepdim=True)
-        # Adjust min_p
-        adjusted_min_p = max_probabilities.mul_(self.min_p)
-        # Identify valid tokens using threshold comparison
-        invalid_token_mask = probability_values < adjusted_min_p
-        # Apply mask using boolean indexing
-        logits[invalid_token_mask] = -float('inf')
+        # Typical sampling: p_i >= min_p * p_max. Since softmax is monotonic,
+        # compare logits directly: logit_i >= max_logit + log(min_p).
+        # This avoids expensive softmax over the vocabulary.
+        max_logits = torch.amax(logits, dim=-1, keepdim=True)
+        threshold = max_logits + self._log_min_p
+        logits.masked_fill_(logits < threshold, float("-inf"))
         return logits
 
 
