{"instance_id": "/root/OmniPerf-Bench/vllm__repo-fbefc8a", "model_patch": "diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nindex f588f40..2f5e999 100644\n--- a/vllm/reasoning/qwen3_reasoning_parser.py\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -31,8 +31,12 @@ class Qwen3ReasoningParser(ReasoningParser):\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n \n+        # Precompile a safe regex (escaped tokens) though the hot path below\n+        # prefers faster string operations.\n         self.reasoning_regex = re.compile(\n-            rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n+            rf\"{re.escape(self.think_start_token)}(.*?){re.escape(self.think_end_token)}\",\n+            re.DOTALL,\n+        )\n \n         if not self.model_tokenizer:\n             raise ValueError(\n@@ -54,10 +58,15 @@ class Qwen3ReasoningParser(ReasoningParser):\n         \"\"\"\n         Extract the content after the end tokens\n         \"\"\"\n-        if self.think_end_token_id not in input_ids[:-1]:\n+        # Find the first occurrence of the end token in a single pass\n+        try:\n+            idx = input_ids.index(self.think_end_token_id)\n+        except ValueError:\n             return []\n-        else:\n-            return input_ids[input_ids.index(self.think_end_token_id) + 1:]\n+        # If the end token is the last element, there is no content after it\n+        if idx >= len(input_ids) - 1:\n+            return []\n+        return input_ids[idx + 1:]\n \n     def extract_reasoning_content_streaming(\n         self,\n@@ -77,73 +86,81 @@ class Qwen3ReasoningParser(ReasoningParser):\n         - 'xyz' goes to content\n         \"\"\"\n         # Skip single special tokens\n-        if len(delta_token_ids) == 1 and (delta_token_ids[0] in [\n-                self.think_start_token_id, self.think_end_token_id\n-        ]):\n+        if (len(delta_token_ids) == 1 and\n+                (delta_token_ids[0] == self.think_start_token_id or\n+                 delta_token_ids[0] == self.think_end_token_id)):\n             return None\n \n-        if self.think_start_token_id in previous_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n-                # <think> in previous, </think> in delta,\n-                # extract reasoning content\n-                end_index = delta_text.find(self.think_end_token)\n+        prev_has_start = self.think_start_token_id in previous_token_ids\n+        prev_has_end = self.think_end_token_id in previous_token_ids\n+        delta_has_start = self.think_start_token_id in delta_token_ids\n+        delta_has_end = self.think_end_token_id in delta_token_ids\n+\n+        start_tok = self.think_start_token\n+        end_tok = self.think_end_token\n+\n+        if prev_has_start:\n+            if delta_has_end:\n+                # <think> in previous, </think> in delta: split once\n+                end_index = delta_text.find(end_tok)\n+                if end_index == -1:\n+                    # Defensive: treat as reasoning continuation\n+                    return DeltaMessage(reasoning_content=delta_text)\n                 reasoning_content = delta_text[:end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                content = delta_text[end_index + len(end_tok):]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n-                                    content=content if content else None)\n-            elif self.think_end_token_id in previous_token_ids:\n-                # <think> in previous, </think> in previous,\n-                # reasoning content continues\n+                                    content=content or None)\n+            elif prev_has_end:\n+                # <think> and </think> already closed previously: content\n                 return DeltaMessage(content=delta_text)\n             else:\n-                # <think> in previous, no </think> in previous or delta,\n-                # reasoning content continues\n+                # Inside reasoning: continue reasoning content\n                 return DeltaMessage(reasoning_content=delta_text)\n-        elif self.think_start_token_id in delta_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n-                # <think> in delta, </think> in delta, extract reasoning content\n-                start_index = delta_text.find(self.think_start_token)\n-                end_index = delta_text.find(self.think_end_token)\n-                reasoning_content = delta_text[start_index +\n-                                               len(self.think_start_token\n-                                                   ):end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+        elif delta_has_start:\n+            if delta_has_end:\n+                # Both <think> and </think> in delta: extract reasoning\n+                start_index = delta_text.find(start_tok)\n+                end_index = delta_text.find(end_tok,\n+                                            start_index + len(start_tok)\n+                                            if start_index != -1 else 0)\n+                if start_index == -1 or end_index == -1:\n+                    # Fallback: treat as content if malformed order\n+                    return DeltaMessage(content=delta_text)\n+                reasoning_content = delta_text[start_index + len(start_tok):\n+                                               end_index]\n+                content = delta_text[end_index + len(end_tok):]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n-                                    content=content if content else None)\n+                                    content=content or None)\n             else:\n-                # <think> in delta, no </think> in delta,\n-                # reasoning content continues\n+                # <think> starts in delta: reasoning continues\n                 return DeltaMessage(reasoning_content=delta_text)\n         else:\n-            # thinking is disabled, just content\n+            # thinking is disabled or not started: plain content\n             return DeltaMessage(content=delta_text)\n \n     def extract_reasoning_content(\n             self, model_output: str, request: ChatCompletionRequest\n     ) -> tuple[Optional[str], Optional[str]]:\n \n-        # Check if the model output contains the <think> tokens.\n-        if (self.think_start_token not in model_output\n-                or self.think_end_token not in model_output):\n-            return None, model_output\n-        else:\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            # Remove the reasoning content from the model output\n-            # Although <think> token is always at the\n-            # beginning of the line, we cannot guarantee that the\n-            # other models will follow this convention.\n-            # Therefore, we need to add :start_index.\n-            start_index = model_output.find(self.think_start_token)\n-            if start_index != -1:\n-                end_index = start_index + len(\n-                    f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-                )\n-                model_output = model_output[:start_index] + \\\n-                                model_output[end_index:]\n-\n-                if len(model_output) == 0:\n-                    return reasoning_content, None\n-\n-            return reasoning_content, model_output\n+        # Fast path using string search to avoid extra regex overhead.\n+        s = model_output\n+        start_tok = self.think_start_token\n+        end_tok = self.think_end_token\n+\n+        start_index = s.find(start_tok)\n+        if start_index == -1:\n+            return None, s\n+\n+        content_start = start_index + len(start_tok)\n+        end_index = s.find(end_tok, content_start)\n+        if end_index == -1:\n+            return None, s\n+\n+        reasoning_content = s[content_start:end_index]\n+        # Remove the reasoning span <think>... </think>\n+        new_output = s[:start_index] + s[end_index + len(end_tok):]\n+\n+        if not new_output:\n+            return reasoning_content, None\n+\n+        return reasoning_content, new_output\n", "model_name_or_path": "gpt-5-2025-08-07"}
