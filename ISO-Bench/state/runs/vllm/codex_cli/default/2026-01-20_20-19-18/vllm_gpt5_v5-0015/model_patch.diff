diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py
index f588f40..2f5e999 100644
--- a/vllm/reasoning/qwen3_reasoning_parser.py
+++ b/vllm/reasoning/qwen3_reasoning_parser.py
@@ -31,8 +31,12 @@ class Qwen3ReasoningParser(ReasoningParser):
         self.think_start_token = "<think>"
         self.think_end_token = "</think>"
 
+        # Precompile a safe regex (escaped tokens) though the hot path below
+        # prefers faster string operations.
         self.reasoning_regex = re.compile(
-            rf"{self.think_start_token}(.*?){self.think_end_token}", re.DOTALL)
+            rf"{re.escape(self.think_start_token)}(.*?){re.escape(self.think_end_token)}",
+            re.DOTALL,
+        )
 
         if not self.model_tokenizer:
             raise ValueError(
@@ -54,10 +58,15 @@ class Qwen3ReasoningParser(ReasoningParser):
         """
         Extract the content after the end tokens
         """
-        if self.think_end_token_id not in input_ids[:-1]:
+        # Find the first occurrence of the end token in a single pass
+        try:
+            idx = input_ids.index(self.think_end_token_id)
+        except ValueError:
             return []
-        else:
-            return input_ids[input_ids.index(self.think_end_token_id) + 1:]
+        # If the end token is the last element, there is no content after it
+        if idx >= len(input_ids) - 1:
+            return []
+        return input_ids[idx + 1:]
 
     def extract_reasoning_content_streaming(
         self,
@@ -77,73 +86,81 @@ class Qwen3ReasoningParser(ReasoningParser):
         - 'xyz' goes to content
         """
         # Skip single special tokens
-        if len(delta_token_ids) == 1 and (delta_token_ids[0] in [
-                self.think_start_token_id, self.think_end_token_id
-        ]):
+        if (len(delta_token_ids) == 1 and
+                (delta_token_ids[0] == self.think_start_token_id or
+                 delta_token_ids[0] == self.think_end_token_id)):
             return None
 
-        if self.think_start_token_id in previous_token_ids:
-            if self.think_end_token_id in delta_token_ids:
-                # <think> in previous, </think> in delta,
-                # extract reasoning content
-                end_index = delta_text.find(self.think_end_token)
+        prev_has_start = self.think_start_token_id in previous_token_ids
+        prev_has_end = self.think_end_token_id in previous_token_ids
+        delta_has_start = self.think_start_token_id in delta_token_ids
+        delta_has_end = self.think_end_token_id in delta_token_ids
+
+        start_tok = self.think_start_token
+        end_tok = self.think_end_token
+
+        if prev_has_start:
+            if delta_has_end:
+                # <think> in previous, </think> in delta: split once
+                end_index = delta_text.find(end_tok)
+                if end_index == -1:
+                    # Defensive: treat as reasoning continuation
+                    return DeltaMessage(reasoning_content=delta_text)
                 reasoning_content = delta_text[:end_index]
-                content = delta_text[end_index + len(self.think_end_token):]
+                content = delta_text[end_index + len(end_tok):]
                 return DeltaMessage(reasoning_content=reasoning_content,
-                                    content=content if content else None)
-            elif self.think_end_token_id in previous_token_ids:
-                # <think> in previous, </think> in previous,
-                # reasoning content continues
+                                    content=content or None)
+            elif prev_has_end:
+                # <think> and </think> already closed previously: content
                 return DeltaMessage(content=delta_text)
             else:
-                # <think> in previous, no </think> in previous or delta,
-                # reasoning content continues
+                # Inside reasoning: continue reasoning content
                 return DeltaMessage(reasoning_content=delta_text)
-        elif self.think_start_token_id in delta_token_ids:
-            if self.think_end_token_id in delta_token_ids:
-                # <think> in delta, </think> in delta, extract reasoning content
-                start_index = delta_text.find(self.think_start_token)
-                end_index = delta_text.find(self.think_end_token)
-                reasoning_content = delta_text[start_index +
-                                               len(self.think_start_token
-                                                   ):end_index]
-                content = delta_text[end_index + len(self.think_end_token):]
+        elif delta_has_start:
+            if delta_has_end:
+                # Both <think> and </think> in delta: extract reasoning
+                start_index = delta_text.find(start_tok)
+                end_index = delta_text.find(end_tok,
+                                            start_index + len(start_tok)
+                                            if start_index != -1 else 0)
+                if start_index == -1 or end_index == -1:
+                    # Fallback: treat as content if malformed order
+                    return DeltaMessage(content=delta_text)
+                reasoning_content = delta_text[start_index + len(start_tok):
+                                               end_index]
+                content = delta_text[end_index + len(end_tok):]
                 return DeltaMessage(reasoning_content=reasoning_content,
-                                    content=content if content else None)
+                                    content=content or None)
             else:
-                # <think> in delta, no </think> in delta,
-                # reasoning content continues
+                # <think> starts in delta: reasoning continues
                 return DeltaMessage(reasoning_content=delta_text)
         else:
-            # thinking is disabled, just content
+            # thinking is disabled or not started: plain content
             return DeltaMessage(content=delta_text)
 
     def extract_reasoning_content(
             self, model_output: str, request: ChatCompletionRequest
     ) -> tuple[Optional[str], Optional[str]]:
 
-        # Check if the model output contains the <think> tokens.
-        if (self.think_start_token not in model_output
-                or self.think_end_token not in model_output):
-            return None, model_output
-        else:
-            # Use a regex to find the reasoning content
-            reasoning_content = self.reasoning_regex.findall(model_output)[0]
-
-            # Remove the reasoning content from the model output
-            # Although <think> token is always at the
-            # beginning of the line, we cannot guarantee that the
-            # other models will follow this convention.
-            # Therefore, we need to add :start_index.
-            start_index = model_output.find(self.think_start_token)
-            if start_index != -1:
-                end_index = start_index + len(
-                    f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
-                )
-                model_output = model_output[:start_index] + \
-                                model_output[end_index:]
-
-                if len(model_output) == 0:
-                    return reasoning_content, None
-
-            return reasoning_content, model_output
+        # Fast path using string search to avoid extra regex overhead.
+        s = model_output
+        start_tok = self.think_start_token
+        end_tok = self.think_end_token
+
+        start_index = s.find(start_tok)
+        if start_index == -1:
+            return None, s
+
+        content_start = start_index + len(start_tok)
+        end_index = s.find(end_tok, content_start)
+        if end_index == -1:
+            return None, s
+
+        reasoning_content = s[content_start:end_index]
+        # Remove the reasoning span <think>... </think>
+        new_output = s[:start_index] + s[end_index + len(end_tok):]
+
+        if not new_output:
+            return reasoning_content, None
+
+        return reasoning_content, new_output
