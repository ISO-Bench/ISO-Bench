diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py
index 1dea711..28b2f31 100644
--- a/vllm/v1/sample/ops/topk_topp_sampler.py
+++ b/vllm/v1/sample/ops/topk_topp_sampler.py
@@ -134,33 +134,115 @@ def apply_top_k_top_p(
     k: Optional[torch.Tensor],
     p: Optional[torch.Tensor],
 ) -> torch.Tensor:
-    """Apply top-k and top-p masks to the logits.
+    """Apply top-k and top-p masks to the logits efficiently.
 
-    This function sorts the logits tensor, which can be slow for large batches.
+    Optimizations:
+    - Avoid a full sort for top-k only by using `torch.topk`.
+    - When both top-k and top-p are provided, first restrict to top-k and
+      apply top-p within the top-k candidates (O(k log k) instead of
+      O(V log V)).
+    - Fall back to a single sort only for top-p-only cases.
     """
-    if k is None and p is None:
+    vocab_size = logits.size(-1)
+
+    # Fast path: nothing to do
+    if (k is None or (isinstance(k, int) and (k >= vocab_size or k <= 0))) \
+            and (p is None):
+        return logits
+
+    # Normalize k into a per-batch tensor if provided.
+    k_tensor: Optional[torch.Tensor]
+    if k is None:
+        k_tensor = None
+    else:
+        if isinstance(k, torch.Tensor):
+            k_tensor = k.to(torch.long)
+        else:
+            # single int for the whole batch
+            k_tensor = torch.tensor([k], device=logits.device, dtype=torch.long)
+            if logits.dim() >= 2 and logits.size(0) > 1:
+                k_tensor = k_tensor.expand(logits.size(0))
+
+        # Clamp to valid range [0, vocab_size]
+        k_tensor = torch.clamp(k_tensor, min=0, max=vocab_size)
+        if torch.all(k_tensor >= vocab_size):
+            # Keeping all tokens â€“ reduce to top-p only (if any)
+            k_tensor = None
+
+    # Helper to mask logits given selected indices to keep.
+    def _mask_with_indices(indices: torch.Tensor, keep_mask: torch.Tensor) -> None:
+        # indices: [B, K], keep_mask: [B, K] True=keep, False=drop
+        mask = torch.ones_like(logits, dtype=torch.bool)
+        mask.scatter_(dim=-1, index=indices, src=~keep_mask)
+        logits.masked_fill_(mask, float('-inf'))
+
+    if p is None and k_tensor is not None:
+        # Top-k only: use a single topk with max(k) then per-row trim.
+        kmax = int(k_tensor.max().item())
+        if kmax <= 0:
+            # Degenerate: mask out everything except ensuring numerical safety
+            logits.fill_(float('-inf'))
+            return logits
+        values, indices = torch.topk(logits, kmax, dim=-1)
+
+        # Build per-row keep mask for first k_i entries
+        arange_k = torch.arange(kmax, device=logits.device)
+        keep_mask = arange_k.unsqueeze(0) < k_tensor.unsqueeze(1)
+        _mask_with_indices(indices, keep_mask)
+        return logits
+
+    if p is not None and k_tensor is not None:
+        # Both top-k and top-p: restrict to top-k first, then apply top-p
+        kmax = int(k_tensor.max().item())
+        if kmax <= 0:
+            logits.fill_(float('-inf'))
+            return logits
+        values, indices = torch.topk(logits, kmax, dim=-1)
+
+        # Mask out positions beyond per-row k before computing softmax
+        arange_k = torch.arange(kmax, device=logits.device)
+        keep_k_mask = arange_k.unsqueeze(0) < k_tensor.unsqueeze(1)
+        # Set values for positions not in per-row k to -inf so they contribute 0
+        if not torch.all(keep_k_mask):
+            values = values.masked_fill(~keep_k_mask, float('-inf'))
+
+        # Compute probabilities within the restricted set
+        probs_topk = values.softmax(dim=-1)
+        cumsum = probs_topk.cumsum(dim=-1)
+        # Broadcast p to shape [B, 1]
+        p_tensor = p if isinstance(p, torch.Tensor) else torch.tensor(p,
+                                                                      device=logits.device,
+                                                                      dtype=probs_topk.dtype)
+        p_tensor = p_tensor.to(probs_topk.dtype).unsqueeze(1)
+        topp_mask = cumsum > p_tensor
+        # Always keep at least one token
+        if topp_mask.numel() > 0:
+            topp_mask[:, 0] = False
+
+        # Final keep mask combines top-k and top-p selections
+        final_keep = keep_k_mask & (~topp_mask)
+        _mask_with_indices(indices, final_keep)
         return logits
+
+    # p is not None and k is None: use a single sort (fallback)
+    # Match original semantics by sorting ascending and removing tokens whose
+    # cumulative probability is <= 1 - p, keeping at least one token.
     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
 
-    if k is not None:
-        # Apply top-k.
-        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B
-        # Get all the top_k values.
-        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))
-        top_k_mask = logits_sort < top_k_mask
-        logits_sort.masked_fill_(top_k_mask, -float("inf"))
-
-    if p is not None:
-        # Apply top-p.
-        probs_sort = logits_sort.softmax(dim=-1)
-        probs_sum = probs_sort.cumsum(dim=-1)
-        top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)
-        # at least one
+    probs_sort = logits_sort.softmax(dim=-1)
+    probs_sum = probs_sort.cumsum(dim=-1)
+    p_tensor = p if isinstance(p, torch.Tensor) else torch.tensor(p,
+                                                                  device=logits.device,
+                                                                  dtype=probs_sort.dtype)
+    p_tensor = p_tensor.to(probs_sort.dtype).unsqueeze(1)
+    top_p_mask = probs_sum <= (1 - p_tensor)
+    # Keep at least one token (the largest at the end of ascending order)
+    if top_p_mask.numel() > 0:
         top_p_mask[:, -1] = False
-        logits_sort.masked_fill_(top_p_mask, -float("inf"))
 
-    # Re-sort the probabilities.
-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)
+    logits_sort.masked_fill_(top_p_mask, float('-inf'))
+    # Scatter back to original order
+    logits = logits.scatter(dim=-1, index=logits_idx, src=logits_sort)
     return logits
 
 
diff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py
index 397a049..2c628a3 100644
--- a/vllm/v1/sample/sampler.py
+++ b/vllm/v1/sample/sampler.py
@@ -200,20 +200,23 @@ class Sampler(nn.Module):
         min_p: torch.Tensor,
     ) -> torch.Tensor:
         """
-        Filters logits using adaptive probability thresholding.
+        Filter logits using adaptive probability thresholding without softmax.
+
+        Keeps tokens whose probability p_i satisfies
+        p_i >= min_p * max_j p_j. Using logits l_i, since
+        p_i / p_max = exp(l_i - l_max), this is equivalent to
+        l_i - l_max >= log(min_p) -> l_i >= l_max + log(min_p).
         """
-        # Convert logits to probability distribution
-        probability_values = torch.nn.functional.softmax(logits, dim=-1)
-        # Calculate maximum probabilities per sequence
-        max_probabilities = torch.amax(probability_values,
-                                       dim=-1,
-                                       keepdim=True)
-        # Reshape min_p for broadcasting
-        adjusted_min_p = min_p.unsqueeze(1) * max_probabilities
-        # Identify valid tokens using threshold comparison
-        valid_token_mask = probability_values >= adjusted_min_p
-        # Apply mask using boolean indexing
-        logits[~valid_token_mask] = -float('inf')
+        # Compute per-row maximum logit and threshold using log(min_p).
+        lmax = logits.max(dim=-1, keepdim=True).values
+        # Ensure min_p is tensor on the same device/dtype
+        if not isinstance(min_p, torch.Tensor):
+            min_p = torch.tensor(min_p, device=logits.device, dtype=logits.dtype)
+        else:
+            min_p = min_p.to(device=logits.device, dtype=logits.dtype)
+        thresh = lmax + torch.log(min_p).unsqueeze(1)
+        # Mask out tokens below threshold.
+        logits.masked_fill_(logits < thresh, float("-inf"))
         return logits
 
     def apply_logits_bias(
