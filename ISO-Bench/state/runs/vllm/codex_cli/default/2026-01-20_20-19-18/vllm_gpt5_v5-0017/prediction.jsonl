{"instance_id": "/root/OmniPerf-Bench/vllm__repo-733e7c9", "model_patch": "diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea711..28b2f31 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -134,33 +134,115 @@ def apply_top_k_top_p(\n     k: Optional[torch.Tensor],\n     p: Optional[torch.Tensor],\n ) -> torch.Tensor:\n-    \"\"\"Apply top-k and top-p masks to the logits.\n+    \"\"\"Apply top-k and top-p masks to the logits efficiently.\n \n-    This function sorts the logits tensor, which can be slow for large batches.\n+    Optimizations:\n+    - Avoid a full sort for top-k only by using `torch.topk`.\n+    - When both top-k and top-p are provided, first restrict to top-k and\n+      apply top-p within the top-k candidates (O(k log k) instead of\n+      O(V log V)).\n+    - Fall back to a single sort only for top-p-only cases.\n     \"\"\"\n-    if k is None and p is None:\n+    vocab_size = logits.size(-1)\n+\n+    # Fast path: nothing to do\n+    if (k is None or (isinstance(k, int) and (k >= vocab_size or k <= 0))) \\\n+            and (p is None):\n+        return logits\n+\n+    # Normalize k into a per-batch tensor if provided.\n+    k_tensor: Optional[torch.Tensor]\n+    if k is None:\n+        k_tensor = None\n+    else:\n+        if isinstance(k, torch.Tensor):\n+            k_tensor = k.to(torch.long)\n+        else:\n+            # single int for the whole batch\n+            k_tensor = torch.tensor([k], device=logits.device, dtype=torch.long)\n+            if logits.dim() >= 2 and logits.size(0) > 1:\n+                k_tensor = k_tensor.expand(logits.size(0))\n+\n+        # Clamp to valid range [0, vocab_size]\n+        k_tensor = torch.clamp(k_tensor, min=0, max=vocab_size)\n+        if torch.all(k_tensor >= vocab_size):\n+            # Keeping all tokens \u2013 reduce to top-p only (if any)\n+            k_tensor = None\n+\n+    # Helper to mask logits given selected indices to keep.\n+    def _mask_with_indices(indices: torch.Tensor, keep_mask: torch.Tensor) -> None:\n+        # indices: [B, K], keep_mask: [B, K] True=keep, False=drop\n+        mask = torch.ones_like(logits, dtype=torch.bool)\n+        mask.scatter_(dim=-1, index=indices, src=~keep_mask)\n+        logits.masked_fill_(mask, float('-inf'))\n+\n+    if p is None and k_tensor is not None:\n+        # Top-k only: use a single topk with max(k) then per-row trim.\n+        kmax = int(k_tensor.max().item())\n+        if kmax <= 0:\n+            # Degenerate: mask out everything except ensuring numerical safety\n+            logits.fill_(float('-inf'))\n+            return logits\n+        values, indices = torch.topk(logits, kmax, dim=-1)\n+\n+        # Build per-row keep mask for first k_i entries\n+        arange_k = torch.arange(kmax, device=logits.device)\n+        keep_mask = arange_k.unsqueeze(0) < k_tensor.unsqueeze(1)\n+        _mask_with_indices(indices, keep_mask)\n+        return logits\n+\n+    if p is not None and k_tensor is not None:\n+        # Both top-k and top-p: restrict to top-k first, then apply top-p\n+        kmax = int(k_tensor.max().item())\n+        if kmax <= 0:\n+            logits.fill_(float('-inf'))\n+            return logits\n+        values, indices = torch.topk(logits, kmax, dim=-1)\n+\n+        # Mask out positions beyond per-row k before computing softmax\n+        arange_k = torch.arange(kmax, device=logits.device)\n+        keep_k_mask = arange_k.unsqueeze(0) < k_tensor.unsqueeze(1)\n+        # Set values for positions not in per-row k to -inf so they contribute 0\n+        if not torch.all(keep_k_mask):\n+            values = values.masked_fill(~keep_k_mask, float('-inf'))\n+\n+        # Compute probabilities within the restricted set\n+        probs_topk = values.softmax(dim=-1)\n+        cumsum = probs_topk.cumsum(dim=-1)\n+        # Broadcast p to shape [B, 1]\n+        p_tensor = p if isinstance(p, torch.Tensor) else torch.tensor(p,\n+                                                                      device=logits.device,\n+                                                                      dtype=probs_topk.dtype)\n+        p_tensor = p_tensor.to(probs_topk.dtype).unsqueeze(1)\n+        topp_mask = cumsum > p_tensor\n+        # Always keep at least one token\n+        if topp_mask.numel() > 0:\n+            topp_mask[:, 0] = False\n+\n+        # Final keep mask combines top-k and top-p selections\n+        final_keep = keep_k_mask & (~topp_mask)\n+        _mask_with_indices(indices, final_keep)\n         return logits\n+\n+    # p is not None and k is None: use a single sort (fallback)\n+    # Match original semantics by sorting ascending and removing tokens whose\n+    # cumulative probability is <= 1 - p, keeping at least one token.\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n-    if k is not None:\n-        # Apply top-k.\n-        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B\n-        # Get all the top_k values.\n-        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n-        top_k_mask = logits_sort < top_k_mask\n-        logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n-\n-    if p is not None:\n-        # Apply top-p.\n-        probs_sort = logits_sort.softmax(dim=-1)\n-        probs_sum = probs_sort.cumsum(dim=-1)\n-        top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n-        # at least one\n+    probs_sort = logits_sort.softmax(dim=-1)\n+    probs_sum = probs_sort.cumsum(dim=-1)\n+    p_tensor = p if isinstance(p, torch.Tensor) else torch.tensor(p,\n+                                                                  device=logits.device,\n+                                                                  dtype=probs_sort.dtype)\n+    p_tensor = p_tensor.to(probs_sort.dtype).unsqueeze(1)\n+    top_p_mask = probs_sum <= (1 - p_tensor)\n+    # Keep at least one token (the largest at the end of ascending order)\n+    if top_p_mask.numel() > 0:\n         top_p_mask[:, -1] = False\n-        logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n-    # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    logits_sort.masked_fill_(top_p_mask, float('-inf'))\n+    # Scatter back to original order\n+    logits = logits.scatter(dim=-1, index=logits_idx, src=logits_sort)\n     return logits\n \n \ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049..2c628a3 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -200,20 +200,23 @@ class Sampler(nn.Module):\n         min_p: torch.Tensor,\n     ) -> torch.Tensor:\n         \"\"\"\n-        Filters logits using adaptive probability thresholding.\n+        Filter logits using adaptive probability thresholding without softmax.\n+\n+        Keeps tokens whose probability p_i satisfies\n+        p_i >= min_p * max_j p_j. Using logits l_i, since\n+        p_i / p_max = exp(l_i - l_max), this is equivalent to\n+        l_i - l_max >= log(min_p) -> l_i >= l_max + log(min_p).\n         \"\"\"\n-        # Convert logits to probability distribution\n-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n-        # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Reshape min_p for broadcasting\n-        adjusted_min_p = min_p.unsqueeze(1) * max_probabilities\n-        # Identify valid tokens using threshold comparison\n-        valid_token_mask = probability_values >= adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[~valid_token_mask] = -float('inf')\n+        # Compute per-row maximum logit and threshold using log(min_p).\n+        lmax = logits.max(dim=-1, keepdim=True).values\n+        # Ensure min_p is tensor on the same device/dtype\n+        if not isinstance(min_p, torch.Tensor):\n+            min_p = torch.tensor(min_p, device=logits.device, dtype=logits.dtype)\n+        else:\n+            min_p = min_p.to(device=logits.device, dtype=logits.dtype)\n+        thresh = lmax + torch.log(min_p).unsqueeze(1)\n+        # Mask out tokens below threshold.\n+        logits.masked_fill_(logits < thresh, float(\"-inf\"))\n         return logits\n \n     def apply_logits_bias(\n", "model_name_or_path": "gpt-5-2025-08-07"}
