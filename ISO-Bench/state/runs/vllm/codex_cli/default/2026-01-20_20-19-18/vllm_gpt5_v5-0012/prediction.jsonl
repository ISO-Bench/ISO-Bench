{"instance_id": "/root/OmniPerf-Bench/vllm__repo-029c71d", "model_patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c222687..aa84ee2 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -369,6 +369,10 @@ class TestPrefixCachingBlockAllocator:\n         assert allocator_dst.get_num_blocks_touched(\n             blocks_to_swap_in, num_lookahead_slots=block_size) == 3\n \n+        # Mark blocks as computed bulk path should be a cheap no-op\n+        # when invoked with empty list, but must not throw.\n+        allocator_dst.mark_blocks_as_computed([])\n+\n     @staticmethod\n     @pytest.mark.parametrize(\"num_blocks\", [1024])\n     @pytest.mark.parametrize(\"block_size\", [16])\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a665..f5372f2 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -1,5 +1,4 @@\n \"\"\"Token blocks.\"\"\"\n-from os.path import commonprefix\n from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n \n from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,\n@@ -106,7 +105,9 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         self._cow_tracker = CopyOnWriteTracker(\n             refcounter=self._refcounter.as_readonly())\n \n-        self.metric_data = CacheMetricData()\n+        # Track cache effectiveness using block granularity equal to\n+        # allocator block size to keep stats representative and cheap.\n+        self.metric_data = CacheMetricData(block_size=block_size)\n \n     # Implements Block.Factory.\n     def _create_block(\n@@ -507,7 +508,33 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        \"\"\"Marks the provided blocks as computed.\n+\n+        This is a fast-path bulk operation used by the scheduler right after\n+        a successful model run. It avoids repeated per-block checks when\n+        detecting computed prefixes on the next scheduling iteration.\n+\n+        Notes:\n+        - Passing an empty list is a valid no-op and returns immediately.\n+        - For blocks in the evictor (inactive), we do not need to update any\n+          state, as they are already considered computed for prefix caching.\n+        \"\"\"\n+        if not block_ids:\n+            return\n+\n+        # Update the in-memory tracker only; this is sufficient for\n+        # get_computed_block_ids(..) fast-path checks.\n+        tracker = self._block_tracker\n+        evictor = self.evictor\n+        for bid in block_ids:\n+            state = tracker.get(bid)\n+            if state is not None and state.active:\n+                state.computed = True\n+            elif bid in evictor:\n+                # Already eligible for reuse; nothing to do.\n+                continue\n+            # If the block id is unknown, silently ignore. This allows callers\n+            # to pass mixed device block ids when only GPU ids are tracked.\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\n@@ -561,10 +588,22 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         if len(computed_seq_block_ids) == 1:\n             return computed_seq_block_ids[0]\n \n-        return commonprefix([\n-            ids for ids in computed_seq_block_ids  # type: ignore\n-            if ids\n-        ])\n+        # Filter out empty lists early.\n+        lists = [ids for ids in computed_seq_block_ids if ids]\n+        if not lists:\n+            return []\n+\n+        # Compute longest common prefix efficiently without importing\n+        # os.path.commonprefix (which is string-oriented and adds overhead).\n+        first = lists[0]\n+        prefix_len = 0\n+        for cols in zip(*lists):\n+            v = cols[0]\n+            if all(x == v for x in cols[1:]):\n+                prefix_len += 1\n+            else:\n+                break\n+        return first[:prefix_len]\n \n     def get_num_blocks_touched(self,\n                                blocks: List[Block],\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451..cd8d115 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,19 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        \"\"\"Mark all blocks of scheduled sequences as computed for caching.\n+\n+        This enables the allocator to acknowledge computed prefixes without\n+        re-checking on subsequent iterations. When caching is disabled, this\n+        becomes a cheap no-op.\n+        \"\"\"\n+        if not self.enable_caching:\n+            return\n+\n+        for seq in seq_group.get_seqs():\n+            block_ids = self.block_tables[seq.seq_id].physical_block_ids\n+            # Empty or single-block prompts are fine; allocator handles it.\n+            self.block_allocator.mark_blocks_as_computed(block_ids)  # type: ignore\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
