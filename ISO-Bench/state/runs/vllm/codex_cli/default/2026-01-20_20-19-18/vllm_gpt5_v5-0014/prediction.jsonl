{"instance_id": "/root/OmniPerf-Bench/vllm__repo-2f38518", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e791453..f544700 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -385,18 +385,28 @@ class InputBatch:\n \n     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:\n         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()\n+        # Allocate once on CPU (pinned if available) and use vectorized ops\n+        # to avoid per-request Python loops.\n         prompt_token_ids_cpu_tensor = torch.empty(\n             (self.num_reqs, max_prompt_len),\n             device=\"cpu\",\n             dtype=torch.int64,\n             pin_memory=self.pin_memory)\n         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()\n+\n+        # Copy token ids for all requests at once.\n         prompt_token_ids[:] = (\n             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])\n-        # Use the value of vocab_size as a pad since we don't have a\n-        # token_id of this value.\n-        for i in range(self.num_reqs):\n-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size\n+\n+        # Pad trailing positions in a single vectorized step instead of a loop.\n+        # Use vocab_size as the padding token since it is guaranteed to be an\n+        # out-of-vocabulary id.\n+        if max_prompt_len > 0 and self.num_reqs > 0:\n+            cols = np.arange(max_prompt_len, dtype=np.int32)\n+            lens = self.num_prompt_tokens[:self.num_reqs]\n+            mask = cols >= lens[:, None]\n+            prompt_token_ids[mask] = self.vocab_size\n+\n         return prompt_token_ids_cpu_tensor.to(device=self.device,\n                                               non_blocking=True)\n \ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54..784c7fe 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -333,8 +333,10 @@ class GPUModelRunner:\n         # NOTE(woosuk): We use torch.index_select instead of np.take here\n         # because torch.index_select is much faster than np.take for large\n         # tensors.\n-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n-                         [block_table_indices].numpy())\n+        # Use torch.index_select for better performance vs. advanced indexing.\n+        block_numbers = torch.index_select(\n+            self.input_batch.block_table_cpu_tensor.flatten(), 0,\n+            torch.from_numpy(block_table_indices)).numpy()\n         block_offsets = positions_np % self.block_size\n         np.add(block_numbers * self.block_size,\n                block_offsets,\n", "model_name_or_path": "gpt-5-2025-08-07"}
