OpenAI Codex v0.87.0 (research preview)
--------
workdir: /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014
model: gpt-5
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: high
reasoning summaries: auto
session id: 019bdd27-9773-7753-9c61-531ffc9c3a09
--------
user
I've uploaded a python code repository in the directory /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014.
Consider the following test script showing an example usage of the repository:

<test_script>
# This is a performance optimization task
# The specific operations to optimize are in the files listed below
# Focus on performance improvements in the target functions

</test_script>

Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?

Basic guidelines:
1. Your task is to make changes to non-test files in the /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 directory to improve the performance of the <test_script>.
2. Make changes while ensuring the repository is functionally equivalent to the original.
3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.
4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.

Follow these steps to improve performance:
1. As a first step, explore the repository structure and understand the target files.
2. Create a script ONLY inside /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/.bench_scratch (e.g., /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/.bench_scratch/test_opt.py) to reproduce and time the example, then execute it with python <filename.py> from the repo root.
3. Profile and identify performance bottlenecks in the target files.
4. Edit the source code of the repository to improve performance.
5. Rebuild and rerun your script to confirm that performance has improved.

Common optimization strategies to consider:
- Reduce unnecessary memory allocations
- Avoid redundant computations
- Optimize data structures and algorithms
- Improve cache locality
- Reduce synchronization overhead
- Use more efficient library calls where available

Target files to optimize:
- vllm/v1/worker/gpu_input_batch.py
- vllm/v1/worker/gpu_model_runner.py

IMPORTANT: You MUST make actual code changes to at least one file.
The task will fail if no files are modified.

## Constraints
- No public API breakage
- All TestPack checks must pass

## Target Files (ONLY modify these)
- `vllm/v1/worker/gpu_input_batch.py`
- `vllm/v1/worker/gpu_model_runner.py`

## Files Modified (statistics)
The following files were changed in the reference optimization:
```
vllm/v1/worker/gpu_input_batch.py  | 13 ++++++++-----
 vllm/v1/worker/gpu_model_runner.py |  1 +
 2 files changed, 9 insertions(+), 5 deletions(-)
```

## TASK COMPLETION COMMAND:
When you have made optimizations:
```bash
git add -A
git reset -q .bench_scratch || true
git commit -m 'Optimize MoE align sum kernels performance'
git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' > /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/model_patch.diff || true
finish
```

START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.

mcp startup: no servers
codex
I’m going to scan the repo and open the two target files to understand current logic and spot hotspots. Then I’ll scaffold a small timing script under `.bench_scratch` to reproduce and baseline.
exec
bash -lc 'ls -la' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 6ms:
total 332
drwxr-xr-x 13 root root  4096 Jan 20 20:45 .
drwxr-xr-x  7 root root   155 Jan 20 20:45 ..
drwxr-xr-x  4 root root  4096 Jan 20 20:45 .buildkite
-rw-r--r--  1 root root   641 Jan 20 20:45 .clang-format
-rw-r--r--  1 root root   345 Jan 20 20:45 .dockerignore
drwxr-xr-x  8 root root  4096 Jan 20 20:45 .git
drwxr-xr-x  5 root root  4096 Jan 20 20:45 .github
-rw-r--r--  1 root root  3683 Jan 20 20:45 .gitignore
-rw-r--r--  1 root root   481 Jan 20 20:45 .readthedocs.yaml
-rw-r--r--  1 root root   496 Jan 20 20:45 .shellcheckrc
-rw-r--r--  1 root root    15 Jan 20 20:45 .yapfignore
-rw-r--r--  1 root root 22710 Jan 20 20:45 CMakeLists.txt
-rw-r--r--  1 root root  5337 Jan 20 20:45 CODE_OF_CONDUCT.md
-rw-r--r--  1 root root   154 Jan 20 20:45 CONTRIBUTING.md
-rw-r--r--  1 root root  1366 Jan 20 20:45 DCO
-rw-r--r--  1 root root 10920 Jan 20 20:45 Dockerfile
-rw-r--r--  1 root root  2395 Jan 20 20:45 Dockerfile.arm
-rw-r--r--  1 root root  2649 Jan 20 20:45 Dockerfile.cpu
-rw-r--r--  1 root root   610 Jan 20 20:45 Dockerfile.hpu
exec
bash -lc "sed -n '1,200p' vllm/v1/worker/gpu_input_batch.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 5ms:
# Datastructures defining an input batch

from dataclasses import dataclass
from typing import TYPE_CHECKING, Dict, List, Optional, Set

import numpy as np
import torch

from vllm.multimodal import MultiModalKwargs
from vllm.sampling_params import SamplingParams, SamplingType
from vllm.v1.sample.metadata import SamplingMetadata

if TYPE_CHECKING:
    from vllm.multimodal.inputs import PlaceholderRange


@dataclass
class CachedRequestState:

    req_id: str
exec
bash -lc "sed -n '200,400p' vllm/v1/worker/gpu_input_batch.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 6ms:
            self.greedy_reqs.add(req_id)
        else:
            self.random_reqs.add(req_id)

        self.top_p_cpu[req_index] = sampling_params.top_p
        if sampling_params.top_p < 1:
            self.top_p_reqs.add(req_id)
        self.top_k_cpu[req_index] = sampling_params.top_k
        if sampling_params.top_k > 0:
            self.top_k_reqs.add(req_id)
        self.frequency_penalties_cpu[req_index] = \
            sampling_params.frequency_penalty
        if sampling_params.frequency_penalty != 0.0:
            self.frequency_penalties_reqs.add(req_id)
        self.presence_penalties_cpu[req_index] = \
            sampling_params.presence_penalty
        if sampling_params.presence_penalty != 0.0:
            self.presence_penalties_reqs.add(req_id)
        self.repetition_penalties_cpu[req_index] = \
            sampling_params.repetition_penalty
exec
bash -lc "sed -n '400,800p' vllm/v1/worker/gpu_input_batch.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 5ms:
        return prompt_token_ids_cpu_tensor.to(device=self.device,
                                              non_blocking=True)

    @property
    def num_reqs(self) -> int:
        return len(self.req_id_to_index)

    @property
    def all_greedy(self) -> bool:
        return len(self.random_reqs) == 0

    @property
    def all_random(self) -> bool:
        return len(self.greedy_reqs) == 0

    @property
    def no_top_p(self) -> bool:
        return len(self.top_p_reqs) == 0

    @property
exec
bash -lc "sed -n '1,200p' vllm/v1/worker/gpu_model_runner.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 6ms:
import gc
import time
from typing import TYPE_CHECKING, Dict, List, Tuple, cast

import numpy as np
import torch
import torch.distributed
import torch.nn as nn

from vllm.config import CompilationLevel, VllmConfig
from vllm.distributed.parallel_state import graph_capture
from vllm.forward_context import set_forward_context
from vllm.inputs import INPUT_REGISTRY
from vllm.logger import init_logger
from vllm.model_executor.model_loader import get_model
from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalKwargs
from vllm.sampling_params import SamplingType
from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,
                        LayerBlockType, cdiv, is_pin_memory_available)
from vllm.v1.attention.backends.flash_attn import (FlashAttentionBackend,
exec
bash -lc "sed -n '200,400p' vllm/v1/worker/gpu_model_runner.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 6ms:
            req_id = req_data.req_id
            req_state = self.requests[req_id]
            req_index = self.input_batch.req_id_to_index[req_id]

            # Update the num_computed_tokens.
            req_state.num_computed_tokens = req_data.num_computed_tokens
            self.input_batch.num_computed_tokens_cpu[req_index] = (
                req_data.num_computed_tokens)

            # Update the block table.
            num_new_blocks = len(req_data.new_block_ids)
            if num_new_blocks == 0:
                continue
            start_index = len(req_state.block_ids)
            end_index = start_index + num_new_blocks
            req_state.block_ids.extend(req_data.new_block_ids)
            self.input_batch.block_table_cpu[
                req_index, start_index:end_index] = req_data.new_block_ids

        req_ids_to_add: List[str] = []
exec
bash -lc "sed -n '400,800p' vllm/v1/worker/gpu_model_runner.py" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 6ms:
            # [A, B, C, D] (i.e., the common prefix is capped by the minimum
            # num_computed_tokens, without plus one).
            # This is because of an implementation detail: We want to always
            # use two kernels for cascade attention. Let's imagine:
            # Request 3's input query: [D]
            # Request 3's kv cache: [A, B, C, D]
            # Request 3's num_computed_tokens: 4 (i.e., [A, B, C, D])
            # If we use [A, B, C, D] as the common prefix for Request 1-3,
            # then Request 3 will be processed only by the first kernel,
            # and the second kernel will get an empty input. While this is not
            # a fundamental problem, our current implementation does not support
            # this case.
            common_prefix_len = min(
                common_prefix_len,
                self.input_batch.num_computed_tokens_cpu[:num_reqs].min())
            # common_prefix_len should be a multiple of the block size.
            common_prefix_len = (common_prefix_len // self.block_size *
                                 self.block_size)
            use_cascade = FlashAttentionBackend.use_cascade_attention(
                common_prefix_len=common_prefix_len,
exec
bash -lc 'rg -n "TODO|NOTE|optimi|non_blocking|numpy|index_select|make_sampling_metadata|_make_prompt_token_ids_tensor|condense|block_table" vllm/v1/worker/gpu_model_runner.py' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 7ms:
5:import numpy as np
85:        # NOTE: mm_input_mapper_client and mm_hasher are only used for memory
116:        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.
143:        # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
150:        self.input_ids_np = self.input_ids_cpu.numpy()
155:        self.positions_np = self.positions_cpu.numpy()
160:        self.slot_mapping_np = self.slot_mapping_cpu.numpy()
165:        self.query_start_loc_np = self.query_start_loc_cpu.numpy()
170:        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
216:            self.input_batch.block_table_cpu[
268:            self.input_batch.condense(removed_req_indices)
278:        self.input_batch.block_table[:num_reqs].copy_(
279:            self.input_batch.block_table_cpu_tensor[:num_reqs],
280:            non_blocking=True)
283:        # TODO: The Python loop can be slow. Optimize.
317:        # NOTE(woosuk): We use torch.index_select instead of np.take here
318:        # because torch.index_select is much faster than np.take for large
320:        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),
322:                           torch.from_numpy(token_indices),
329:        # NOTE(woosuk): We can't simply use `token_indices // block_size` here







thinking
**Analyzing code modifications**

I’m considering modifications in the target commit which include adding a line in `gpu_model_runner.py`. They might be introducing a `.pin_memory` flag or converting data types for indexing, perhaps fixing `block_numbers` to `int32` for better speed. I'm also thinking they could change the indexing method from numpy to `torch.index_select` to avoid bridging. I’ve noticed they use slicing with numpy indexing, which may lead to tensor conversion, but PyTorch can handle numpy array indexing.
thinking
**Considering tensor indexing**

I’m not completely sure, but indexing a torch tensor with a numpy array might convert it to a torch tensor with a long data type. The code uses `.numpy()` afterward, but indexing before that likely returns a torch tensor which could lead to errors if it isn't on the CPU or contiguous. The current setup may have inefficiencies due to the overhead of converting to Python slicing. They should switch to using `torch.index_select` for better performance rather than relying on direct indexing.
thinking
**Exploring tensor optimization**

They could process the data directly on the torch tensor to avoid converting to numpy, then send it to the CPU. They’re computing `slot_mapping_np` with numpy operations that could be expressed in torch as well, like `block_numbers.astype(np.int32)*block_size + block_offsets`. But I see that they're using a problematic conversion with `block_numbers`. Adding lines for gather or index_select would complicate things. Most changes appear in `gpu_input_batch.py`, where optimizing the loops could improve performance significantly.
thinking
**Streamlining token assignment**

I see an opportunity to optimize by replacing the assignment loop with vectorized operations using broadcasting and masks. For each row `i`, I can fill columns greater than or equal to `num_prompt_tokens[i]` with a padding value. This could involve creating an arange for the columns and comparing it with `self.num_prompt_tokens`, followed by using `np.where`. Instead of looping for assignment, I could allocate a full array with padding and then copy only the necessary tokens using masks. This way, we avoid Python loops altogether!
thinking
**Optimizing tensor operations**

I’m considering ways to improve memory efficiency by reducing unnecessary copying of arrays through slicing and views. For prompt token IDs, I can use vectorization for filling with padding and avoid overhead from Python loops. Instead of copying with numpy, maybe I should use torch operations directly to keep it on the CPU. I’m also looking to refine the masking process to streamline filling, ensuring I manage memory effectively. Implementing these changes should help make the entire process more efficient with minimal overhead.
thinking
**Implementing optimization tests**

I’m working on optimizing memory usage and speed in my tensor operations by using pinned memory for faster H2D copying. The goal is to replace inefficient looping with vectorized operations for filling token IDs. I plan to create a benchmarking script that simulates multiple requests and tests the performance of the `_make_prompt_token_ids_tensor` function. This script will be crucial for establishing a baseline measurement and assessing improvements post-modification, allowing performance comparisons effectively. It sounds like a comprehensive task, but I'm ready to get started!
thinking
**Creating benchmark script**

I'm excited to implement the benchmark script! This script will help me assess the performance of the modifications I plan to make. I'll focus on setting up a simulation of multiple requests to test the `_make_prompt_token_ids_tensor` function specifically. By doing this, I can gather baseline performance data before making changes and then compare it afterward to see if my optimizations have improved the efficiency. I'm looking forward to putting this together!
exec
bash -lc "python -V && mkdir -p .bench_scratch && python - << 'PY'
import time, random
import torch
from vllm.v1.worker.gpu_input_batch import InputBatch, CachedRequestState
from vllm.sampling_params import SamplingParams

# Config
N_REQS = 256
MAX_LEN = 2048
BLOCK_SIZE = 16
VOCAB_SIZE = 32000

ib = InputBatch(
    max_num_reqs=N_REQS,
    max_model_len=MAX_LEN,
    max_num_blocks_per_req=(MAX_LEN + BLOCK_SIZE - 1)//BLOCK_SIZE,
    device=torch.device('cpu'),
    pin_memory=False,
    vocab_size=VOCAB_SIZE,
)

rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(1, 1024)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    # No outputs yet
    out_ids = []
    # Block ids covering prompt
    num_blocks = (prompt_len + BLOCK_SIZE - 1)//BLOCK_SIZE
    block_ids = list(range(num_blocks))
    sp = SamplingParams()
    crs = CachedRequestState(
        req_id=f\"req{i}\",
        prompt_token_ids=prompt_ids,
        prompt=None,
        mm_inputs=[],
        mm_positions=[],
        sampling_params=sp,
        generator=None,
        block_ids=block_ids,
        num_computed_tokens=prompt_len,
        output_token_ids=out_ids,
    )
    ib.add_request(crs, req_index=i)

# Warmup
for _ in range(3):
    t = ib._make_prompt_token_ids_tensor()

# Timed runs
iters = 10
start = time.perf_counter()
for _ in range(iters):
    t = ib._make_prompt_token_ids_tensor()
end = time.perf_counter()
print(f\"_make_prompt_token_ids_tensor avg: {(end-start)/iters*1000:.3f} ms\")
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 exited 1 in 2.73s:
Python 3.11.13
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/engine/arg_utils.py", line 11, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/config.py", line 22, in <module>
    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/model_executor/__init__.py", line 1, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/model_executor/parameter.py", line 7, in <module>
    from vllm.distributed import get_tensor_model_parallel_rank
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/distributed/__init__.py", line 1, in <module>
    from .communication_op import *
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/distributed/communication_op.py", line 6, in <module>
    from .parallel_state import get_tp_group
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/distributed/parallel_state.py", line 38, in <module>


thinking
**Addressing module imports**

I’m figuring out the complexities of importing the vllm module. It’s causing multiple dependency issues and missing modules. For instance, using InputBatch from vllm.v1.worker.gpu_input_batch triggers imports that I might want to avoid. I'll look into importing directly using relative paths instead—this could help side-step the unwanted dependency cascade. If needed, I can adjust sys.path or use importlib.util to load the module without invoking the entire vllm package, aiming for a cleaner import process!
thinking
**Refining function implementation**

I'm focusing on a function that relies on `self.vocab_size`, `self.token_ids_cpu`, and `self.num_prompt_tokens`, with no external dependencies. This is promising! I can define a simple dummy dataclass called `CachedRequestState` within the same module namespace to keep things clean. My plan is to modify the benchmark script to load both `InputBatch` and `CachedRequestState` from the loaded module, which will help me avoid any unwanted vllm imports. I’m ready to write this out again!
exec
bash -lc "python - << 'PY'
import time, random, importlib.util, sys
from pathlib import Path
root = Path('.').resolve()
path = root / 'vllm' / 'v1' / 'worker' / 'gpu_input_batch.py'
spec = importlib.util.spec_from_file_location('gpu_input_batch_mod', str(path))
mod = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = mod
spec.loader.exec_module(mod)
InputBatch = mod.InputBatch
CachedRequestState = mod.CachedRequestState

# Config
N_REQS = 256
MAX_LEN = 2048
BLOCK_SIZE = 16
VOCAB_SIZE = 32000

ib = InputBatch(
    max_num_reqs=N_REQS,
    max_model_len=MAX_LEN,
    max_num_blocks_per_req=(MAX_LEN + BLOCK_SIZE - 1)//BLOCK_SIZE,
    device=__import__('torch').device('cpu'),
    pin_memory=False,
    vocab_size=VOCAB_SIZE,
)

rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(1, 1024)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    out_ids = []
    num_blocks = (prompt_len + BLOCK_SIZE - 1)//BLOCK_SIZE
    block_ids = list(range(num_blocks))
    # Minimal sampling params replacement: we don't use it
    class SP: pass
    sp = SP(); sp.temperature=1.0; sp.sampling_type=type('T',(),{'GREEDY':0})
    sp.top_p=1.0; sp.top_k=0; sp.frequency_penalty=0.0; sp.presence_penalty=0.0; sp.repetition_penalty=1.0; sp.min_tokens=0; sp.all_stop_token_ids=set(); sp.logprobs=None; sp.prompt_logprobs=False
    crs = CachedRequestState(
        req_id=f\"req{i}\",
        prompt_token_ids=prompt_ids,
        prompt=None,
        mm_inputs=[],
        mm_positions=[],
        sampling_params=sp,
        generator=None,
        block_ids=block_ids,
        num_computed_tokens=prompt_len,
        output_token_ids=out_ids,
    )
    ib.add_request(crs, req_index=i)

# Warmup
for _ in range(3):
    t = ib._make_prompt_token_ids_tensor()

# Timed runs
iters = 5
start = time.perf_counter()
for _ in range(iters):
    t = ib._make_prompt_token_ids_tensor()
end = time.perf_counter()
print(f\"Baseline _make_prompt_token_ids_tensor avg: {(end-start)/iters*1000:.3f} ms\")
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 exited 1 in 2.98s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "<stdin>", line 8, in <module>
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/v1/worker/gpu_input_batch.py", line 9, in <module>
    from vllm.multimodal import MultiModalKwargs
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/engine/arg_utils.py", line 11, in <module>
    from vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/config.py", line 22, in <module>
    from vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/model_executor/__init__.py", line 1, in <module>
    from vllm.model_executor.parameter import (BasevLLMParameter,
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/model_executor/parameter.py", line 7, in <module>
    from vllm.distributed import get_tensor_model_parallel_rank
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/distributed/__init__.py", line 1, in <module>
    from .communication_op import *
exec
bash -lc "python - << 'PY'
import types, sys
# Stub required modules for importing gpu_input_batch without full vllm deps
m_mm = types.ModuleType('vllm.multimodal')
class MultiModalKwargs(dict):
    pass
m_mm.MultiModalKwargs = MultiModalKwargs
sys.modules['vllm.multimodal'] = m_mm

m_sp = types.ModuleType('vllm.sampling_params')
class SamplingType:
    GREEDY = object()
    RANDOM_SEED = object()
class SamplingParams:
    def __init__(self):
        self.temperature = 1.0
        self.sampling_type = SamplingType.GREEDY
        self.top_p = 1.0
        self.top_k = 0
        self.frequency_penalty = 0.0
        self.presence_penalty = 0.0
        self.repetition_penalty = 1.0
        self.min_tokens = 0
        self.all_stop_token_ids = set()
        self.logprobs = None
        self.prompt_logprobs = False
m_sp.SamplingType = SamplingType
m_sp.SamplingParams = SamplingParams
sys.modules['vllm.sampling_params'] = m_sp

m_md = types.ModuleType('vllm.v1.sample.metadata')
class SamplingMetadata:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
m_md.SamplingMetadata = SamplingMetadata
sys.modules['vllm.v1.sample.metadata'] = m_md

# Now import the module by path
import importlib.util, time, random
from pathlib import Path
path = Path('vllm/v1/worker/gpu_input_batch.py').resolve()
spec = importlib.util.spec_from_file_location('gpu_input_batch_mod', str(path))
mod = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = mod
spec.loader.exec_module(mod)
InputBatch = mod.InputBatch
CachedRequestState = mod.CachedRequestState
SamplingParams = m_sp.SamplingParams
SamplingType = m_sp.SamplingType

import torch
# Config
N_REQS = 256
MAX_LEN = 2048
BLOCK_SIZE = 16
VOCAB_SIZE = 32000

ib = InputBatch(
    max_num_reqs=N_REQS,
    max_model_len=MAX_LEN,
    max_num_blocks_per_req=(MAX_LEN + BLOCK_SIZE - 1)//BLOCK_SIZE,
    device=torch.device('cpu'),
    pin_memory=False,
    vocab_size=VOCAB_SIZE,
)

rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(1, 1024)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    out_ids = []
    num_blocks = (prompt_len + BLOCK_SIZE - 1)//BLOCK_SIZE
    block_ids = list(range(num_blocks))
    sp = SamplingParams()
    crs = CachedRequestState(
        req_id=f\"req{i}\",
        prompt_token_ids=prompt_ids,
        prompt=None,
        mm_inputs=[],
        mm_positions=[],
        sampling_params=sp,
        generator=None,
        block_ids=block_ids,
        num_computed_tokens=prompt_len,
        output_token_ids=out_ids,
    )
    ib.add_request(crs, req_index=i)

# Warmup
for _ in range(3):
    t = ib._make_prompt_token_ids_tensor()

iters = 5
start = time.perf_counter()
for _ in range(iters):
    t = ib._make_prompt_token_ids_tensor()
end = time.perf_counter()
print(f\"Baseline _make_prompt_token_ids_tensor avg: {(end-start)/iters*1000:.3f} ms\")
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 1.54s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Baseline _make_prompt_token_ids_tensor avg: 1.195 ms

thinking
**Considering advanced indexing**

I'm thinking about whether I should fill the pad and only copy the left tokens using advanced indexing. But honestly, the vectorized mask seems like a decent option too. I’ll have to apply the patch as well, but I'm curious if using the advanced indexing method might yield better results. It feels like there's a bit to explore here, so I want to ensure I'm making the right choice!
file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/v1/worker/gpu_input_batch.py
@@ -387,2 +387,4 @@
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
@@ -393,8 +395,16 @@
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
apply_patch(auto_approved=true) exited 0 in 5ms:
Success. Updated the following files:
M vllm/v1/worker/gpu_input_batch.py
file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)


file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)







thinking
**Creating a benchmarking script**

I need to create a benchmarking script in .bench_scratch to time the _make_prompt_token_ids_tensor function. Using similar stub injection should help avoid full vllm imports. The given test script is blank but outlines the need for timing example usage. I’ll craft test_opt.py using the local import path method and stubs. Even though we've changed files, I can still run the script to measure times and compare improvements to a previous baseline measurement. I might need to implement vectorized and naïve methods for this.
thinking
**Timing functions for performance comparison**

I’m planning to create a script that times both the old and new functions. I’ll replicate the old method inside the script and temporarily override InputBatch._make_prompt_token_ids_tensor with this naive implementation for a baseline comparison. After that, I'll switch back to the vectorized version. This way, I can accurately compare performance. Additionally, I need to make a minor update to gpu_model_runner.py, focusing on optimizing my computation of block_numbers using flattening and numpy indexing.
thinking
**Optimizing tensor indexing**

I’m considering how flatten()[block_table_indices] returns a torch tensor. It looks like using torch indexing with a numpy array may introduce conversion overhead. In the previous implementation, they opted for torch.index_select for speed, but for block_numbers, they used flatten()[block_table_indices].numpy(), which could trigger a copy. I think I can optimize this by switching to torch.index_select to maintain consistency with the earlier approach. I would then access .numpy() to leverage numpy functionality while avoiding those overheads.
thinking
**Implementing performance improvements**

I need to calculate block_offsets using positions_np % self.block_size, and then map these with np.add to get the final values. Currently, block_numbers is a numpy array, and if I keep it as torch, I'll have to convert back to numpy, which isn't ideal. Instead, I can convert the index to a torch CPU tensor with pin_memory. This way, I can keep using torch.index_select for efficiency. It looks like a simple change that’ll reduce overhead with minimal code modification — just a line or two. Let's implement that!
thinking
**Creating and running benchmark tests**

I’ll start by creating the .bench_scratch/test_opt.py script as planned to compute times and show performance improvements. It’s important to handle any environment dependencies that might cause import issues, so I’ll stub the necessary modules like 'vllm.multimodal' and others. We won't import gpu_model_runner to avoid heavy dependencies. I’ll focus on timing InputBatch._make_prompt_token_ids_tensor and possibly make_sampling_metadata.

Once the script is ready, I'll apply the patch to gpu_model_runner.py, ensuring it works with our modifications, and run the script to verify everything.
file update
M /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/v1/worker/gpu_model_runner.py
@@ -335,4 +335,6 @@
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
apply_patch(auto_approved=true) exited 0 in 8ms:
Success. Updated the following files:
M vllm/v1/worker/gpu_model_runner.py
file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

exec
bash -lc "python - << 'PY'
import types, sys
# Stubs for imports used by gpu_input_batch for isolated timing
m_mm = types.ModuleType('vllm.multimodal')
class MultiModalKwargs(dict): pass
m_mm.MultiModalKwargs = MultiModalKwargs
sys.modules['vllm.multimodal'] = m_mm

m_sp = types.ModuleType('vllm.sampling_params')
class SamplingType:
    GREEDY = object()
    RANDOM_SEED = object()
class SamplingParams:
    def __init__(self):
        self.temperature = 1.0
        self.sampling_type = SamplingType.GREEDY
        self.top_p = 1.0
        self.top_k = 0
        self.frequency_penalty = 0.0
        self.presence_penalty = 0.0
        self.repetition_penalty = 1.0
        self.min_tokens = 0
        self.all_stop_token_ids = set()
        self.logprobs = None
        self.prompt_logprobs = False
m_sp.SamplingType = SamplingType
m_sp.SamplingParams = SamplingParams
sys.modules['vllm.sampling_params'] = m_sp

m_md = types.ModuleType('vllm.v1.sample.metadata')
class SamplingMetadata:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
m_md.SamplingMetadata = SamplingMetadata
sys.modules['vllm.v1.sample.metadata'] = m_md

import importlib.util, time, random
from pathlib import Path
p = Path('vllm/v1/worker/gpu_input_batch.py').resolve()
spec = importlib.util.spec_from_file_location('gpu_input_batch_mod', str(p))
mod = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = mod
spec.loader.exec_module(mod)

import torch
InputBatch = mod.InputBatch
CachedRequestState = mod.CachedRequestState
SamplingParams = m_sp.SamplingParams

# Config
N_REQS = 256
MAX_LEN = 2048
BLOCK_SIZE = 16
VOCAB_SIZE = 32000

ib = InputBatch(
    max_num_reqs=N_REQS,
    max_model_len=MAX_LEN,
    max_num_blocks_per_req=(MAX_LEN + BLOCK_SIZE - 1)//BLOCK_SIZE,
    device=torch.device('cpu'),
    pin_memory=False,
    vocab_size=VOCAB_SIZE,
)

rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(1, 1024)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    out_ids = []
    num_blocks = (prompt_len + BLOCK_SIZE - 1)//BLOCK_SIZE
    block_ids = list(range(num_blocks))
    sp = SamplingParams()
    crs = CachedRequestState(
        req_id=f\"req{i}\",
        prompt_token_ids=prompt_ids,
        prompt=None,
        mm_inputs=[],
        mm_positions=[],
        sampling_params=sp,
        generator=None,
        block_ids=block_ids,
        num_computed_tokens=prompt_len,
        output_token_ids=out_ids,
    )
    ib.add_request(crs, req_index=i)

# Warmup
for _ in range(3):
    ib._make_prompt_token_ids_tensor()

iters = 10
import numpy as np
start = time.perf_counter()
for _ in range(iters):
    ib._make_prompt_token_ids_tensor()
end = time.perf_counter()
print(f\"Optimized _make_prompt_token_ids_tensor avg: {(end-start)/iters*1000:.3f} ms\")
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 1.41s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Optimized _make_prompt_token_ids_tensor avg: 0.430 ms
file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

exec
bash -lc "mkdir -p .bench_scratch && cat > .bench_scratch/test_opt.py << 'PY'
import types, sys, time, random
# Stub only minimal imports to run the targeted hotpaths without full install
m_mm = types.ModuleType('vllm.multimodal')
class MultiModalKwargs(dict): pass
m_mm.MultiModalKwargs = MultiModalKwargs
sys.modules['vllm.multimodal'] = m_mm

m_sp = types.ModuleType('vllm.sampling_params')
class SamplingType:
    GREEDY = object()
    RANDOM_SEED = object()
class SamplingParams:
    def __init__(self):
        self.temperature = 1.0
        self.sampling_type = SamplingType.GREEDY
        self.top_p = 1.0
        self.top_k = 0
        self.frequency_penalty = 0.0
        self.presence_penalty = 0.0
        self.repetition_penalty = 1.0
        self.min_tokens = 0
        self.all_stop_token_ids = set()
        self.logprobs = None
        self.prompt_logprobs = False
m_sp.SamplingType = SamplingType
m_sp.SamplingParams = SamplingParams
sys.modules['vllm.sampling_params'] = m_sp

m_md = types.ModuleType('vllm.v1.sample.metadata')
class SamplingMetadata:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
m_md.SamplingMetadata = SamplingMetadata
sys.modules['vllm.v1.sample.metadata'] = m_md

import importlib.util
from pathlib import Path
p = Path('vllm/v1/worker/gpu_input_batch.py').resolve()
spec = importlib.util.spec_from_file_location('gpu_input_batch_mod', str(p))
mod = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = mod
spec.loader.exec_module(mod)
InputBatch = mod.InputBatch
CachedRequestState = mod.CachedRequestState
from types import SimpleNamespace

import numpy as np
import torch

# Construct a synthetic scheduler_output-like object for _prepare_inputs path
p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
mod2 = importlib.util.module_from_spec(spec2)
sys.modules[spec2.name] = mod2
spec2.loader.exec_module(mod2)

# Build an InputBatch with synthetic requests
N_REQS = 128
MAX_LEN = 1024
BLOCK_SIZE = 16
VOCAB_SIZE = 32000
ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
                device=torch.device('cpu'), pin_memory=False, vocab_size=VOCAB_SIZE)
rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(200, 800)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    block_ids = list(range((prompt_len+BLOCK_SIZE-1)//BLOCK_SIZE))
    sp = SamplingParams()
    crs = CachedRequestState(req_id=f'req{i}', prompt_token_ids=prompt_ids,
        prompt=None, mm_inputs=[], mm_positions=[], sampling_params=sp,
        generator=None, block_ids=block_ids, num_computed_tokens=prompt_len,
        output_token_ids=[])
    ib.add_request(crs, req_index=i)

# Fake GPUModelRunner bits needed for _prepare_inputs
class FakeSched:
    def __init__(self, ib):
        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
        self.num_common_prefix_blocks = 0

# Instantiate runner with minimal fields to hit numpy/torch paths only
class DummyCfg:
    def __init__(self):
        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
        self.model_config = SimpleNamespace(is_multimodal_model=False,
            get_sliding_window=lambda: None,
            max_model_len=MAX_LEN,
            get_vocab_size=lambda: VOCAB_SIZE,
            get_num_layers_by_block_type=lambda *a, **k: 1,
            get_num_attention_heads=lambda *a, **k: 32,
            get_num_kv_heads=lambda *a, **k: 32,
            get_head_size=lambda *a, **k: 128,
            get_hidden_size=lambda *a, **k: 4096,
            enforce_eager=False)
        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
            max_num_seqs=N_REQS,
            max_num_encoder_input_tokens=0,
            encoder_cache_size=0)
        self.lora_config = self.load_config = self.parallel_config = \\
            self.speculative_config = self.prompt_adapter_config = \\
            self.observability_config = SimpleNamespace()

cfg = DummyCfg()

gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
# Swap in our prepared input_batch to avoid full init data mismatch
gr.input_batch = ib

# Baseline prompt tensor timing
for _ in range(3):
    ib._make_prompt_token_ids_tensor()
start = time.perf_counter()
for _ in range(20):
    ib._make_prompt_token_ids_tensor()
print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)

# Exercise _prepare_inputs for indexing hot path
sched = FakeSched(ib)
# Warm
for _ in range(2):
    gr._prepare_inputs(sched)
start = time.perf_counter()
for _ in range(10):
    gr._prepare_inputs(sched)
print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
PY" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 8ms:

file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

exec
bash -lc 'python .bench_scratch/test_opt.py' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 exited 1 in 1.35s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
Traceback (most recent call last):
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/.bench_scratch/test_opt.py", line 55, in <module>
    spec2.loader.exec_module(mod2)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/vllm/v1/worker/gpu_model_runner.py", line 10, in <module>
    from vllm.config import CompilationLevel, VllmConfig
ModuleNotFoundError: No module named 'vllm.config'
file update:
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

file update
A /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/.bench_scratch/test_opt.py
import types, sys, time, random
# Stub only minimal imports to run the targeted hotpaths without full install
m_mm = types.ModuleType('vllm.multimodal')
class MultiModalKwargs(dict): pass
m_mm.MultiModalKwargs = MultiModalKwargs
sys.modules['vllm.multimodal'] = m_mm

m_sp = types.ModuleType('vllm.sampling_params')
class SamplingType:
    GREEDY = object()
    RANDOM_SEED = object()
class SamplingParams:
    def __init__(self):
        self.temperature = 1.0
        self.sampling_type = SamplingType.GREEDY
        self.top_p = 1.0
        self.top_k = 0
        self.frequency_penalty = 0.0
        self.presence_penalty = 0.0
        self.repetition_penalty = 1.0
        self.min_tokens = 0
        self.all_stop_token_ids = set()
        self.logprobs = None
        self.prompt_logprobs = False
m_sp.SamplingType = SamplingType
m_sp.SamplingParams = SamplingParams
sys.modules['vllm.sampling_params'] = m_sp

m_md = types.ModuleType('vllm.v1.sample.metadata')
class SamplingMetadata:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
m_md.SamplingMetadata = SamplingMetadata
sys.modules['vllm.v1.sample.metadata'] = m_md

import importlib.util
from pathlib import Path
p = Path('vllm/v1/worker/gpu_input_batch.py').resolve()
spec = importlib.util.spec_from_file_location('gpu_input_batch_mod', str(p))
mod = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = mod
spec.loader.exec_module(mod)
InputBatch = mod.InputBatch
CachedRequestState = mod.CachedRequestState

import numpy as np
import torch

# Build an InputBatch with synthetic requests
N_REQS = 256
MAX_LEN = 2048
BLOCK_SIZE = 16
VOCAB_SIZE = 32000
ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
                device=torch.device('cpu'), pin_memory=False, vocab_size=VOCAB_SIZE)
rand = random.Random(0)
for i in range(N_REQS):
    prompt_len = rand.randint(200, 800)
    prompt_ids = [rand.randint(0, VOCAB_SIZE-1) for _ in range(prompt_len)]
    block_ids = list(range((prompt_len+BLOCK_SIZE-1)//BLOCK_SIZE))
    sp = SamplingParams()
    crs = CachedRequestState(req_id=f'req{i}', prompt_token_ids=prompt_ids,
        prompt=None, mm_inputs=[], mm_positions=[], sampling_params=sp,
        generator=None, block_ids=block_ids, num_computed_tokens=prompt_len,
        output_token_ids=[])
    ib.add_request(crs, req_index=i)

# Baseline/optimized prompt tensor timing
for _ in range(3):
    ib._make_prompt_token_ids_tensor()
start = time.perf_counter()
for _ in range(50):
    ib._make_prompt_token_ids_tensor()
print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)

print('OK')

apply_patch(auto_approved=true) exited 0 in 9ms:
Success. Updated the following files:
A .bench_scratch/test_opt.py
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

exec
bash -lc 'python .bench_scratch/test_opt.py' in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 1.47s:
/opt/conda/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
prompt_tensor_ms 0.17810903955250978
OK
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

Plan update
  ✓ Explore repo and target files
  ✓ Create timing script under .bench_scratch
  ✓ Profile bottlenecks in target paths
  ✓ Optimize cpu prompt padding and block fetch
  ✓ Re-run timing to confirm improvement
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

exec
bash -lc "git add -A && git reset -q .bench_scratch || true && git commit -m 'Optimize MoE align sum kernels performance' && git diff "'$(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . '"':(exclude).bench_scratch' > /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/model_patch.diff || true ; echo 'DONE' && echo 'Model patch at: /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/model_patch.diff'" in /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014 succeeded in 141ms:
[agent/vllm_core/b55ed6ef b02f8a0] Optimize MoE align sum kernels performance
 2 files changed, 18 insertions(+), 6 deletions(-)
fatal: Not a valid object name origin/HEAD
DONE
Model patch at: /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/model_patch.diff
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

codex
Here’s what I changed to speed up the hot paths and validated with a local timing script.

What I did
- Optimized CPU prompt padding in `vllm/v1/worker/gpu_input_batch.py: _make_prompt_token_ids_tensor`:
  - Replaced the per-request Python loop padding with a fully vectorized NumPy mask.
  - Kept bulk copy of token IDs and pinned CPU tensor; just eliminated the slow loop.
- Optimized block index fetch in `vllm/v1/worker/gpu_model_runner.py`:
  - Switched advanced NumPy indexing on a flattened torch tensor to `torch.index_select` and then `.numpy()`, which is faster for large arrays.

Reproduction script
- Added `.bench_scratch/test_opt.py` to measure the prompt tensor preparation path without installing full dependencies by stubbing minimal imports.
- Baseline vs optimized measurement (representative run on this machine):
  - `_make_prompt_token_ids_tensor` dropped from ~1.195 ms to ~0.430 ms in isolated timing, and ~0.178 ms for a 50-iteration run within the scratch script.

Why this helps
- The previous padding loop incurred Python overhead proportional to number of requests; vectorization applies pad in a single pass and avoids per-row iteration.
- Using `torch.index_select` avoids slower NumPy advanced indexing on large flattened tensors and keeps the operation in optimized PyTorch kernels before converting to NumPy.

Files changed
- vllm/v1/worker/gpu_input_batch.py:366
- vllm/v1/worker/gpu_model_runner.py:336

Next steps
- If you want, I can run broader profiling across the rest of the batching path or validate on GPU with a small mocked scheduler to measure end-to-end gains.
- Ready to extend similar vectorization to other CPU-side prep in `_prepare_inputs` if needed.

Patch saved
- Model patch at: /root/OmniPerf-Bench/perf-agents-bench/.work/worktrees/vllm_core/vllm_gpt5_v5-0014/model_patch.diff
file update:
diff --git a/.bench_scratch/test_opt.py b/.bench_scratch/test_opt.py
index 71701784f46325401a1af0c11da8c69c700ba9fc..2ffe504c8b096a6a938512c052c802ff821df38a
--- a/.bench_scratch/test_opt.py
+++ b/.bench_scratch/test_opt.py
@@ -42,21 +42,13 @@
 spec.loader.exec_module(mod)
 InputBatch = mod.InputBatch
 CachedRequestState = mod.CachedRequestState
-from types import SimpleNamespace

 import numpy as np
 import torch

-# Construct a synthetic scheduler_output-like object for _prepare_inputs path
-p2 = Path('vllm/v1/worker/gpu_model_runner.py').resolve()
-spec2 = importlib.util.spec_from_file_location('gpu_model_runner_mod', str(p2))
-mod2 = importlib.util.module_from_spec(spec2)
-sys.modules[spec2.name] = mod2
-spec2.loader.exec_module(mod2)
-
 # Build an InputBatch with synthetic requests
-N_REQS = 128
-MAX_LEN = 1024
+N_REQS = 256
+MAX_LEN = 2048
 BLOCK_SIZE = 16
 VOCAB_SIZE = 32000
 ib = InputBatch(N_REQS, MAX_LEN, (MAX_LEN+BLOCK_SIZE-1)//BLOCK_SIZE,
@@ -73,56 +65,13 @@
         output_token_ids=[])
     ib.add_request(crs, req_index=i)

-# Fake GPUModelRunner bits needed for _prepare_inputs
-class FakeSched:
-    def __init__(self, ib):
-        self.total_num_scheduled_tokens = sum(1 for _ in range(ib.num_reqs))
-        self.num_scheduled_tokens = {rid:1 for rid in ib.req_ids[:ib.num_reqs]}
-        self.num_common_prefix_blocks = 0
-
-# Instantiate runner with minimal fields to hit numpy/torch paths only
-class DummyCfg:
-    def __init__(self):
-        self.compilation_config = SimpleNamespace(level=0, capture_sizes=[])
-        self.model_config = SimpleNamespace(is_multimodal_model=False,
-            get_sliding_window=lambda: None,
-            max_model_len=MAX_LEN,
-            get_vocab_size=lambda: VOCAB_SIZE,
-            get_num_layers_by_block_type=lambda *a, **k: 1,
-            get_num_attention_heads=lambda *a, **k: 32,
-            get_num_kv_heads=lambda *a, **k: 32,
-            get_head_size=lambda *a, **k: 128,
-            get_hidden_size=lambda *a, **k: 4096,
-            enforce_eager=False)
-        self.cache_config = SimpleNamespace(block_size=BLOCK_SIZE, cache_dtype='auto')
-        self.scheduler_config = SimpleNamespace(max_num_batched_tokens=N_REQS,
-            max_num_seqs=N_REQS,
-            max_num_encoder_input_tokens=0,
-            encoder_cache_size=0)
-        self.lora_config = self.load_config = self.parallel_config = \
-            self.speculative_config = self.prompt_adapter_config = \
-            self.observability_config = SimpleNamespace()
-
-cfg = DummyCfg()
-
-gr = mod2.GPUModelRunner(cfg, torch.device('cpu'))
-# Swap in our prepared input_batch to avoid full init data mismatch
-gr.input_batch = ib
-
-# Baseline prompt tensor timing
+# Baseline/optimized prompt tensor timing
 for _ in range(3):
     ib._make_prompt_token_ids_tensor()
 start = time.perf_counter()
-for _ in range(20):
+for _ in range(50):
     ib._make_prompt_token_ids_tensor()
-print('prompt_tensor_ms', (time.perf_counter()-start)/20*1000)
+print('prompt_tensor_ms', (time.perf_counter()-start)/50*1000)
+
+print('OK')

-# Exercise _prepare_inputs for indexing hot path
-sched = FakeSched(ib)
-# Warm
-for _ in range(2):
-    gr._prepare_inputs(sched)
-start = time.perf_counter()
-for _ in range(10):
-    gr._prepare_inputs(sched)
-print('prepare_inputs_ms', (time.perf_counter()-start)/10*1000)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300fe06b4915c22705fa45b2f7e9cbd063..f544700a57cef8e968a878892da7774ec055e2e9
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -385,18 +385,28 @@

     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:
         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()
+        # Allocate once on CPU (pinned if available) and use vectorized ops
+        # to avoid per-request Python loops.
         prompt_token_ids_cpu_tensor = torch.empty(
             (self.num_reqs, max_prompt_len),
             device="cpu",
             dtype=torch.int64,
             pin_memory=self.pin_memory)
         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()
+
+        # Copy token ids for all requests at once.
         prompt_token_ids[:] = (
             self.token_ids_cpu[:self.num_reqs, :max_prompt_len])
-        # Use the value of vocab_size as a pad since we don't have a
-        # token_id of this value.
-        for i in range(self.num_reqs):
-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size
+
+        # Pad trailing positions in a single vectorized step instead of a loop.
+        # Use vocab_size as the padding token since it is guaranteed to be an
+        # out-of-vocabulary id.
+        if max_prompt_len > 0 and self.num_reqs > 0:
+            cols = np.arange(max_prompt_len, dtype=np.int32)
+            lens = self.num_prompt_tokens[:self.num_reqs]
+            mask = cols >= lens[:, None]
+            prompt_token_ids[mask] = self.vocab_size
+
         return prompt_token_ids_cpu_tensor.to(device=self.device,
                                               non_blocking=True)

diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8e0a0bc8bc2dc75b3db977385fc79c2c..784c7fe3db3c480cc0786ce1901e5e3b5678d841
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -333,8 +333,10 @@
         # NOTE(woosuk): We use torch.index_select instead of np.take here
         # because torch.index_select is much faster than np.take for large
         # tensors.
-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()
-                         [block_table_indices].numpy())
+        # Use torch.index_select for better performance vs. advanced indexing.
+        block_numbers = torch.index_select(
+            self.input_batch.block_table_cpu_tensor.flatten(), 0,
+            torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,

tokens used
200,306