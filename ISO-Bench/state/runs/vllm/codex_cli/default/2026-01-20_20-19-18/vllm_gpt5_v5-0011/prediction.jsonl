{"instance_id": "/root/OmniPerf-Bench/vllm__repo-a4d577b", "model_patch": "diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex dfe7102..f0eb00d 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -10,15 +10,20 @@ def get_token_bin_counts_and_mask(\n     vocab_size: int,\n     num_seqs: int,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    # Compute the bin counts for the tokens.\n-    # vocab_size + 1 for padding.\n+    \"\"\"Compute per-sequence token frequency and presence mask.\n+\n+    Uses a single scatter_add pass to build counts, then derives the mask.\n+    The input may include a padding sentinel equal to ``vocab_size``; we\n+    construct the buffer with ``vocab_size + 1`` to safely index it and then\n+    slice away the sentinel column.\n+    \"\"\"\n     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n+    one_src = torch.ones_like(tokens, dtype=bin_counts.dtype)\n+    bin_counts.scatter_add_(1, tokens, one_src)\n     bin_counts = bin_counts[:, :vocab_size]\n-    mask = bin_counts > 0\n-\n+    mask = bin_counts.ne(0)\n     return bin_counts, mask\n \n \n@@ -41,18 +46,39 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n     repetition_penalties: The repetition penalties of shape (num_seqs, )\n     \"\"\"\n     num_seqs, vocab_size = logits.shape\n-    _, prompt_mask = get_token_bin_counts_and_mask(prompt_tokens_tensor,\n-                                                   vocab_size, num_seqs)\n+    # Prompt presence: only need a mask, not counts.\n+    # Build boolean mask with a lightweight scatter.\n+    prompt_presence_full = torch.zeros((num_seqs, vocab_size + 1),\n+                                       dtype=torch.bool,\n+                                       device=logits.device)\n+    prompt_presence_full.scatter_(1,\n+                                  prompt_tokens_tensor,\n+                                  torch.ones_like(prompt_tokens_tensor,\n+                                                  dtype=torch.bool))\n+    prompt_mask = prompt_presence_full[:, :vocab_size]\n+\n+    # Output: need both counts and mask for frequency/presence penalties.\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze_(dim=1).repeat(\n-        1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n-    # We follow the definition in OpenAI API.\n-    # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n-    logits -= frequency_penalties.unsqueeze_(dim=1) * output_bin_counts\n-    logits -= presence_penalties.unsqueeze_(dim=1) * output_mask\n+\n+    # Repetition penalty: apply per sequence without materializing a repeated\n+    # (B, V) tensor. For tokens seen in either prompt or output, scale logits\n+    # by penalty depending on their sign per OpenAI API behavior.\n+    rep = repetition_penalties.unsqueeze(1)  # (B, 1)\n+    seen_mask = prompt_mask | output_mask\n+    if seen_mask.any():\n+        # For positive logits -> divide by rep; non-positive -> multiply by rep\n+        pos_mask = logits > 0\n+        # Only modify where seen_mask is true.\n+        # Compute scales lazily with broadcasting to avoid large repeats.\n+        scale_pos = (1.0 / rep)\n+        scale_neg = rep\n+        logits = torch.where(seen_mask & pos_mask, logits * scale_pos,\n+                             logits)\n+        logits = torch.where(seen_mask & (~pos_mask), logits * scale_neg,\n+                             logits)\n+\n+    # Frequency and presence penalties as in OpenAI API.\n+    logits -= frequency_penalties.unsqueeze(1) * output_bin_counts\n+    logits -= presence_penalties.unsqueeze(1) * output_mask\n     return logits\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 2743100..bfe0379 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -97,6 +97,19 @@ def apply_top_k_top_p(\n     \"\"\"\n     if no_top_k and no_top_p:\n         return logits\n+    # Fast path for top-k only using topk threshold without full sort.\n+    if not no_top_k and no_top_p:\n+        V = logits.size(1)\n+        k_clamped = torch.clamp(k.to(torch.long), 1, V)\n+        # Compute top-k values per row; threshold is the smallest among them.\n+        topk_vals, _ = torch.topk(logits, k_clamped.max().item(), dim=-1)\n+        # For rows with k < k_max, the kth value is at position k-1.\n+        # Build per-row thresholds by gathering from topk_vals.\n+        kth_indices = (k_clamped - 1).clamp(min=0)\n+        thresh = topk_vals.gather(1, kth_indices.unsqueeze(1)).squeeze(1)\n+        mask = logits < thresh.unsqueeze(1)\n+        return logits.masked_fill(mask, -float(\"inf\"))\n+    # General path falls back to sort for combined top-k and top-p\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n     if not no_top_k:\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex ec6374d..9624b5c 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -200,18 +200,17 @@ class Sampler(nn.Module):\n         \"\"\"\n         Filters logits using adaptive probability thresholding.\n         \"\"\"\n-        # Convert logits to probability distribution\n-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n-        # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Reshape min_p for broadcasting\n-        adjusted_min_p = min_p.unsqueeze(1) * max_probabilities\n-        # Identify valid tokens using threshold comparison\n-        valid_token_mask = probability_values >= adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[~valid_token_mask] = -float('inf')\n+        # Avoid softmax; use inequality p_i >= min_p * p_max.\n+        # Since p_i / p_max = exp(l_i - l_max), this is equivalent to\n+        # l_i - l_max >= log(min_p) when 0 < min_p <= 1.\n+        if torch.all(min_p <= 0):\n+            return logits\n+        l_max, _ = logits.max(dim=-1, keepdim=True)\n+        # Clamp min_p to (0,1] to avoid log(0) and undefined values.\n+        safe_min_p = torch.clamp(min_p, min=1e-9, max=1.0).unsqueeze(1)\n+        thresh = l_max + torch.log(safe_min_p)\n+        valid_mask = logits >= thresh\n+        logits.masked_fill_(~valid_mask, -float(\"inf\"))\n         return logits\n \n     def apply_logits_bias(\n@@ -219,11 +218,20 @@ class Sampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> torch.Tensor:\n-        # TODO(houseroad): this implementation is extremely inefficient.\n-        # One idea is implement this as a PyTorch C++ op, and we may\n-        # even optimize the logit_bias layout.\n-        for i, logit_bias in enumerate(sampling_metadata.logit_bias):\n-            if logit_bias:\n-                for token_id, bias in logit_bias.items():\n-                    logits[i, token_id] += bias\n+        # Vectorized application of per-request logit bias using index_put_.\n+        rows = []\n+        cols = []\n+        vals = []\n+        for i, bias_map in enumerate(sampling_metadata.logit_bias):\n+            if not bias_map:\n+                continue\n+            for token_id, bias in bias_map.items():\n+                rows.append(i)\n+                cols.append(int(token_id))\n+                vals.append(float(bias))\n+        if rows:\n+            index = (torch.tensor(rows, device=logits.device, dtype=torch.long),\n+                     torch.tensor(cols, device=logits.device, dtype=torch.long))\n+            values = torch.tensor(vals, device=logits.device, dtype=logits.dtype)\n+            logits.index_put_(index, values, accumulate=True)\n         return logits\ndiff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex cb7411a..c699e91 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -484,12 +484,14 @@ class InputBatch:\n             pin_memory=self.pin_memory,\n         )\n         prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()\n-        prompt_token_ids[:] = self.token_ids_cpu[:self.\n-                                                 num_reqs, :max_prompt_len]\n-        # Use the value of vocab_size as a pad since we don't have a\n-        # token_id of this value.\n-        for i in range(self.num_reqs):\n-            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size\n+        # Copy the leading tokens from the flat token buffer.\n+        prompt_token_ids[:] = self.token_ids_cpu[:self.num_reqs, :max_prompt_len]\n+        # Vectorized padding: set tokens beyond prompt length to vocab_size.\n+        if self.num_reqs > 0 and max_prompt_len > 0:\n+            lengths = self.num_prompt_tokens[:self.num_reqs]\n+            col_idx = np.arange(max_prompt_len, dtype=np.int32)\n+            mask = col_idx >= lengths[:, None]\n+            prompt_token_ids[mask] = self.vocab_size\n         return prompt_token_ids_cpu_tensor.to(device=self.device,\n                                               non_blocking=True)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
