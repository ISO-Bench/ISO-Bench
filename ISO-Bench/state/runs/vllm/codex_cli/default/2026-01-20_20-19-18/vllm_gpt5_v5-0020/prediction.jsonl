{"instance_id": "/root/OmniPerf-Bench/vllm__repo-a732900", "model_patch": "diff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py\nindex fda4d00..12b01f3 100644\n--- a/vllm/distributed/device_communicators/pynccl.py\n+++ b/vllm/distributed/device_communicators/pynccl.py\n@@ -1,4 +1,6 @@\n from typing import Optional, Union\n+import ctypes\n+import numpy as np\n \n # ===================== import region =====================\n import torch\n@@ -73,13 +75,21 @@ class PyNcclCommunicator:\n             self.unique_id = ncclUniqueId()\n \n         if not isinstance(group, StatelessProcessGroup):\n-            tensor = torch.ByteTensor(list(self.unique_id.internal))\n+            # Broadcast the ncclUniqueId bytes efficiently without\n+            # intermediate Python list materialization.\n+            uid_size = ctypes.sizeof(ncclUniqueId)\n+            tensor = torch.empty(uid_size, dtype=torch.uint8)\n             ranks = dist.get_process_group_ranks(group)\n+            if self.rank == 0:\n+                src_bytes = bytes(self.unique_id.internal)\n+                tensor.copy_(torch.from_numpy(np.frombuffer(src_bytes,\n+                                                           dtype=np.uint8)))\n             # arg `src` in `broadcast` is the global rank\n             dist.broadcast(tensor, src=ranks[0], group=group)\n-            byte_list = tensor.tolist()\n-            for i, byte in enumerate(byte_list):\n-                self.unique_id.internal[i] = byte\n+            # Copy the received bytes back into the ctypes structure.\n+            np_arr = tensor.numpy()\n+            ctypes.memmove(ctypes.addressof(self.unique_id), np_arr.ctypes.data,\n+                           uid_size)\n         else:\n             self.unique_id = group.broadcast_obj(self.unique_id, src=0)\n         if isinstance(device, int):\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex a92b77e..c1b4717 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -792,8 +792,61 @@ def async_tensor_h2d(\n     return t.to(device=target_device, non_blocking=True)\n \n \n+def device_arange(\n+    n: int,\n+    dtype: torch.dtype,\n+    target_device: Union[str, torch.device],\n+    *,\n+    pin_memory: bool = False,\n+) -> torch.Tensor:\n+    \"\"\"Create a 1-D tensor [0, 1, ..., n-1] directly on the target device.\n+\n+    This is a fast-path alternative to constructing ``list(range(n))`` and\n+    passing it through ``async_tensor_h2d``. It avoids intermediate Python\n+    list materialization and host-to-device copies by generating the tensor\n+    directly on the destination device when possible.\n+\n+    - If ``target_device`` is a CPU device, this returns a CPU tensor and\n+      optionally pins it when ``pin_memory`` is True.\n+    - If ``target_device`` is a CUDA/XPU device, this creates the tensor on\n+      that device without an intermediate host copy.\n+    \"\"\"\n+    # Construct directly on the destination device to avoid H2D copies.\n+    t = torch.arange(n, dtype=dtype, device=target_device)\n+    # Preserve semantics for callers expecting a pinned CPU tensor.\n+    if pin_memory and (isinstance(target_device, str)\n+                       and target_device.startswith(\"cpu\")):\n+        t = t.pin_memory()\n+    if pin_memory and isinstance(target_device, torch.device) and \\\n+            target_device.type == \"cpu\":\n+        t = t.pin_memory()\n+    return t\n+\n+\n+_DTYPE_SIZE_CACHE = {\n+    torch.float16: 2,\n+    torch.bfloat16: 2,\n+    torch.float32: 4,\n+    torch.float64: 8,\n+    torch.int8: 1,\n+    torch.uint8: 1,\n+    torch.int16: 2,\n+    torch.int32: 4,\n+    torch.int64: 8,\n+    torch.bool: 1,\n+}\n+\n+\n def get_dtype_size(dtype: torch.dtype) -> int:\n-    \"\"\"Get the size of the data type in bytes.\"\"\"\n+    \"\"\"Get the size of the data type in bytes.\n+\n+    Uses a small cached lookup for common dtypes to avoid constructing\n+    temporary tensors on hot paths. Falls back to ``element_size()`` for\n+    uncommon dtypes.\n+    \"\"\"\n+    size = _DTYPE_SIZE_CACHE.get(dtype)\n+    if size is not None:\n+        return size\n     return torch.tensor([], dtype=dtype).element_size()\n \n \n@@ -1938,7 +1991,13 @@ def _check_multiproc_method():\n         os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n \n \n+@lru_cache(maxsize=1)\n def get_mp_context():\n+    \"\"\"Return the multiprocessing context using configured start method.\n+\n+    Cached to avoid repeating environment checks and context creation on\n+    hot paths where this helper may be queried frequently.\n+    \"\"\"\n     _check_multiproc_method()\n     mp_method = envs.VLLM_WORKER_MULTIPROC_METHOD\n     return multiprocessing.get_context(mp_method)\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex a2c2ceb..733fc1e 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -14,7 +14,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n-from vllm.utils import PyObjectCache, async_tensor_h2d\n+from vllm.utils import PyObjectCache, async_tensor_h2d, device_arange\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -249,11 +249,13 @@ class StatefulModelInput(BroadcastableModelInput):\n \n         assert self.frozen_model_input is not None\n         assert self.frozen_model_input.sampling_metadata is not None\n-        self.frozen_model_input.sampling_metadata.selected_token_indices =  \\\n-            async_tensor_h2d(list(range(self.num_queries)),\n-                             dtype=torch.long,\n-                             target_device=device,\n-                             pin_memory=pin_memory)\n+        # Create indices directly on target device to avoid Python list\n+        # materialization and H2D copies.\n+        self.frozen_model_input.sampling_metadata.selected_token_indices = \\\n+            device_arange(self.num_queries,\n+                          dtype=torch.long,\n+                          target_device=device,\n+                          pin_memory=pin_memory)\n \n     def maybe_advance_frozen_model_input(self, device: str, pin_memory: bool):\n         \"\"\"\n", "model_name_or_path": "gpt-5-2025-08-07"}
