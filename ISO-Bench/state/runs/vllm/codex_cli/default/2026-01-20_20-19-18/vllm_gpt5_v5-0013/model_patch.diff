diff --git a/requirements-common.txt b/requirements-common.txt
index b7c94cb..981c9cc 100644
--- a/requirements-common.txt
+++ b/requirements-common.txt
@@ -1,6 +1,7 @@
 psutil
 sentencepiece  # Required for LLaMA tokenizer.
 numpy < 2.0.0
+orjson  # fast JSON handling for telemetry/logging paths
 requests >= 2.26.0
 tqdm
 blake3
diff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py
index 9b116e0..a8e89bf 100644
--- a/vllm/v1/spec_decode/ngram_proposer.py
+++ b/vllm/v1/spec_decode/ngram_proposer.py
@@ -41,9 +41,15 @@ class NgramProposer:
               followed that pattern. Here we will return [4,2,3] because 
               we only have three tokens after the match.
         """
-        # TODO: Use c++ to implement the _find_subarray_kmp to
-        # improve the efficiency
-        return self._find_subarray_kmp(context_token_ids, n, k)
+        # Fast path: use a bytes-level search on contiguous int32 views.
+        # This leverages CPython's optimized substring search in C
+        # and avoids Python-level per-element loops.
+        try:
+            result = self._find_subarray_bytes(context_token_ids, n, k)
+            return result
+        except Exception:
+            # Fallback to KMP on any unexpected edge conditions.
+            return self._find_subarray_kmp(context_token_ids, n, k)
 
     @staticmethod
     def _kmp_lps_array(pattern: List[int]) -> List[int]:
@@ -104,3 +110,46 @@ class NgramProposer:
 
         # Y not found
         return None
+
+    @staticmethod
+    def _find_subarray_bytes(
+        context_token_ids: np.ndarray,
+        n: int,
+        k: int,
+    ) -> Optional[np.ndarray]:
+        """Find the next k tokens using a bytes-level search.
+
+        Converts tokens to a contiguous int32 view and searches for the last
+        n-token pattern within the previous context prefix using CPython's
+        highly optimized substring search. This avoids Python loops while
+        keeping memory overhead modest.
+        """
+        context_len = int(context_token_ids.shape[0])
+        assert n > 0
+        if context_len <= n:
+            return None
+
+        # Ensure contiguous int32 views for zero-copy bytes export on slices.
+        # We operate on slices to avoid copying the whole array.
+        # Note: Using np.asarray(..., dtype=np.int32) may copy only when
+        # needed and keeps overhead low for typical int32 inputs.
+        prefix_arr32 = np.asarray(
+            np.ascontiguousarray(context_token_ids[:context_len - n]),
+            dtype=np.int32,
+        )
+        pattern_arr32 = np.asarray(
+            np.ascontiguousarray(context_token_ids[-n:]),
+            dtype=np.int32,
+        )
+
+        # Convert to bytes and search within the prefix only to avoid matching
+        # the trailing pattern itself. Each int32 contributes 4 bytes.
+        haystack = prefix_arr32.tobytes(order="C")
+        needle = pattern_arr32.tobytes(order="C")
+        pos_bytes = haystack.find(needle)
+        if pos_bytes < 0:
+            return None
+
+        start_idx = pos_bytes // 4  # byte offset -> token index
+        # Return from the original array to preserve dtype and views.
+        return context_token_ids[start_idx + n:start_idx + n + k]
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 0ecc00a..63c4aad 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -492,7 +492,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # because torch.index_select is much faster than np.take for large
         # tensors.
         block_table_cpu = self.input_batch.block_table.get_cpu_tensor()
-        block_numbers = block_table_cpu.flatten()[block_table_indices].numpy()
+        # Use torch.index_select on a flat view to avoid NumPy advanced
+        # indexing overhead and unnecessary intermediate allocations.
+        block_table_flat = block_table_cpu.view(-1)
+        block_numbers = torch.index_select(
+            block_table_flat, 0, torch.from_numpy(block_table_indices)).numpy()
         block_offsets = positions_np % self.block_size
         np.add(block_numbers * self.block_size,
                block_offsets,
