{"instance_id": "/root/OmniPerf-Bench/vllm__repo-cc466a3", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 1f19d20..6aae521 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -153,11 +153,13 @@ def _apply_min_tokens_penalty(\n     logits: torch.Tensor,\n     sampling_metadata: SamplingMetadata,\n ) -> torch.Tensor:\n-    \"\"\"Apply min_tokens penalty which sets stop tokens to -inf if min_tokens\n-        have not been generated yet\n+    \"\"\"Apply ``min_tokens`` penalty by setting stop tokens to ``-inf``.\n+\n+    If a sequence has generated fewer than ``min_tokens`` tokens, we mask out\n+    all stop tokens for that sequence to prevent early stopping. This\n+    implementation avoids building large Python lists of index pairs and uses\n+    vectorized advanced indexing on the device for better performance.\n     \"\"\"\n-    # list of indices in logits that will be set to -inf\n-    logits_to_penalize: List[Tuple[int, int]] = []\n     logits_applied = 0\n     for seq_group in sampling_metadata.seq_groups:\n         seq_ids = seq_group.seq_ids\n@@ -172,24 +174,28 @@ def _apply_min_tokens_penalty(\n         start_idx = sample_indices[0]\n         min_tokens = sampling_params.min_tokens\n         token_ids_to_penalize = sampling_params.all_stop_token_ids\n-        if min_tokens > 0 and token_ids_to_penalize:\n-            seqs_to_penalize = []\n-            for j, seq_id in enumerate(seq_ids):\n-                seq_data = seq_group.seq_data[seq_id]\n-                if len(seq_data.output_token_ids) < min_tokens:\n-                    seqs_to_penalize.append(j)\n-\n-            if seqs_to_penalize:\n-                # convert to the index into logits\n-                seqs_to_penalize = [start_idx + j for j in seqs_to_penalize]\n-                # itertools.product pairs each seq index with every token id\n-                logits_to_penalize.extend(\n-                    itertools.product(seqs_to_penalize, token_ids_to_penalize))\n-\n-    if logits_to_penalize:\n-        # use zip and * to group indices along each dimension\n-        # eg. [ (1,2), (1,3), (5,6) ] -> ( (1,1,5), (2,3,6) )\n-        logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\n+        if min_tokens <= 0 or not token_ids_to_penalize:\n+            continue\n+\n+        # Identify parent sequences that still need the penalty applied.\n+        seqs_to_penalize: List[int] = []\n+        for j, seq_id in enumerate(seq_ids):\n+            seq_data = seq_group.seq_data[seq_id]\n+            if len(seq_data.output_token_ids) < min_tokens:\n+                seqs_to_penalize.append(start_idx + j)\n+\n+        if not seqs_to_penalize:\n+            continue\n+\n+        # Vectorized in-device masking for all (row, stop_token) combinations.\n+        row_idx = torch.tensor(seqs_to_penalize,\n+                               device=logits.device,\n+                               dtype=torch.long)\n+        col_idx = torch.tensor(list(token_ids_to_penalize),\n+                               device=logits.device,\n+                               dtype=torch.long)\n+        # Broadcasting advanced indexing creates a (num_rows, num_cols) view.\n+        logits[row_idx.unsqueeze(1), col_idx] = -float(\"inf\")\n \n     # verifies that no rows in logits were missed unexpectedly\n     assert logits_applied == logits.shape[0]\n@@ -255,16 +261,22 @@ def _apply_min_p(\n     logits: torch.Tensor,\n     min_p: torch.Tensor,\n ) -> torch.Tensor:\n+    \"\"\"Apply minimum probability filtering without an extra softmax pass.\n+\n+    A token is kept only if ``p_i >= min_p * p_max`` where ``p_i`` are the\n+    softmax probabilities and ``p_max`` is the maximum probability in the row.\n+    Since ``p_i / p_max = exp(logit_i - logit_max)``, this inequality is\n+    equivalent to ``logit_i >= logit_max + log(min_p)``. We therefore avoid\n+    computing a full softmax by comparing logits directly using a row-wise\n+    threshold computed in float32 for numerical stability.\n     \"\"\"\n-    Adapted from\n-    https://github.com/oobabooga/text-generation-webui/blob/3146124ec01f02c8fb1650a6517cf1b60b537aaf/modules/sampler_hijack.py#L16C17-L16C17\n-    \"\"\"\n-    probs = torch.softmax(logits, dim=-1)\n-    top_probs, _ = probs.max(dim=-1, keepdim=True)\n-    scaled_min_p = min_p.unsqueeze_(dim=1) * top_probs\n-    tokens_to_remove = probs < scaled_min_p\n-    logits = logits.masked_fill_(tokens_to_remove, -float(\"inf\"))\n-\n+    # Compute row-wise maximum logits in float32 for numerical stability.\n+    max_logits_f32 = logits.max(dim=-1, keepdim=True).values.to(torch.float32)\n+    # Per-row log(min_p). log(0) -> -inf is correct (keeps all tokens).\n+    log_min_p_f32 = torch.log(min_p.to(torch.float32)).unsqueeze(1)\n+    threshold = max_logits_f32 + log_min_p_f32\n+    # Mask tokens whose logits fall below the threshold.\n+    logits = logits.masked_fill(logits < threshold, -float(\"inf\"))\n     return logits\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
