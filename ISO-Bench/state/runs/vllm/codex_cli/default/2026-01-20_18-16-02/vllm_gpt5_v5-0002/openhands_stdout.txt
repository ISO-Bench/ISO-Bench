- I optimized vllm/model_executor/layers/sampler.py focusing on two hot paths:
  - Min-tokens penalty: removed Python-side Cartesian product and switched to vectorized, on-device advanced indexing to mask stop tokens. Cuts Python overhead and host-device syncs.