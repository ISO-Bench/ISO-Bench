{"instance_id": "/root/OmniPerf-Bench/vllm__repo-2b04c20", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 0026031..bc84fce 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -532,26 +532,38 @@ def moe_align_block_size_triton(\n     expert_ids: torch.Tensor,\n     num_tokens_post_pad: torch.Tensor,\n ) -> None:\n+    # Ensure contiguous accesses for better memory throughput.\n+    if not topk_ids.is_contiguous():\n+        topk_ids = topk_ids.contiguous()\n+\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n+    # Allocate once and explicitly zero to avoid repeated small inits.\n+    tokens_cnts = torch.empty((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    tokens_cnts.zero_()\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n+    cumsum.zero_()\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n+    # Tune Triton launch params to reduce latency of these tiny kernels.\n     moe_align_block_size_stage1[grid](\n         topk_ids,\n         tokens_cnts,\n         num_experts,\n         numel,\n         tokens_per_thread,\n+        num_warps=4,\n+        num_stages=1,\n     )\n     moe_align_block_size_stage2[grid](\n         tokens_cnts,\n         num_experts,\n+        num_warps=4,\n+        num_stages=1,\n     )\n     moe_align_block_size_stage3[(1, )](\n         num_tokens_post_pad,\n@@ -559,6 +571,8 @@ def moe_align_block_size_triton(\n         cumsum,\n         num_experts,\n         block_size,\n+        num_warps=1,\n+        num_stages=1,\n     )\n     moe_align_block_size_stage4[grid](\n         topk_ids,\n@@ -570,6 +584,8 @@ def moe_align_block_size_triton(\n         block_size,\n         numel,\n         tokens_per_thread,\n+        num_warps=4,\n+        num_stages=1,\n     )\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
