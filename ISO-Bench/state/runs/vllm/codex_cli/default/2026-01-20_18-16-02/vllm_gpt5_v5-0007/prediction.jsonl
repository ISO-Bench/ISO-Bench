{"instance_id": "/root/OmniPerf-Bench/vllm__repo-2a0309a", "model_patch": "diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex acc3a94..241194b 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -4,7 +4,7 @@ from typing import Dict, List, Optional\n import torch\n \n \n-@dataclass\n+@dataclass(slots=True)\n class SamplerOutput:\n \n     # [num_reqs]\n@@ -22,7 +22,7 @@ class SamplerOutput:\n \n # ModelRunnerOutput is serialized and sent to the scheduler process.\n # This is expensive for torch.Tensor so prefer to use List instead.\n-@dataclass\n+@dataclass(slots=True)\n class ModelRunnerOutput:\n \n     # [num_reqs]\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 7cd42ca..48f7010 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -39,16 +39,19 @@ class Sampler(nn.Module):\n             topk_logprobs = None\n             topk_indices = None\n \n-        # Use float32 for the logits.\n-        logits = logits.to(torch.float32)\n+        # Use float32 for the logits only if needed.\n+        # Avoid unnecessary dtype conversion when already float32.\n+        if logits.dtype is not torch.float32:\n+            logits = logits.to(torch.float32)\n         # Apply penalties (e.g., min_tokens, freq_penalties).\n         logits = self.apply_penalties(logits, sampling_metadata)\n         # Apply temperature.\n         logits = self.apply_temperature(logits, sampling_metadata.temperature)\n         # Sample the next token.\n         sampled = self.sample(logits, sampling_metadata)\n-        # Use int32 to reduce the tensor size.\n-        sampled = sampled.to(torch.int32)\n+        # Use int32 to reduce the tensor size; avoid no-op cast.\n+        if sampled.dtype is not torch.int32:\n+            sampled = sampled.to(torch.int32)\n \n         # NOTE: CPU-GPU synchronization happens here.\n         sampler_output = SamplerOutput(\n@@ -108,7 +111,12 @@ class Sampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        logprobs = logits.log_softmax(dim=-1, dtype=torch.float32)\n+        # Compute logprobs in float32 but avoid extra cast if logits is\n+        # already float32.\n+        if logits.dtype is torch.float32:\n+            logprobs = logits.log_softmax(dim=-1)\n+        else:\n+            logprobs = logits.log_softmax(dim=-1, dtype=torch.float32)\n         # FIXME: Mask the sampled token_id, get topk logprobs,\n         # and concatenate the topk with the sampled token_id.\n         topk_logprobs, topk_indices = torch.topk(\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4b3c325..10cfd72 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -339,16 +339,14 @@ class GPUModelRunner:\n         self.input_batch.block_table.commit(num_reqs)\n \n         # Get the number of scheduled tokens for each request.\n-        # TODO: The Python loop can be slow. Optimize.\n-        num_scheduled_tokens = []\n-        max_num_scheduled_tokens = 0\n-        for req_id in self.input_batch.req_ids[:num_reqs]:\n-            assert req_id is not None\n-            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n-            num_scheduled_tokens.append(num_tokens)\n-            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n-                                           num_tokens)\n-        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n+        # Avoid Python max() in a loop and build the numpy array directly.\n+        num_scheduled_tokens = np.fromiter(\n+            (scheduler_output.num_scheduled_tokens[req_id]\n+             for req_id in self.input_batch.req_ids[:num_reqs]),\n+            dtype=np.int32,\n+            count=num_reqs,\n+        )\n+        max_num_scheduled_tokens = int(num_scheduled_tokens.max())\n         assert max_num_scheduled_tokens > 0\n \n         # Get request indices.\n@@ -799,14 +797,18 @@ class GPUModelRunner:\n                     # This relies on cuda-specific torch-internal impl details\n                     generator.set_offset(generator.get_offset() - 4)\n \n+        # Avoid creating CPU tensors when not needed; use non_blocking copy\n+        # to overlap D2H with subsequent CPU work when possible.\n         if sampler_output.logprob_token_ids is None:\n             logprob_token_ids = None\n         else:\n-            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n+            logprob_token_ids = sampler_output.logprob_token_ids.to(\n+                device=\"cpu\", non_blocking=True)\n         if sampler_output.logprobs is None:\n             logprobs = None\n         else:\n-            logprobs = sampler_output.logprobs.cpu()\n+            logprobs = sampler_output.logprobs.to(device=\"cpu\",\n+                                                  non_blocking=True)\n \n         # num_reqs entries should be non-None\n         assert all(\n", "model_name_or_path": "gpt-5-2025-08-07"}
