{"instance_id": "/root/OmniPerf-Bench/vllm__repo-25373b6", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer.py b/vllm/model_executor/layers/mamba/mamba_mixer.py\nindex 796c8d9..1833a51 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer.py\n@@ -141,7 +141,9 @@ class MambaMixer(CustomOp):\n         attn_metadata: AttentionMetadata = get_forward_context().attn_metadata\n \n         # 1. Gated MLP's linear projection\n-        projected_states = self.in_proj(hidden_states)[0].transpose(-2, -1)\n+        # Cast to projection weight dtype to avoid implicit conversions\n+        hs = hidden_states.to(self.in_proj.weight.dtype)\n+        projected_states = self.in_proj(hs)[0].transpose(-2, -1)\n         hidden_states, gate = projected_states.chunk(2, dim=-2)\n \n         # 2. Convolution sequence transformation\n@@ -178,12 +180,14 @@ class MambaMixer(CustomOp):\n         # 3. State Space Model sequence transformation\n         # 3.a. input varying initialization of time_step, B and C\n \n+        # Ensure inputs to GEMM-backed layers are contiguous for better kernel\n+        # selection (especially with fused/LoRA paths)\n         if self.is_lora_enabled:\n-            #   lora kernel requires contiguous tensor\n             ssm_parameters = self.x_proj(\n                 hidden_states.transpose(-2, -1).contiguous())[0]\n         else:\n-            ssm_parameters = self.x_proj(hidden_states.transpose(-2, -1))[0]\n+            ssm_parameters = self.x_proj(\n+                hidden_states.transpose(-2, -1).contiguous())[0]\n \n         time_step, B, C = torch.split(\n             ssm_parameters,\n@@ -235,11 +239,11 @@ class MambaMixer(CustomOp):\n             scan_outputs = scan_outputs.transpose(0, 1)\n \n         # 4. Final linear projection\n+        # Keep output projection input contiguous to reduce potential reorders\n         if self.is_lora_enabled:\n-            #  lora kernel requires contiguous tensor\n             contextualized_states = self.out_proj(\n                 scan_outputs.transpose(-2, -1).contiguous())[0]\n         else:\n             contextualized_states = self.out_proj(\n-                scan_outputs.transpose(-2, -1))[0]\n+                scan_outputs.transpose(-2, -1).contiguous())[0]\n         return contextualized_states\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 36edac2..c5380f5 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -492,7 +492,9 @@ class MambaMixer2(MambaBase, CustomOp):\n         groups_time_state_size = self.n_groups * self.ssm_state_size\n \n         # 1. Gated MLP's linear projection\n-        projected_states, _ = self.in_proj(hidden_states)\n+        # Cast to weight dtype to avoid internal type conversions\n+        projected_states, _ = self.in_proj(\n+            hidden_states.to(self.in_proj.weight.dtype))\n \n         if mup_vector is not None:\n             projected_states = projected_states * mup_vector\n@@ -512,7 +514,7 @@ class MambaMixer2(MambaBase, CustomOp):\n \n         # - get hidden_states, B and C after depthwise convolution.\n         split_hidden_states_B_C_fn = lambda hidden_states_B_C: torch.split(\n-            hidden_states_B_C,\n+            hidden_states_B_C.contiguous(),\n             [\n                 self.intermediate_size // self.tp_size,\n                 groups_time_state_size // self.tp_size,\ndiff --git a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\nindex 3f67fc3..84002b8 100644\n--- a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n+++ b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n@@ -264,7 +264,8 @@ def selective_state_update(state,\n         assert dt_bias.shape == (nheads, dim)\n     if state_batch_indices is not None:\n         assert state_batch_indices.shape == (batch, )\n-    out = torch.empty_like(x)\n+    # Pre-allocate output as contiguous to match Triton kernel expectations\n+    out = torch.empty_like(x).contiguous()\n     grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch, nheads)\n     z_strides = ((z.stride(0), z.stride(1), z.stride(2)) if z is not None else\n                  (0, 0, 0))\ndiff --git a/vllm/model_executor/layers/mamba/ops/ssd_combined.py b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\nindex b121275..8615839 100644\n--- a/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n+++ b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n@@ -50,15 +50,15 @@ def _mamba_chunk_scan_combined_fwd(x,\n         assert D.shape == (nheads, headdim) or D.shape == (nheads, )\n     if seq_idx is not None:\n         assert seq_idx.shape == (batch, seqlen)\n+    # Ensure last-dim contiguous for better memory access in Triton/GEMMs\n     if B.stride(-1) != 1:\n         B = B.contiguous()\n     if C.stride(-1) != 1:\n         C = C.contiguous()\n-    if x.stride(-1) != 1 and x.stride(\n-            1) != 1:  # Either M or K dimension should be contiguous\n+    # Prefer contiguous inputs to avoid implicit reorders in kernels\n+    if x.stride(-1) != 1 and x.stride(1) != 1:\n         x = x.contiguous()\n-    if z is not None and z.stride(-1) != 1 and z.stride(\n-            1) != 1:  # Either M or K dimension should be contiguous\n+    if z is not None and z.stride(-1) != 1 and z.stride(1) != 1:\n         z = z.contiguous()\n     if D is not None and D.stride(-1) != 1:\n         D = D.contiguous()\n", "model_name_or_path": "gpt-5-2025-08-07"}
