{"instance_id": "/root/OmniPerf-Bench/vllm__repo-084a01f", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 5c37679..53ac9d7 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -201,7 +201,10 @@ def _apply_min_tokens_penalty(\n         have not been generated yet\n     \"\"\"\n     # list of indices in logits that will be set to -inf\n-    logits_to_penalize: List[Tuple[int, int]] = []\n+    # Accumulate how many rows of logits we touched for validation,\n+    # but avoid building large Python lists of index pairs. Doing\n+    # vectorized index assignment per group is faster and reduces\n+    # Python-side overhead.\n     logits_applied = 0\n     for seq_group in sampling_metadata.seq_groups:\n         seq_ids = seq_group.seq_ids\n@@ -217,23 +220,24 @@ def _apply_min_tokens_penalty(\n         min_tokens = sampling_params.min_tokens\n         token_ids_to_penalize = sampling_params.all_stop_token_ids\n         if min_tokens > 0 and token_ids_to_penalize:\n+            # Identify the sequences in this group that still fall under the\n+            # min_tokens constraint.\n             seqs_to_penalize: List[int] = []\n             for j, seq_id in enumerate(seq_ids):\n                 seq_data = seq_group.seq_data[seq_id]\n                 if len(seq_data.output_token_ids) < min_tokens:\n-                    seqs_to_penalize.append(j)\n+                    seqs_to_penalize.append(start_idx + j)\n \n             if seqs_to_penalize:\n-                # convert to the index into logits\n-                seqs_to_penalize = [start_idx + j for j in seqs_to_penalize]\n-                # itertools.product pairs each seq index with every token id\n-                logits_to_penalize.extend(\n-                    itertools.product(seqs_to_penalize, token_ids_to_penalize))\n-\n-    if logits_to_penalize:\n-        # use zip and * to group indices along each dimension\n-        # eg. [ (1,2), (1,3), (5,6) ] -> ( (1,1,5), (2,3,6) )\n-        logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\n+                # Vectorized cross-index assignment on device avoids building\n+                # a large Cartesian-product list on CPU.\n+                row_idx = torch.as_tensor(seqs_to_penalize,\n+                                          device=logits.device)\n+                # Ensure a stable, indexable container (set -> list) and move\n+                # directly to the target device.\n+                col_idx = torch.tensor(list(token_ids_to_penalize),\n+                                       device=logits.device)\n+                logits[row_idx.unsqueeze(1), col_idx] = -float(\"inf\")\n \n     # verifies that no rows in logits were missed unexpectedly\n     assert logits_applied == logits.shape[0]\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 390b5d1..4aa92fa 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -266,15 +266,17 @@ def _prepare_seq_groups(\n         \"\"\"\n \n         if sampling_params.prompt_logprobs is not None:\n+            # Directly extend from range (no intermediate list).\n             prompt_logprob_indices.extend(\n                 range(logit_idx, logit_idx + prompt_logprob_len))\n             logit_idx += prompt_logprob_len\n         if do_sample:\n+            # Avoid creating temporary lists when extending; zip is an\n+            # iterator and extend consumes it directly.\n             sample_indices.extend(range(logit_idx, logit_idx + sample_len))\n             categorized_sample_indices[sampling_params.sampling_type].extend(\n-                list(\n-                    zip(range(logit_idx, logit_idx + sample_len),\n-                        range(sample_idx, sample_idx + sample_len))))\n+                zip(range(logit_idx, logit_idx + sample_len),\n+                    range(sample_idx, sample_idx + sample_len)))\n             logit_idx += sample_len\n             sample_idx += sample_len\n \n@@ -437,8 +439,13 @@ class SamplingTensors:\n                 if seq_group.do_sample:\n                     for seq_id in seq_ids:\n                         seq_data = seq_group.seq_data[seq_id]\n-                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n-                        output_tokens.append(list(seq_data.output_token_ids))\n+                        # Reuse existing in-memory token lists to minimize\n+                        # Python allocations; make_tensor_with_pad copies\n+                        # data into a new NumPy array anyway.\n+                        prompt_tokens.append(\n+                            seq_data.get_prompt_token_ids_list())\n+                        output_tokens.append(\n+                            seq_data.get_output_token_ids_list())\n \n         sampling_tensors = SamplingTensors.from_lists(\n             temperatures, top_ps, top_ks, min_ps, presence_penalties,\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 0cd4c7e..aa74c84 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -221,6 +221,15 @@ class SequenceData:\n     def get_output_token_ids(self) -> Tuple[int, ...]:\n         return self.output_token_ids\n \n+    # Performance helpers for internal code paths that immediately copy\n+    # these containers (e.g., padding into new NumPy arrays). Returning the\n+    # underlying lists avoids temporary tuple/list constructions.\n+    def get_prompt_token_ids_list(self) -> List[int]:\n+        return self._prompt_token_ids\n+\n+    def get_output_token_ids_list(self) -> List[int]:\n+        return self._output_token_ids\n+\n     @property\n     def stage(self) -> SequenceStage:\n         return self._stage\n", "model_name_or_path": "gpt-5-2025-08-07"}
