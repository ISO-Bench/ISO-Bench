{"instance_id": "/root/OmniPerf-Bench/vllm__repo-3d92516", "model_patch": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 4198550..c2d778c 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -316,25 +316,26 @@ class Scheduler:\n         if isinstance(request_id, str):\n             request_id = (request_id, )\n         request_ids = set(request_id)\n-        for state_queue in [self.waiting, self.running, self.swapped]:\n-            aborted_groups: List[SequenceGroup] = []\n+        # Rebuild each queue once to avoid repeated O(n) deque.remove calls.\n+        for attr in (\"waiting\", \"running\", \"swapped\"):\n+            state_queue: Deque[SequenceGroup] = getattr(self, attr)\n+            if not state_queue:\n+                continue\n+\n+            kept: Deque[SequenceGroup] = deque()\n             for seq_group in state_queue:\n-                if not request_ids:\n-                    # Using 'break' here may add two extra iterations,\n-                    # but is acceptable to reduce complexity .\n-                    break\n-                if seq_group.request_id in request_ids:\n-                    # Appending aborted group into pending list.\n-                    aborted_groups.append(seq_group)\n-                    request_ids.remove(seq_group.request_id)\n-            for aborted_group in aborted_groups:\n-                # Remove the sequence group from the state queue.\n-                state_queue.remove(aborted_group)\n-                for seq in aborted_group.get_seqs():\n-                    if seq.is_finished():\n-                        continue\n-                    seq.status = SequenceStatus.FINISHED_ABORTED\n-                    self.free_seq(seq)\n+                rid = seq_group.request_id\n+                if rid in request_ids:\n+                    request_ids.remove(rid)\n+                    for seq in seq_group.get_seqs():\n+                        if seq.is_finished():\n+                            continue\n+                        seq.status = SequenceStatus.FINISHED_ABORTED\n+                        self.free_seq(seq)\n+                else:\n+                    kept.append(seq_group)\n+\n+            setattr(self, attr, kept)\n \n     def has_unfinished_seqs(self) -> bool:\n         return len(self.waiting) != 0 or len(self.running) != 0 or len(\n@@ -594,7 +595,7 @@ class Scheduler:\n         seq_groups: List[SequenceGroup] = []\n         # We don't sort waiting queue because we assume it is sorted.\n         # Copy the queue so that the input queue is not modified.\n-        waiting_queue = deque([s for s in waiting_queue])\n+        waiting_queue = deque(waiting_queue)\n \n         leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n         while self._passed_delay(time.time()) and waiting_queue:\n@@ -1064,7 +1065,7 @@ class Scheduler:\n         # Delay scheduling prompts to let waiting queue fill up\n         if self.scheduler_config.delay_factor > 0 and self.waiting:\n             earliest_arrival_time = min(\n-                [e.metrics.arrival_time for e in self.waiting])\n+                e.metrics.arrival_time for e in self.waiting)\n             passed_delay = (\n                 (now - earliest_arrival_time) >\n                 (self.scheduler_config.delay_factor * self.last_prompt_latency)\n@@ -1097,10 +1098,8 @@ class Scheduler:\n         sequences (e.g., running beam search), it means it is in decoding\n         phase, so chunking doesn't happen.\n         \"\"\"\n-        num_new_tokens = 0\n         seqs = seq_group.get_seqs(status=status)\n-        for seq in seqs:\n-            num_new_tokens += seq.get_num_new_tokens()\n+        num_new_tokens = sum(seq.get_num_new_tokens() for seq in seqs)\n         # Chunk if a running request cannot fit in.\n         # If number of seq > 1, it means it is doing beam search in a\n         # decode phase. Do not chunk in that case.\n", "model_name_or_path": "gpt-5-2025-08-07"}
