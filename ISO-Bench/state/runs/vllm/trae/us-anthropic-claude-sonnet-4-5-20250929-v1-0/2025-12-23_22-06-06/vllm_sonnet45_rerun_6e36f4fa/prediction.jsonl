{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-dd2a6a8", "model_patch": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 4c2f715..3270c35 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -349,7 +349,7 @@ class Scheduler:\n         # It lets the model know that any state associated with these requests\n         # can and must be released after the current step.\n         # This is used to evict the finished requests from the Mamba cache.\n-        self._finished_requests_ids: List[str] = list()\n+        self._finished_requests_ids: List[str] = []\n         # Time at previous scheduling step\n         self.prev_time = 0.0\n         # Did we schedule a prompt at previous step?\n@@ -486,7 +486,7 @@ class Scheduler:\n     def get_and_reset_finished_requests_ids(self) -> List[str]:\n         \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n         finished_requests_ids = self._finished_requests_ids\n-        self._finished_requests_ids = list()\n+        self._finished_requests_ids = []\n         return finished_requests_ids\n \n     def _schedule_running(\n@@ -874,7 +874,7 @@ class Scheduler:\n \n         # Queue requests that couldn't be scheduled.\n         waiting_queue.extendleft(leftover_waiting_sequences)\n-        if len(seq_groups) > 0:\n+        if seq_groups:\n             self.prev_prompt = True\n \n         return SchedulerPrefillOutputs(\n@@ -935,14 +935,14 @@ class Scheduler:\n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n         # Update new running requests.\n-        if len(prefills.seq_groups) > 0:\n-            self.running.extend([s.seq_group for s in prefills.seq_groups])\n+        if prefills.seq_groups:\n+            self.running.extend(s.seq_group for s in prefills.seq_groups)\n \n         self.running.extend(running_scheduled.decode_seq_groups_list)\n \n-        if len(swapped_in.decode_seq_groups) > 0:\n+        if swapped_in.decode_seq_groups:\n             self.running.extend(\n-                [s.seq_group for s in swapped_in.decode_seq_groups])\n+                s.seq_group for s in swapped_in.decode_seq_groups)\n \n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n@@ -1028,15 +1028,15 @@ class Scheduler:\n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n         # Update new running requests.\n-        self.running.extend([s.seq_group for s in prefills.seq_groups])\n+        self.running.extend(s.seq_group for s in prefills.seq_groups)\n         self.running.extend(\n-            [s.seq_group for s in running_scheduled.decode_seq_groups])\n+            s.seq_group for s in running_scheduled.decode_seq_groups)\n         self.running.extend(\n-            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n+            s.seq_group for s in running_scheduled.prefill_seq_groups)\n         self.running.extend(\n-            [s.seq_group for s in swapped_in.decode_seq_groups])\n+            s.seq_group for s in swapped_in.decode_seq_groups)\n         self.running.extend(\n-            [s.seq_group for s in swapped_in.prefill_seq_groups])\n+            s.seq_group for s in swapped_in.prefill_seq_groups)\n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n         return SchedulerOutputs(\n@@ -1315,7 +1315,7 @@ class Scheduler:\n \n         for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n             cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n-            if len(cows) > 0:\n+            if cows:\n                 blocks_to_copy.extend(cows)\n \n     def _preempt(\n", "model_name_or_path": "gpt-5-2025-08-07"}
