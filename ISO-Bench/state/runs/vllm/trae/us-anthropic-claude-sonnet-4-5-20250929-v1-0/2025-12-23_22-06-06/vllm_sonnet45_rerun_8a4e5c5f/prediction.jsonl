{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-76b4944", "model_patch": "diff --git a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\nindex 4e82424..111488c 100644\n--- a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n+++ b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n@@ -102,15 +102,18 @@ async def handle_request():\n         global count\n         global prefill_instances\n         global prefill_cv\n+        # Optimize: Cache list conversion and avoid repeated modulo operations\n         with prefill_cv:\n             prefill_list = list(prefill_instances.items())\n-            prefill_addr, prefill_zmq_addr = prefill_list[count % len(prefill_list)]\n+            prefill_idx = count % len(prefill_list)\n+            prefill_addr, prefill_zmq_addr = prefill_list[prefill_idx]\n \n         global decode_instances\n         global decode_cv\n         with decode_cv:\n             decode_list = list(decode_instances.items())\n-            decode_addr, decode_zmq_addr = decode_list[count % len(decode_list)]\n+            decode_idx = count % len(decode_list)\n+            decode_addr, decode_zmq_addr = decode_list[decode_idx]\n \n         print(\n             f\"handle_request count: {count}, [HTTP:{prefill_addr}, \"\n@@ -119,10 +122,8 @@ async def handle_request():\n         )\n         count += 1\n \n-        request_id = (\n-            f\"___prefill_addr_{prefill_zmq_addr}___decode_addr_\"\n-            f\"{decode_zmq_addr}_{random_uuid()}\"\n-        )\n+        # Optimize: Use single f-string for request_id construction\n+        request_id = f\"___prefill_addr_{prefill_zmq_addr}___decode_addr_{decode_zmq_addr}_{random_uuid()}\"\n \n         # finish prefill\n         async for _ in forward_request(\ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\nindex 52f589a..ab23e71 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n@@ -40,12 +40,14 @@ class ReqMeta:\n     def make_meta(request_id: str, token_ids: list[int], block_ids: list[int],\n                   block_size: int) -> \"ReqMeta\":\n         valid_num_tokens = len(token_ids)\n-        token_ids_tensor = torch.tensor(token_ids)\n-        block_ids_tensor = torch.tensor(block_ids)\n+        # Optimize: Use torch.as_tensor for faster conversion from lists\n+        token_ids_tensor = torch.as_tensor(token_ids, dtype=torch.long)\n+        block_ids_tensor = torch.as_tensor(block_ids, dtype=torch.long)\n         num_blocks = block_ids_tensor.shape[0]\n-        block_offsets = torch.arange(0, block_size)\n-        slot_mapping = block_offsets.reshape((1, block_size)) + \\\n-                block_ids_tensor.reshape((num_blocks, 1)) * block_size\n+        block_offsets = torch.arange(0, block_size, dtype=torch.long)\n+        # Optimize: Use view instead of reshape when possible for zero-copy\n+        slot_mapping = block_offsets.view(1, block_size) + \\\n+                block_ids_tensor.view(num_blocks, 1) * block_size\n         slot_mapping = slot_mapping.flatten()[:valid_num_tokens]\n \n         return ReqMeta(\n@@ -192,21 +194,24 @@ class P2pNcclConnector(KVConnectorBase_V1):\n \n         # Load the KV for each request each layer\n         for request in metadata.requests:\n+            # Optimize: Cache request_id to avoid repeated attribute access\n+            request_id = request.request_id\n+            slot_mapping = request.slot_mapping\n             for layer_name in forward_context.no_compile_layers:\n                 attn_layer = forward_context.no_compile_layers[layer_name]\n                 kv_cache_layer = attn_layer.kv_cache[ \\\n                     forward_context.virtual_engine]\n \n-                kv_cache = self.p2p_nccl_engine.recv_tensor(\n-                    request.request_id + \"#\" + layer_name)\n+                # Optimize: Use f-string for faster string concatenation\n+                tensor_id = f\"{request_id}#{layer_name}\"\n+                kv_cache = self.p2p_nccl_engine.recv_tensor(tensor_id)\n \n                 if kv_cache is None:\n-                    logger.warning(\"\ud83d\udea7src_kv_cache is None, %s\",\n-                                   request.request_id)\n+                    logger.warning(\"\ud83d\udea7src_kv_cache is None, %s\", request_id)\n                     continue\n \n                 inject_kv_into_layer(kv_cache_layer, kv_cache,\n-                                     request.slot_mapping, request.request_id)\n+                                     slot_mapping, request_id)\n \n     def wait_for_layer_load(self, layer_name: str) -> None:\n         \"\"\"Blocking until the KV for a specific layer is loaded into vLLM's\n@@ -260,10 +265,12 @@ class P2pNcclConnector(KVConnectorBase_V1):\n         for request in connector_metadata.requests:\n             request_id = request.request_id\n             ip, port = self.parse_request_id(request_id, True)\n-            remote_address = ip + \":\" + str(port + self._rank)\n+            # Optimize: Use f-string for faster string concatenation\n+            remote_address = f\"{ip}:{port + self._rank}\"\n             kv_cache = extract_kv_from_layer(kv_layer, request.slot_mapping)\n-            self.p2p_nccl_engine.send_tensor(request_id + \"#\" + layer_name,\n-                                             kv_cache, remote_address)\n+            # Optimize: Use f-string for faster string concatenation\n+            tensor_id = f\"{request_id}#{layer_name}\"\n+            self.p2p_nccl_engine.send_tensor(tensor_id, kv_cache, remote_address)\n \n     def wait_for_save(self):\n         if self.is_producer:\ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\nindex 6c9ccb2..18910c0 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n@@ -31,18 +31,18 @@ DEFAULT_MEM_POOL_SIZE_GB = 32\n \n @contextmanager\n def set_p2p_nccl_context(num_channels: str):\n-    original_values: dict[str, Any] = {}\n-    env_vars = [\n+    # Optimize: Use tuple for immutable sequence (faster than list)\n+    env_vars = (\n         'NCCL_MAX_NCHANNELS',\n         'NCCL_MIN_NCHANNELS',\n         'NCCL_CUMEM_ENABLE',\n         'NCCL_BUFFSIZE',\n         'NCCL_PROTO',  # LL,LL128,SIMPLE\n         'NCCL_ALGO',  # RING,TREE\n-    ]\n+    )\n \n-    for var in env_vars:\n-        original_values[var] = os.environ.get(var)\n+    # Optimize: Use dict comprehension for faster initialization\n+    original_values = {var: os.environ.get(var) for var in env_vars}\n \n     logger.info(\"set_p2p_nccl_context, original_values: %s\", original_values)\n \n@@ -52,9 +52,10 @@ def set_p2p_nccl_context(num_channels: str):\n         os.environ['NCCL_CUMEM_ENABLE'] = '1'\n         yield\n     finally:\n-        for var in env_vars:\n-            if original_values[var] is not None:\n-                os.environ[var] = original_values[var]\n+        # Optimize: Iterate over items() to avoid double lookup\n+        for var, value in original_values.items():\n+            if value is not None:\n+                os.environ[var] = value\n             else:\n                 os.environ.pop(var, None)\n \n@@ -369,24 +370,26 @@ class P2pNcclEngine:\n \n                     if data[\"ret\"] == 0:\n                         comm, rank = self.comms[remote_address.decode()]\n-                        self._send(comm, tensor.to(self.device), rank ^ 1,\n-                                   self.send_stream)\n+                        # Optimize: Only move tensor if it's not already on the device\n+                        if tensor.device != self.device:\n+                            tensor = tensor.to(self.device)\n+                        self._send(comm, tensor, rank ^ 1, self.send_stream)\n                 else:\n                     logger.warning(\n                         \"\ud83d\udea7Unexpected, Received message from %s, data:%s\",\n                         remote_address, data)\n \n     def _have_sent_tensor_id(self, tensor_id: str):\n-        request_id = tensor_id.split('#')[0]\n-        if request_id not in self.send_request_id_to_tensor_ids:\n-            self.send_request_id_to_tensor_ids[request_id] = set()\n-        self.send_request_id_to_tensor_ids[request_id].add(tensor_id)\n+        # Optimize: Use partition for faster string splitting\n+        request_id = tensor_id.partition('#')[0]\n+        # Optimize: Use setdefault to avoid double lookup\n+        self.send_request_id_to_tensor_ids.setdefault(request_id, set()).add(tensor_id)\n \n     def _have_received_tensor_id(self, tensor_id: str):\n-        request_id = tensor_id.split('#')[0]\n-        if request_id not in self.recv_request_id_to_tensor_ids:\n-            self.recv_request_id_to_tensor_ids[request_id] = set()\n-        self.recv_request_id_to_tensor_ids[request_id].add(tensor_id)\n+        # Optimize: Use partition for faster string splitting\n+        request_id = tensor_id.partition('#')[0]\n+        # Optimize: Use setdefault to avoid double lookup\n+        self.recv_request_id_to_tensor_ids.setdefault(request_id, set()).add(tensor_id)\n \n     def _send_async(self):\n         while True:\n@@ -440,7 +443,10 @@ class P2pNcclEngine:\n                 response.decode())\n             return False\n \n-        self._send(comm, tensor.to(self.device), rank ^ 1, self.send_stream)\n+        # Optimize: Only move tensor if it's not already on the device\n+        if tensor.device != self.device:\n+            tensor = tensor.to(self.device)\n+        self._send(comm, tensor, rank ^ 1, self.send_stream)\n \n         if self.send_type == \"PUT_ASYNC\":\n             self._have_sent_tensor_id(tensor_id)\n@@ -462,20 +468,21 @@ class P2pNcclEngine:\n         \"\"\"\n \n         # Clear the buffer upon request completion.\n-        for request_id in finished_req_ids:\n-            for layer_name in forward_context.no_compile_layers:\n-                tensor_id = request_id + \"#\" + layer_name\n-                if tensor_id in self.recv_store:\n-                    with self.recv_store_cv:\n+        # Optimize: Use with statement once outside the loop\n+        with self.recv_store_cv:\n+            for request_id in finished_req_ids:\n+                for layer_name in forward_context.no_compile_layers:\n+                    # Optimize: Use f-string for faster string concatenation\n+                    tensor_id = f\"{request_id}#{layer_name}\"\n+                    if tensor_id in self.recv_store:\n                         tensor = self.recv_store.pop(tensor_id, None)\n-                        self.send_request_id_to_tensor_ids.pop(\n-                            request_id, None)\n-                        self.recv_request_id_to_tensor_ids.pop(\n-                            request_id, None)\n-                    addr = 0\n-                    if isinstance(tensor, tuple):\n-                        addr, _, _ = tensor\n-                        self.pool.free(addr)\n+                        addr = 0\n+                        if isinstance(tensor, tuple):\n+                            addr, _, _ = tensor\n+                            self.pool.free(addr)\n+                # Optimize: Remove request tracking outside the layer loop\n+                self.send_request_id_to_tensor_ids.pop(request_id, None)\n+                self.recv_request_id_to_tensor_ids.pop(request_id, None)\n \n         # TODO:Retrieve requests that have already sent the KV cache.\n         finished_sending: set[str] = set()\n", "model_name_or_path": "gpt-5-2025-08-07"}
