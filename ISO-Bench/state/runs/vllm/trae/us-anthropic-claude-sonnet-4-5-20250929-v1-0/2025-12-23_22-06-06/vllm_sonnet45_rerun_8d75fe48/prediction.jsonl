{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-388596c", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a..c931b69 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -258,7 +258,7 @@ def scaled_fp8_quant(\n     else:\n         output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e..c5b9eb7 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -204,7 +204,7 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                   layer.weight_scale[idx])\n \n                 layer.weight[start:end, :] = per_tensor_quantize(\n-                    weight_dq, layer.weight_scale.max())\n+                    weight_dq, max_w_scale)\n                 start = end\n             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n \n@@ -309,6 +309,4 @@ def per_tensor_quantize(tensor: torch.Tensor,\n def per_tensor_dequantize(\n         tensor: torch.Tensor, inv_scale: Union[float,\n                                                torch.Tensor]) -> torch.Tensor:\n-    fake_qweight = tensor.to(torch.float16)\n-    dq_weight = fake_qweight * inv_scale\n-    return dq_weight\n+    return tensor.to(torch.float16) * inv_scale\n", "model_name_or_path": "gpt-5-2025-08-07"}
