diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index 14e4134..5ba0916 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -38,6 +38,11 @@ assert _LONG_INFO.min == _MOCK_LONG_INFO.min
 assert _LONG_INFO.max == _MOCK_LONG_INFO.max
 
 
+# Optimize: Pre-define factory functions to avoid lambda overhead
+def _get_current_time() -> int:
+    return int(time.time())
+
+
 class OpenAIBaseModel(BaseModel):
     # OpenAI API does allow extra fields
     model_config = ConfigDict(extra="allow")
@@ -73,7 +78,7 @@ class ErrorResponse(OpenAIBaseModel):
 class ModelPermission(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"modelperm-{random_uuid()}")
     object: str = "model_permission"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     allow_create_engine: bool = False
     allow_sampling: bool = True
     allow_logprobs: bool = True
@@ -88,7 +93,7 @@ class ModelPermission(OpenAIBaseModel):
 class ModelCard(OpenAIBaseModel):
     id: str
     object: str = "model"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     owned_by: str = "vllm"
     root: Optional[str] = None
     parent: Optional[str] = None
@@ -1018,7 +1023,7 @@ class CompletionResponseChoice(OpenAIBaseModel):
 class CompletionResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"cmpl-{random_uuid()}")
     object: str = "text_completion"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     choices: List[CompletionResponseChoice]
     usage: UsageInfo
@@ -1041,7 +1046,7 @@ class CompletionResponseStreamChoice(OpenAIBaseModel):
 class CompletionStreamResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"cmpl-{random_uuid()}")
     object: str = "text_completion"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     choices: List[CompletionResponseStreamChoice]
     usage: Optional[UsageInfo] = Field(default=None)
@@ -1056,7 +1061,7 @@ class EmbeddingResponseData(OpenAIBaseModel):
 class EmbeddingResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"embd-{random_uuid()}")
     object: str = "list"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     data: List[EmbeddingResponseData]
     usage: UsageInfo
@@ -1071,7 +1076,7 @@ class PoolingResponseData(OpenAIBaseModel):
 class PoolingResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"pool-{random_uuid()}")
     object: str = "list"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     data: List[PoolingResponseData]
     usage: UsageInfo
@@ -1086,7 +1091,7 @@ class ScoreResponseData(OpenAIBaseModel):
 class ScoreResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"embd-{random_uuid()}")
     object: str = "list"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     data: List[ScoreResponseData]
     usage: UsageInfo
@@ -1161,7 +1166,7 @@ class ChatCompletionResponseChoice(OpenAIBaseModel):
 class ChatCompletionResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"chatcmpl-{random_uuid()}")
     object: Literal["chat.completion"] = "chat.completion"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     choices: List[ChatCompletionResponseChoice]
     usage: UsageInfo
@@ -1185,7 +1190,7 @@ class ChatCompletionResponseStreamChoice(OpenAIBaseModel):
 class ChatCompletionStreamResponse(OpenAIBaseModel):
     id: str = Field(default_factory=lambda: f"chatcmpl-{random_uuid()}")
     object: Literal["chat.completion.chunk"] = "chat.completion.chunk"
-    created: int = Field(default_factory=lambda: int(time.time()))
+    created: int = Field(default_factory=_get_current_time)
     model: str
     choices: List[ChatCompletionResponseStreamChoice]
     usage: Optional[UsageInfo] = Field(default=None)
diff --git a/vllm/envs.py b/vllm/envs.py
index 1e68326..a7b59e9 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -478,11 +478,17 @@ environment_variables: Dict[str, Callable[[], Any]] = {
 
 # end-env-vars-definition
 
+# Cache for environment variable values to avoid repeated os.getenv() calls
+_env_var_cache: Dict[str, Any] = {}
+
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation of environment variables with caching
     if name in environment_variables:
-        return environment_variables[name]()
+        # Check cache first
+        if name not in _env_var_cache:
+            _env_var_cache[name] = environment_variables[name]()
+        return _env_var_cache[name]
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index b4d3e44..b1f4e33 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -133,9 +133,11 @@ class AsyncLLM(EngineClient):
         """Add new request to the AsyncLLM."""
 
         # 1) Create a new output queue for the request.
+        # Optimize: check request_id before creating queue
         if self.output_processor.is_request_active(request_id):
             raise ValueError(f"Request id {request_id} already running.")
-        queue: asyncio.Queue[RequestOutput] = asyncio.Queue()
+        # Optimize: use unbounded queue for better performance
+        queue: asyncio.Queue[RequestOutput] = asyncio.Queue(maxsize=0)
 
         # 2) Convert Input --> Request.
         request = self.processor.process_inputs(request_id, prompt, params,
diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py
index 19b8900..c47e8ef 100644
--- a/vllm/v1/engine/core_client.py
+++ b/vllm/v1/engine/core_client.py
@@ -111,7 +111,8 @@ class InprocClient(EngineCoreClient):
         self.engine_core.add_request(request)
 
     def abort_requests(self, request_ids: List[str]) -> None:
-        if len(request_ids) > 0:
+        # Optimize: use truthiness check instead of len() > 0
+        if request_ids:
             self.engine_core.abort_requests(request_ids)
 
     def shutdown(self) -> None:
@@ -231,7 +232,8 @@ class SyncMPClient(MPClient):
         self._send_input(EngineCoreRequestType.ADD, request)
 
     def abort_requests(self, request_ids: List[str]) -> None:
-        if len(request_ids) > 0:
+        # Optimize: use truthiness check instead of len() > 0
+        if request_ids:
             self._send_input(EngineCoreRequestType.ABORT, request_ids)
 
     def profile(self, is_start: bool = True) -> None:
@@ -273,7 +275,8 @@ class AsyncMPClient(MPClient):
         await self._send_input(EngineCoreRequestType.ADD, request)
 
     async def abort_requests_async(self, request_ids: List[str]) -> None:
-        if len(request_ids) > 0:
+        # Optimize: use truthiness check instead of len() > 0
+        if request_ids:
             await self._send_input(EngineCoreRequestType.ABORT, request_ids)
 
     async def profile_async(self, is_start: bool = True) -> None:
diff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py
index 749f4f5..e71cf9a 100644
--- a/vllm/v1/engine/output_processor.py
+++ b/vllm/v1/engine/output_processor.py
@@ -134,9 +134,11 @@ class OutputProcessor:
         request_outputs: List[RequestOutput] = []
         reqs_to_abort: List[str] = []
         iteration_stats = IterationStats(self.log_stats)
+        # Optimize: cache request_states dict for faster lookups
+        request_states = self.request_states
         for engine_core_output in engine_core_outputs:
             req_id = engine_core_output.request_id
-            req_state = self.request_states.get(req_id)
+            req_state = request_states.get(req_id)
             if req_state is None:
                 # Ignore output for already-aborted request.
                 continue
@@ -163,7 +165,7 @@ class OutputProcessor:
 
                 # Free completed requests.
                 if request_output.finished:
-                    self.request_states.pop(req_id)
+                    request_states.pop(req_id)
                     if not engine_core_output.finished:
                         # If req not finished in EngineCore, but Detokenizer
                         # detected stop string, abort needed in EngineCore.
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index 4545016..aaca458 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -46,9 +46,10 @@ class Request:
 
         self.prompt = prompt
         self.prompt_token_ids = prompt_token_ids
-        self.num_prompt_tokens = len(self.prompt_token_ids)
+        self.num_prompt_tokens = len(prompt_token_ids)
         self._output_token_ids: List[int] = []
-        self._all_token_ids: List[int] = self.prompt_token_ids.copy()
+        # Optimize: use list() constructor instead of copy() for better performance
+        self._all_token_ids: List[int] = list(prompt_token_ids)
         self.num_computed_tokens = 0
 
         # Multi-modal related
@@ -95,10 +96,13 @@ class Request:
         self,
         token_ids: Union[int, List[int]],
     ) -> None:
+        # Optimize: handle single int case more efficiently
         if isinstance(token_ids, int):
-            token_ids = [token_ids]
-        self._output_token_ids.extend(token_ids)
-        self._all_token_ids.extend(token_ids)
+            self._output_token_ids.append(token_ids)
+            self._all_token_ids.append(token_ids)
+        else:
+            self._output_token_ids.extend(token_ids)
+            self._all_token_ids.extend(token_ids)
 
     @property
     def num_tokens(self) -> int:
