{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-bc8a8ce", "model_patch": "diff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py\nindex 3a4c259..16ceab1 100644\n--- a/vllm/v1/sample/logits_processor.py\n+++ b/vllm/v1/sample/logits_processor.py\n@@ -229,7 +229,7 @@ class MinPLogitsProcessor(LogitsProcessor):\n         super().__init__()\n         self.min_p_count: int = 0\n \n-        self.min_p_cpu_tensor = torch.zeros((max_num_reqs, ),\n+        self.min_p_cpu_tensor = torch.empty((max_num_reqs, ),\n                                             dtype=torch.float32,\n                                             device=\"cpu\",\n                                             pin_memory=pin_memory)\n@@ -259,32 +259,34 @@ class MinPLogitsProcessor(LogitsProcessor):\n             return\n \n         needs_update = False\n+        min_p_cpu = self.min_p_cpu  # Cache attribute access\n+        \n         # Process added requests.\n         for index, params, _ in batch_update.added:\n             min_p = params.min_p if isinstance(params, SamplingParams) else 0.0\n-            if self.min_p_cpu[index] != min_p:\n+            if min_p_cpu[index] != min_p:\n                 needs_update = True\n-                self.min_p_cpu[index] = min_p\n+                min_p_cpu[index] = min_p\n             if min_p:\n                 self.min_p_count += 1\n \n         if self.min_p_count:\n             # Process removed requests.\n-            needs_update |= bool(batch_update.removed)\n-            for index in batch_update.removed:\n-                if self.min_p_cpu[index]:\n-                    self.min_p_count -= 1\n+            if batch_update.removed:\n+                needs_update = True\n+                for index in batch_update.removed:\n+                    if min_p_cpu[index]:\n+                        self.min_p_count -= 1\n \n             # Process moved requests, unidirectional (a->b) and swap (a<->b)\n             for adx, bdx, direct in batch_update.moved:\n-                change = (min_p_a :=\n-                          self.min_p_cpu[adx]) != (min_p_b :=\n-                                                   self.min_p_cpu[bdx])\n-                needs_update |= change\n-                if change:\n-                    self.min_p_cpu[bdx] = min_p_a\n+                min_p_a = min_p_cpu[adx]\n+                min_p_b = min_p_cpu[bdx]\n+                if min_p_a != min_p_b:\n+                    needs_update = True\n+                    min_p_cpu[bdx] = min_p_a\n                     if direct == MoveDirectionality.SWAP:\n-                        self.min_p_cpu[adx] = min_p_b\n+                        min_p_cpu[adx] = min_p_b\n \n         # Update tensors if needed.\n         size = batch_update.batch_size\n@@ -305,12 +307,10 @@ class MinPLogitsProcessor(LogitsProcessor):\n         max_probabilities = torch.amax(probability_values,\n                                        dim=-1,\n                                        keepdim=True)\n-        # Adjust min_p\n-        adjusted_min_p = max_probabilities.mul_(self.min_p)\n-        # Identify valid tokens using threshold comparison\n-        invalid_token_mask = probability_values < adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[invalid_token_mask] = -float('inf')\n+        # Adjust min_p and identify invalid tokens in one step\n+        adjusted_min_p = max_probabilities * self.min_p\n+        # Apply mask using boolean indexing - avoid creating intermediate mask\n+        logits.masked_fill_(probability_values < adjusted_min_p, -float('inf'))\n         return logits\n \n \n@@ -322,7 +322,7 @@ class LogitBiasLogitsProcessor(LogitsProcessor):\n         self.device = device\n         self.pin_memory = pin_memory\n \n-        self.bias_tensor: torch.Tensor = torch.tensor(())\n+        self.bias_tensor: torch.Tensor = torch.empty(0, dtype=torch.float32, device=device)\n         self.logits_slice = (self._device_tensor([], torch.int32),\n                              self._device_tensor([], torch.int32))\n \n@@ -370,22 +370,30 @@ class LogitBiasLogitsProcessor(LogitsProcessor):\n \n         # Update tensors if needed.\n         if needs_update:\n-            reqs, tok_ids, biases = [], [], []\n-            for req, lb in self.biases.items():\n-                reqs.extend([req] * len(lb))\n-                tok_ids.extend(lb.keys())\n-                biases.extend(lb.values())\n-\n-            self.bias_tensor = self._device_tensor(biases, torch.float32)\n-            self.logits_slice = (self._device_tensor(reqs, torch.int32),\n-                                 self._device_tensor(tok_ids, torch.int32))\n+            # Use list comprehensions for better performance\n+            items = [(req, tok_id, bias) \n+                    for req, lb in self.biases.items() \n+                    for tok_id, bias in lb.items()]\n+            if items:\n+                reqs, tok_ids, biases = zip(*items)\n+                self.bias_tensor = self._device_tensor(list(biases), torch.float32)\n+                self.logits_slice = (self._device_tensor(list(reqs), torch.int32),\n+                                     self._device_tensor(list(tok_ids), torch.int32))\n+            else:\n+                self.bias_tensor = torch.empty(0, dtype=torch.float32, device=self.device)\n+                self.logits_slice = (torch.empty(0, dtype=torch.int32, device=self.device),\n+                                    torch.empty(0, dtype=torch.int32, device=self.device))\n \n     def _device_tensor(self, data: list, dtype: torch.dtype) -> torch.Tensor:\n-        return (torch.tensor(data,\n-                             device=\"cpu\",\n-                             dtype=dtype,\n-                             pin_memory=self.pin_memory).to(device=self.device,\n-                                                            non_blocking=True))\n+        if self.device.type == \"cpu\":\n+            return torch.tensor(data, device=\"cpu\", dtype=dtype)\n+        if self.pin_memory:\n+            return torch.tensor(data,\n+                               device=\"cpu\",\n+                               dtype=dtype,\n+                               pin_memory=True).to(device=self.device,\n+                                                   non_blocking=True)\n+        return torch.tensor(data, device=self.device, dtype=dtype)\n \n     def apply(self, logits: torch.Tensor) -> torch.Tensor:\n         if self.biases:\n@@ -470,21 +478,28 @@ class MinTokensLogitsProcessor(LogitsProcessor):\n \n         # Update tensors if needed.\n         if needs_update:\n-            reqs: list[int] = []\n-            tok_ids: list[int] = []\n-            for req, (_, _, stop_tok_ids) in self.min_toks.items():\n-                reqs.extend([req] * len(stop_tok_ids))\n-                tok_ids.extend(stop_tok_ids)\n-\n-            self.logits_slice = (self._device_tensor(reqs, torch.int32),\n-                                 self._device_tensor(tok_ids, torch.int32))\n+            # Use list comprehensions for better performance\n+            items = [(req, tok_id) \n+                    for req, (_, _, stop_tok_ids) in self.min_toks.items()\n+                    for tok_id in stop_tok_ids]\n+            if items:\n+                reqs, tok_ids = zip(*items)\n+                self.logits_slice = (self._device_tensor(list(reqs), torch.int32),\n+                                     self._device_tensor(list(tok_ids), torch.int32))\n+            else:\n+                self.logits_slice = (torch.empty(0, dtype=torch.int32, device=self.device),\n+                                    torch.empty(0, dtype=torch.int32, device=self.device))\n \n     def _device_tensor(self, data: list, dtype: torch.dtype) -> torch.Tensor:\n-        return (torch.tensor(data,\n-                             device=\"cpu\",\n-                             dtype=dtype,\n-                             pin_memory=self.pin_memory).to(device=self.device,\n-                                                            non_blocking=True))\n+        if self.device.type == \"cpu\":\n+            return torch.tensor(data, device=\"cpu\", dtype=dtype)\n+        if self.pin_memory:\n+            return torch.tensor(data,\n+                               device=\"cpu\",\n+                               dtype=dtype,\n+                               pin_memory=True).to(device=self.device,\n+                                                   non_blocking=True)\n+        return torch.tensor(data, device=self.device, dtype=dtype)\n \n     def apply(self, logits: torch.Tensor) -> torch.Tensor:\n         if self.min_toks:\n", "model_name_or_path": "gpt-5-2025-08-07"}
