diff --git a/vllm/config.py b/vllm/config.py
index 6bfe94b..86a7582 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -253,10 +253,14 @@ def get_field(cls: ConfigType, name: str) -> Field:
     default factory fields in `EngineArgs`."""
     if not is_dataclass(cls):
         raise TypeError("The given class is not a dataclass.")
-    cls_fields = {f.name: f for f in fields(cls)}
-    if name not in cls_fields:
+    # Optimize: directly iterate to find the field instead of creating a dict
+    named_field = None
+    for f in fields(cls):
+        if f.name == name:
+            named_field = f
+            break
+    if named_field is None:
         raise ValueError(f"Field '{name}' not found in {cls.__name__}.")
-    named_field: Field = cls_fields[name]
     if (default_factory := named_field.default_factory) is not MISSING:
         return field(default_factory=default_factory)
     if (default := named_field.default) is not MISSING:
@@ -4831,7 +4835,10 @@ class VllmConfig:
                 self.model_config is not None and \
                     not self.model_config.enforce_eager:
 
-                possible_sizes = [1, 2, 4] + [8 * i for i in range(1, 1025)]
+                # Pre-compute possible sizes list for better performance
+                # This generates [1, 2, 4, 8, 16, 24, ..., 8192]
+                possible_sizes = [1, 2, 4]
+                possible_sizes.extend(range(8, 8193, 8))
                 if self.parallel_config.tensor_parallel_size > 1 and \
                     self.compilation_config.pass_config.enable_sequence_parallelism:
                     possible_sizes = self.update_sizes_for_sequence_parallelism(
diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff741..60347c1 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -221,8 +221,8 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
     # If set, vllm will use precompiled binaries (*.so)
     "VLLM_USE_PRECOMPILED":
-    lambda: bool(os.environ.get("VLLM_USE_PRECOMPILED")) or bool(
-        os.environ.get("VLLM_PRECOMPILED_WHEEL_LOCATION")),
+    lambda: bool(os.getenv("VLLM_USE_PRECOMPILED")) or bool(
+        os.getenv("VLLM_PRECOMPILED_WHEEL_LOCATION")),
 
     # Whether to force using nightly wheel in python build.
     # This is used for testing the nightly wheel in python build.
@@ -285,30 +285,30 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # If true, will load models from ModelScope instead of Hugging Face Hub.
     # note that the value is true or false, not numbers
     "VLLM_USE_MODELSCOPE":
-    lambda: os.environ.get("VLLM_USE_MODELSCOPE", "False").lower() == "true",
+    lambda: os.getenv("VLLM_USE_MODELSCOPE", "False").lower() == "true",
 
     # Interval in seconds to log a warning message when the ring buffer is full
     "VLLM_RINGBUFFER_WARNING_INTERVAL":
-    lambda: int(os.environ.get("VLLM_RINGBUFFER_WARNING_INTERVAL", "60")),
+    lambda: int(os.getenv("VLLM_RINGBUFFER_WARNING_INTERVAL", "60")),
 
     # path to cudatoolkit home directory, under which should be bin, include,
     # and lib directories.
     "CUDA_HOME":
-    lambda: os.environ.get("CUDA_HOME", None),
+    lambda: os.getenv("CUDA_HOME", None),
 
     # Path to the NCCL library file. It is needed because nccl>=2.19 brought
     # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234
     "VLLM_NCCL_SO_PATH":
-    lambda: os.environ.get("VLLM_NCCL_SO_PATH", None),
+    lambda: os.getenv("VLLM_NCCL_SO_PATH", None),
 
     # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl
     # library file in the locations specified by `LD_LIBRARY_PATH`
     "LD_LIBRARY_PATH":
-    lambda: os.environ.get("LD_LIBRARY_PATH", None),
+    lambda: os.getenv("LD_LIBRARY_PATH", None),
 
     # flag to control if vllm should use triton flash attention
     "VLLM_USE_TRITON_FLASH_ATTN":
-    lambda: (os.environ.get("VLLM_USE_TRITON_FLASH_ATTN", "True").lower() in
+    lambda: (os.getenv("VLLM_USE_TRITON_FLASH_ATTN", "True").lower() in
              ("true", "1")),
 
     # Use separate prefill and decode kernels for V1 attention instead of
@@ -321,59 +321,59 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # Force vllm to use a specific flash-attention version (2 or 3), only valid
     # when using the flash-attention backend.
     "VLLM_FLASH_ATTN_VERSION":
-    lambda: maybe_convert_int(os.environ.get("VLLM_FLASH_ATTN_VERSION", None)),
+    lambda: maybe_convert_int(os.getenv("VLLM_FLASH_ATTN_VERSION", None)),
 
     # Internal flag to enable Dynamo fullgraph capture
     "VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE":
     lambda: bool(
-        os.environ.get("VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE", "1") != "0"),
+        os.getenv("VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE", "1") != "0"),
 
     # Feature flag to enable/disable Inductor standalone compile.
     # In torch <= 2.7 we ignore this flag; in torch >= 2.8 this is
     # enabled by default.
     "VLLM_USE_STANDALONE_COMPILE":
-    lambda: os.environ.get("VLLM_USE_STANDALONE_COMPILE", "1") == "1",
+    lambda: os.getenv("VLLM_USE_STANDALONE_COMPILE", "1") == "1",
 
     # local rank of the process in the distributed setting, used to determine
     # the GPU device id
     "LOCAL_RANK":
-    lambda: int(os.environ.get("LOCAL_RANK", "0")),
+    lambda: int(os.getenv("LOCAL_RANK", "0")),
 
     # used to control the visible devices in the distributed setting
     "CUDA_VISIBLE_DEVICES":
-    lambda: os.environ.get("CUDA_VISIBLE_DEVICES", None),
+    lambda: os.getenv("CUDA_VISIBLE_DEVICES", None),
 
     # timeout for each iteration in the engine
     "VLLM_ENGINE_ITERATION_TIMEOUT_S":
-    lambda: int(os.environ.get("VLLM_ENGINE_ITERATION_TIMEOUT_S", "60")),
+    lambda: int(os.getenv("VLLM_ENGINE_ITERATION_TIMEOUT_S", "60")),
 
     # API key for vLLM API server
     "VLLM_API_KEY":
-    lambda: os.environ.get("VLLM_API_KEY", None),
+    lambda: os.getenv("VLLM_API_KEY", None),
 
     # Whether to log responses from API Server for debugging
     "VLLM_DEBUG_LOG_API_SERVER_RESPONSE":
-    lambda: os.environ.get("VLLM_DEBUG_LOG_API_SERVER_RESPONSE", "False"
+    lambda: os.getenv("VLLM_DEBUG_LOG_API_SERVER_RESPONSE", "False"
                            ).lower() == "true",
 
     # S3 access information, used for tensorizer to load model from S3
     "S3_ACCESS_KEY_ID":
-    lambda: os.environ.get("S3_ACCESS_KEY_ID", None),
+    lambda: os.getenv("S3_ACCESS_KEY_ID", None),
     "S3_SECRET_ACCESS_KEY":
-    lambda: os.environ.get("S3_SECRET_ACCESS_KEY", None),
+    lambda: os.getenv("S3_SECRET_ACCESS_KEY", None),
     "S3_ENDPOINT_URL":
-    lambda: os.environ.get("S3_ENDPOINT_URL", None),
+    lambda: os.getenv("S3_ENDPOINT_URL", None),
 
     # Usage stats collection
     "VLLM_USAGE_STATS_SERVER":
-    lambda: os.environ.get("VLLM_USAGE_STATS_SERVER", "https://stats.vllm.ai"),
+    lambda: os.getenv("VLLM_USAGE_STATS_SERVER", "https://stats.vllm.ai"),
     "VLLM_NO_USAGE_STATS":
-    lambda: os.environ.get("VLLM_NO_USAGE_STATS", "0") == "1",
+    lambda: os.getenv("VLLM_NO_USAGE_STATS", "0") == "1",
     "VLLM_DO_NOT_TRACK":
-    lambda: (os.environ.get("VLLM_DO_NOT_TRACK", None) or os.environ.get(
+    lambda: (os.getenv("VLLM_DO_NOT_TRACK", None) or os.getenv(
         "DO_NOT_TRACK", None) or "0") == "1",
     "VLLM_USAGE_SOURCE":
-    lambda: os.environ.get("VLLM_USAGE_SOURCE", "production"),
+    lambda: os.getenv("VLLM_USAGE_SOURCE", "production"),
 
     # Logging configuration
     # If set to 0, vllm will not configure logging
@@ -600,7 +600,7 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
     # Timeout in seconds for keeping HTTP connections alive in API server
     "VLLM_HTTP_TIMEOUT_KEEP_ALIVE":
-    lambda: int(os.environ.get("VLLM_HTTP_TIMEOUT_KEEP_ALIVE", "5")),
+    lambda: int(os.getenv("VLLM_HTTP_TIMEOUT_KEEP_ALIVE", "5")),
 
     # a list of plugin names to load, separated by commas.
     # if this is not set, it means all plugins will be loaded
@@ -822,11 +822,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
     # Randomize inputs during dummy runs when using Data Parallel
     "VLLM_RANDOMIZE_DP_DUMMY_INPUTS":
-    lambda: os.environ.get("VLLM_RANDOMIZE_DP_DUMMY_INPUTS", "0") == "1",
+    lambda: os.getenv("VLLM_RANDOMIZE_DP_DUMMY_INPUTS", "0") == "1",
 
     # Whether to use S3 path for model loading in CI via RunAI Streamer
     "VLLM_CI_USE_S3":
-    lambda: os.environ.get("VLLM_CI_USE_S3", "0") == "1",
+    lambda: os.getenv("VLLM_CI_USE_S3", "0") == "1",
 
     # Use model_redirect to redirect the model name to a local folder.
     # `model_redirect` can be a json file mapping the model between
@@ -835,23 +835,23 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # or a space separated values table file:
     # meta-llama/Llama-3.2-1B   /tmp/Llama-3.2-1B
     "VLLM_MODEL_REDIRECT_PATH":
-    lambda: os.environ.get("VLLM_MODEL_REDIRECT_PATH", None),
+    lambda: os.getenv("VLLM_MODEL_REDIRECT_PATH", None),
 
     # Whether to use atomicAdd reduce in gptq/awq marlin kernel.
     "VLLM_MARLIN_USE_ATOMIC_ADD":
-    lambda: os.environ.get("VLLM_MARLIN_USE_ATOMIC_ADD", "0") == "1",
+    lambda: os.getenv("VLLM_MARLIN_USE_ATOMIC_ADD", "0") == "1",
 
     # Whether to turn on the outlines cache for V0
     # This cache is unbounded and on disk, so it's not safe to use in
     # an environment with potentially malicious users.
     "VLLM_V0_USE_OUTLINES_CACHE":
-    lambda: os.environ.get("VLLM_V0_USE_OUTLINES_CACHE", "0") == "1",
+    lambda: os.getenv("VLLM_V0_USE_OUTLINES_CACHE", "0") == "1",
 
     # Whether to turn on the outlines cache for V1
     # This cache is unbounded and on disk, so it's not safe to use in
     # an environment with potentially malicious users.
     "VLLM_V1_USE_OUTLINES_CACHE":
-    lambda: os.environ.get("VLLM_V1_USE_OUTLINES_CACHE", "0") == "1",
+    lambda: os.getenv("VLLM_V1_USE_OUTLINES_CACHE", "0") == "1",
 
     # Gap between padding buckets for the forward pass. So we have
     # 8, we will run forward pass with [16, 24, 32, ...].
@@ -859,7 +859,7 @@ environment_variables: dict[str, Callable[[], Any]] = {
     lambda: int(os.environ["VLLM_TPU_BUCKET_PADDING_GAP"])
     if "VLLM_TPU_BUCKET_PADDING_GAP" in os.environ else 0,
     "VLLM_TPU_MOST_MODEL_LEN":
-    lambda: maybe_convert_int(os.environ.get("VLLM_TPU_MOST_MODEL_LEN", None)),
+    lambda: maybe_convert_int(os.getenv("VLLM_TPU_MOST_MODEL_LEN", None)),
 
     # Allow use of DeepGemm kernels for fused moe ops.
     "VLLM_USE_DEEP_GEMM":
@@ -995,11 +995,17 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
 # --8<-- [end:env-vars-definition]
 
+# Cache for environment variable values to avoid repeated lambda evaluations
+_env_vars_cache: dict[str, Any] = {}
+
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation of environment variables with caching
     if name in environment_variables:
-        return environment_variables[name]()
+        # Check cache first for performance
+        if name not in _env_vars_cache:
+            _env_vars_cache[name] = environment_variables[name]()
+        return _env_vars_cache[name]
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
@@ -1021,6 +1027,8 @@ def set_vllm_use_v1(use_v1: bool):
             "explicitly by the user. Please raise this as a Github "
             "Issue and explicitly set VLLM_USE_V1=0 or 1.")
     os.environ["VLLM_USE_V1"] = "1" if use_v1 else "0"
+    # Clear cache for this variable since it was explicitly set
+    _env_vars_cache.pop("VLLM_USE_V1", None)
 
 
 def compute_hash() -> str:
