{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6..b3e75c1 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1322,7 +1322,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n             # multiplying the list, to avoid Dynamo from treating them as\n             # tensor aliasing.\n             kv_caches = [\n-                torch.tensor([], dtype=torch.float32, device=self.device)\n+                torch.empty(0, dtype=torch.float32, device=self.device)\n                 for _ in range(num_layers)\n             ]\n             finished_requests_ids = [seq.request_id for seq in seqs]\n@@ -1437,10 +1437,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = self.max_batchsize_to_capture\n-        input_tokens = torch.zeros(max_batch_size,\n+        input_tokens = torch.empty(max_batch_size,\n                                    dtype=torch.long,\n                                    device=self.device)\n-        input_positions = torch.zeros(max_batch_size,\n+        input_positions = torch.empty(max_batch_size,\n                                       dtype=torch.long,\n                                       device=self.device)\n         if self.model_config.uses_mrope:\n@@ -1568,12 +1568,12 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         \"\"\"\n         # During the decode phase encoder_input_ids and encoder_positions are\n         # unset. Do the same thing for graph capture.\n-        capture_inputs[\"encoder_input_ids\"] = torch.tensor([],\n-                                                           dtype=torch.long,\n-                                                           device=self.device)\n-        capture_inputs[\"encoder_positions\"] = torch.tensor([],\n-                                                           dtype=torch.long,\n-                                                           device=self.device)\n+        capture_inputs[\"encoder_input_ids\"] = torch.empty(0,\n+                                                          dtype=torch.long,\n+                                                          device=self.device)\n+        capture_inputs[\"encoder_positions\"] = torch.empty(0,\n+                                                          dtype=torch.long,\n+                                                          device=self.device)\n \n     @property\n     def vocab_size(self) -> int:\n@@ -1750,8 +1750,10 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                     model_forward_end)\n                 orig_model_forward_time = 0.0\n                 if intermediate_tensors is not None:\n-                    orig_model_forward_time = intermediate_tensors.tensors.get(\n-                        \"model_forward_time\", torch.tensor(0.0)).item()\n+                    model_forward_time_tensor = intermediate_tensors.tensors.get(\n+                        \"model_forward_time\", None)\n+                    if model_forward_time_tensor is not None:\n+                        orig_model_forward_time = model_forward_time_tensor.item()\n                 hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                     torch.tensor(model_forward_time + orig_model_forward_time))\n             return hidden_or_intermediate_states\n@@ -1778,8 +1780,10 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                 model_forward_end)\n             orig_model_forward_time = 0.0\n             if intermediate_tensors is not None:\n-                orig_model_forward_time = intermediate_tensors.tensors.get(\n-                    \"model_forward_time\", torch.tensor(0.0)).item()\n+                model_forward_time_tensor = intermediate_tensors.tensors.get(\n+                    \"model_forward_time\", None)\n+                if model_forward_time_tensor is not None:\n+                    orig_model_forward_time = model_forward_time_tensor.item()\n             # If there are multiple workers, we are still tracking the latency\n             # from the start time of the driver worker to the end time of the\n             # driver worker. The model forward time will then end up covering\n", "model_name_or_path": "gpt-5-2025-08-07"}
