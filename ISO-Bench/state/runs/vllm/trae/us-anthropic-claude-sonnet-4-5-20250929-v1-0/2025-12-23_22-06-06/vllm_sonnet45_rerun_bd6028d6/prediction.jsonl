{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-802329d", "model_patch": "diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9d..a518777 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -51,8 +51,12 @@ class Llama4MoE(nn.Module):\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         router_scores, router_indices = torch.topk(gating_output, topk, dim=-1)\n-        router_scores = torch.sigmoid(router_scores.float()).to(\n-            hidden_states.dtype)\n+        # Optimize: avoid intermediate float conversion if already in correct dtype\n+        if router_scores.dtype == hidden_states.dtype:\n+            router_scores = torch.sigmoid(router_scores)\n+        else:\n+            router_scores = torch.sigmoid(router_scores.float()).to(\n+                hidden_states.dtype)\n         return (router_scores, router_indices.to(torch.int32))\n \n     def __init__(self,\n@@ -205,9 +209,10 @@ class Llama4Attention(nn.Module):\n         )\n \n     def _get_attn_scale(self, positions: torch.Tensor) -> torch.Tensor:\n-        floor = torch.floor((positions + 1.0) / self.floor_scale)\n-        attn_scale = torch.log(floor + 1.0) * self.attn_scale + 1.0\n-\n+        # Optimize: fuse operations to reduce intermediate tensor allocations\n+        # floor = torch.floor((positions + 1.0) / self.floor_scale)\n+        # attn_scale = torch.log(floor + 1.0) * self.attn_scale + 1.0\n+        attn_scale = torch.log(torch.floor((positions + 1.0) / self.floor_scale) + 1.0).mul_(self.attn_scale).add_(1.0)\n         return attn_scale.unsqueeze(-1)\n \n     def forward(\n@@ -221,10 +226,13 @@ class Llama4Attention(nn.Module):\n         if self.rotary_emb is not None:\n             q, k = self.rotary_emb(positions, q, k)\n         if self.qk_norm is not None:\n+            # Optimize: cache original dtype to avoid repeated attribute access\n+            q_dtype = q.dtype\n             q = q.reshape(-1, self.num_heads, self.head_dim)\n-            q = self.qk_norm(q.float()).reshape(-1, self.q_size).to(q.dtype)\n+            q = self.qk_norm(q.float()).reshape(-1, self.q_size).to(q_dtype)\n+            k_dtype = k.dtype\n             k = k.reshape(-1, self.num_kv_heads, self.head_dim)\n-            k = self.qk_norm(k.float()).reshape(-1, self.kv_size).to(k.dtype)\n+            k = self.qk_norm(k.float()).reshape(-1, self.kv_size).to(k_dtype)\n \n         # We are applying temperature tuning (https://arxiv.org/abs/2501.19399)\n         # to NoPE layers, where the inference-time temperature tuning function\n@@ -236,7 +244,8 @@ class Llama4Attention(nn.Module):\n         # and (before) attention.\n         if self.attn_temperature_tuning and self.nope:\n             attn_scale = self._get_attn_scale(positions)\n-            q = (q * attn_scale).to(q.dtype)\n+            # Optimize: use in-place multiplication if possible, avoid redundant .to()\n+            q = q * attn_scale\n         attn_output = self.attn(q, k, v)\n         output, _ = self.o_proj(attn_output)\n         return output\n@@ -371,10 +380,8 @@ class Llama4Model(LlamaModel):\n                 expert_map = self.layers[\n                     layer_idx].feed_forward.experts.expert_map\n                 if expert_map is not None:\n-                    local_expert_indices = (expert_map != -1) \\\n-                                            .nonzero() \\\n-                                            .flatten() \\\n-                                            .to(new_loaded_weight.device)\n+                    # Optimize: use nonzero(as_tuple=True)[0] instead of nonzero().flatten()\n+                    local_expert_indices = (expert_map != -1).nonzero(as_tuple=True)[0].to(new_loaded_weight.device)\n                     new_loaded_weight = new_loaded_weight[local_expert_indices]\n                     expert_id = local_expert_indices[0].item()\n             else:\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434..50a528c 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -470,7 +470,9 @@ def merge_multimodal_embeddings(\n         This updates ``inputs_embeds`` in place.\n     \"\"\"\n     if isinstance(placeholder_token_id, list):\n+        # Optimize: specify dtype to avoid implicit conversion\n         placeholder_token_id = torch.tensor(placeholder_token_id,\n+                                            dtype=input_ids.dtype,\n                                             device=input_ids.device)\n         return _merge_multimodal_embeddings(\n             inputs_embeds,\n@@ -654,7 +656,7 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):\n     ) -> IntermediateTensors:\n         return IntermediateTensors({\n             key:\n-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)\n+            torch.empty((batch_size, hidden_size), dtype=dtype, device=device)\n             for key in keys\n         })\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
