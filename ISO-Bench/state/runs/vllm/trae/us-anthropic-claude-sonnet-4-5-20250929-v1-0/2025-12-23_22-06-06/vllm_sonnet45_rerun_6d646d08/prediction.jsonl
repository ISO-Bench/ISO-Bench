{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-95a178f", "model_patch": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 87b3d21..99a8a2b 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1150,7 +1150,7 @@ class HiddenStates(msgspec.Struct, array_like=True,\n             # Adding dummy hidden_states to this to maintain same shape\n             self.second_last_token_hidden_states = torch.cat([\n                 self.second_last_token_hidden_states,\n-                torch.zeros_like(hidden_states)\n+                torch.empty_like(hidden_states)\n                 if second_last_token_hidden_states is None else\n                 second_last_token_hidden_states\n             ])\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 8a3c99a..eb0032f 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1225,8 +1225,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = self.max_batchsize_to_capture\n-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n+        input_tokens = torch.empty(max_batch_size, dtype=torch.long).cuda()\n+        input_positions = torch.empty(max_batch_size, dtype=torch.long).cuda()\n \n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex be0c75b..2c1e7c8 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -298,7 +298,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         # if CPU is ahead.\n         if self.is_driver_worker and get_pp_group().is_last_rank:\n             if self.pinned_sampled_token_ids is None:\n-                self.pinned_sampled_token_ids = torch.zeros(\n+                self.pinned_sampled_token_ids = torch.empty(\n                     (self.scheduler_config.max_num_seqs, 1),\n                     dtype=torch.long,\n                     device=\"cpu\",\n", "model_name_or_path": "gpt-5-2025-08-07"}
