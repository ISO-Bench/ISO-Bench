{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-b0e96aa", "model_patch": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex 7978730..2663871 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -15,6 +15,7 @@ import importlib.metadata\n import importlib.util\n import inspect\n import ipaddress\n+import itertools\n import multiprocessing\n import os\n import re\n@@ -327,15 +328,12 @@ class PyObjectCache:\n         self._obj_builder = obj_builder\n         self._index = 0\n \n-        self._obj_cache = []\n-        for _ in range(128):\n-            self._obj_cache.append(self._obj_builder())\n+        self._obj_cache = [self._obj_builder() for _ in range(128)]\n \n     def _grow_cache(self):\n         # Double the size of the cache\n         num_objs = len(self._obj_cache)\n-        for _ in range(num_objs):\n-            self._obj_cache.append(self._obj_builder())\n+        self._obj_cache.extend([self._obj_builder() for _ in range(num_objs)])\n \n     def get_object(self):\n         \"\"\"Returns a pre-allocated cached object. If there is not enough\n@@ -658,10 +656,10 @@ def create_kv_caches_with_random_flash(\n     key_value_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n     scale = head_size**-0.5\n \n-    key_caches: list[torch.Tensor] = []\n-    value_caches: list[torch.Tensor] = []\n+    key_caches: list[torch.Tensor] = [None] * num_layers  # type: ignore\n+    value_caches: list[torch.Tensor] = [None] * num_layers  # type: ignore\n \n-    for _ in range(num_layers):\n+    for i in range(num_layers):\n         key_value_cache = torch.empty(size=key_value_cache_shape,\n                                       dtype=torch_dtype,\n                                       device=device)\n@@ -672,8 +670,8 @@ def create_kv_caches_with_random_flash(\n         else:\n             raise ValueError(\n                 f\"Does not support key cache of type {cache_dtype}\")\n-        key_caches.append(key_value_cache[:, 0])\n-        value_caches.append(key_value_cache[:, 1])\n+        key_caches[i] = key_value_cache[:, 0]\n+        value_caches[i] = key_value_cache[:, 1]\n     return key_caches, value_caches\n \n \n@@ -699,10 +697,10 @@ def create_kv_caches_with_random(\n     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n \n     scale = head_size**-0.5\n-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n+    x = 16 // get_dtype_size(torch_dtype)\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n-    key_caches: list[torch.Tensor] = []\n-    for _ in range(num_layers):\n+    key_caches: list[torch.Tensor] = [None] * num_layers  # type: ignore\n+    for i in range(num_layers):\n         key_cache = torch.empty(size=key_cache_shape,\n                                 dtype=torch_dtype,\n                                 device=device)\n@@ -713,11 +711,11 @@ def create_kv_caches_with_random(\n         else:\n             raise ValueError(\n                 f\"Does not support key cache of type {cache_dtype}\")\n-        key_caches.append(key_cache)\n+        key_caches[i] = key_cache\n \n     value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n-    value_caches: list[torch.Tensor] = []\n-    for _ in range(num_layers):\n+    value_caches: list[torch.Tensor] = [None] * num_layers  # type: ignore\n+    for i in range(num_layers):\n         value_cache = torch.empty(size=value_cache_shape,\n                                   dtype=torch_dtype,\n                                   device=device)\n@@ -728,7 +726,7 @@ def create_kv_caches_with_random(\n         else:\n             raise ValueError(\n                 f\"Does not support value cache of type {cache_dtype}\")\n-        value_caches.append(value_cache)\n+        value_caches[i] = value_cache\n     return key_caches, value_caches\n \n \n@@ -778,10 +776,13 @@ def make_ndarray_with_pad(\n         # Unlike for most functions, map is faster than a genexpr over `len`\n         max_len = max(map(len, x), default=0)\n \n-    padded_x = np.full((len(x), max_len), pad, dtype=dtype)\n-    for ind, blocktb in enumerate(x):\n-        assert len(blocktb) <= max_len\n-        padded_x[ind, :len(blocktb)] = blocktb\n+    num_rows = len(x)\n+    padded_x = np.full((num_rows, max_len), pad, dtype=dtype)\n+    for ind in range(num_rows):\n+        blocktb = x[ind]\n+        blocktb_len = len(blocktb)\n+        assert blocktb_len <= max_len\n+        padded_x[ind, :blocktb_len] = blocktb\n \n     return padded_x\n \n@@ -822,6 +823,7 @@ def async_tensor_h2d(\n     return t.to(device=target_device, non_blocking=True)\n \n \n+@lru_cache(maxsize=32)\n def get_dtype_size(dtype: torch.dtype) -> int:\n     \"\"\"Get the size of the data type in bytes.\"\"\"\n     return torch.tensor([], dtype=dtype).element_size()\n@@ -847,7 +849,7 @@ def is_list_of(\n \n def flatten_2d_lists(lists: Iterable[Iterable[T]]) -> list[T]:\n     \"\"\"Flatten a list of lists to a single list.\"\"\"\n-    return [item for sublist in lists for item in sublist]\n+    return list(itertools.chain.from_iterable(lists))\n \n \n def full_groupby(values: Iterable[_V], *, key: Callable[[_V], _K]):\n", "model_name_or_path": "gpt-5-2025-08-07"}
