diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py
index 36e5e17..4eb6ec5 100644
--- a/vllm/spec_decode/ngram_worker.py
+++ b/vllm/spec_decode/ngram_worker.py
@@ -66,11 +66,13 @@ class NGramWorker(NonLLMProposerWorkerBase):
         for idx, seq_group_metadata in enumerate(
                 execute_model_req.seq_group_metadata_list):
             seq_data = next(iter(seq_group_metadata.seq_data.values()))
-
-            input_ids = torch.as_tensor(seq_data.get_token_ids(),
+            
+            # Optimize: Get token_ids once to avoid multiple calls
+            token_ids = seq_data.get_token_ids()
+            input_ids = torch.as_tensor(token_ids,
                                         dtype=torch.long,
                                         device=self.device)
-            input_length = seq_data.get_len()
+            input_length = len(token_ids)
 
             for ngram_size in range(
                     min(self.ngram_prompt_lookup_max, input_length - 1),
@@ -97,16 +99,18 @@ class NGramWorker(NonLLMProposerWorkerBase):
                 first_match = matches.max(dim=-1)
                 if first_match.values.item():
                     proposal_start_idx = first_match.indices.add_(ngram_size)
-                    spec_indices = (
-                        proposal_start_idx).repeat(sample_len) + torch.arange(
-                            sample_len, device=self.device)
+                    # Optimize: Use addcdiv pattern to avoid intermediate repeat
+                    spec_indices = torch.arange(
+                        sample_len, device=self.device, dtype=torch.long)
+                    spec_indices.add_(proposal_start_idx)
                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)
                     res = input_ids.gather(dim=-1, index=spec_indices)
                     token_id_list.append(res)
+                    # Optimize: Avoid dtype conversion by using float32 directly
                     token_prob_list.append(
                         torch.nn.functional.one_hot(
                             res,
-                            num_classes=self.vocab_size).to(torch.float32))
+                            num_classes=self.vocab_size).float())
                     has_spec_out = True
                     break
             else:
@@ -125,7 +129,7 @@ class NGramWorker(NonLLMProposerWorkerBase):
                     SamplerOutput(
                         outputs=None,
                         sampled_token_probs=token_prob_list[idx],
-                        logprobs=torch.zeros((sample_len, self.vocab_size),
+                        logprobs=torch.empty((sample_len, self.vocab_size),
                                              dtype=torch.float32,
                                              device=self.device),
                         sampled_token_ids=token_id_list[idx],
