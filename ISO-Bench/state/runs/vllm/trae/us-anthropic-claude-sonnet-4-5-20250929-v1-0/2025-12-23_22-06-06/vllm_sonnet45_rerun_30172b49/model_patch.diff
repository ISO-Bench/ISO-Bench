diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py
index dfe7102..f0b9d92 100644
--- a/vllm/model_executor/layers/utils.py
+++ b/vllm/model_executor/layers/utils.py
@@ -12,9 +12,10 @@ def get_token_bin_counts_and_mask(
 ) -> Tuple[torch.Tensor, torch.Tensor]:
     # Compute the bin counts for the tokens.
     # vocab_size + 1 for padding.
-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),
+    bin_counts = torch.empty((num_seqs, vocab_size + 1),
                              dtype=torch.long,
                              device=tokens.device)
+    bin_counts.fill_(0)
     bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))
     bin_counts = bin_counts[:, :vocab_size]
     mask = bin_counts > 0
diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py
index df1da89..20496aa 100644
--- a/vllm/v1/sample/rejection_sampler.py
+++ b/vllm/v1/sample/rejection_sampler.py
@@ -100,9 +100,10 @@ class RejectionSampler(nn.Module):
                                                  logits.device)
         target_probs = _create_greedy_token_probs(target_token_ids, vocab_size,
                                                   logits.device)
-        uniform_samples = torch.zeros(batch_size,
+        uniform_samples = torch.empty(batch_size,
                                       max_spec_len + 1,
                                       device=logits.device)
+        uniform_samples.fill_(0)
 
         sampled_token_ids, _, _ = fs.chain_speculative_sampling(
             draft_probs,
@@ -147,9 +148,11 @@ class RejectionSampler(nn.Module):
         # Identify valid positions (non-padding).
         valid_mask = output_token_ids != INVALID_TOKEN_ID
         # Generate mask with bonus token.
+        bonus_zeros = torch.empty(accept_mask.size(0), 1, device=accept_mask.device)
+        bonus_zeros.fill_(0)
         generate_mask = torch.cat([
             accept_mask,
-            torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)
+            bonus_zeros
         ],
                                   dim=1).to(torch.bool) & valid_mask
         zeros_mask = (generate_mask == 0)
@@ -171,16 +174,16 @@ def _create_greedy_token_probs(
 ) -> torch.Tensor:
     batch_size, num_tokens = token_ids.shape
 
-    token_probs = torch.zeros(batch_size,
+    token_probs = torch.empty(batch_size,
                               num_tokens,
                               vocab_size,
                               dtype=torch.float,
                               device=out_device)
+    token_probs.fill_(0)
 
     # Ignore INVALID_TOKEN_ID.
     valid_mask = (token_ids != INVALID_TOKEN_ID)
-    valid_indices = token_ids.clone()
-    valid_indices[~valid_mask] = 0
+    valid_indices = torch.where(valid_mask, token_ids, 0)
 
     token_probs.scatter_(dim=2,
                          index=valid_indices.unsqueeze(-1),
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index cb7411a..4cd4efb 100644
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -70,7 +70,7 @@ class InputBatch:
         # Find a way to reduce the CPU memory usage.
         # This buffer is not directly transferred to the GPU, so it does not
         # need to be pinned.
-        self.token_ids_cpu_tensor = torch.zeros(
+        self.token_ids_cpu_tensor = torch.empty(
             (max_num_reqs, max_model_len),
             device="cpu",
             dtype=torch.int32,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5754422..096430c 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -155,10 +155,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         self.num_sms = self.device_properties.multi_processor_count
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
         # None in the first PP rank. The rest are set after load_model.
@@ -176,16 +176,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             # identical position IDs, making M-RoPE functionally equivalent to
             # 1D-RoPE.
             # See page 5 of https://arxiv.org/abs/2409.12191
-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
                 pin_memory=self.pin_memory)
 
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -198,27 +198,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
@@ -1382,9 +1382,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,
                     layer_spec.head_size)
                 dtype = layer_spec.dtype
-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,
+                kv_caches[layer_name] = torch.empty(kv_cache_shape,
                                                     dtype=dtype,
                                                     device=self.device)
+                kv_caches[layer_name].fill_(0)
             else:
                 raise NotImplementedError
 
diff --git a/vllm/v1/worker/tpu_model_runner.py b/vllm/v1/worker/tpu_model_runner.py
index 4ee6853..a0a126a 100644
--- a/vllm/v1/worker/tpu_model_runner.py
+++ b/vllm/v1/worker/tpu_model_runner.py
@@ -743,13 +743,13 @@ class TPUModelRunner:
         exec_mode = ExecutionMode(exec_mode)
         if exec_mode.is_prefill():
             seq_len = (seq_len + 15) // 16 * 16
-            token_ids = torch.zeros((num_tokens, seq_len),
+            token_ids = torch.empty((num_tokens, seq_len),
                                     dtype=torch.int32,
                                     device=self.device)
-            position_ids = torch.zeros((num_tokens, seq_len),
+            position_ids = torch.empty((num_tokens, seq_len),
                                        dtype=torch.int32,
                                        device=self.device)
-            slot_mapping = torch.zeros((num_tokens, seq_len),
+            slot_mapping = torch.empty((num_tokens, seq_len),
                                        dtype=torch.int64,
                                        device=self.device)
             if exec_mode == ExecutionMode.PREFILL:
@@ -770,7 +770,7 @@ class TPUModelRunner:
                                           dtype=torch.int32,
                                           device=self.device)
 
-                block_tables = torch.zeros(
+                block_tables = torch.empty(
                     (num_tokens, self.max_num_blocks_per_req),
                     dtype=torch.int32,
                     device=self.device)
@@ -790,16 +790,16 @@ class TPUModelRunner:
                 )
         else:
             assert seq_len == 1
-            token_ids = torch.zeros((num_tokens, seq_len),
+            token_ids = torch.empty((num_tokens, seq_len),
                                     dtype=torch.int32,
                                     device=self.device)
-            position_ids = torch.zeros((num_tokens, seq_len),
+            position_ids = torch.empty((num_tokens, seq_len),
                                        dtype=torch.int32,
                                        device=self.device)
-            slot_mapping = torch.zeros((num_tokens, seq_len),
+            slot_mapping = torch.empty((num_tokens, seq_len),
                                        dtype=torch.int64,
                                        device=self.device)
-            block_tables = torch.zeros(
+            block_tables = torch.empty(
                 (num_tokens, self.max_num_blocks_per_req),
                 dtype=torch.int32,
                 device=self.device)
@@ -938,10 +938,12 @@ class TPUModelRunner:
                     layer_spec.head_size)
                 dtype = layer_spec.dtype
 
-                tpu_k_cache = torch.zeros(kv_cache_shape,
+                tpu_k_cache = torch.empty(kv_cache_shape,
                                           dtype=dtype,
                                           device=self.device)
-                tpu_v_cache = torch.zeros_like(tpu_k_cache)
+                tpu_k_cache.fill_(0)
+                tpu_v_cache = torch.empty_like(tpu_k_cache)
+                tpu_v_cache.fill_(0)
 
                 kv_caches[layer_name] = (tpu_k_cache, tpu_v_cache)
             else:
