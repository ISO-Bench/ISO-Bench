{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-a4d577b", "model_patch": "diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex dfe7102..f0b9d92 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -12,9 +12,10 @@ def get_token_bin_counts_and_mask(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     # Compute the bin counts for the tokens.\n     # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n+    bin_counts = torch.empty((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n+    bin_counts.fill_(0)\n     bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n     bin_counts = bin_counts[:, :vocab_size]\n     mask = bin_counts > 0\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex df1da89..20496aa 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -100,9 +100,10 @@ class RejectionSampler(nn.Module):\n                                                  logits.device)\n         target_probs = _create_greedy_token_probs(target_token_ids, vocab_size,\n                                                   logits.device)\n-        uniform_samples = torch.zeros(batch_size,\n+        uniform_samples = torch.empty(batch_size,\n                                       max_spec_len + 1,\n                                       device=logits.device)\n+        uniform_samples.fill_(0)\n \n         sampled_token_ids, _, _ = fs.chain_speculative_sampling(\n             draft_probs,\n@@ -147,9 +148,11 @@ class RejectionSampler(nn.Module):\n         # Identify valid positions (non-padding).\n         valid_mask = output_token_ids != INVALID_TOKEN_ID\n         # Generate mask with bonus token.\n+        bonus_zeros = torch.empty(accept_mask.size(0), 1, device=accept_mask.device)\n+        bonus_zeros.fill_(0)\n         generate_mask = torch.cat([\n             accept_mask,\n-            torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)\n+            bonus_zeros\n         ],\n                                   dim=1).to(torch.bool) & valid_mask\n         zeros_mask = (generate_mask == 0)\n@@ -171,16 +174,16 @@ def _create_greedy_token_probs(\n ) -> torch.Tensor:\n     batch_size, num_tokens = token_ids.shape\n \n-    token_probs = torch.zeros(batch_size,\n+    token_probs = torch.empty(batch_size,\n                               num_tokens,\n                               vocab_size,\n                               dtype=torch.float,\n                               device=out_device)\n+    token_probs.fill_(0)\n \n     # Ignore INVALID_TOKEN_ID.\n     valid_mask = (token_ids != INVALID_TOKEN_ID)\n-    valid_indices = token_ids.clone()\n-    valid_indices[~valid_mask] = 0\n+    valid_indices = torch.where(valid_mask, token_ids, 0)\n \n     token_probs.scatter_(dim=2,\n                          index=valid_indices.unsqueeze(-1),\ndiff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex cb7411a..4cd4efb 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -70,7 +70,7 @@ class InputBatch:\n         # Find a way to reduce the CPU memory usage.\n         # This buffer is not directly transferred to the GPU, so it does not\n         # need to be pinned.\n-        self.token_ids_cpu_tensor = torch.zeros(\n+        self.token_ids_cpu_tensor = torch.empty(\n             (max_num_reqs, max_model_len),\n             device=\"cpu\",\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 5754422..096430c 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -155,10 +155,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -176,16 +176,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -198,27 +198,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1382,9 +1382,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,\n                     layer_spec.head_size)\n                 dtype = layer_spec.dtype\n-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n+                kv_caches[layer_name] = torch.empty(kv_cache_shape,\n                                                     dtype=dtype,\n                                                     device=self.device)\n+                kv_caches[layer_name].fill_(0)\n             else:\n                 raise NotImplementedError\n \ndiff --git a/vllm/v1/worker/tpu_model_runner.py b/vllm/v1/worker/tpu_model_runner.py\nindex 4ee6853..a0a126a 100644\n--- a/vllm/v1/worker/tpu_model_runner.py\n+++ b/vllm/v1/worker/tpu_model_runner.py\n@@ -743,13 +743,13 @@ class TPUModelRunner:\n         exec_mode = ExecutionMode(exec_mode)\n         if exec_mode.is_prefill():\n             seq_len = (seq_len + 15) // 16 * 16\n-            token_ids = torch.zeros((num_tokens, seq_len),\n+            token_ids = torch.empty((num_tokens, seq_len),\n                                     dtype=torch.int32,\n                                     device=self.device)\n-            position_ids = torch.zeros((num_tokens, seq_len),\n+            position_ids = torch.empty((num_tokens, seq_len),\n                                        dtype=torch.int32,\n                                        device=self.device)\n-            slot_mapping = torch.zeros((num_tokens, seq_len),\n+            slot_mapping = torch.empty((num_tokens, seq_len),\n                                        dtype=torch.int64,\n                                        device=self.device)\n             if exec_mode == ExecutionMode.PREFILL:\n@@ -770,7 +770,7 @@ class TPUModelRunner:\n                                           dtype=torch.int32,\n                                           device=self.device)\n \n-                block_tables = torch.zeros(\n+                block_tables = torch.empty(\n                     (num_tokens, self.max_num_blocks_per_req),\n                     dtype=torch.int32,\n                     device=self.device)\n@@ -790,16 +790,16 @@ class TPUModelRunner:\n                 )\n         else:\n             assert seq_len == 1\n-            token_ids = torch.zeros((num_tokens, seq_len),\n+            token_ids = torch.empty((num_tokens, seq_len),\n                                     dtype=torch.int32,\n                                     device=self.device)\n-            position_ids = torch.zeros((num_tokens, seq_len),\n+            position_ids = torch.empty((num_tokens, seq_len),\n                                        dtype=torch.int32,\n                                        device=self.device)\n-            slot_mapping = torch.zeros((num_tokens, seq_len),\n+            slot_mapping = torch.empty((num_tokens, seq_len),\n                                        dtype=torch.int64,\n                                        device=self.device)\n-            block_tables = torch.zeros(\n+            block_tables = torch.empty(\n                 (num_tokens, self.max_num_blocks_per_req),\n                 dtype=torch.int32,\n                 device=self.device)\n@@ -938,10 +938,12 @@ class TPUModelRunner:\n                     layer_spec.head_size)\n                 dtype = layer_spec.dtype\n \n-                tpu_k_cache = torch.zeros(kv_cache_shape,\n+                tpu_k_cache = torch.empty(kv_cache_shape,\n                                           dtype=dtype,\n                                           device=self.device)\n-                tpu_v_cache = torch.zeros_like(tpu_k_cache)\n+                tpu_k_cache.fill_(0)\n+                tpu_v_cache = torch.empty_like(tpu_k_cache)\n+                tpu_v_cache.fill_(0)\n \n                 kv_caches[layer_name] = (tpu_k_cache, tpu_v_cache)\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
