{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3d92516", "model_patch": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 4198550..eadecc8 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -66,7 +66,7 @@ class SchedulingBudget:\n     def subtract_num_batched_tokens(self, req_id: str,\n                                     num_batched_tokens: int):\n         if req_id in self._requeset_ids_num_batched_tokens:\n-            self._requeset_ids_num_batched_tokens.remove(req_id)\n+            self._requeset_ids_num_batched_tokens.discard(req_id)\n             self._num_batched_tokens -= num_batched_tokens\n \n     def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n@@ -78,7 +78,7 @@ class SchedulingBudget:\n \n     def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n         if req_id in self._requeset_ids_num_curr_seqs:\n-            self._requeset_ids_num_curr_seqs.remove(req_id)\n+            self._requeset_ids_num_curr_seqs.discard(req_id)\n             self._num_curr_seqs -= num_curr_seqs\n \n     @property\n@@ -326,7 +326,7 @@ class Scheduler:\n                 if seq_group.request_id in request_ids:\n                     # Appending aborted group into pending list.\n                     aborted_groups.append(seq_group)\n-                    request_ids.remove(seq_group.request_id)\n+                    request_ids.discard(seq_group.request_id)\n             for aborted_group in aborted_groups:\n                 # Remove the sequence group from the state queue.\n                 state_queue.remove(aborted_group)\n@@ -594,10 +594,11 @@ class Scheduler:\n         seq_groups: List[SequenceGroup] = []\n         # We don't sort waiting queue because we assume it is sorted.\n         # Copy the queue so that the input queue is not modified.\n-        waiting_queue = deque([s for s in waiting_queue])\n+        waiting_queue = deque(waiting_queue)\n \n         leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n-        while self._passed_delay(time.time()) and waiting_queue:\n+        now = time.time()\n+        while self._passed_delay(now) and waiting_queue:\n             seq_group = waiting_queue[0]\n \n             waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n@@ -979,9 +980,7 @@ class Scheduler:\n             cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n \n             for src, dests in cows.items():\n-                if src not in blocks_to_copy:\n-                    blocks_to_copy[src] = []\n-                blocks_to_copy[src].extend(dests)\n+                blocks_to_copy.setdefault(src, []).extend(dests)\n \n     def _preempt(\n         self,\n@@ -1064,7 +1063,7 @@ class Scheduler:\n         # Delay scheduling prompts to let waiting queue fill up\n         if self.scheduler_config.delay_factor > 0 and self.waiting:\n             earliest_arrival_time = min(\n-                [e.metrics.arrival_time for e in self.waiting])\n+                (e.metrics.arrival_time for e in self.waiting))\n             passed_delay = (\n                 (now - earliest_arrival_time) >\n                 (self.scheduler_config.delay_factor * self.last_prompt_latency)\n", "model_name_or_path": "gpt-5-2025-08-07"}
