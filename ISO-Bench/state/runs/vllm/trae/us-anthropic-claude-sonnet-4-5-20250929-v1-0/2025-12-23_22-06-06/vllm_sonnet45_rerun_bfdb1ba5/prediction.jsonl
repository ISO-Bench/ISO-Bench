{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-cf2f084", "model_patch": "diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 4421739..b1a80a9 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -25,7 +25,7 @@ TOKENIZERS = [\n \n def _run_incremental_decode(tokenizer, all_input_ids,\n                             skip_special_tokens: bool):\n-    decoded_text = \"\"\n+    decoded_texts = []\n     offset = 0\n     token_offset = 0\n     prev_tokens = None\n@@ -37,12 +37,12 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n             offset,\n             token_offset,\n             skip_special_tokens=skip_special_tokens)\n-        decoded_text += text\n+        decoded_texts.append(text)\n         if prev_tokens is None:\n             prev_tokens = new_tokens\n         else:\n-            prev_tokens += new_tokens\n-    return decoded_text\n+            prev_tokens.extend(new_tokens)\n+    return \"\".join(decoded_texts)\n \n \n @pytest.mark.parametrize(\"truth\", TRUTH)\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19..f540cff 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -26,6 +26,8 @@ def get_cached_tokenizer(\n     tokenizer_all_special_tokens_extended = (\n         tokenizer.all_special_tokens_extended)\n     tokenizer_all_special_tokens = set(tokenizer.all_special_tokens)\n+    tokenizer_added_vocab = tokenizer.get_added_vocab()\n+    tokenizer_is_fast = tokenizer.is_fast\n \n     class CachedTokenizer(tokenizer.__class__):\n \n@@ -41,6 +43,13 @@ def get_cached_tokenizer(\n         def all_special_tokens_extended(self):\n             return tokenizer_all_special_tokens_extended\n \n+        def get_added_vocab(self):\n+            return tokenizer_added_vocab\n+\n+        @property\n+        def is_fast(self):\n+            return tokenizer_is_fast\n+\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     tokenizer.__class__ = CachedTokenizer\n@@ -137,7 +146,8 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    # Optimization: tokenizer.all_special_tokens is already a set from get_cached_tokenizer\n+    all_special_tokens = tokenizer.all_special_tokens\n     for token in output_tokens:\n         if skip_special_tokens and token in all_special_tokens:\n             continue\n@@ -189,7 +199,9 @@ def detokenize_incrementally(\n         # Put new_token_id in a list so skip_special_tokens is respected\n         new_tokens = tokenizer.convert_ids_to_tokens(\n             [new_token_id], skip_special_tokens=skip_special_tokens)\n-        output_tokens = prev_tokens + new_tokens\n+        # Optimization: Use list copy and extend instead of concatenation\n+        output_tokens = prev_tokens.copy()\n+        output_tokens.extend(new_tokens)\n \n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n", "model_name_or_path": "gpt-5-2025-08-07"}
