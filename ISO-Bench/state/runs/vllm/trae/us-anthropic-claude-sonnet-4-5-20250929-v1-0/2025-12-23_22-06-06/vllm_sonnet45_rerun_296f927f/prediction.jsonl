{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0032903", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex fec6d61..145ccaa 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -443,9 +443,6 @@ class MambaMixer2(CustomOp):\n                 cache_indices=mamba_cache_params.state_indices_tensor,\n                 query_start_loc=attn_metadata.query_start_loc).transpose(\n                     0, 1)[:seq_len]\n-\n-            # TODO: Why is this needed?\n-            hidden_states_B_C = hidden_states_B_C.contiguous()\n         else:\n             hidden_states_B_C = causal_conv1d_update(\n                 hidden_states_B_C,\n@@ -471,9 +468,11 @@ class MambaMixer2(CustomOp):\n \n             initial_states = None\n             if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+                # Optimize: use advanced indexing instead of loop for zeroing\n+                indices_to_zero = mamba_cache_params.state_indices_tensor[\n+                    ~has_initial_states]\n+                if len(indices_to_zero) > 0:\n+                    mamba_cache_params.ssm_state[indices_to_zero] = 0\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -499,19 +498,22 @@ class MambaMixer2(CustomOp):\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            # Optimize: use advanced indexing instead of loop for copying\n+            mamba_cache_params.ssm_state[\n+                mamba_cache_params.state_indices_tensor] = varlen_state\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n         else:\n \n             n_groups = self.n_groups // self.tp_size\n-            A = self.A[:, None, ...][:, :, None].expand(\n+            # Optimize: simplify indexing operations\n+            A = self.A[:, None, None].expand(\n                 -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n-            dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n-            D = self.D[:, None, ...].expand(-1, self.head_dim)\n+            # Optimize: simplify indexing operations\n+            dt_bias = self.dt_bias[:, None].expand(-1, self.head_dim)\n+            D = self.D[:, None].expand(-1, self.head_dim)\n             B = B.view(-1, n_groups, B.shape[1] // n_groups)\n             C = C.view(-1, n_groups, C.shape[1] // n_groups)\n             hidden_states_reshaped = hidden_states.view(\n", "model_name_or_path": "gpt-5-2025-08-07"}
