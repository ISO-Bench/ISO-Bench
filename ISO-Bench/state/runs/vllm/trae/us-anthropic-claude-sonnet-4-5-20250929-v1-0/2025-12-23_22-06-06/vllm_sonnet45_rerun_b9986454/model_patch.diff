diff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py
index 1ae765a..031ee8e 100644
--- a/vllm/model_executor/layers/quantization/moe_wna16.py
+++ b/vllm/model_executor/layers/quantization/moe_wna16.py
@@ -212,7 +212,7 @@ class MoeWNA16Method(FusedMoEMethodBase):
         layer.register_parameter("w2_qweight", w2_qweight)
         set_weight_attrs(w2_qweight, extra_weight_attrs)
 
-        w13_scales = torch.nn.Parameter(torch.zeros(
+        w13_scales = torch.nn.Parameter(torch.empty(
             num_experts,
             2 * intermediate_size_per_partition,
             hidden_size // group_size,
@@ -221,7 +221,7 @@ class MoeWNA16Method(FusedMoEMethodBase):
         layer.register_parameter("w13_scales", w13_scales)
         set_weight_attrs(w13_scales, extra_weight_attrs)
 
-        w2_scales = torch.nn.Parameter(torch.zeros(
+        w2_scales = torch.nn.Parameter(torch.empty(
             num_experts,
             hidden_size,
             intermediate_size_per_partition // group_size,
@@ -231,7 +231,7 @@ class MoeWNA16Method(FusedMoEMethodBase):
         set_weight_attrs(w2_scales, extra_weight_attrs)
 
         if self.quant_config.has_zp:
-            w13_qzeros = torch.nn.Parameter(torch.zeros(
+            w13_qzeros = torch.nn.Parameter(torch.empty(
                 num_experts,
                 2 * intermediate_size_per_partition // bit8_pack_factor,
                 hidden_size // group_size,
@@ -240,7 +240,7 @@ class MoeWNA16Method(FusedMoEMethodBase):
             layer.register_parameter("w13_qzeros", w13_qzeros)
             set_weight_attrs(w13_qzeros, extra_weight_attrs)
 
-            w2_qzeros = torch.nn.Parameter(torch.zeros(
+            w2_qzeros = torch.nn.Parameter(torch.empty(
                 num_experts,
                 hidden_size // bit8_pack_factor,
                 intermediate_size_per_partition // group_size,
