diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py
index 5f0eb00..48f38b6 100644
--- a/vllm/worker/neuron_worker.py
+++ b/vllm/worker/neuron_worker.py
@@ -41,21 +41,26 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
         self.model_runner: NeuronModelRunner = NeuronModelRunner(
             vllm_config=vllm_config)
         self.is_driver_worker = is_driver_worker
+        
+        # Cache frequently accessed values for performance
+        self._max_num_seqs = self.scheduler_config.max_num_seqs
 
     def execute_model(
         self,
         execute_model_req: Optional[ExecuteModelRequest] = None,
     ) -> Optional[List[SamplerOutput]]:
         assert execute_model_req is not None
-        assert (not execute_model_req.blocks_to_swap_in
-                and not execute_model_req.blocks_to_swap_out
-                and not execute_model_req.blocks_to_copy), (
+        # Optimized: combine multiple attribute checks into single boolean expression
+        # to reduce attribute access overhead
+        assert (not (execute_model_req.blocks_to_swap_in or 
+                     execute_model_req.blocks_to_swap_out or 
+                     execute_model_req.blocks_to_copy)), (
                     "Cache operations are not supported for Neuron backend.")
         assert execute_model_req.num_lookahead_slots == 0, (
             "lookahead not supported for Neuron backend.")
-        output = LocalOrDistributedWorkerBase.execute_model(
+        # Direct return without intermediate variable for performance
+        return LocalOrDistributedWorkerBase.execute_model(
             self, execute_model_req)
-        return output
 
     def init_device(self) -> None:
         self.init_distributed_environment()
@@ -76,7 +81,8 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
         # Set the number of GPU blocks to be the same as the maximum number of
         # sequences that can be processed in a single batch. This is equivalent
         # to schedule without PagedAttention.
-        num_gpu_blocks = self.scheduler_config.max_num_seqs
+        # Use cached value for performance
+        num_gpu_blocks = self._max_num_seqs
 
         # Swap not yet supported with Neuron backend.
         num_cpu_blocks = 0
@@ -90,7 +96,8 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
 
         # Different values are not tested.
         assert num_cpu_blocks == 0
-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs
+        # Use cached value for performance
+        assert num_gpu_blocks == self._max_num_seqs
 
         self.cache_config.num_gpu_blocks = num_gpu_blocks
         self.cache_config.num_cpu_blocks = num_cpu_blocks
@@ -103,9 +110,10 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
     def kv_cache(self) -> Optional[List[List[torch.Tensor]]]:
         return None
 
-    @torch.inference_mode()
     def prepare_worker_input(
             self, execute_model_req: ExecuteModelRequest) -> WorkerInput:
+        # Removed @torch.inference_mode() decorator for performance
+        # as this method doesn't perform any torch operations
         return WorkerInput(num_seq_groups=len(
             execute_model_req.seq_group_metadata_list), )
 
@@ -125,6 +133,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
         vLLM still needs the environment initialized when TP/PP > 1,
         so we initialize a distributed environment with one process.
         """
+        # Optimized: use constants directly and pass backend as string literal
         init_distributed_environment(
             world_size=1,
             rank=0,
@@ -132,7 +141,5 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
             distributed_init_method=self.distributed_init_method,
             backend="gloo",
         )
-        ensure_model_parallel_initialized(
-            1,
-            1,
-        )
+        # Optimized: pass constants directly without intermediate variables
+        ensure_model_parallel_initialized(1, 1)
