{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3014c92", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 9de2338..9a23350 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -56,8 +56,14 @@ def _apply_rotary_emb_torch(\n     sin: torch.Tensor,\n     is_neox_style: bool,\n ) -> torch.Tensor:\n-    cos = cos.unsqueeze(-2).to(x.dtype)\n-    sin = sin.unsqueeze(-2).to(x.dtype)\n+    # Optimize: convert dtype once and unsqueeze in-place\n+    if cos.dtype != x.dtype:\n+        cos = cos.to(x.dtype)\n+    if sin.dtype != x.dtype:\n+        sin = sin.to(x.dtype)\n+    cos = cos.unsqueeze(-2)\n+    sin = sin.unsqueeze(-2)\n+    \n     if is_neox_style:\n         x1, x2 = torch.chunk(x, 2, dim=-1)\n     else:\n@@ -152,21 +158,33 @@ class RotaryEmbedding(CustomOp):\n \n         query_shape = query.shape\n         query = query.view(num_tokens, -1, self.head_size)\n-        query_rot = query[..., :self.rotary_dim]\n-        query_pass = query[..., self.rotary_dim:]\n-        query_rot = _apply_rotary_emb_torch(query_rot, cos, sin,\n-                                            self.is_neox_style)\n-        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n+        \n+        # Optimize: avoid cat/reshape when rotary_dim == head_size\n+        if self.rotary_dim == self.head_size:\n+            query = _apply_rotary_emb_torch(query, cos, sin, self.is_neox_style)\n+            query = query.reshape(query_shape)\n+        else:\n+            query_rot = query[..., :self.rotary_dim]\n+            query_pass = query[..., self.rotary_dim:]\n+            query_rot = _apply_rotary_emb_torch(query_rot, cos, sin,\n+                                                self.is_neox_style)\n+            query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n \n         # key may be None in some cases, e.g. cross-layer KV sharing\n         if key is not None:\n             key_shape = key.shape\n             key = key.view(num_tokens, -1, self.head_size)\n-            key_rot = key[..., :self.rotary_dim]\n-            key_pass = key[..., self.rotary_dim:]\n-            key_rot = _apply_rotary_emb_torch(key_rot, cos, sin,\n-                                              self.is_neox_style)\n-            key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n+            \n+            # Optimize: avoid cat/reshape when rotary_dim == head_size\n+            if self.rotary_dim == self.head_size:\n+                key = _apply_rotary_emb_torch(key, cos, sin, self.is_neox_style)\n+                key = key.reshape(key_shape)\n+            else:\n+                key_rot = key[..., :self.rotary_dim]\n+                key_pass = key[..., self.rotary_dim:]\n+                key_rot = _apply_rotary_emb_torch(key_rot, cos, sin,\n+                                                  self.is_neox_style)\n+                key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n         return query, key\n \n     def forward_cuda(\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 520d8fb..929ff92 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -223,19 +223,19 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self._init_device_properties()\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n-        self.query_start_loc = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc = torch.empty(self.max_num_reqs + 1,\n                                            dtype=torch.int32,\n                                            device=self.device)\n-        self.seq_lens = torch.zeros(self.max_num_reqs,\n+        self.seq_lens = torch.empty(self.max_num_reqs,\n                                     dtype=torch.int32,\n                                     device=self.device)\n-        self.slot_mapping = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping = torch.empty(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=self.device)\n \n@@ -254,10 +254,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n@@ -266,7 +266,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # Only relevant for models using ALiBi (e.g, MPT)\n         self.use_alibi = check_use_alibi(model_config)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -280,21 +280,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -2052,7 +2052,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # NOTE(woosuk): Here, we should use int32 because the sampler uses\n             # int32 for bonus_token_ids. If the dtype mismatches, re-compilation\n             # will occur at runtime.\n-            bonus_token_ids = torch.zeros(num_reqs,\n+            bonus_token_ids = torch.empty(num_reqs,\n                                           device=self.device,\n                                           dtype=torch.int32)\n             self.rejection_sampler(\n@@ -2087,7 +2087,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         dummy_metadata = PoolingMetadata(\n             prompt_lens=torch.tensor([h.shape[0] for h in hidden_states_list],\n                                      device=self.device),\n-            prompt_token_ids=torch.zeros((num_reqs, req_num_tokens),\n+            prompt_token_ids=torch.empty((num_reqs, req_num_tokens),\n                                          dtype=torch.int32,\n                                          device=self.device),\n             pooling_params=[PoolingParams()] * num_reqs)\n", "model_name_or_path": "gpt-5-2025-08-07"}
