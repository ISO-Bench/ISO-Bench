{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3e1c76c", "model_patch": "diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex da09bb7..d6eae67 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -100,13 +100,13 @@ class TritonMLAState(AttentionState):\n                                               PAD_SLOT_ID,\n                                               dtype=torch.long,\n                                               device=self.runner.device)\n-        self._graph_seq_lens = torch.ones(max_batch_size,\n-                                          dtype=torch.int32,\n-                                          device=self.runner.device)\n+        self._graph_seq_lens = torch.empty(max_batch_size,\n+                                           dtype=torch.int32,\n+                                           device=self.runner.device)\n         self._graph_block_tables = torch.from_numpy(\n             self.runner.graph_block_tables).to(device=self.runner.device)\n \n-        self._positions = torch.zeros((max_batch_size, ),\n+        self._positions = torch.empty((max_batch_size, ),\n                                       dtype=torch.long,\n                                       device=self.runner.device)\n \n@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):\n         B = q_nope.shape[0]\n \n         q = torch.cat([q_nope, q_pe], dim=-1)\n-        o = torch.zeros(B,\n+        o = torch.empty(B,\n                         self.num_heads,\n                         self.kv_lora_rank,\n                         dtype=q.dtype,\ndiff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 83055d6..062446d 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -56,7 +56,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,\n     new_shape_perm = list(w_q_perm.shape)\n     new_shape_perm[-1] *= pack_factor\n \n-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)\n+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)\n     for i in range(pack_factor):\n         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask\n \n@@ -105,7 +105,7 @@ def permute_rows(q_w: torch.Tensor,\n     orig_device = q_w.device\n     k_size, _ = q_w.shape\n \n-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)\n+    g_idx = torch.empty((k_size, ), dtype=torch.int32)\n     for i in range(k_size):\n         g_idx[i] = i // group_size\n \ndiff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py\nindex 0b44f0d..0526d02 100644\n--- a/vllm/model_executor/models/deepseek_v3.py\n+++ b/vllm/model_executor/models/deepseek_v3.py\n@@ -556,11 +556,11 @@ class DeepseekV3ForCausalLM(nn.Module, SupportsPP):\n             device: torch.device) -> IntermediateTensors:\n         return IntermediateTensors({\n             \"hidden_states\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n             \"residual\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n         })\n", "model_name_or_path": "gpt-5-2025-08-07"}
