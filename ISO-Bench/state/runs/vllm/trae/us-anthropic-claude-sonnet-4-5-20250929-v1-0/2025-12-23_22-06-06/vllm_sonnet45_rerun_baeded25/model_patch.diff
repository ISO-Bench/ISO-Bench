diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index da09bb7..d6eae67 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -100,13 +100,13 @@ class TritonMLAState(AttentionState):
                                               PAD_SLOT_ID,
                                               dtype=torch.long,
                                               device=self.runner.device)
-        self._graph_seq_lens = torch.ones(max_batch_size,
-                                          dtype=torch.int32,
-                                          device=self.runner.device)
+        self._graph_seq_lens = torch.empty(max_batch_size,
+                                           dtype=torch.int32,
+                                           device=self.runner.device)
         self._graph_block_tables = torch.from_numpy(
             self.runner.graph_block_tables).to(device=self.runner.device)
 
-        self._positions = torch.zeros((max_batch_size, ),
+        self._positions = torch.empty((max_batch_size, ),
                                       dtype=torch.long,
                                       device=self.runner.device)
 
@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         B = q_nope.shape[0]
 
         q = torch.cat([q_nope, q_pe], dim=-1)
-        o = torch.zeros(B,
+        o = torch.empty(B,
                         self.num_heads,
                         self.kv_lora_rank,
                         dtype=q.dtype,
diff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py
index 83055d6..062446d 100644
--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py
@@ -56,7 +56,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,
     new_shape_perm = list(w_q_perm.shape)
     new_shape_perm[-1] *= pack_factor
 
-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)
     for i in range(pack_factor):
         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask
 
@@ -105,7 +105,7 @@ def permute_rows(q_w: torch.Tensor,
     orig_device = q_w.device
     k_size, _ = q_w.shape
 
-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)
+    g_idx = torch.empty((k_size, ), dtype=torch.int32)
     for i in range(k_size):
         g_idx[i] = i // group_size
 
diff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py
index 0b44f0d..0526d02 100644
--- a/vllm/model_executor/models/deepseek_v3.py
+++ b/vllm/model_executor/models/deepseek_v3.py
@@ -556,11 +556,11 @@ class DeepseekV3ForCausalLM(nn.Module, SupportsPP):
             device: torch.device) -> IntermediateTensors:
         return IntermediateTensors({
             "hidden_states":
-            torch.zeros((batch_size, self.config.hidden_size),
+            torch.empty((batch_size, self.config.hidden_size),
                         dtype=dtype,
                         device=device),
             "residual":
-            torch.zeros((batch_size, self.config.hidden_size),
+            torch.empty((batch_size, self.config.hidden_size),
                         dtype=dtype,
                         device=device),
         })
