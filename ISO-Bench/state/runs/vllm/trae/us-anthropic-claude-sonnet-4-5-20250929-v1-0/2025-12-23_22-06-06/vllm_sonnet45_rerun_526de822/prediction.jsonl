{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-56fe4c2", "model_patch": "diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\nindex 3ff1621..2cd32ca 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n@@ -82,7 +82,7 @@ def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n \n     # Apply scale at end.\n-    masks_scale_a = masks_scale_am[:, None] & (tl.arange(0, 1) < 1)[:, None]\n+    masks_scale_a = masks_scale_am[:, None]\n     scale_a = tl.load(scale_a_ptrs[:, None], masks_scale_a)\n     # Need to broadcast to the appropriate size, if scale_a is already\n     # (BLOCK_SIZE_M, 1) then it will broadcast to its own shape. Same goes\n@@ -90,10 +90,10 @@ def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,\n     scale_a = scale_a.broadcast_to((BLOCK_SIZE_M, 1))\n     accumulator = scale_a * accumulator.to(tl.float32)\n \n-    masks_scale_b = masks_scale_bn[:, None] & (tl.arange(0, 1) < 1)[None, :]\n+    masks_scale_b = masks_scale_bn[:, None]\n     scale_b = tl.load(scale_b_ptrs[:, None], masks_scale_b)\n     scale_b = scale_b.broadcast_to((BLOCK_SIZE_N, 1))\n-    accumulator = scale_b.T * accumulator.to(tl.float32)\n+    accumulator = scale_b.T * accumulator\n \n     # Convert to output format.\n     c = accumulator.to(c_ptr.type.element_ty)\n@@ -109,8 +109,6 @@ def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,\n     # Save output\n     offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M).to(tl.int64)\n     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N).to(tl.int64)\n-    offs_cm = offs_cm.to(tl.int64)\n-    offs_cn = offs_cn.to(tl.int64)\n     c_ptrs = (c_ptr + stride_cm * offs_cm[:, None] +\n               stride_cn * offs_cn[None, :])\n     c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n@@ -181,4 +179,4 @@ def triton_scaled_mm(input: torch.Tensor,\n                            BLOCK_SIZE_SCALE_A=block_size_sa,\n                            BLOCK_SIZE_SCALE_B=block_size_sb)\n \n-    return result.to(out_dtype)\n+    return result\n", "model_name_or_path": "gpt-5-2025-08-07"}
