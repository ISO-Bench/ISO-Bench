diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index 5b56437..42e986e 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -192,7 +192,7 @@ def scaled_fp8_quant(
 ) -> Tuple[torch.Tensor, torch.Tensor]:
     output = torch.empty_like(input, dtype=torch.float8_e4m3fn)
     if scale is None:
-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)
+        scale = torch.empty(1, device=input.device, dtype=torch.float32)
         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)
     else:
         vllm_ops.static_scaled_fp8_quant(output, input, scale)
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index b57e1dd..b76b999 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -199,7 +199,7 @@ class Fp8LinearMethod(LinearMethodBase):
                                                   layer.weight_scale[idx])
 
                 layer.weight[start:end, :] = per_tensor_quantize(
-                    weight_dq, layer.weight_scale.max())
+                    weight_dq, max_w_scale)
                 start = end
             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)
 
@@ -248,7 +248,9 @@ class Fp8LinearMethod(LinearMethodBase):
 
 def all_close_1d(x: torch.Tensor) -> bool:
     assert len(x.shape) == 1
-    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))
+    if x.shape[0] <= 1:
+        return True
+    return torch.allclose(x, x[0].expand_as(x))
 
 
 def per_tensor_quantize(tensor: torch.Tensor,
@@ -260,6 +262,4 @@ def per_tensor_quantize(tensor: torch.Tensor,
 
 def per_tensor_dequantize(tensor: torch.Tensor,
                           inv_scale: float) -> torch.Tensor:
-    fake_qweight = tensor.to(torch.float16)
-    dq_weight = fake_qweight * inv_scale
-    return dq_weight
+    return tensor.to(torch.float16) * inv_scale
