{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3a1e648", "model_patch": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a4..3a6eafb 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -21,7 +21,8 @@ def create_logits_tensor(token_ids: list[list[int]],\n     \"\"\"Helper function to create logits tensor that \n        will produce desired token ids on argmax\"\"\"\n     num_total_tokens = sum(len(tokens) for tokens in token_ids)\n-    logits = torch.full((num_total_tokens, vocab_size), -100.0, device=DEVICE)\n+    logits = torch.empty((num_total_tokens, vocab_size), device=DEVICE)\n+    logits.fill_(-100.0)\n     start_loc = 0\n     for tokens in token_ids:\n         for j, token_id in enumerate(tokens):\n@@ -345,8 +346,10 @@ def estimate_rejection_sampling_pdf(\n                                             num_samples, k)\n \n     # Bonus tokens not used but required.\n-    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n-                                  device=DEVICE).repeat(num_samples, 1)\n+    bonus_token_ids = torch.empty((1, 1), dtype=torch.int64,\n+                                  device=DEVICE)\n+    bonus_token_ids.fill_(0)\n+    bonus_token_ids = bonus_token_ids.repeat(num_samples, 1)\n \n     sampling_metadata = create_sampling_metadata(all_greedy=False)\n     output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62..4bf54db 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -133,9 +133,9 @@ class RejectionSampler(nn.Module):\n             # Generate mask with bonus token.\n             generate_mask = torch.cat([\n                 accept_mask,\n-                torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)\n+                torch.zeros(accept_mask.size(0), 1, dtype=torch.bool, device=accept_mask.device)\n             ],\n-                                      dim=1).to(torch.bool) & valid_mask\n+                                      dim=1) & valid_mask\n             zeros_mask = (generate_mask == 0)\n             first_zero_idx = zeros_mask.float().argmax(dim=1)\n             # Figure out which rows actually contain at least one zero.\n@@ -204,16 +204,18 @@ class RejectionSampler(nn.Module):\n             # Generate mask with bonus tokens.\n             generate_mask = torch.cat([\n                 accept_mask,\n-                torch.zeros(batch_size, 1, device=accept_mask.device)\n+                torch.zeros(batch_size, 1, dtype=torch.bool, device=accept_mask.device)\n             ],\n-                                      dim=1).to(torch.bool)\n+                                      dim=1)\n             zeros_mask = (generate_mask == 0)\n             first_zero_idx = zeros_mask.float().argmax(dim=1)\n+            invalid_column = torch.empty((batch_size, 1),\n+                                        dtype=torch.int,\n+                                        device=accept_mask.device)\n+            invalid_column.fill_(INVALID_TOKEN_ID)\n             output_token_ids = torch.cat([\n                 accepted_token_ids,\n-                torch.full((batch_size, 1),\n-                           fill_value=INVALID_TOKEN_ID,\n-                           device=accept_mask.device)\n+                invalid_column\n             ],\n                                          dim=1)\n             output_token_ids[torch.arange(batch_size),\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 6601538..4203854 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -1281,8 +1281,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         logits = self.model.compute_logits(hidden_states, None)\n         num_reqs = logits.size(0)\n \n-        dummy_tensors = lambda v: torch.full(\n-            (num_reqs, ), v, device=self.device)\n+        def dummy_tensors(v):\n+            t = torch.empty((num_reqs, ), device=self.device)\n+            t.fill_(v)\n+            return t\n \n         dummy_metadata = SamplingMetadata(\n             temperature=dummy_tensors(0.5),\n", "model_name_or_path": "gpt-5-2025-08-07"}
