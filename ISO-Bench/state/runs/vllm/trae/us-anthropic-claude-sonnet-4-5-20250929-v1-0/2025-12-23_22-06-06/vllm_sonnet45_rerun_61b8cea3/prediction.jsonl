{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-526078a", "model_patch": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101..243e859 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -155,7 +155,8 @@ def create_and_prepopulate_kv_cache(\n         perm = torch.arange(\n             1, blocks_end)  # Sequential order starting from block 1\n \n-    inv_perm = torch.zeros(blocks_end, dtype=torch.long, device=device)\n+    inv_perm = torch.empty(blocks_end, dtype=torch.long, device=device)\n+    inv_perm[0] = 0\n     inv_perm[1:] = torch.argsort(\n         perm) + 1  # Add 1 to account for starting from block 1\n     kv_cache[:, 1:blocks_end, ...] = kv_cache[:, perm, ...]\n@@ -172,14 +173,15 @@ def create_and_prepopulate_kv_cache(\n \n         # Create a realistic slot mapping that corresponds to the block table\n     for i in range(batch_size):\n-        token_offsets = torch.arange(int(query_lens[i])) + int(context_lens[i])\n+        # Optimize: Create tensor directly on target device\n+        token_offsets = torch.arange(int(query_lens[i]), device=device) + int(context_lens[i])\n         block_indices = token_offsets // block_size\n         token_inter_block_offsets = token_offsets % block_size\n         start = common_attn_metadata.query_start_loc_cpu[i]\n         end = common_attn_metadata.query_start_loc_cpu[i + 1]\n         slot_mapping[start:end] = block_table[\n             i,\n-            block_indices] * block_size + token_inter_block_offsets.to(device)\n+            block_indices] * block_size + token_inter_block_offsets\n \n     return kv_cache\n \n@@ -359,10 +361,11 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n         #  (context_len + i)\n         kv_len = s_len\n         offset = context_len\n-        attn_mask = torch.full((q_len, kv_len),\n-                               float('-inf'),\n+        # Optimize: Use empty + fill_ instead of torch.full\n+        attn_mask = torch.empty((q_len, kv_len),\n                                device=device,\n                                dtype=dtype)\n+        attn_mask.fill_(float('-inf'))\n         for i in range(q_len):\n             attn_mask[i, :offset + i + 1] = 0.0\n \ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdd..2d944a6 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -43,9 +43,10 @@ def create_common_attn_metadata(\n         max_block_idx: int = 1000) -> CommonAttentionMetadata:\n     \"\"\"Create CommonAttentionMetadata from a BatchSpec and ModelParams.\"\"\"\n     # Create query start locations\n-    query_start_loc = torch.zeros(batch_spec.batch_size + 1,\n+    query_start_loc = torch.empty(batch_spec.batch_size + 1,\n                                   dtype=torch.int32,\n                                   device=device)\n+    query_start_loc[0] = 0\n     query_start_loc[1:] = torch.tensor(batch_spec.query_lens,\n                                        dtype=torch.int32,\n                                        device=device).cumsum(0)\ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26..d1d6f3d 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -422,12 +422,14 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                 < block_table_bounds.unsqueeze(1))\n         paged_kv_indices = block_table_tensor[mask]\n \n-        paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n-                        dtype=block_table_bounds.dtype,\n-                        device=block_table_bounds.device),\n-            block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n-        ])\n+        # Optimize: Use empty tensor with explicit zero assignment instead of zeros\n+        cumsum_result = block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n+        paged_kv_indptr = torch.empty(\n+            cumsum_result.shape[0] + 1,\n+            dtype=block_table_bounds.dtype,\n+            device=block_table_bounds.device)\n+        paged_kv_indptr[0] = 0\n+        paged_kv_indptr[1:] = cumsum_result\n \n         paged_kv_last_page_len = seq_lens % page_size\n         paged_kv_last_page_len = torch.where(paged_kv_last_page_len == 0,\n", "model_name_or_path": "gpt-5-2025-08-07"}
