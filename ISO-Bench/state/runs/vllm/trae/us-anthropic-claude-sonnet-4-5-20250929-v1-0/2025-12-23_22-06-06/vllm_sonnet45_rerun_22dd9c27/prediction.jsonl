{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-a6d795d", "model_patch": "diff --git a/vllm/attention/ops/triton_unified_attention.py b/vllm/attention/ops/triton_unified_attention.py\nindex c65f095..82bd2de 100644\n--- a/vllm/attention/ops/triton_unified_attention.py\n+++ b/vllm/attention/ops/triton_unified_attention.py\n@@ -195,9 +195,7 @@ def kernel_unified_attention_2d(\n         seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1\n \n         # S : (BLOCK_M, BLOCK_SIZE)\n-        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)\n-\n-        S += scale * tl.dot(Q, K)\n+        S = scale * tl.dot(Q, K)\n \n         if USE_SOFTCAP:\n             S = apply_softcap(S, softcap)\n@@ -414,9 +412,7 @@ def kernel_unified_attention_3d(\n         seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1\n \n         # S : (BLOCK_M, BLOCK_SIZE)\n-        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)\n-\n-        S += scale * tl.dot(Q, K)\n+        S = scale * tl.dot(Q, K)\n \n         if USE_SOFTCAP:\n             S = apply_softcap(S, softcap)\n@@ -585,7 +581,6 @@ def unified_attention(\n \n     use_alibi_slopes = alibi_slopes is not None\n \n-    block_size = v.shape[1]\n     num_seqs = len(seqused_k)\n     num_query_heads = q.shape[1]\n     num_kv_heads = k.shape[2]\n", "model_name_or_path": "gpt-5-2025-08-07"}
