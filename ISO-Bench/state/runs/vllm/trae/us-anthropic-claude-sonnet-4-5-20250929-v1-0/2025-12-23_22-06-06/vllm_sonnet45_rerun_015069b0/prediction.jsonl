{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-fbefc8a", "model_patch": "diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nindex f588f40..ceb6023 100644\n--- a/vllm/reasoning/qwen3_reasoning_parser.py\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -30,6 +30,10 @@ class Qwen3ReasoningParser(ReasoningParser):\n         super().__init__(tokenizer)\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n+        \n+        # Cache token lengths for performance\n+        self.think_start_token_len = len(self.think_start_token)\n+        self.think_end_token_len = len(self.think_end_token)\n \n         self.reasoning_regex = re.compile(\n             rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n@@ -54,10 +58,15 @@ class Qwen3ReasoningParser(ReasoningParser):\n         \"\"\"\n         Extract the content after the end tokens\n         \"\"\"\n-        if self.think_end_token_id not in input_ids[:-1]:\n+        # Optimize: single pass to find the token and extract in one go\n+        try:\n+            end_idx = input_ids.index(self.think_end_token_id)\n+            # Check if it's not the last element\n+            if end_idx < len(input_ids) - 1:\n+                return input_ids[end_idx + 1:]\n+            return []\n+        except ValueError:\n             return []\n-        else:\n-            return input_ids[input_ids.index(self.think_end_token_id) + 1:]\n \n     def extract_reasoning_content_streaming(\n         self,\n@@ -82,13 +91,18 @@ class Qwen3ReasoningParser(ReasoningParser):\n         ]):\n             return None\n \n-        if self.think_start_token_id in previous_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        # Cache token ID presence checks for performance\n+        start_in_prev = self.think_start_token_id in previous_token_ids\n+        end_in_delta = self.think_end_token_id in delta_token_ids\n+        start_in_delta = self.think_start_token_id in delta_token_ids\n+\n+        if start_in_prev:\n+            if end_in_delta:\n                 # <think> in previous, </think> in delta,\n                 # extract reasoning content\n                 end_index = delta_text.find(self.think_end_token)\n                 reasoning_content = delta_text[:end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                content = delta_text[end_index + self.think_end_token_len:]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n                                     content=content if content else None)\n             elif self.think_end_token_id in previous_token_ids:\n@@ -99,15 +113,14 @@ class Qwen3ReasoningParser(ReasoningParser):\n                 # <think> in previous, no </think> in previous or delta,\n                 # reasoning content continues\n                 return DeltaMessage(reasoning_content=delta_text)\n-        elif self.think_start_token_id in delta_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        elif start_in_delta:\n+            if end_in_delta:\n                 # <think> in delta, </think> in delta, extract reasoning content\n                 start_index = delta_text.find(self.think_start_token)\n                 end_index = delta_text.find(self.think_end_token)\n-                reasoning_content = delta_text[start_index +\n-                                               len(self.think_start_token\n-                                                   ):end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                reasoning_content = delta_text[start_index + \n+                                               self.think_start_token_len:end_index]\n+                content = delta_text[end_index + self.think_end_token_len:]\n                 return DeltaMessage(reasoning_content=reasoning_content,\n                                     content=content if content else None)\n             else:\n@@ -123,27 +136,25 @@ class Qwen3ReasoningParser(ReasoningParser):\n     ) -> tuple[Optional[str], Optional[str]]:\n \n         # Check if the model output contains the <think> tokens.\n-        if (self.think_start_token not in model_output\n-                or self.think_end_token not in model_output):\n+        # Optimize: find start_index first to avoid redundant searches\n+        start_index = model_output.find(self.think_start_token)\n+        if start_index == -1:\n             return None, model_output\n-        else:\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            # Remove the reasoning content from the model output\n-            # Although <think> token is always at the\n-            # beginning of the line, we cannot guarantee that the\n-            # other models will follow this convention.\n-            # Therefore, we need to add :start_index.\n-            start_index = model_output.find(self.think_start_token)\n-            if start_index != -1:\n-                end_index = start_index + len(\n-                    f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-                )\n-                model_output = model_output[:start_index] + \\\n-                                model_output[end_index:]\n-\n-                if len(model_output) == 0:\n-                    return reasoning_content, None\n-\n-            return reasoning_content, model_output\n+        \n+        end_token_index = model_output.find(self.think_end_token, start_index)\n+        if end_token_index == -1:\n+            return None, model_output\n+        \n+        # Extract reasoning content directly without regex\n+        reasoning_start = start_index + self.think_start_token_len\n+        reasoning_content = model_output[reasoning_start:end_token_index]\n+        \n+        # Remove the reasoning content from the model output\n+        # Optimize: calculate end_index without f-string formatting\n+        end_index = end_token_index + self.think_end_token_len\n+        model_output = model_output[:start_index] + model_output[end_index:]\n+        \n+        if len(model_output) == 0:\n+            return reasoning_content, None\n+\n+        return reasoning_content, model_output\n", "model_name_or_path": "gpt-5-2025-08-07"}
