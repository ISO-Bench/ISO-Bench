diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py
index f588f40..ceb6023 100644
--- a/vllm/reasoning/qwen3_reasoning_parser.py
+++ b/vllm/reasoning/qwen3_reasoning_parser.py
@@ -30,6 +30,10 @@ class Qwen3ReasoningParser(ReasoningParser):
         super().__init__(tokenizer)
         self.think_start_token = "<think>"
         self.think_end_token = "</think>"
+        
+        # Cache token lengths for performance
+        self.think_start_token_len = len(self.think_start_token)
+        self.think_end_token_len = len(self.think_end_token)
 
         self.reasoning_regex = re.compile(
             rf"{self.think_start_token}(.*?){self.think_end_token}", re.DOTALL)
@@ -54,10 +58,15 @@ class Qwen3ReasoningParser(ReasoningParser):
         """
         Extract the content after the end tokens
         """
-        if self.think_end_token_id not in input_ids[:-1]:
+        # Optimize: single pass to find the token and extract in one go
+        try:
+            end_idx = input_ids.index(self.think_end_token_id)
+            # Check if it's not the last element
+            if end_idx < len(input_ids) - 1:
+                return input_ids[end_idx + 1:]
+            return []
+        except ValueError:
             return []
-        else:
-            return input_ids[input_ids.index(self.think_end_token_id) + 1:]
 
     def extract_reasoning_content_streaming(
         self,
@@ -82,13 +91,18 @@ class Qwen3ReasoningParser(ReasoningParser):
         ]):
             return None
 
-        if self.think_start_token_id in previous_token_ids:
-            if self.think_end_token_id in delta_token_ids:
+        # Cache token ID presence checks for performance
+        start_in_prev = self.think_start_token_id in previous_token_ids
+        end_in_delta = self.think_end_token_id in delta_token_ids
+        start_in_delta = self.think_start_token_id in delta_token_ids
+
+        if start_in_prev:
+            if end_in_delta:
                 # <think> in previous, </think> in delta,
                 # extract reasoning content
                 end_index = delta_text.find(self.think_end_token)
                 reasoning_content = delta_text[:end_index]
-                content = delta_text[end_index + len(self.think_end_token):]
+                content = delta_text[end_index + self.think_end_token_len:]
                 return DeltaMessage(reasoning_content=reasoning_content,
                                     content=content if content else None)
             elif self.think_end_token_id in previous_token_ids:
@@ -99,15 +113,14 @@ class Qwen3ReasoningParser(ReasoningParser):
                 # <think> in previous, no </think> in previous or delta,
                 # reasoning content continues
                 return DeltaMessage(reasoning_content=delta_text)
-        elif self.think_start_token_id in delta_token_ids:
-            if self.think_end_token_id in delta_token_ids:
+        elif start_in_delta:
+            if end_in_delta:
                 # <think> in delta, </think> in delta, extract reasoning content
                 start_index = delta_text.find(self.think_start_token)
                 end_index = delta_text.find(self.think_end_token)
-                reasoning_content = delta_text[start_index +
-                                               len(self.think_start_token
-                                                   ):end_index]
-                content = delta_text[end_index + len(self.think_end_token):]
+                reasoning_content = delta_text[start_index + 
+                                               self.think_start_token_len:end_index]
+                content = delta_text[end_index + self.think_end_token_len:]
                 return DeltaMessage(reasoning_content=reasoning_content,
                                     content=content if content else None)
             else:
@@ -123,27 +136,25 @@ class Qwen3ReasoningParser(ReasoningParser):
     ) -> tuple[Optional[str], Optional[str]]:
 
         # Check if the model output contains the <think> tokens.
-        if (self.think_start_token not in model_output
-                or self.think_end_token not in model_output):
+        # Optimize: find start_index first to avoid redundant searches
+        start_index = model_output.find(self.think_start_token)
+        if start_index == -1:
             return None, model_output
-        else:
-            # Use a regex to find the reasoning content
-            reasoning_content = self.reasoning_regex.findall(model_output)[0]
-
-            # Remove the reasoning content from the model output
-            # Although <think> token is always at the
-            # beginning of the line, we cannot guarantee that the
-            # other models will follow this convention.
-            # Therefore, we need to add :start_index.
-            start_index = model_output.find(self.think_start_token)
-            if start_index != -1:
-                end_index = start_index + len(
-                    f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
-                )
-                model_output = model_output[:start_index] + \
-                                model_output[end_index:]
-
-                if len(model_output) == 0:
-                    return reasoning_content, None
-
-            return reasoning_content, model_output
+        
+        end_token_index = model_output.find(self.think_end_token, start_index)
+        if end_token_index == -1:
+            return None, model_output
+        
+        # Extract reasoning content directly without regex
+        reasoning_start = start_index + self.think_start_token_len
+        reasoning_content = model_output[reasoning_start:end_token_index]
+        
+        # Remove the reasoning content from the model output
+        # Optimize: calculate end_index without f-string formatting
+        end_index = end_token_index + self.think_end_token_len
+        model_output = model_output[:start_index] + model_output[end_index:]
+        
+        if len(model_output) == 0:
+            return reasoning_content, None
+
+        return reasoning_content, model_output
