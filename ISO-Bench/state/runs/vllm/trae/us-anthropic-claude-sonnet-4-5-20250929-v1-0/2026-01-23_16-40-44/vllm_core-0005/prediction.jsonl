{"instance_id": "vllm-project__vllm-f508e03", "model_patch": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex 1211e6b..cc52ebe 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -156,3 +156,72 @@ def test_models_with_fp8_kv_cache(\n         name_0=\"no_chunked_prefill\",\n         name_1=\"chunked_prefill\",\n     )\n+\n+\n+@pytest.mark.parametrize(\"model\", [\"facebook/opt-125m\"])\n+@pytest.mark.parametrize(\"dtype\", [\"half\"])\n+@pytest.mark.parametrize(\"max_tokens\", [16])\n+@pytest.mark.parametrize(\"chunked_prefill_token_size\", [4])\n+def test_chunked_prefill_batched_block_marking(\n+    vllm_runner,\n+    example_prompts,\n+    model: str,\n+    dtype: str,\n+    max_tokens: int,\n+    chunked_prefill_token_size: int,\n+) -> None:\n+    \"\"\"Test that batched block marking works correctly with chunked prefill.\"\"\"\n+    \n+    # Use multiple prompts to test batching\n+    prompts = example_prompts[:4]\n+    \n+    with vllm_runner(\n+        model,\n+        dtype=dtype,\n+        max_num_batched_tokens=chunked_prefill_token_size,\n+        enable_chunked_prefill=True,\n+        tensor_parallel_size=1,\n+    ) as vllm_model:\n+        # Generate outputs with chunked prefill\n+        outputs = vllm_model.generate_greedy(prompts, max_tokens)\n+        \n+        # Verify that all prompts generated outputs\n+        assert len(outputs) == len(prompts)\n+        for output in outputs:\n+            assert len(output[1]) > 0\n+\n+\n+@pytest.mark.parametrize(\"model\", [\"facebook/opt-125m\"])\n+@pytest.mark.parametrize(\"dtype\", [\"half\"])\n+@pytest.mark.parametrize(\"max_tokens\", [16])\n+def test_chunked_prefill_with_prefix_caching(\n+    vllm_runner,\n+    example_prompts,\n+    model: str,\n+    dtype: str,\n+    max_tokens: int,\n+) -> None:\n+    \"\"\"Test that prefix caching works correctly with batched block marking.\"\"\"\n+    \n+    # Use prompts with common prefixes\n+    common_prefix = \"Hello, this is a test prompt about \"\n+    prompts = [\n+        common_prefix + \"artificial intelligence.\",\n+        common_prefix + \"machine learning.\",\n+        common_prefix + \"deep learning.\",\n+    ]\n+    \n+    with vllm_runner(\n+        model,\n+        dtype=dtype,\n+        enable_chunked_prefill=True,\n+        enable_prefix_caching=True,\n+        tensor_parallel_size=1,\n+    ) as vllm_model:\n+        # Generate outputs\n+        outputs = vllm_model.generate_greedy(prompts, max_tokens)\n+        \n+        # Verify that all prompts generated outputs\n+        assert len(outputs) == len(prompts)\n+        for output in outputs:\n+            assert len(output[1]) > 0\ndiff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex cd306b9..163c61e 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -595,3 +595,38 @@ def test_sliding_window_multi_seq():\n \n     # assert all blocks are free now\n     assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n+\n+\n+def test_mark_blocks_as_computed_batched():\n+    \"\"\"Test that mark_blocks_as_computed works with batched seq_groups.\"\"\"\n+    block_size = 4\n+    num_cpu_blocks = 4\n+    num_gpu_blocks = 4\n+    block_manager = BlockSpaceManagerV1(block_size,\n+                                        num_cpu_blocks,\n+                                        num_gpu_blocks,\n+                                        watermark=0,\n+                                        enable_caching=True)\n+\n+    # Create multiple sequence groups\n+    seq_groups = []\n+    for i in range(3):\n+        prompt, seq_group = create_dummy_prompt(str(i), block_size * 2)\n+        block_manager.allocate(seq_group)\n+        seq_groups.append(seq_group)\n+\n+    # Append tokens to sequences to create full blocks\n+    for seq_group in seq_groups:\n+        for seq in seq_group.get_seqs():\n+            for _ in range(block_size):\n+                seq.append_token_id(0, {0: Logprob(0.0)})\n+            block_manager.append_slots(seq, block_size)\n+\n+    # Mark all blocks as computed in a single batched call\n+    block_manager.mark_blocks_as_computed(seq_groups)\n+\n+    # Verify that blocks are marked as computed\n+    for seq_group in seq_groups:\n+        for seq in seq_group.get_seqs():\n+            computed_blocks = block_manager.get_all_computed_blocks(seq)\n+            assert len(computed_blocks) > 0\ndiff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex 6d9c2f3..86e81e3 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -562,3 +562,32 @@ def test_chunked_prefill_max_seqs():\n     assert len(get_sequence_groups(out)) == max_seqs\n     assert not running[0].is_prefill()\n     assert not running[1].is_prefill()\n+\n+\n+def test_batched_mark_blocks_as_computed():\n+    \"\"\"Test that batched mark_blocks_as_computed is called correctly.\"\"\"\n+    block_size = 4\n+    max_seqs = 60\n+    max_model_len = 80\n+    scheduler_config = SchedulerConfig(\n+        max_token_budget=100,\n+        max_num_seqs=max_seqs,\n+        max_model_len=max_model_len,\n+        enable_chunked_prefill=True,\n+    )\n+    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n+    cache_config.num_cpu_blocks = 16\n+    cache_config.num_gpu_blocks = 16\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+\n+    # Add multiple sequence groups\n+    num_seq_groups = 5\n+    for i in range(num_seq_groups):\n+        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size * 2)\n+        scheduler.add_seq_group(seq_group)\n+\n+    # Schedule all sequences\n+    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n+    \n+    # Verify that all sequences were scheduled\n+    assert len(get_sequence_groups(out)) == num_seq_groups\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 6667233..f6923bb 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -684,13 +684,14 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n     def compute_full_blocks_in_seq(self, seq: Sequence):\n         if seq.seq_id not in self.block_tables:\n             return\n-        max_full_block = seq.get_len() // self.block_size - 1\n         block_table = self.block_tables[seq.seq_id]\n+        max_full_block = seq.get_len() // self.block_size - 1\n         if max_full_block == -1:\n             return\n-        for i in reversed(range(max_full_block)):\n+        # Optimize: Mark blocks as computed in forward order and stop early\n+        for i in range(max_full_block):\n             if block_table[i].computed:\n-                break\n+                continue\n             block_table[i].computed = True\n \n     def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n@@ -718,10 +719,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n         return commonprefix([ids for ids in ids_list if ids != []])\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_groups: List[SequenceGroup]):\n         if self.enable_caching:\n-            for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq)\n+            for seq_group in seq_groups:\n+                for seq in seq_group.get_seqs():\n+                    self.compute_full_blocks_in_seq(seq)\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\n         if device == Device.GPU:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex 7d2db43..833cba9 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -290,7 +290,7 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             self._last_access_blocks_tracker.update_last_access(\n                 seq.seq_id, now)\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_groups: List[SequenceGroup]):\n         # If prefix caching is enabled, mark immutable blocks as computed\n         # right after they have been scheduled (for prefill). This assumes\n         # the scheduler is synchronous so blocks are actually computed when\ndiff --git a/vllm/core/embedding_model_block_manager.py b/vllm/core/embedding_model_block_manager.py\nindex f16f66e..e696df2 100644\n--- a/vllm/core/embedding_model_block_manager.py\n+++ b/vllm/core/embedding_model_block_manager.py\n@@ -80,7 +80,7 @@ class EmbeddingModelBlockSpaceManager(BlockSpaceManager):\n                                       seq_group: List[Sequence]) -> List[int]:\n         return []\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_groups: List[SequenceGroup]):\n         pass\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\ndiff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex becd0d2..1982417 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -115,7 +115,7 @@ class BlockSpaceManager(ABC):\n         pass\n \n     @abstractmethod\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_groups: List[SequenceGroup]):\n         pass\n \n     @abstractmethod\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex fbc53af..eecab6b 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -1143,7 +1143,9 @@ class Scheduler:\n                 encoder_seq_data = None\n                 cross_block_table = None\n \n-            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            # Optimize: Cache the result of get_seqs to avoid multiple calls\n+            running_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n+            for seq in running_seqs:\n                 seq_id = seq.seq_id\n                 seq_data[seq_id] = seq.data\n                 block_tables[seq_id] = self.block_manager.get_block_table(seq)\n@@ -1152,7 +1154,7 @@ class Scheduler:\n             if self.cache_config.enable_prefix_caching:\n                 common_computed_block_nums = (\n                     self.block_manager.get_common_computed_block_ids(\n-                        seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n+                        running_seqs))\n \n             do_sample = True\n             is_prompt = seq_group.is_prefill()\n@@ -1224,9 +1226,11 @@ class Scheduler:\n         # batch will have been computed before the next scheduling invocation.\n         # This is because the engine assumes that a failure in model execution\n         # will crash the vLLM instance / will not retry.\n-        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n-            self.block_manager.mark_blocks_as_computed(\n-                scheduled_seq_group.seq_group)\n+        seq_groups_to_mark = [\n+            scheduled_seq_group.seq_group\n+            for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups\n+        ]\n+        self.block_manager.mark_blocks_as_computed(seq_groups_to_mark)\n \n         self._seq_group_metadata_cache[self.next_cache_id].reset()\n \ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4e..1b43cdb 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -510,14 +510,21 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         # have to be updated.\n         if prefix_cache_hit:\n             assert computed_block_nums is not None\n-            context_len = len(computed_block_nums) * self.block_size\n-            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n-                seq_idx][context_len:]\n-            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n-                seq_idx][context_len:]\n+            # Optimize: compute context_len once and reuse\n+            num_computed_blocks = len(computed_block_nums)\n+            context_len = num_computed_blocks * self.block_size\n+            \n+            # Optimize: use in-place slicing to avoid creating new list objects\n+            input_tokens = inter_data.input_tokens[seq_idx]\n+            input_positions = inter_data.input_positions[seq_idx]\n+            \n+            # Only slice if there are tokens to skip\n+            if context_len > 0:\n+                inter_data.input_tokens[seq_idx] = input_tokens[context_len:]\n+                inter_data.input_positions[seq_idx] = input_positions[context_len:]\n+            \n             inter_data.context_lens[seq_idx] = context_len\n-            inter_data.query_lens[\n-                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n+            inter_data.query_lens[seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n \n     def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                     seq_idx: int,\n@@ -641,32 +648,37 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         \"\"\"Finalize the builder intermediate data and\n         create on-device tensors.\n         \"\"\"\n+        # Optimize: Use list comprehension to flatten data more efficiently\n         # Combine and flatten intermediate data.\n-        input_tokens = []\n-        for inter_data in self.inter_data_list:\n-            for cur_input_tokens in inter_data.input_tokens:\n-                input_tokens.extend(cur_input_tokens)\n+        input_tokens = [\n+            token\n+            for inter_data in self.inter_data_list\n+            for cur_input_tokens in inter_data.input_tokens\n+            for token in cur_input_tokens\n+        ]\n \n         if not input_tokens:\n             # This may happen when all prefill requests hit\n             # prefix caching and there is no decode request.\n             return self.model_input_cls()\n \n-        input_positions = []\n-        for inter_data in self.inter_data_list:\n-            for cur_input_positions in inter_data.input_positions:\n-                input_positions.extend(cur_input_positions)\n+        input_positions = [\n+            pos\n+            for inter_data in self.inter_data_list\n+            for cur_input_positions in inter_data.input_positions\n+            for pos in cur_input_positions\n+        ]\n \n+        # Optimize: Combine loops to reduce iterations\n         seq_lens = []\n+        query_lens = []\n         max_decode_seq_len = 0\n         for inter_data in self.inter_data_list:\n             seq_lens.extend(inter_data.seq_lens)\n+            query_lens.extend(inter_data.query_lens)\n             if not inter_data.is_prompt:\n                 max_decode_seq_len = max(max_decode_seq_len,\n                                          max(inter_data.seq_lens))\n-        query_lens = []\n-        for inter_data in self.inter_data_list:\n-            query_lens.extend(inter_data.query_lens)\n \n         # Mapping from request IDs to sequence IDs. Used for Jamba models\n         # that manages the cache by itself.\n", "model_name_or_path": "gpt-5-2025-08-07"}
