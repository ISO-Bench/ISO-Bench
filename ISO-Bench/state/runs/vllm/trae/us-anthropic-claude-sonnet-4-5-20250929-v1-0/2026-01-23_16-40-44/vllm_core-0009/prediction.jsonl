{"instance_id": "vllm-project__vllm-67abdbb", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca6..995841c 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -671,7 +671,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         block_table = self.block_tables[seq.seq_id]\n         if max_full_block == -1:\n             return\n-        for i in reversed(range(max_full_block)):\n+        for i in range(max_full_block - 1, -1, -1):\n             if block_table[i].computed:\n                 break\n             block_table[i].computed = True\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477ef..f8572a2 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -129,6 +129,8 @@ class SequenceData:\n         # The number of tokens that are computed (that run against the model).\n         self._num_computed_tokens = 0\n         self._stage: SequenceStage = SequenceStage.PREFILL\n+        # Cache the total length to avoid recomputing it\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n \n         self._update_cached_all_tokens()\n \n@@ -144,6 +146,7 @@ class SequenceData:\n     def prompt_token_ids(self, new_prompt_token_ids) -> None:\n         self._prompt_token_ids = array('l', new_prompt_token_ids)\n         self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n         self._update_cached_all_tokens()\n \n     @property\n@@ -157,6 +160,7 @@ class SequenceData:\n     @output_token_ids.setter\n     def output_token_ids(self, new_output_token_ids) -> None:\n         self._output_token_ids = array('l', new_output_token_ids)\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n         self._update_cached_all_tokens()\n \n     @property\n@@ -167,9 +171,10 @@ class SequenceData:\n         self._output_token_ids.append(token_id)\n         self._cached_all_token_ids.append(token_id)\n         self.cumulative_logprob += logprob\n+        self._cached_len += 1\n \n     def get_len(self) -> int:\n-        return len(self._output_token_ids) + len(self._prompt_token_ids)\n+        return self._cached_len\n \n     def get_prompt_len(self) -> int:\n         return len(self._prompt_token_ids)\n", "model_name_or_path": "gpt-5-2025-08-07"}
