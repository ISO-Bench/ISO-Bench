{"instance_id": "vllm-project__vllm-51f8aa9", "model_patch": "diff --git a/tests/core/block/test_block_manager_v2.py b/tests/core/block/test_block_manager_v2.py\nindex d0ca09c..34e6620 100644\n--- a/tests/core/block/test_block_manager_v2.py\n+++ b/tests/core/block/test_block_manager_v2.py\n@@ -5,7 +5,7 @@ from vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n from vllm.core.interfaces import AllocStatus\n from vllm.sequence import Logprob, SequenceStatus\n-from vllm.utils import chunk_list\n+from vllm.utils import cdiv\n \n from ..utils import (create_dummy_prompt, create_seq_group,\n                      create_seq_group_encoder_decoder)\n@@ -248,11 +248,9 @@ def test_append_slots(block_size, prompt_len, num_slots_to_append,\n                            block_manager.get_num_free_gpu_blocks())\n \n     # Expect consumed blocks to be new blocks required to support the new slots.\n-    expected_consumed_blocks = len(\n-        chunk_list(\n-            list(\n-                range(prompt_len + num_slots_to_append + num_lookahead_slots)),\n-            block_size)) - len(chunk_list(list(range(prompt_len)), block_size))\n+    # Optimized: use cdiv instead of chunk_list for better performance\n+    total_tokens = prompt_len + num_slots_to_append + num_lookahead_slots\n+    expected_consumed_blocks = cdiv(total_tokens, block_size) - cdiv(prompt_len, block_size)\n     assert num_consumed_blocks == expected_consumed_blocks\n \n \ndiff --git a/tests/core/block/test_cpu_gpu_block_allocator.py b/tests/core/block/test_cpu_gpu_block_allocator.py\nindex 15b76d9..dbf237a 100644\n--- a/tests/core/block/test_cpu_gpu_block_allocator.py\n+++ b/tests/core/block/test_cpu_gpu_block_allocator.py\n@@ -1,7 +1,7 @@\n import pytest\n \n from vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\n-from vllm.utils import Device, chunk_list\n+from vllm.utils import Device\n \n \n @pytest.mark.parametrize(\"num_cpu_blocks\", [0, 512])\n@@ -58,10 +58,13 @@ def test_allocate_immutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n \n     unique_token_ids = list(\n         range((num_cpu_blocks + num_gpu_blocks) * block_size))\n-    gpu_token_ids = chunk_list(unique_token_ids[:num_gpu_blocks * block_size],\n-                               block_size)\n-    cpu_token_ids = chunk_list(unique_token_ids[num_gpu_blocks * block_size:],\n-                               block_size)\n+    \n+    # Optimized: inline chunking to avoid intermediate list allocations\n+    gpu_token_ids = [unique_token_ids[i:i + block_size] \n+                     for i in range(0, num_gpu_blocks * block_size, block_size)]\n+    cpu_token_ids = [unique_token_ids[i:i + block_size] \n+                     for i in range(num_gpu_blocks * block_size, \n+                                    len(unique_token_ids), block_size)]\n \n     assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\ndiff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex 49e63c2..daa8f4f 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -262,8 +262,14 @@ class BlockTable:\n \n         block_token_ids = []\n         tail_token_ids = []\n-        for cur_token_ids in chunk_list(token_ids, self._block_size):\n-            if len(cur_token_ids) == self._block_size:\n+        \n+        # Optimized chunking to avoid intermediate list allocations\n+        num_tokens = len(token_ids)\n+        block_size = self._block_size\n+        for i in range(0, num_tokens, block_size):\n+            end_idx = min(i + block_size, num_tokens)\n+            cur_token_ids = token_ids[i:end_idx]\n+            if len(cur_token_ids) == block_size:\n                 block_token_ids.append(cur_token_ids)\n             else:\n                 tail_token_ids.append(cur_token_ids)\n@@ -351,6 +357,12 @@ class BlockTable:\n         \"\"\"\n         first_chunk_size = self._block_size - (self._num_full_slots %\n                                                self._block_size)\n-        token_blocks = [token_ids[:first_chunk_size]] + chunk_list(\n-            token_ids[first_chunk_size:], self._block_size)\n+        token_blocks = [token_ids[:first_chunk_size]]\n+        \n+        # Optimized chunking to avoid intermediate list allocations\n+        remaining = token_ids[first_chunk_size:]\n+        block_size = self._block_size\n+        for i in range(0, len(remaining), block_size):\n+            token_blocks.append(remaining[i:i + block_size])\n+        \n         return token_blocks\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex f272e23..2a56210 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -828,7 +828,7 @@ class PrefixCachingBlock(Block):\n         - int: The computed hash value for the block.\n         \"\"\"\n         assert (prev_block_hash is None) == is_first_block\n-        return hash((is_first_block, prev_block_hash, *cur_block_token_ids))\n+        return hash((is_first_block, prev_block_hash, tuple(cur_block_token_ids)))\n \n \n class ComputedBlocksTracker:\ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 87508a1..6638eef 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -81,6 +81,9 @@ _MODELS = {**_GENERATION_MODELS, **_EMBEDDING_MODELS}\n # out of tree models\n _OOT_MODELS: Dict[str, Type[nn.Module]] = {}\n \n+# Cache for loaded model classes\n+_MODEL_CLASSES_CACHE: Dict[str, Type[nn.Module]] = {}\n+\n # Models not supported by ROCm.\n _ROCM_UNSUPPORTED_MODELS: List[str] = []\n \n@@ -100,6 +103,10 @@ class ModelRegistry:\n \n     @staticmethod\n     def load_model_cls(model_arch: str) -> Optional[Type[nn.Module]]:\n+        # Check cache first\n+        if model_arch in _MODEL_CLASSES_CACHE:\n+            return _MODEL_CLASSES_CACHE[model_arch]\n+            \n         if model_arch in _OOT_MODELS:\n             return _OOT_MODELS[model_arch]\n         if model_arch not in _MODELS:\n@@ -117,7 +124,13 @@ class ModelRegistry:\n         module_name, model_cls_name = _MODELS[model_arch]\n         module = importlib.import_module(\n             f\"vllm.model_executor.models.{module_name}\")\n-        return getattr(module, model_cls_name, None)\n+        model_cls = getattr(module, model_cls_name, None)\n+        \n+        # Cache the result\n+        if model_cls is not None:\n+            _MODEL_CLASSES_CACHE[model_arch] = model_cls\n+        \n+        return model_cls\n \n     @staticmethod\n     def get_supported_archs() -> List[str]:\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 1cebf68..b69082e 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -127,6 +127,9 @@ class SequenceData:\n         # The number of tokens that are computed (that run against the model).\n         self._num_computed_tokens = 0\n         self._stage: SequenceStage = SequenceStage.PREFILL\n+        \n+        # Cache lengths for performance\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n \n         self._update_cached_all_tokens()\n \n@@ -142,6 +145,7 @@ class SequenceData:\n     def prompt_token_ids(self, new_prompt_token_ids) -> None:\n         self._prompt_token_ids = list(new_prompt_token_ids)\n         self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n         self._update_cached_all_tokens()\n \n     @property\n@@ -151,15 +155,17 @@ class SequenceData:\n     @output_token_ids.setter\n     def output_token_ids(self, new_output_token_ids) -> None:\n         self._output_token_ids = list(new_output_token_ids)\n+        self._cached_len = len(self._prompt_token_ids) + len(self._output_token_ids)\n         self._update_cached_all_tokens()\n \n     def append_token_id(self, token_id: int, logprob: float) -> None:\n         self._output_token_ids.append(token_id)\n         self._cached_all_token_ids.append(token_id)\n+        self._cached_len += 1\n         self.cumulative_logprob += logprob\n \n     def get_len(self) -> int:\n-        return len(self._output_token_ids) + len(self._prompt_token_ids)\n+        return self._cached_len\n \n     def get_prompt_len(self) -> int:\n         return len(self._prompt_token_ids)\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex f3025a6..28202ea 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -422,7 +422,8 @@ def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:\n \n def cdiv(a: int, b: int) -> int:\n     \"\"\"Ceiling division.\"\"\"\n-    return -(a // -b)\n+    # Optimized ceiling division\n+    return (a + b - 1) // b\n \n \n def _generate_random_fp8(\n", "model_name_or_path": "gpt-5-2025-08-07"}
