{"instance_id": "vllm-project__vllm-25373b6", "model_patch": "diff --git a/tests/kernels/mamba/test_mamba_ssm.py b/tests/kernels/mamba/test_mamba_ssm.py\nindex 8dece26..97e40df 100644\n--- a/tests/kernels/mamba/test_mamba_ssm.py\n+++ b/tests/kernels/mamba/test_mamba_ssm.py\n@@ -183,29 +183,25 @@ def selective_scan_opcheck_fn(u,\n     \"\"\"if return_last_state is True, returns (out, last_state)\n     last_state has shape (batch, dim, dstate).\n     \"\"\"\n-    if u.stride(-1) != 1:\n-        u = u.contiguous()\n-    if delta.stride(-1) != 1:\n-        delta = delta.contiguous()\n-    if D is not None:\n-        D = D.contiguous()\n-    if B.stride(-1) != 1:\n-        B = B.contiguous()\n-    if C.stride(-1) != 1:\n-        C = C.contiguous()\n-    if z is not None and z.stride(-1) != 1:\n-        z = z.contiguous()\n-    if B.dim() == 3 and cu_seq_len is None:\n+    u = u if u.stride(-1) == 1 else u.contiguous()\n+    delta = delta if delta.stride(-1) == 1 else delta.contiguous()\n+    D = D if D is None or D.is_contiguous() else D.contiguous()\n+    B = B if B.stride(-1) == 1 else B.contiguous()\n+    C = C if C.stride(-1) == 1 else C.contiguous()\n+    z = z if z is None or z.stride(-1) == 1 else z.contiguous()\n+    \n+    B_dim = B.dim()\n+    if B_dim == 3 and cu_seq_len is None:\n         B = B.unsqueeze(1)\n-    if B.dim() == 2 and cu_seq_len is not None:\n+    elif B_dim == 2 and cu_seq_len is not None:\n         B = B.unsqueeze(0)\n-    if C.dim() == 3 and cu_seq_len is None:\n+    \n+    C_dim = C.dim()\n+    if C_dim == 3 and cu_seq_len is None:\n         C = C.unsqueeze(1)\n-    if C.dim() == 2 and cu_seq_len is not None:\n+    elif C_dim == 2 and cu_seq_len is not None:\n         C = C.unsqueeze(0)\n \n-    # Disable test_autograd_registration for now as it seems to trigger\n-    # a bogus error.\n     opcheck(torch.ops._C.selective_scan_fwd,\n             (u, delta, A, B, C, D, z, delta_bias, delta_softplus, cu_seq_len,\n              cache_indices, has_initial_state, ssm_states, pad_slot_id),\n@@ -242,46 +238,36 @@ def test_selective_scan(is_variable_B, is_variable_C, varBC_groups, has_D,\n     batch_size = 1\n     dim = 4\n     dstate = 8\n-    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype))\n-    A_ref = A.clone()\n+    A = -0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)\n     if not is_variable_B:\n         B_shape = [dim, dstate]\n     elif varBC_groups == 1:\n         B_shape = [batch_size, dstate, seqlen]\n     else:\n         B_shape = [batch_size, varBC_groups, dstate, seqlen]\n-    B = torch.randn(B_shape,\n-                    device=device,\n+    B = torch.randn(B_shape, device=device,\n                     dtype=wtype if not is_variable_B else itype)\n-    B_ref = B.clone()\n     if not is_variable_C:\n         C_shape = [dim, dstate]\n     elif varBC_groups == 1:\n         C_shape = [batch_size, dstate, seqlen]\n     else:\n         C_shape = [batch_size, varBC_groups, dstate, seqlen]\n-    C = torch.randn(C_shape,\n-                    device=device,\n+    C = torch.randn(C_shape, device=device,\n                     dtype=wtype if not is_variable_C else itype)\n-    C_ref = C.clone()\n     D = torch.randn(dim, device=device, dtype=torch.float32) if has_D else None\n-    D_ref = D.clone()\n-    z = torch.randn(batch_size, dim, seqlen, device=device,\n-                    dtype=itype) if has_z else None\n-    z_ref = z.clone() if has_z else None\n-    delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)\n-                  ) if has_delta_bias else None\n+    z = torch.randn(batch_size, dim, seqlen, device=device, dtype=itype) if has_z else None\n+    delta_bias = 0.5 * torch.rand(dim, device=device, dtype=torch.float32) if has_delta_bias else None\n     u = torch.randn(batch_size, dim, seqlen, device=device, dtype=itype)\n-    u_ref = u.clone()\n-    delta = (0.5 *\n-             torch.rand(batch_size, dim, seqlen, device=device, dtype=itype))\n-    delta_ref = delta.clone()\n+    delta = 0.5 * torch.rand(batch_size, dim, seqlen, device=device, dtype=itype)\n     state_shape = (batch_size, u.shape[1], int(A.shape[1]))\n-    state = torch.randn(state_shape,\n-                        device=u.device,\n-                        dtype=itype,\n-                        requires_grad=False)\n-    state_ref = state.clone()\n+    state = torch.randn(state_shape, device=u.device, dtype=itype, requires_grad=False)\n+    \n+    # Clone for reference implementation\n+    A_ref, B_ref, C_ref = A.clone(), B.clone(), C.clone()\n+    D_ref = D.clone() if D is not None else None\n+    z_ref = z.clone() if z is not None else None\n+    u_ref, delta_ref, state_ref = u.clone(), delta.clone(), state.clone()\n     out = None\n     out_ref = None\n     outs = []\n@@ -455,37 +441,29 @@ def test_selective_scan_varlen(with_padding, is_variable_B, is_variable_C,\n \n     dim = 4\n     dstate = 8\n-    A = (-0.5 * torch.rand(dim, dstate, device=device, dtype=wtype))\n-    A_ref = A.clone()\n+    A = -0.5 * torch.rand(dim, dstate, device=device, dtype=wtype)\n     B_shape = [varBC_groups, dstate, seqlen]\n-    B = torch.randn(B_shape,\n-                    device=device,\n+    B = torch.randn(B_shape, device=device,\n                     dtype=wtype if not is_variable_B else itype)\n-    B_ref = B.clone()\n     C_shape = [varBC_groups, dstate, seqlen]\n-    C = torch.randn(C_shape,\n-                    device=device,\n+    C = torch.randn(C_shape, device=device,\n                     dtype=wtype if not is_variable_C else itype)\n-    C_ref = C.clone()\n     D = torch.randn(dim, device=device, dtype=torch.float32) if has_D else None\n-    D_ref = D.clone()\n     z = torch.randn(dim, seqlen, device=device, dtype=itype)\n-    z_ref = z.clone()\n-    delta_bias = (0.5 * torch.rand(dim, device=device, dtype=torch.float32)\n-                  ) if has_delta_bias else None\n+    delta_bias = 0.5 * torch.rand(dim, device=device, dtype=torch.float32) if has_delta_bias else None\n     u = torch.randn(dim, seqlen, device=device, dtype=itype)\n-    u_ref = u.clone()\n-    delta = (0.5 * torch.rand(dim, seqlen, device=device, dtype=itype))\n-    delta_ref = delta.clone()\n-    out = None\n-    out_ref = None\n-\n+    delta = 0.5 * torch.rand(dim, seqlen, device=device, dtype=itype)\n+    \n     prev_state_shape = (total_entries, u.shape[0], int(A.shape[1]))\n-    prev_state = torch.randn(prev_state_shape,\n-                             device=u.device,\n-                             dtype=itype,\n-                             requires_grad=False)\n+    prev_state = torch.randn(prev_state_shape, device=u.device, dtype=itype, requires_grad=False)\n+    \n+    # Clone for reference implementation\n+    A_ref, B_ref, C_ref = A.clone(), B.clone(), C.clone()\n+    D_ref = D.clone() if D is not None else None\n+    z_ref, u_ref, delta_ref = z.clone(), u.clone(), delta.clone()\n     prev_state_ref = prev_state.clone()\n+    out = None\n+    out_ref = None\n     state_indices = torch.randperm(total_entries,\n                                    dtype=torch.int32,\n                                    device=u.device)[:batch_size]\n@@ -588,7 +566,7 @@ def test_selective_state_update_with_batch_indices(with_padding, dim, dstate,\n     C = torch.randn(padded_batch_size, dstate, device=device)\n     D = torch.randn(dim, device=device)\n     z = torch.randn_like(x) if has_z else None\n-    state_ref = state[state_indices, :].clone()\n+    state_ref = state[state_indices].clone()\n     state_before = state.clone()\n     out = selective_state_update(state,\n                                  x,\n@@ -690,7 +668,7 @@ def test_selective_state_update_with_heads_with_batch_indices(\n     B = torch.randn(batch_size, ngroups, dstate, device=device)\n     C = torch.randn(batch_size, ngroups, dstate, device=device)\n     z = torch.randn_like(x) if has_z else None\n-    state_ref = state[state_indices, :].detach().clone()\n+    state_ref = state[state_indices].detach().clone()\n     out = selective_state_update(state,\n                                  x,\n                                  dt,\ndiff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py\nindex 00c1a29..735d143 100644\n--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py\n+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py\n@@ -90,19 +90,11 @@ def generate_random_inputs(batch_size,\n                            device='cuda'):\n \n     current_platform.seed_everything(0)\n-    A = (-torch.exp(torch.rand(n_heads, dtype=itype, device=device)))\n-    dt = F.softplus(\n-        torch.randn(batch_size, seqlen, n_heads, dtype=itype, device=device) -\n-        4)\n-    X = torch.randn((batch_size, seqlen, n_heads, d_head),\n-                    dtype=itype,\n-                    device=device)\n-    B = torch.randn((batch_size, seqlen, n_heads, d_head),\n-                    dtype=itype,\n-                    device=device)\n-    C = torch.randn((batch_size, seqlen, n_heads, d_head),\n-                    dtype=itype,\n-                    device=device)\n+    A = -torch.exp(torch.rand(n_heads, dtype=itype, device=device))\n+    dt = F.softplus(torch.randn(batch_size, seqlen, n_heads, dtype=itype, device=device) - 4)\n+    X = torch.randn(batch_size, seqlen, n_heads, d_head, dtype=itype, device=device)\n+    B = torch.randn(batch_size, seqlen, n_heads, d_head, dtype=itype, device=device)\n+    C = torch.randn(batch_size, seqlen, n_heads, d_head, dtype=itype, device=device)\n \n     return A, dt, X, B, C\n \n@@ -144,8 +136,8 @@ def generate_continuous_batched_examples(example_lens_by_batch,\n             last_taken[i] = (c + x) % full_length\n             exhausted[i] = last_taken[i] == 0\n \n-        return (torch.concat([x[i, s:e] for i, (s, e) in enumerate(indices)\n-                              ]).unsqueeze(0) for x in (dt, X, B, C))\n+        return (torch.concat([x[i, s:e] for i, (s, e) in enumerate(indices)]).unsqueeze(0) \n+                for x in (dt, X, B, C))\n \n     # internal function that maps \"n\" to the appropriate right boundary\n     # value when forming continuous batches from examples of length given\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer.py b/vllm/model_executor/layers/mamba/mamba_mixer.py\nindex 796c8d9..bc28701 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer.py\n@@ -200,11 +200,9 @@ class MambaMixer(CustomOp):\n \n         discrete_time_step = self.dt_proj(time_step)[0].transpose(-2, -1)\n         # 3.c perform the recurrence y \u2190 SSM(A, B, C)(x)\n-        time_proj_bias = (self.dt_proj.bias.float() if hasattr(\n-            self.dt_proj, \"bias\") else None)\n+        time_proj_bias = self.dt_proj.bias.float() if hasattr(self.dt_proj, \"bias\") else None\n \n-        if attn_metadata.query_start_loc is not None \\\n-            and attn_metadata.context_lens_tensor is not None:\n+        if attn_metadata.query_start_loc is not None and attn_metadata.context_lens_tensor is not None:\n             scan_outputs = selective_scan_fn(\n                 hidden_states,\n                 mamba_cache_params.ssm_state,\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 36edac2..3e25a7b 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -523,10 +523,8 @@ class MambaMixer2(MambaBase, CustomOp):\n \n         if envs.VLLM_USE_V1 and attn_metadata is None:\n             # V1 profile run\n-            hidden_states_B_C = (hidden_states_B_C.transpose(\n-                0, 1).clone().transpose(0, 1)).contiguous()\n-            hidden_states, _B, _C = split_hidden_states_B_C_fn(\n-                hidden_states_B_C)\n+            hidden_states_B_C = hidden_states_B_C.contiguous()\n+            hidden_states, _B, _C = split_hidden_states_B_C_fn(hidden_states_B_C)\n             hidden_states = self.norm(hidden_states, gate)\n             out, _ = self.out_proj(hidden_states)\n             return out\n@@ -588,24 +586,17 @@ class MambaMixer2(MambaBase, CustomOp):\n         # Process prefill requests\n         if has_prefill:\n             # 2. Convolution sequence transformation\n-            # - \"cache_indices\" updates the conv_state cache in positions\n-            #   pointed to by \"state_indices_tensor\"\n-            x = hidden_states_B_C_p.transpose(\n-                0, 1)  # this is the form that causal-conv see\n+            x = hidden_states_B_C_p.transpose(0, 1)\n             if mamba2_metadata.cu_seqlen is None:\n-                mamba2_metadata = update_metadata(x, query_start_loc_p,\n-                                                  mamba2_metadata)\n+                mamba2_metadata = update_metadata(x, query_start_loc_p, mamba2_metadata)\n             hidden_states_B_C_p = causal_conv1d_fn(\n-                x,\n-                conv_weights,\n-                self.conv1d.bias,\n+                x, conv_weights, self.conv1d.bias,\n                 activation=self.activation,\n                 conv_states=conv_state,\n                 has_initial_state=has_initial_states_p,\n                 cache_indices=state_indices_tensor_p,\n                 metadata=mamba2_metadata,\n-                query_start_loc=query_start_loc_p).transpose(\n-                    0, 1)[:num_prefill_tokens]\n+                query_start_loc=query_start_loc_p).transpose(0, 1)[:num_prefill_tokens]\n \n             hidden_states_p, B_p, C_p = split_hidden_states_B_C_fn(\n                 hidden_states_B_C_p)\ndiff --git a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\nindex 3f67fc3..97858e0 100644\n--- a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n+++ b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n@@ -227,110 +227,50 @@ def selective_state_update(state,\n         out: (batch, dim) or (batch, nheads, dim)\n     \"\"\"\n     has_heads = state.dim() > 3\n-    if state.dim() == 3:\n-        state = state.unsqueeze(1)\n-    if x.dim() == 2:\n-        x = x.unsqueeze(1)\n-    if dt.dim() == 2:\n-        dt = dt.unsqueeze(1)\n-    if A.dim() == 2:\n-        A = A.unsqueeze(0)\n-    if B.dim() == 2:\n-        B = B.unsqueeze(1)\n-    if C.dim() == 2:\n-        C = C.unsqueeze(1)\n-    if D is not None and D.dim() == 1:\n-        D = D.unsqueeze(0)\n-    if z is not None and z.dim() == 2:\n-        z = z.unsqueeze(1)\n-    if dt_bias is not None and dt_bias.dim() == 1:\n-        dt_bias = dt_bias.unsqueeze(0)\n+    state = state if state.dim() == 4 else state.unsqueeze(1)\n+    x = x if x.dim() == 3 else x.unsqueeze(1)\n+    dt = dt if dt.dim() == 3 else dt.unsqueeze(1)\n+    A = A if A.dim() == 3 else A.unsqueeze(0)\n+    B = B if B.dim() == 3 else B.unsqueeze(1)\n+    C = C if C.dim() == 3 else C.unsqueeze(1)\n+    D = D if D is None or D.dim() == 2 else D.unsqueeze(0)\n+    z = z if z is None or z.dim() == 3 else z.unsqueeze(1)\n+    dt_bias = dt_bias if dt_bias is None or dt_bias.dim() == 2 else dt_bias.unsqueeze(0)\n \n     _, nheads, dim, dstate = state.shape\n     batch = x.shape[0]\n-\n-    assert x.shape == (batch, nheads, dim)\n-    assert dt.shape == x.shape\n-    assert A.shape == (nheads, dim, dstate)\n     ngroups = B.shape[1]\n-    assert nheads % ngroups == 0, \"nheads must be divisible by ngroups\"\n-    assert B.shape == (batch, ngroups, dstate)\n-    assert C.shape == B.shape\n-    if D is not None:\n-        assert D.shape == (nheads, dim)\n-    if z is not None:\n-        assert z.shape == x.shape\n-    if dt_bias is not None:\n-        assert dt_bias.shape == (nheads, dim)\n-    if state_batch_indices is not None:\n-        assert state_batch_indices.shape == (batch, )\n+    \n     out = torch.empty_like(x)\n     grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch, nheads)\n-    z_strides = ((z.stride(0), z.stride(1), z.stride(2)) if z is not None else\n-                 (0, 0, 0))\n-    # We don't want autotune since it will overwrite the state\n-    # We instead tune by hand.\n+    z_strides = (z.stride(0), z.stride(1), z.stride(2)) if z is not None else (0, 0, 0)\n+    \n     BLOCK_SIZE_M, num_warps = ((32, 4) if dstate <= 16 else\n-                               ((16, 4) if dstate <= 32 else\n-                                ((8, 4) if dstate <= 64 else\n-                                 ((4, 4) if dstate <= 128 else ((4, 8))))))\n-    tie_hdim = A.stride(-1) == 0 and A.stride(-2) == 0 and dt.stride(\n-        -1) == 0 and dt_bias.stride(-1) == 0\n+                               (16, 4) if dstate <= 32 else\n+                               (8, 4) if dstate <= 64 else\n+                               (4, 4) if dstate <= 128 else (4, 8))\n+    tie_hdim = (A.stride(-1) == 0 and A.stride(-2) == 0 and \n+                dt.stride(-1) == 0 and \n+                (dt_bias is None or dt_bias.stride(-1) == 0))\n     with torch.cuda.device(x.device.index):\n         _selective_scan_update_kernel[grid](\n-            state,\n-            x,\n-            dt,\n-            dt_bias,\n-            A,\n-            B,\n-            C,\n-            D,\n-            z,\n-            out,\n-            state_batch_indices,\n-            pad_slot_id,\n-            batch,\n-            nheads,\n-            dim,\n-            dstate,\n-            nheads // ngroups,\n-            state.stride(0),\n-            state.stride(1),\n-            state.stride(2),\n-            state.stride(3),\n-            x.stride(0),\n-            x.stride(1),\n-            x.stride(2),\n-            dt.stride(0),\n-            dt.stride(1),\n-            dt.stride(2),\n-            *(dt_bias.stride(0),\n-              dt_bias.stride(1)) if dt_bias is not None else 0,\n-            A.stride(0),\n-            A.stride(1),\n-            A.stride(2),\n-            B.stride(0),\n-            B.stride(1),\n-            B.stride(2),\n-            C.stride(0),\n-            C.stride(1),\n-            C.stride(2),\n+            state, x, dt, dt_bias, A, B, C, D, z, out,\n+            state_batch_indices, pad_slot_id,\n+            batch, nheads, dim, dstate, nheads // ngroups,\n+            state.stride(0), state.stride(1), state.stride(2), state.stride(3),\n+            x.stride(0), x.stride(1), x.stride(2),\n+            dt.stride(0), dt.stride(1), dt.stride(2),\n+            *(dt_bias.stride(0), dt_bias.stride(1)) if dt_bias is not None else 0,\n+            A.stride(0), A.stride(1), A.stride(2),\n+            B.stride(0), B.stride(1), B.stride(2),\n+            C.stride(0), C.stride(1), C.stride(2),\n             *(D.stride(0), D.stride(1)) if D is not None else 0,\n-            z_strides[0],\n-            z_strides[1],\n-            z_strides[2],\n-            out.stride(0),\n-            out.stride(1),\n-            out.stride(2),\n-            dt_softplus,\n-            tie_hdim,\n-            BLOCK_SIZE_M,\n+            z_strides[0], z_strides[1], z_strides[2],\n+            out.stride(0), out.stride(1), out.stride(2),\n+            dt_softplus, tie_hdim, BLOCK_SIZE_M,\n             num_warps=num_warps,\n         )\n-    if not has_heads:\n-        out = out.squeeze(1)\n-    return out\n+    return out if has_heads else out.squeeze(1)\n \n \n def selective_scan_fn(u,\n@@ -383,32 +323,29 @@ def selective_scan_fn(u,\n         output: (dim, total_length) for varlen or (batch, dim, seqlen) \n                 supports inplace replacement\n     \"\"\"\n-    if u.stride(-1) != 1:\n-        u = u.contiguous()\n-    if delta.stride(-1) != 1:\n-        delta = delta.contiguous()\n-    if D is not None:\n-        D = D.contiguous()\n-    if B.stride(-1) != 1:\n-        B = B.contiguous()\n-    if C.stride(-1) != 1:\n-        C = C.contiguous()\n-    if z is not None and z.stride(-1) != 1:\n-        z = z.contiguous()\n-    if B.dim() == 3 and query_start_loc is None:\n-        B = B.unsqueeze(1)\n-    if B.dim() == 2 and query_start_loc is not None:\n-        B = B.unsqueeze(0)\n-    if C.dim() == 3 and query_start_loc is None:\n-        C = C.unsqueeze(1)\n-    if C.dim() == 2 and query_start_loc is not None:\n-        C = C.unsqueeze(0)\n+    # Only make contiguous if necessary\n+    u_contig = u if u.stride(-1) == 1 else u.contiguous()\n+    delta_contig = delta if delta.stride(-1) == 1 else delta.contiguous()\n+    D_contig = D if D is None or D.is_contiguous() else D.contiguous()\n+    B_contig = B if B.stride(-1) == 1 else B.contiguous()\n+    C_contig = C if C.stride(-1) == 1 else C.contiguous()\n+    z_contig = z if z is None or z.stride(-1) == 1 else z.contiguous()\n+    \n+    # Handle dimension adjustments\n+    B_dim = B_contig.dim()\n+    C_dim = C_contig.dim()\n+    if B_dim == 3 and query_start_loc is None:\n+        B_contig = B_contig.unsqueeze(1)\n+    elif B_dim == 2 and query_start_loc is not None:\n+        B_contig = B_contig.unsqueeze(0)\n+    if C_dim == 3 and query_start_loc is None:\n+        C_contig = C_contig.unsqueeze(1)\n+    elif C_dim == 2 and query_start_loc is not None:\n+        C_contig = C_contig.unsqueeze(0)\n \n-    ops.selective_scan_fwd(u, delta, A, B, C, D, z, delta_bias, delta_softplus,\n+    ops.selective_scan_fwd(u_contig, delta_contig, A, B_contig, C_contig, D_contig, \n+                           z_contig, delta_bias, delta_softplus,\n                            query_start_loc, cache_indices, has_initial_state,\n                            ssm_states, pad_slot_id)\n \n-    if z is None:\n-        return delta  # output written inplace to delta\n-    else:\n-        return z  # output written inplace to z\n+    return z_contig if z is not None else delta_contig\ndiff --git a/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py b/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\nindex 61eff0c..d2b70c6 100644\n--- a/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\n+++ b/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\n@@ -502,11 +502,10 @@ def _chunk_scan_fwd(\n         out_x = None\n \n     grid = lambda META: (\n-        triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(\n-            headdim, META['BLOCK_SIZE_N']), batch * nchunks\n-        if chunk_offsets is None else len(chunk_offsets), nheads)\n-    z_strides = ((z.stride(0), z.stride(1), z.stride(2),\n-                  z.stride(3)) if z is not None else (0, 0, 0, 0))\n+        triton.cdiv(chunk_size, META['BLOCK_SIZE_M']) * triton.cdiv(headdim, META['BLOCK_SIZE_N']), \n+        batch * nchunks if chunk_offsets is None else len(chunk_offsets), \n+        nheads)\n+    z_strides = (z.stride(0), z.stride(1), z.stride(2), z.stride(3)) if z is not None else (0, 0, 0, 0)\n     _chunk_scan_fwd_kernel[grid](\n         cb,\n         x,\n@@ -554,8 +553,7 @@ def _chunk_scan_fwd(\n         dA_cumsum.stride(2),\n         dA_cumsum.stride(1),\n         dA_cumsum.stride(3),\n-        *((seq_idx.stride(0), seq_idx.stride(1)) if seq_idx is not None else\n-          (0, 0)),\n+        *((seq_idx.stride(0), seq_idx.stride(1)) if seq_idx is not None else (0, 0)),\n         C.stride(0),\n         C.stride(1),\n         C.stride(2),\n@@ -566,9 +564,8 @@ def _chunk_scan_fwd(\n         states.stride(3),\n         states.stride(4),\n         *((initial_states.stride(0), initial_states.stride(1),\n-           initial_states.stride(2),\n-           initial_states.stride(3)) if initial_states is not None else\n-          (0, 0, 0, 0)),\n+           initial_states.stride(2), initial_states.stride(3)) \n+          if initial_states is not None else (0, 0, 0, 0)),\n         D.stride(0) if D is not None else 0,\n         True,\n         D is not None,\ndiff --git a/vllm/model_executor/layers/mamba/ops/ssd_combined.py b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\nindex b121275..4ff2c34 100644\n--- a/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n+++ b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n@@ -50,18 +50,11 @@ def _mamba_chunk_scan_combined_fwd(x,\n         assert D.shape == (nheads, headdim) or D.shape == (nheads, )\n     if seq_idx is not None:\n         assert seq_idx.shape == (batch, seqlen)\n-    if B.stride(-1) != 1:\n-        B = B.contiguous()\n-    if C.stride(-1) != 1:\n-        C = C.contiguous()\n-    if x.stride(-1) != 1 and x.stride(\n-            1) != 1:  # Either M or K dimension should be contiguous\n-        x = x.contiguous()\n-    if z is not None and z.stride(-1) != 1 and z.stride(\n-            1) != 1:  # Either M or K dimension should be contiguous\n-        z = z.contiguous()\n-    if D is not None and D.stride(-1) != 1:\n-        D = D.contiguous()\n+    B = B if B.stride(-1) == 1 else B.contiguous()\n+    C = C if C.stride(-1) == 1 else C.contiguous()\n+    x = x if x.stride(-1) == 1 or x.stride(1) == 1 else x.contiguous()\n+    z = z if z is None or z.stride(-1) == 1 or z.stride(1) == 1 else z.contiguous()\n+    D = D if D is None or D.stride(-1) == 1 else D.contiguous()\n     if initial_states is not None:\n         if cu_seqlens is None:\n             assert initial_states.shape == (batch, nheads, headdim, dstate)\ndiff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py\nindex a4ded2b..6781b52 100644\n--- a/vllm/model_executor/models/phi4flash.py\n+++ b/vllm/model_executor/models/phi4flash.py\n@@ -316,9 +316,7 @@ class Phi4Mamba(nn.Module):\n             return out[0], yoco_key_values\n \n         # 1. Gated MLP's linear projection\n-        # projected_states = self.in_proj(hidden_states)[0].transpose(-2, -1)\n-        projected_states = self.in_proj(\n-            hidden_states.to(self.in_proj.weight.dtype))[0].transpose(-2, -1)\n+        projected_states = self.in_proj(hidden_states.to(self.in_proj.weight.dtype))[0].transpose(-2, -1)\n         hidden_states, gate = projected_states.chunk(2, dim=-2)\n \n         # 2. Convolution sequence transformation\ndiff --git a/vllm/model_executor/models/plamo2.py b/vllm/model_executor/models/plamo2.py\nindex 9bc577c..949a19e 100644\n--- a/vllm/model_executor/models/plamo2.py\n+++ b/vllm/model_executor/models/plamo2.py\n@@ -262,8 +262,6 @@ class Plamo2MambaMixer(nn.Module):\n         # Process prefill requests\n         if has_prefill:\n             # 2. Convolution sequence transformation\n-            # - \"cache_indices\" updates the conv_state cache in positions\n-            # pointed to by \"mamba_cache_params.state_indices_tensor\"\n             hidden_states_p = causal_conv1d_fn(\n                 hidden_states_p.transpose(0, 1),\n                 conv_weights,\n@@ -272,13 +270,7 @@ class Plamo2MambaMixer(nn.Module):\n                 conv_states=mamba_cache_params.conv_state,\n                 has_initial_state=mamba2_metadata.has_initial_states,\n                 cache_indices=state_indices_tensor_p,\n-                query_start_loc=query_start_loc_p)\n-            hidden_states_p = hidden_states_p.transpose(0, 1)\n-            hidden_states_p = hidden_states_p[:num_prefill_tokens]\n-            # In some instances, the following `bcdt_proj` op\n-            # requires contiguous inputs\n-            # (e.g. if the Marlin kernel is used).\n-            hidden_states_p = hidden_states_p.contiguous()\n+                query_start_loc=query_start_loc_p).transpose(0, 1)[:num_prefill_tokens].contiguous()\n \n             B, C, dt = self._project_ssm_parameters(hidden_states_p)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
