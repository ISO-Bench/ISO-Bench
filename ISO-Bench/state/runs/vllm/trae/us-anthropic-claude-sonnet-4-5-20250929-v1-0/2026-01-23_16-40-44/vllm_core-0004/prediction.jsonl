{"instance_id": "vllm-project__vllm-1d35662", "model_patch": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 329b03a..6568acd 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -154,6 +154,36 @@ class GrammarCompilerCache:\n         return cls._cache[cache_key]\n \n \n+class GrammarValidationCache:\n+    \"\"\"\n+    Cache for validated Grammar objects to avoid redundant validation.\n+    \n+    This cache stores validated Grammar objects by their string representation,\n+    avoiding the overhead of re-validating the same grammar/schema multiple times.\n+    \"\"\"\n+    _cache: dict[str, bool] = {}\n+\n+    @classmethod\n+    def validate_json_schema(cls, json_str: str) -> None:\n+        \"\"\"Validate a JSON schema, using cache if available.\"\"\"\n+        if json_str not in cls._cache:\n+            try:\n+                xgr.Grammar.from_json_schema(json_str)\n+                cls._cache[json_str] = True\n+            except RuntimeError as err:\n+                raise ValueError(str(err)) from err\n+\n+    @classmethod\n+    def validate_ebnf_grammar(cls, grammar_str: str) -> None:\n+        \"\"\"Validate an EBNF grammar, using cache if available.\"\"\"\n+        if grammar_str not in cls._cache:\n+            try:\n+                xgr.Grammar.from_ebnf(grammar_str)\n+                cls._cache[grammar_str] = True\n+            except RuntimeError as err:\n+                raise ValueError(str(err)) from err\n+\n+\n @dataclass\n class GrammarConfig:\n     \"\"\"Serializable configuration for grammar compilation\"\"\"\n@@ -184,10 +214,7 @@ class GrammarConfig:\n             # Validate the schema and raise ValueError here if it is invalid.\n             # This is to avoid exceptions in model execution, which will crash\n             # the engine worker process.\n-            try:\n-                xgr.Grammar.from_json_schema(json_str)\n-            except RuntimeError as err:\n-                raise ValueError(str(err)) from err\n+            GrammarValidationCache.validate_json_schema(json_str)\n \n             return cls(json_str=json_str,\n                        vocab_size=model_config.hf_text_config.vocab_size,\n@@ -211,10 +238,7 @@ class GrammarConfig:\n             # Validate the grammar and raise ValueError here if it is invalid.\n             # This is to avoid exceptions in model execution, which will crash\n             # the engine worker process.\n-            try:\n-                xgr.Grammar.from_ebnf(grammar_str)\n-            except RuntimeError as err:\n-                raise ValueError(str(err)) from err\n+            GrammarValidationCache.validate_ebnf_grammar(grammar_str)\n \n             return cls(grammar_str=grammar_str,\n                        vocab_size=model_config.hf_text_config.vocab_size,\n@@ -231,10 +255,7 @@ class GrammarConfig:\n             )\n         elif guided_params.choice:\n             choice_str = GrammarConfig.choice_as_grammar(guided_params.choice)\n-            try:\n-                xgr.Grammar.from_ebnf(choice_str)\n-            except RuntimeError as err:\n-                raise ValueError(str(err)) from err\n+            GrammarValidationCache.validate_ebnf_grammar(choice_str)\n \n             return cls(\n                 grammar_str=choice_str,\n@@ -270,6 +291,8 @@ class XGrammarLogitsProcessor:\n \n     ctx: xgr.CompiledGrammar | None = None\n     token_bitmask: torch.Tensor = None  # type: ignore[assignment]\n+    token_bitmask_device: torch.Tensor = None  # type: ignore[assignment]\n+    cached_device: torch.device | None = None\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n@@ -284,6 +307,8 @@ class XGrammarLogitsProcessor:\n         self.matchers = []\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n+        self.token_bitmask_device = None  # type: ignore[assignment]\n+        self.cached_device = None\n         self.prefilled = False\n \n     def _ensure_ctx(self):\n@@ -313,20 +338,22 @@ class XGrammarLogitsProcessor:\n                 self.batch_size, self.config.vocab_size)\n \n         if not self.prefilled:\n-            # Have not sampled a token yet\n+            # Have not sampled a token yet - just fill bitmask for initial state\n             self.prefilled = True\n+            for i, matcher in enumerate(self.matchers):\n+                if not matcher.is_terminated():\n+                    # @ubospica: ideally, fill_next_token_bitmask should be\n+                    # parallelized with model decoding\n+                    # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)\n         else:\n+            # Combined loop: accept token and fill bitmask in single iteration\n+            # This reduces the number of is_terminated() checks by half\n+            sampled_token = input_ids[-1]\n             for i, matcher in enumerate(self.matchers):\n                 if not matcher.is_terminated():\n-                    sampled_token = input_ids[-1]\n-                    assert self.matchers[i].accept_token(sampled_token)\n-\n-        for i, matcher in enumerate(self.matchers):\n-            if not matcher.is_terminated():\n-                # @ubospica: ideally, fill_next_token_bitmask should be\n-                # parallelized with model decoding\n-                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n-                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+                    assert matcher.accept_token(sampled_token)\n+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)\n \n         # token_bitmask is a CPU tensor for use with accept_token and\n         # fill_next_token_bitmask so we move it to the device of scores\n@@ -337,11 +364,18 @@ class XGrammarLogitsProcessor:\n             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n             scores = scores.to(\"cpu\").float().unsqueeze(0)\n \n+        # Cache the device-transferred token_bitmask to avoid repeated transfers\n+        if self.cached_device != scores.device:\n+            self.token_bitmask_device = self.token_bitmask.to(scores.device, non_blocking=True)\n+            self.cached_device = scores.device\n+        else:\n+            # Update the cached device tensor with new bitmask data\n+            self.token_bitmask_device.copy_(self.token_bitmask, non_blocking=True)\n+        \n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n         # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(\n-            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n+        xgr.apply_token_bitmask_inplace(scores, self.token_bitmask_device)\n         if device_type != \"cuda\":\n             scores = scores.to(dtype).to(device_type).squeeze()\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
