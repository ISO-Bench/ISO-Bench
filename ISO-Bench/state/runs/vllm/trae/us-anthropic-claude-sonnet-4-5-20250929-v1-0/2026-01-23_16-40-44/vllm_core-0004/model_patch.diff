diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index 329b03a..6568acd 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -154,6 +154,36 @@ class GrammarCompilerCache:
         return cls._cache[cache_key]
 
 
+class GrammarValidationCache:
+    """
+    Cache for validated Grammar objects to avoid redundant validation.
+    
+    This cache stores validated Grammar objects by their string representation,
+    avoiding the overhead of re-validating the same grammar/schema multiple times.
+    """
+    _cache: dict[str, bool] = {}
+
+    @classmethod
+    def validate_json_schema(cls, json_str: str) -> None:
+        """Validate a JSON schema, using cache if available."""
+        if json_str not in cls._cache:
+            try:
+                xgr.Grammar.from_json_schema(json_str)
+                cls._cache[json_str] = True
+            except RuntimeError as err:
+                raise ValueError(str(err)) from err
+
+    @classmethod
+    def validate_ebnf_grammar(cls, grammar_str: str) -> None:
+        """Validate an EBNF grammar, using cache if available."""
+        if grammar_str not in cls._cache:
+            try:
+                xgr.Grammar.from_ebnf(grammar_str)
+                cls._cache[grammar_str] = True
+            except RuntimeError as err:
+                raise ValueError(str(err)) from err
+
+
 @dataclass
 class GrammarConfig:
     """Serializable configuration for grammar compilation"""
@@ -184,10 +214,7 @@ class GrammarConfig:
             # Validate the schema and raise ValueError here if it is invalid.
             # This is to avoid exceptions in model execution, which will crash
             # the engine worker process.
-            try:
-                xgr.Grammar.from_json_schema(json_str)
-            except RuntimeError as err:
-                raise ValueError(str(err)) from err
+            GrammarValidationCache.validate_json_schema(json_str)
 
             return cls(json_str=json_str,
                        vocab_size=model_config.hf_text_config.vocab_size,
@@ -211,10 +238,7 @@ class GrammarConfig:
             # Validate the grammar and raise ValueError here if it is invalid.
             # This is to avoid exceptions in model execution, which will crash
             # the engine worker process.
-            try:
-                xgr.Grammar.from_ebnf(grammar_str)
-            except RuntimeError as err:
-                raise ValueError(str(err)) from err
+            GrammarValidationCache.validate_ebnf_grammar(grammar_str)
 
             return cls(grammar_str=grammar_str,
                        vocab_size=model_config.hf_text_config.vocab_size,
@@ -231,10 +255,7 @@ class GrammarConfig:
             )
         elif guided_params.choice:
             choice_str = GrammarConfig.choice_as_grammar(guided_params.choice)
-            try:
-                xgr.Grammar.from_ebnf(choice_str)
-            except RuntimeError as err:
-                raise ValueError(str(err)) from err
+            GrammarValidationCache.validate_ebnf_grammar(choice_str)
 
             return cls(
                 grammar_str=choice_str,
@@ -270,6 +291,8 @@ class XGrammarLogitsProcessor:
 
     ctx: xgr.CompiledGrammar | None = None
     token_bitmask: torch.Tensor = None  # type: ignore[assignment]
+    token_bitmask_device: torch.Tensor = None  # type: ignore[assignment]
+    cached_device: torch.device | None = None
     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)
     batch_size: int = field(default=1)
     prefilled: bool = field(default=False)
@@ -284,6 +307,8 @@ class XGrammarLogitsProcessor:
         self.matchers = []
         self.batch_size = 1
         self.token_bitmask = None  # type: ignore[assignment]
+        self.token_bitmask_device = None  # type: ignore[assignment]
+        self.cached_device = None
         self.prefilled = False
 
     def _ensure_ctx(self):
@@ -313,20 +338,22 @@ class XGrammarLogitsProcessor:
                 self.batch_size, self.config.vocab_size)
 
         if not self.prefilled:
-            # Have not sampled a token yet
+            # Have not sampled a token yet - just fill bitmask for initial state
             self.prefilled = True
+            for i, matcher in enumerate(self.matchers):
+                if not matcher.is_terminated():
+                    # @ubospica: ideally, fill_next_token_bitmask should be
+                    # parallelized with model decoding
+                    # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303
+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)
         else:
+            # Combined loop: accept token and fill bitmask in single iteration
+            # This reduces the number of is_terminated() checks by half
+            sampled_token = input_ids[-1]
             for i, matcher in enumerate(self.matchers):
                 if not matcher.is_terminated():
-                    sampled_token = input_ids[-1]
-                    assert self.matchers[i].accept_token(sampled_token)
-
-        for i, matcher in enumerate(self.matchers):
-            if not matcher.is_terminated():
-                # @ubospica: ideally, fill_next_token_bitmask should be
-                # parallelized with model decoding
-                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303
-                matcher.fill_next_token_bitmask(self.token_bitmask, i)
+                    assert matcher.accept_token(sampled_token)
+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)
 
         # token_bitmask is a CPU tensor for use with accept_token and
         # fill_next_token_bitmask so we move it to the device of scores
@@ -337,11 +364,18 @@ class XGrammarLogitsProcessor:
             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22
             scores = scores.to("cpu").float().unsqueeze(0)
 
+        # Cache the device-transferred token_bitmask to avoid repeated transfers
+        if self.cached_device != scores.device:
+            self.token_bitmask_device = self.token_bitmask.to(scores.device, non_blocking=True)
+            self.cached_device = scores.device
+        else:
+            # Update the cached device tensor with new bitmask data
+            self.token_bitmask_device.copy_(self.token_bitmask, non_blocking=True)
+        
         # Note: In this method, if the tensors have different dimensions
         # on CPU device fails, but on GPU it runs without error. Hence the
         # unsqueeze above for scores, to match the token bitmask shape
-        xgr.apply_token_bitmask_inplace(
-            scores, self.token_bitmask.to(scores.device, non_blocking=True))
+        xgr.apply_token_bitmask_inplace(scores, self.token_bitmask_device)
         if device_type != "cuda":
             scores = scores.to(dtype).to(device_type).squeeze()
 
