{"instance_id": "vllm-project__vllm-90f1e55", "model_patch": "diff --git a/csrc/ops.h b/csrc/ops.h\nindex 7f3e6b6..ab47500 100644\n--- a/csrc/ops.h\n+++ b/csrc/ops.h\n@@ -252,14 +252,6 @@ void get_cutlass_moe_mm_data(\n     const int64_t num_experts, const int64_t n, const int64_t k,\n     const std::optional<torch::Tensor>& blockscale_offsets);\n \n-void get_cutlass_pplx_moe_mm_data(torch::Tensor& expert_offsets,\n-                                  torch::Tensor& problem_sizes1,\n-                                  torch::Tensor& problem_sizes2,\n-                                  const torch::Tensor& expert_num_tokens,\n-                                  const int64_t num_local_experts,\n-                                  const int64_t padded_m, const int64_t n,\n-                                  const int64_t k);\n-\n void cutlass_scaled_mm_azp(torch::Tensor& out, torch::Tensor const& a,\n                            torch::Tensor const& b,\n                            torch::Tensor const& a_scales,\ndiff --git a/csrc/quantization/cutlass_w8a8/moe/moe_data.cu b/csrc/quantization/cutlass_w8a8/moe/moe_data.cu\nindex 623c9a2..31e1678 100644\n--- a/csrc/quantization/cutlass_w8a8/moe/moe_data.cu\n+++ b/csrc/quantization/cutlass_w8a8/moe/moe_data.cu\n@@ -157,37 +157,3 @@ void get_cutlass_moe_mm_data_caller(\n       static_cast<int32_t*>(atomic_buffer.data_ptr()), topk_ids.numel(),\n       topk_ids.size(1));\n }\n-\n-__global__ void compute_pplx_data(int32_t* expert_offsets,\n-                                  int32_t* problem_sizes1,\n-                                  int32_t* problem_sizes2,\n-                                  const int32_t* __restrict__ expert_num_tokens,\n-                                  const int padded_m, const int n,\n-                                  const int k) {\n-  int expert_idx = threadIdx.x;\n-\n-  expert_offsets[expert_idx] = expert_idx * padded_m;\n-  problem_sizes1[expert_idx * 3] = expert_num_tokens[expert_idx];\n-  problem_sizes1[expert_idx * 3 + 1] = 2 * n;\n-  problem_sizes1[expert_idx * 3 + 2] = k;\n-  problem_sizes2[expert_idx * 3] = expert_num_tokens[expert_idx];\n-  problem_sizes2[expert_idx * 3 + 1] = k;\n-  problem_sizes2[expert_idx * 3 + 2] = n;\n-}\n-\n-void get_cutlass_pplx_moe_mm_data_caller(torch::Tensor& expert_offsets,\n-                                         torch::Tensor& problem_sizes1,\n-                                         torch::Tensor& problem_sizes2,\n-                                         const torch::Tensor& expert_num_tokens,\n-                                         const int64_t num_local_experts,\n-                                         const int64_t padded_m,\n-                                         const int64_t n, const int64_t k) {\n-  auto stream = at::cuda::getCurrentCUDAStream(expert_offsets.device().index());\n-\n-  compute_pplx_data<<<1, num_local_experts, 0, stream>>>(\n-      static_cast<int32_t*>(expert_offsets.data_ptr()),\n-      static_cast<int32_t*>(problem_sizes1.data_ptr()),\n-      static_cast<int32_t*>(problem_sizes2.data_ptr()),\n-      static_cast<const int32_t*>(expert_num_tokens.data_ptr()), padded_m, n,\n-      k);\n-}\ndiff --git a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu\nindex 31b6048..3f721c4 100644\n--- a/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu\n+++ b/csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu\n@@ -65,14 +65,6 @@ void get_cutlass_moe_mm_data_caller(\n     torch::Tensor& input_permutation, torch::Tensor& output_permutation,\n     const int64_t num_experts, const int64_t n, const int64_t k,\n     const std::optional<torch::Tensor>& blockscale_offsets);\n-\n-void get_cutlass_pplx_moe_mm_data_caller(torch::Tensor& expert_offsets,\n-                                         torch::Tensor& problem_sizes1,\n-                                         torch::Tensor& problem_sizes2,\n-                                         const torch::Tensor& expert_num_tokens,\n-                                         const int64_t num_local_experts,\n-                                         const int64_t padded_m,\n-                                         const int64_t n, const int64_t k);\n #endif\n \n void cutlass_scaled_mm_azp_sm75(torch::Tensor& c, torch::Tensor const& a,\n@@ -270,30 +262,6 @@ void get_cutlass_moe_mm_data(\n       version_num, \". Required capability: 90 or 100\");\n }\n \n-void get_cutlass_pplx_moe_mm_data(torch::Tensor& expert_offsets,\n-                                  torch::Tensor& problem_sizes1,\n-                                  torch::Tensor& problem_sizes2,\n-                                  const torch::Tensor& expert_num_tokens,\n-                                  const int64_t num_local_experts,\n-                                  const int64_t padded_m, const int64_t n,\n-                                  const int64_t k) {\n-  // This function currently gets compiled only if we have a valid cutlass moe\n-  // mm to run it for.\n-  int32_t version_num = get_sm_version_num();\n-#if (defined ENABLE_CUTLASS_MOE_SM90 && ENABLE_CUTLASS_MOE_SM90) || \\\n-    (defined ENABLE_CUTLASS_MOE_SM100 && ENABLE_CUTLASS_MOE_SM100)\n-  get_cutlass_pplx_moe_mm_data_caller(expert_offsets, problem_sizes1,\n-                                      problem_sizes2, expert_num_tokens,\n-                                      num_local_experts, padded_m, n, k);\n-  return;\n-#endif\n-  TORCH_CHECK_NOT_IMPLEMENTED(\n-      false,\n-      \"No compiled get_cutlass_pplx_moe_mm_data: no cutlass_scaled_mm kernel \"\n-      \"for CUDA device capability: \",\n-      version_num, \". Required capability: 90 or 100\");\n-}\n-\n void cutlass_scaled_mm_azp(torch::Tensor& c, torch::Tensor const& a,\n                            torch::Tensor const& b,\n                            torch::Tensor const& a_scales,\ndiff --git a/csrc/torch_bindings.cpp b/csrc/torch_bindings.cpp\nindex 79e2575..b2932f1 100644\n--- a/csrc/torch_bindings.cpp\n+++ b/csrc/torch_bindings.cpp\n@@ -467,22 +467,6 @@ TORCH_LIBRARY_EXPAND(TORCH_EXTENSION_NAME, ops) {\n       {stride_tag});\n   ops.impl(\"get_cutlass_moe_mm_data\", torch::kCUDA, &get_cutlass_moe_mm_data);\n \n-  // A function that computes data required to run fused MoE with w8a8 grouped\n-  // GEMM and PPLX. It takes expert_num_tokens and non_zero_expert_idxs\n-  // as an input, and computes expert_offsets (token start indices of each\n-  // expert). In addition to this, it computes problem sizes for each expert's\n-  // multiplication used by the two mms called from fused MoE operation.\n-  ops.def(\n-      \"get_cutlass_pplx_moe_mm_data(Tensor! expert_offsets, \"\n-      \"                             Tensor! problem_sizes1, \"\n-      \"                             Tensor! problem_sizes2, \"\n-      \"                             Tensor expert_num_tokens, \"\n-      \"                             int num_local_experts, int padded_m, \"\n-      \"                             int n, int k) -> ()\",\n-      {stride_tag});\n-  ops.impl(\"get_cutlass_pplx_moe_mm_data\", torch::kCUDA,\n-           &get_cutlass_pplx_moe_mm_data);\n-\n   // Check if cutlass scaled_mm supports block quantization (used by DeepSeekV3)\n   ops.def(\n       \"cutlass_scaled_mm_supports_block_fp8(int cuda_device_capability) -> \"\ndiff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex cf296a3..39a2e47 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -909,30 +909,6 @@ def shuffle_rows(input_tensor: torch.Tensor, dst2src_map: torch.Tensor):\n     return output_tensor\n \n \n-def get_cutlass_pplx_moe_mm_data(expert_offsets: torch.Tensor,\n-                                 problem_sizes1: torch.Tensor,\n-                                 problem_sizes2: torch.Tensor,\n-                                 expert_num_tokens: torch.Tensor,\n-                                 num_local_experts: int, padded_m: int, n: int,\n-                                 k: int):\n-    \"\"\"\n-    Prepare data necessary to perform CUTLASS grouped matrix multiplications\n-    used in CUTLASS-based fused MoE.\n-\n-    The function takes in expert_num_tokens (token count per expert) and\n-    non_zero_expert_idxs (consecutive indices of experts with non-zero token \n-    counts) and uses them to compute:\n-    - expert_offsets: Indices that mark at which token index each expert begins\n-                      its computation.\n-    - problem_sizes1, problem_sizes2: MxNxK sizes of each expert's\n-                                      multiplication in two grouped MMs used in\n-                                      the fused MoE operation.\n-    \"\"\"\n-    return torch.ops._C.get_cutlass_pplx_moe_mm_data(\n-        expert_offsets, problem_sizes1, problem_sizes2, expert_num_tokens,\n-        num_local_experts, padded_m, n, k)\n-\n-\n def cutlass_moe_mm(out_tensors: torch.Tensor, a_tensors: torch.Tensor,\n                    b_tensors: torch.Tensor, a_scales: torch.Tensor,\n                    b_scales: torch.Tensor, expert_offsets: torch.Tensor,\ndiff --git a/vllm/model_executor/layers/fused_moe/cutlass_moe.py b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\nindex ff49d7b..a7f9704 100644\n--- a/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n@@ -106,19 +106,22 @@ def run_cutlass_moe_fp8(\n     if use_batched_format:\n         assert expert_num_tokens is not None\n \n-        expert_offsets = torch.empty((local_E),\n-                                     dtype=torch.int32,\n-                                     device=device)\n+        expert_offsets = torch.arange(0, local_E * padded_M, padded_M,\n+                                     dtype=torch.int32, device=device)\n+        \n         problem_sizes1 = torch.empty((local_E, 3),\n                                      dtype=torch.int32,\n                                      device=device)\n+        problem_sizes1[:, 0] = expert_num_tokens\n+        problem_sizes1[:, 1] = 2 * N\n+        problem_sizes1[:, 2] = K\n+        \n         problem_sizes2 = torch.empty((local_E, 3),\n                                      dtype=torch.int32,\n                                      device=device)\n-\n-        ops.get_cutlass_pplx_moe_mm_data(expert_offsets, problem_sizes1,\n-                                         problem_sizes2, expert_num_tokens,\n-                                         local_E, padded_M, N, K)\n+        problem_sizes2[:, 0] = expert_num_tokens\n+        problem_sizes2[:, 1] = K\n+        problem_sizes2[:, 2] = N\n \n         w1_scale = w1_scale.reshape(w1_scale.size(0), -1)\n         w2_scale = w2_scale.reshape(w2_scale.size(0), -1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
