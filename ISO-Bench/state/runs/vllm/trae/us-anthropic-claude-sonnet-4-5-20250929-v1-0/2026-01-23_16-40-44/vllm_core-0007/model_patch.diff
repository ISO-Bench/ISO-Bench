diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py
index acc3a94..32aee44 100644
--- a/vllm/v1/outputs.py
+++ b/vllm/v1/outputs.py
@@ -8,7 +8,7 @@ import torch
 class SamplerOutput:
 
     # [num_reqs]
-    sampled_token_ids: List[int]
+    sampled_token_ids: torch.Tensor
 
     # [num_reqs, max_num_logprobs + 1]
     logprob_token_ids: Optional[torch.Tensor]
diff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py
index 7cd42ca..cc2f357 100644
--- a/vllm/v1/sample/sampler.py
+++ b/vllm/v1/sample/sampler.py
@@ -47,12 +47,10 @@ class Sampler(nn.Module):
         logits = self.apply_temperature(logits, sampling_metadata.temperature)
         # Sample the next token.
         sampled = self.sample(logits, sampling_metadata)
-        # Use int32 to reduce the tensor size.
-        sampled = sampled.to(torch.int32)
 
         # NOTE: CPU-GPU synchronization happens here.
         sampler_output = SamplerOutput(
-            sampled_token_ids=sampled.tolist(),
+            sampled_token_ids=sampled,
             logprob_token_ids=topk_indices,
             logprobs=topk_logprobs,
             prompt_logprob_token_ids=None,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 4b3c325..dca8235 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -201,6 +201,19 @@ class GPUModelRunner:
                                         device="cpu",
                                         pin_memory=self.pin_memory)
         self.seq_lens_np = self.seq_lens_cpu.numpy()
+        
+        # Persistent buffers for cascade attention.
+        self.cu_prefix_query_lens = torch.zeros(2,
+                                                dtype=torch.int32,
+                                                device=self.device)
+        self.prefix_kv_lens = torch.zeros(1,
+                                          dtype=torch.int32,
+                                          device=self.device)
+        self.suffix_kv_lens_cpu = torch.zeros(self.max_num_reqs,
+                                              dtype=torch.int32,
+                                              device="cpu",
+                                              pin_memory=self.pin_memory)
+        self.suffix_kv_lens_np = self.suffix_kv_lens_cpu.numpy()
 
     def _update_states(self, scheduler_output: "SchedulerOutput") -> None:
         # Remove stopped requests from the cached states.
@@ -497,16 +510,18 @@ class GPUModelRunner:
             )
 
         if use_cascade:
-            # TODO: Optimize.
-            cu_prefix_query_lens = torch.tensor(
-                [0, total_num_scheduled_tokens],
-                dtype=torch.int32,
-                device=self.device)
-            prefix_kv_lens = torch.tensor([common_prefix_len],
-                                          dtype=torch.int32,
-                                          device=self.device)
-            suffix_kv_lens = (self.seq_lens_np[:num_reqs] - common_prefix_len)
-            suffix_kv_lens = torch.from_numpy(suffix_kv_lens).to(self.device)
+            # Use persistent buffers to avoid creating new tensors.
+            self.cu_prefix_query_lens[0] = 0
+            self.cu_prefix_query_lens[1] = total_num_scheduled_tokens
+            cu_prefix_query_lens = self.cu_prefix_query_lens
+            
+            self.prefix_kv_lens[0] = common_prefix_len
+            prefix_kv_lens = self.prefix_kv_lens
+            
+            np.subtract(self.seq_lens_np[:num_reqs], common_prefix_len,
+                       out=self.suffix_kv_lens_np[:num_reqs])
+            suffix_kv_lens = self.suffix_kv_lens_cpu[:num_reqs].to(
+                self.device, non_blocking=True)
         else:
             cu_prefix_query_lens = None
             prefix_kv_lens = None
@@ -775,10 +790,23 @@ class GPUModelRunner:
             sampling_metadata=sampling_metadata,
         )
 
-        sampled_token_ids = sampler_output.sampled_token_ids
+        sampled_token_ids_tensor = sampler_output.sampled_token_ids
         # TODO(woosuk): The following loop can be slow since it iterates over
         # the requests one by one. Optimize.
         num_reqs = self.input_batch.num_reqs
+        
+        # NOTE: CPU-GPU synchronization happens here.
+        sampled_token_ids_cpu = sampled_token_ids_tensor.cpu()
+        if sampler_output.logprob_token_ids is None:
+            logprob_token_ids = None
+        else:
+            logprob_token_ids = sampler_output.logprob_token_ids.cpu()
+        if sampler_output.logprobs is None:
+            logprobs = None
+        else:
+            logprobs = sampler_output.logprobs.cpu()
+        
+        sampled_token_ids_list = sampled_token_ids_cpu.tolist()
         for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):
             assert req_id is not None
             req_state = self.requests[req_id]
@@ -787,7 +815,7 @@ class GPUModelRunner:
             assert seq_len <= req_state.num_tokens
             if seq_len == req_state.num_tokens:
                 # Append the sampled token to the output token ids.
-                token_id = sampled_token_ids[i]
+                token_id = sampled_token_ids_list[i]
                 self.input_batch.token_ids_cpu[i, seq_len] = token_id
                 self.input_batch.num_tokens[i] += 1
                 req_state.output_token_ids.append(token_id)
@@ -799,15 +827,6 @@ class GPUModelRunner:
                     # This relies on cuda-specific torch-internal impl details
                     generator.set_offset(generator.get_offset() - 4)
 
-        if sampler_output.logprob_token_ids is None:
-            logprob_token_ids = None
-        else:
-            logprob_token_ids = sampler_output.logprob_token_ids.cpu()
-        if sampler_output.logprobs is None:
-            logprobs = None
-        else:
-            logprobs = sampler_output.logprobs.cpu()
-
         # num_reqs entries should be non-None
         assert all(
             req_id is not None for req_id in
@@ -817,7 +836,7 @@ class GPUModelRunner:
         model_runner_output = ModelRunnerOutput(
             req_ids=req_ids,
             req_id_to_index=self.input_batch.req_id_to_index,
-            sampled_token_ids=sampled_token_ids,
+            sampled_token_ids=sampled_token_ids_list,
             logprob_token_ids_cpu=logprob_token_ids,
             logprobs_cpu=logprobs,
         )
