{"instance_id": "vllm-project__vllm-2a0309a", "model_patch": "diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex acc3a94..32aee44 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -8,7 +8,7 @@ import torch\n class SamplerOutput:\n \n     # [num_reqs]\n-    sampled_token_ids: List[int]\n+    sampled_token_ids: torch.Tensor\n \n     # [num_reqs, max_num_logprobs + 1]\n     logprob_token_ids: Optional[torch.Tensor]\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 7cd42ca..cc2f357 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -47,12 +47,10 @@ class Sampler(nn.Module):\n         logits = self.apply_temperature(logits, sampling_metadata.temperature)\n         # Sample the next token.\n         sampled = self.sample(logits, sampling_metadata)\n-        # Use int32 to reduce the tensor size.\n-        sampled = sampled.to(torch.int32)\n \n         # NOTE: CPU-GPU synchronization happens here.\n         sampler_output = SamplerOutput(\n-            sampled_token_ids=sampled.tolist(),\n+            sampled_token_ids=sampled,\n             logprob_token_ids=topk_indices,\n             logprobs=topk_logprobs,\n             prompt_logprob_token_ids=None,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4b3c325..dca8235 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -201,6 +201,19 @@ class GPUModelRunner:\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n         self.seq_lens_np = self.seq_lens_cpu.numpy()\n+        \n+        # Persistent buffers for cascade attention.\n+        self.cu_prefix_query_lens = torch.zeros(2,\n+                                                dtype=torch.int32,\n+                                                device=self.device)\n+        self.prefix_kv_lens = torch.zeros(1,\n+                                          dtype=torch.int32,\n+                                          device=self.device)\n+        self.suffix_kv_lens_cpu = torch.zeros(self.max_num_reqs,\n+                                              dtype=torch.int32,\n+                                              device=\"cpu\",\n+                                              pin_memory=self.pin_memory)\n+        self.suffix_kv_lens_np = self.suffix_kv_lens_cpu.numpy()\n \n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n@@ -497,16 +510,18 @@ class GPUModelRunner:\n             )\n \n         if use_cascade:\n-            # TODO: Optimize.\n-            cu_prefix_query_lens = torch.tensor(\n-                [0, total_num_scheduled_tokens],\n-                dtype=torch.int32,\n-                device=self.device)\n-            prefix_kv_lens = torch.tensor([common_prefix_len],\n-                                          dtype=torch.int32,\n-                                          device=self.device)\n-            suffix_kv_lens = (self.seq_lens_np[:num_reqs] - common_prefix_len)\n-            suffix_kv_lens = torch.from_numpy(suffix_kv_lens).to(self.device)\n+            # Use persistent buffers to avoid creating new tensors.\n+            self.cu_prefix_query_lens[0] = 0\n+            self.cu_prefix_query_lens[1] = total_num_scheduled_tokens\n+            cu_prefix_query_lens = self.cu_prefix_query_lens\n+            \n+            self.prefix_kv_lens[0] = common_prefix_len\n+            prefix_kv_lens = self.prefix_kv_lens\n+            \n+            np.subtract(self.seq_lens_np[:num_reqs], common_prefix_len,\n+                       out=self.suffix_kv_lens_np[:num_reqs])\n+            suffix_kv_lens = self.suffix_kv_lens_cpu[:num_reqs].to(\n+                self.device, non_blocking=True)\n         else:\n             cu_prefix_query_lens = None\n             prefix_kv_lens = None\n@@ -775,10 +790,23 @@ class GPUModelRunner:\n             sampling_metadata=sampling_metadata,\n         )\n \n-        sampled_token_ids = sampler_output.sampled_token_ids\n+        sampled_token_ids_tensor = sampler_output.sampled_token_ids\n         # TODO(woosuk): The following loop can be slow since it iterates over\n         # the requests one by one. Optimize.\n         num_reqs = self.input_batch.num_reqs\n+        \n+        # NOTE: CPU-GPU synchronization happens here.\n+        sampled_token_ids_cpu = sampled_token_ids_tensor.cpu()\n+        if sampler_output.logprob_token_ids is None:\n+            logprob_token_ids = None\n+        else:\n+            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n+        if sampler_output.logprobs is None:\n+            logprobs = None\n+        else:\n+            logprobs = sampler_output.logprobs.cpu()\n+        \n+        sampled_token_ids_list = sampled_token_ids_cpu.tolist()\n         for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n             assert req_id is not None\n             req_state = self.requests[req_id]\n@@ -787,7 +815,7 @@ class GPUModelRunner:\n             assert seq_len <= req_state.num_tokens\n             if seq_len == req_state.num_tokens:\n                 # Append the sampled token to the output token ids.\n-                token_id = sampled_token_ids[i]\n+                token_id = sampled_token_ids_list[i]\n                 self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                 self.input_batch.num_tokens[i] += 1\n                 req_state.output_token_ids.append(token_id)\n@@ -799,15 +827,6 @@ class GPUModelRunner:\n                     # This relies on cuda-specific torch-internal impl details\n                     generator.set_offset(generator.get_offset() - 4)\n \n-        if sampler_output.logprob_token_ids is None:\n-            logprob_token_ids = None\n-        else:\n-            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n-        if sampler_output.logprobs is None:\n-            logprobs = None\n-        else:\n-            logprobs = sampler_output.logprobs.cpu()\n-\n         # num_reqs entries should be non-None\n         assert all(\n             req_id is not None for req_id in\n@@ -817,7 +836,7 @@ class GPUModelRunner:\n         model_runner_output = ModelRunnerOutput(\n             req_ids=req_ids,\n             req_id_to_index=self.input_batch.req_id_to_index,\n-            sampled_token_ids=sampled_token_ids,\n+            sampled_token_ids=sampled_token_ids_list,\n             logprob_token_ids_cpu=logprob_token_ids,\n             logprobs_cpu=logprobs,\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
