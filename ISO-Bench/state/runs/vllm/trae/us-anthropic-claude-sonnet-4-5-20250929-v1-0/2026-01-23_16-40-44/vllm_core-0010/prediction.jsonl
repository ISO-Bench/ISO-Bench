{"instance_id": "vllm-project__vllm-270a5da", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex b53a540..6956750 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -503,15 +503,29 @@ class MambaMixer2(CustomOp):\n         else:\n \n             n_groups = self.n_groups // self.tp_size\n-            A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n-            dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n-            dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n-            D = self.D[:, None, ...].expand(-1, self.head_dim)\n-            B = B.view(-1, n_groups, B.shape[1] // n_groups)\n-            C = C.view(-1, n_groups, C.shape[1] // n_groups)\n+            num_heads_per_rank = self.num_heads // self.tp_size\n+            \n+            # Optimize tensor preparations to reduce overhead\n+            # Use unsqueeze instead of indexing with None for clarity and potential optimization\n+            # Pre-compute dimensions to avoid repeated calculations\n+            A = self.A.unsqueeze(1).unsqueeze(2).expand(\n+                num_heads_per_rank, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n+            \n+            # Optimize dt expansion - use unsqueeze for better performance\n+            dt = dt.unsqueeze(2).expand(-1, -1, self.head_dim)\n+            \n+            # Optimize dt_bias and D expansions\n+            dt_bias = self.dt_bias.unsqueeze(1).expand(num_heads_per_rank, self.head_dim)\n+            D = self.D.unsqueeze(1).expand(num_heads_per_rank, self.head_dim)\n+            \n+            # Pre-compute B and C dimensions\n+            b_c_dim = B.shape[1] // n_groups\n+            B = B.view(-1, n_groups, b_c_dim)\n+            C = C.view(-1, n_groups, b_c_dim)\n+            \n+            # Reshape hidden states\n             hidden_states_reshaped = hidden_states.view(\n-                -1, self.num_heads // self.tp_size, self.head_dim)\n+                -1, num_heads_per_rank, self.head_dim)\n \n             # - the hidden is reshaped into number of current batches\n             # - in this case there is no more prefill, so the batches gen\n@@ -535,7 +549,7 @@ class MambaMixer2(CustomOp):\n                 state_batch_indices=mamba_cache_params.state_indices_tensor,\n             )\n             hidden_states = hidden_states.view(\n-                -1, (self.num_heads // self.tp_size) * self.head_dim)\n+                -1, num_heads_per_rank * self.head_dim)\n \n         # # 4. gated MLP\n         hidden_states = self.norm(hidden_states, gate)\n", "model_name_or_path": "gpt-5-2025-08-07"}
