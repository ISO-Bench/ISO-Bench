{"instance_id": "vllm-project__vllm-cc466a3", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 1f19d20..2be6124 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -156,9 +156,11 @@ def _apply_min_tokens_penalty(\n     \"\"\"Apply min_tokens penalty which sets stop tokens to -inf if min_tokens\n         have not been generated yet\n     \"\"\"\n-    # list of indices in logits that will be set to -inf\n-    logits_to_penalize: List[Tuple[int, int]] = []\n+    # Collect all sequence indices and token ids that need penalizing\n+    seq_indices_to_penalize: List[int] = []\n+    token_ids_to_penalize_list: List[int] = []\n     logits_applied = 0\n+    \n     for seq_group in sampling_metadata.seq_groups:\n         seq_ids = seq_group.seq_ids\n         sampling_params = seq_group.sampling_params\n@@ -173,23 +175,23 @@ def _apply_min_tokens_penalty(\n         min_tokens = sampling_params.min_tokens\n         token_ids_to_penalize = sampling_params.all_stop_token_ids\n         if min_tokens > 0 and token_ids_to_penalize:\n-            seqs_to_penalize = []\n+            # Find sequences that need penalizing\n+            seqs_needing_penalty = []\n             for j, seq_id in enumerate(seq_ids):\n                 seq_data = seq_group.seq_data[seq_id]\n                 if len(seq_data.output_token_ids) < min_tokens:\n-                    seqs_to_penalize.append(j)\n+                    seqs_needing_penalty.append(start_idx + j)\n \n-            if seqs_to_penalize:\n-                # convert to the index into logits\n-                seqs_to_penalize = [start_idx + j for j in seqs_to_penalize]\n-                # itertools.product pairs each seq index with every token id\n-                logits_to_penalize.extend(\n-                    itertools.product(seqs_to_penalize, token_ids_to_penalize))\n+            if seqs_needing_penalty:\n+                # For each sequence, penalize all stop tokens\n+                num_tokens = len(token_ids_to_penalize)\n+                for seq_idx in seqs_needing_penalty:\n+                    seq_indices_to_penalize.extend([seq_idx] * num_tokens)\n+                    token_ids_to_penalize_list.extend(token_ids_to_penalize)\n \n-    if logits_to_penalize:\n-        # use zip and * to group indices along each dimension\n-        # eg. [ (1,2), (1,3), (5,6) ] -> ( (1,1,5), (2,3,6) )\n-        logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\n+    # Apply penalties using advanced indexing if there are any\n+    if seq_indices_to_penalize:\n+        logits[seq_indices_to_penalize, token_ids_to_penalize_list] = -float(\"inf\")\n \n     # verifies that no rows in logits were missed unexpectedly\n     assert logits_applied == logits.shape[0]\n@@ -207,10 +209,22 @@ def _apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n     output_bin_counts, output_mask = _get_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n \n-    repetition_penalties = repetition_penalties[:, None].repeat(1, vocab_size)\n-    repetition_penalties[~(prompt_mask | output_mask)] = 1.0\n-    logits = torch.where(logits > 0, logits / repetition_penalties,\n-                         logits * repetition_penalties)\n+    # Compute combined mask\n+    combined_mask = prompt_mask | output_mask\n+    \n+    # Apply repetition penalties using masked operations\n+    # Expand penalties for broadcasting: (num_seqs,) -> (num_seqs, 1)\n+    repetition_penalties_expanded = repetition_penalties.unsqueeze(1)\n+    \n+    # Apply penalties only where tokens appeared (combined_mask is True)\n+    # For positive logits: divide by penalty\n+    # For negative/zero logits: multiply by penalty\n+    # For non-masked positions: keep original value\n+    positive_mask = (logits > 0) & combined_mask\n+    negative_mask = (logits <= 0) & combined_mask\n+    \n+    logits = torch.where(positive_mask, logits / repetition_penalties_expanded, logits)\n+    logits = torch.where(negative_mask, logits * repetition_penalties_expanded, logits)\n \n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n@@ -242,8 +256,11 @@ def _apply_top_k_top_p(\n     logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n-    src = torch.arange(logits_idx.shape[-1],\n-                       device=logits_idx.device).expand_as(logits_idx)\n+    # Create inverse permutation to unsort\n+    vocab_size = logits_idx.shape[-1]\n+    src = torch.arange(vocab_size, device=logits_idx.device, dtype=logits_idx.dtype)\n+    # Expand only once using unsqueeze and expand\n+    src = src.unsqueeze(0).expand_as(logits_idx)\n     logits_idx_inv = torch.empty_like(logits_idx).scatter_(dim=-1,\n                                                            index=logits_idx,\n                                                            src=src)\n@@ -829,26 +846,32 @@ def _get_prompt_logprob_if_needed(\n     # Find prompt logprobs\n     prompt_logprobs: Optional[PromptLogprobs] = None\n     if (is_prompt and sampling_params.prompt_logprobs is not None):\n-        prompt_logprobs = []\n         num_logprobs = sampling_params.prompt_logprobs\n         next_prompt_tokens = _get_next_prompt_tokens(seq_group)\n-        for token_id in next_prompt_tokens:\n+        num_tokens = len(next_prompt_tokens)\n+        \n+        # Batch extract selected logprobs and ranks for all prompt tokens\n+        selected_logprobs_slice = selected_logprobs[selected_logprobs_idx:selected_logprobs_idx + num_tokens]\n+        ranks_slice = ranks[selected_logprobs_idx:selected_logprobs_idx + num_tokens]\n+        \n+        prompt_logprobs = []\n+        for i, token_id in enumerate(next_prompt_tokens):\n             # Calculate the prompt logprob of the real prompt tokens.\n             # Use tuple here for performance (to use to_list()).\n             # {token_id: (logprob, rank_from_vocab)}\n             prompt_logprobs_dict: Dict[int, Tuple[float, int]] = {\n-                token_id: (selected_logprobs[selected_logprobs_idx].item(),\n-                           ranks[selected_logprobs_idx].item())\n+                token_id: (selected_logprobs_slice[i].item(),\n+                           ranks_slice[i].item())\n             }\n \n             # Add top K prompt logprobs along with its rank.\n             if num_logprobs > 0:\n                 prompt_logprobs_dict.update(\n                     zip(\n-                        top_token_ids[top_logprob_idx, :num_logprobs].tolist(),\n+                        top_token_ids[top_logprob_idx + i, :num_logprobs].tolist(),\n                         zip(\n                             top_logprobs[\n-                                top_logprob_idx, :num_logprobs].tolist(),\n+                                top_logprob_idx + i, :num_logprobs].tolist(),\n                             # This is ranks. Since top_logprob is sorted,\n                             # we can just use a range here.\n                             range(1, num_logprobs + 1))))\n@@ -856,9 +879,9 @@ def _get_prompt_logprob_if_needed(\n                 token_id: Logprob(*logprob_and_rank)\n                 for token_id, logprob_and_rank in prompt_logprobs_dict.items()\n             })\n-            # + 1 to go to the next prompt token.\n-            top_logprob_idx += 1\n-            selected_logprobs_idx += 1\n+        \n+        top_logprob_idx += num_tokens\n+        selected_logprobs_idx += num_tokens\n     return prompt_logprobs, top_logprob_idx, selected_logprobs_idx\n \n \n@@ -882,19 +905,21 @@ def _get_sampled_logprob_if_needed(\n \n     if seq_group.do_sample:\n         assert len(next_token_ids) > 0\n-        for (next_token_id, parent_id) in zip(next_token_ids, parent_seq_ids):\n+        num_samples = len(next_token_ids)\n+        \n+        # Batch extract selected logprobs and ranks for all sampled tokens\n+        selected_logprobs_slice = selected_logprobs[selected_logprobs_idx:selected_logprobs_idx + num_samples]\n+        ranks_slice = ranks[selected_logprobs_idx:selected_logprobs_idx + num_samples]\n+        \n+        for i, (next_token_id, parent_id) in enumerate(zip(next_token_ids, parent_seq_ids)):\n             # Calculate the sample logprob of the real sampled tokens.\n             # Use tuple here for performance (to use to_list()).\n             # token_id: (logprob, rank_from_vocab)\n             sampled_logprobs_dict: Dict[int, Tuple[float, int]] = {\n                 next_token_id:\n-                (selected_logprobs[selected_logprobs_idx].item(),\n-                 ranks[selected_logprobs_idx].item())\n+                (selected_logprobs_slice[i].item(),\n+                 ranks_slice[i].item())\n             }\n-            # +1 to go to the next sampled token. Note that\n-            # selected_logprobs can contain duplicates unlike top_logprobs\n-            # when beam search is enabled.\n-            selected_logprobs_idx += 1\n \n             # Second, add top K logprobs along with its rank.\n             if num_logprobs >= 0:\n@@ -913,6 +938,8 @@ def _get_sampled_logprob_if_needed(\n                 for token_id, logprob_and_rank in\n                 sampled_logprobs_dict.items()\n             })\n+        \n+        selected_logprobs_idx += num_samples\n         # There are len(seq_ids) number of sampled tokens for the current\n         # sequence group in top_logprobs. Jump to the next seq_group.\n         top_logprob_idx += len(seq_ids)\n", "model_name_or_path": "gpt-5-2025-08-07"}
