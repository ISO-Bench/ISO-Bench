{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..2a405b66c 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4717,6 +4717,23 @@ class VllmConfig:\n \n         disable_chunked_prefill_reasons: list[str] = []\n \n+        # Allow users to force-disable the hybrid KV cache manager regardless of\n+        # detected compatibility, which can improve latency in some scenarios.\n+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:\n+            self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+\n+        # Optional escape hatch to disable chunked prefill entirely via env var.\n+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:\n+            logger.info(\"Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.\")\n+            self.scheduler_config.chunked_prefill_enabled = False\n+            self.scheduler_config.long_prefill_token_threshold = 0\n+            self.scheduler_config.max_num_batched_tokens = max(\n+                self.scheduler_config.max_model_len,\n+                DEFAULT_MAX_NUM_BATCHED_TOKENS)\n+            if self.cache_config is not None:\n+                self.cache_config.enable_prefix_caching = False\n+\n+\n         if self.model_config and self.model_config.pooler_config:\n             pooling_type = self.model_config.pooler_config.pooling_type\n             if pooling_type is None or pooling_type.lower() != \"last\":\n@@ -4769,12 +4786,23 @@ class VllmConfig:\n                 # Hybrid KV cache manager is not compatible with KV events.\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n-                self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif \\\n+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning_once(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\"\n+                    )\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..596886e43 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -165,6 +165,18 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:\n     return int(value)\n \n \n+\n+def getenv_bool(name: str, default: bool = False) -> bool:\n+    \"\"\"Fast path for boolean env var parsing with common conventions.\n+    Accepts \"1\" or \"true\" (case-insensitive) as True. Returns default if unset.\n+    \"\"\"\n+    val = os.getenv(name)\n+    if val is None:\n+        return default\n+    val = val.strip().lower()\n+    return val == \"1\" or val == \"true\"\n+\n+\n def get_vllm_port() -> Optional[int]:\n     \"\"\"Get the port from VLLM_PORT environment variable.\n \n@@ -948,6 +960,19 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # implement and support a subset of all possible layouts.\n     \"VLLM_KV_CACHE_LAYOUT\":\n     lambda: os.getenv(\"VLLM_KV_CACHE_LAYOUT\", None),\n+    # If set, allow using hybrid KV cache manager together with chunked local attention.\n+    # Default is disabled due to latency regressions; set to 1 to enable.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: getenv_bool(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", False),\n+    \n+    # Force-disable hybrid KV cache manager (overrides compatibility checks).\n+    \"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\", False),\n+\n+    # Disable chunked prefill across the board to reduce latency when needed.\n+    \"VLLM_DISABLE_CHUNKED_PREFILL\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_CHUNKED_PREFILL\", False),\n+\n \n     # Enable checking whether the generated logits contain NaNs,\n     # indicating corrupted output. Useful for debugging low level bugs\n@@ -997,9 +1022,12 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n \n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation with simple caching to avoid repeated parsing\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        value = environment_variables[name]()\n+        # cache the resolved value as a module attribute for fast subsequent access\n+        setattr(sys.modules[__name__], name, value)\n+        return value\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n@@ -1037,10 +1065,8 @@ def compute_hash() -> str:\n \n     # summarize environment variables\n     def factorize(name: str):\n-        if __getattr__(name):\n-            factors.append(__getattr__(name))\n-        else:\n-            factors.append(\"None\")\n+        val = __getattr__(name)\n+        factors.append(val if val else \"None\")\n \n     # The values of envs may affects the computation graph.\n     # TODO(DefTruth): hash all environment variables?\n", "model_name_or_path": "gpt-5-2025-08-07"}
