diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/config.py b/vllm/config.py
index 6bfe94b76..2a405b66c 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -4717,6 +4717,23 @@ class VllmConfig:
 
         disable_chunked_prefill_reasons: list[str] = []
 
+        # Allow users to force-disable the hybrid KV cache manager regardless of
+        # detected compatibility, which can improve latency in some scenarios.
+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:
+            self.scheduler_config.disable_hybrid_kv_cache_manager = True
+
+        # Optional escape hatch to disable chunked prefill entirely via env var.
+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:
+            logger.info("Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.")
+            self.scheduler_config.chunked_prefill_enabled = False
+            self.scheduler_config.long_prefill_token_threshold = 0
+            self.scheduler_config.max_num_batched_tokens = max(
+                self.scheduler_config.max_model_len,
+                DEFAULT_MAX_NUM_BATCHED_TOKENS)
+            if self.cache_config is not None:
+                self.cache_config.enable_prefix_caching = False
+
+
         if self.model_config and self.model_config.pooler_config:
             pooling_type = self.model_config.pooler_config.pooling_type
             if pooling_type is None or pooling_type.lower() != "last":
@@ -4769,12 +4786,23 @@ class VllmConfig:
                 # Hybrid KV cache manager is not compatible with KV events.
                 self.scheduler_config.disable_hybrid_kv_cache_manager = True
             if self.model_config is not None and \
-                self.model_config.attention_chunk_size is not None and \
-                self.speculative_config is not None and \
-                self.speculative_config.use_eagle():
-                # Hybrid KV cache manager is not yet supported with chunked
-                # local attention + eagle.
-                self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                self.model_config.attention_chunk_size is not None:
+                if self.speculative_config is not None and \
+                    self.speculative_config.use_eagle():
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention + eagle.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                elif \
+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:
+                    logger.warning_once(
+                        "There is a latency regression when using chunked local"
+                        " attention with the hybrid KV cache manager. Disabling"
+                        " it, by default. To enable it, set the environment "
+                        "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1."
+                    )
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
 
     def update_sizes_for_sequence_parallelism(self,
                                               possible_sizes: list) -> list:
diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff74151..596886e43 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -165,6 +165,18 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:
     return int(value)
 
 
+
+def getenv_bool(name: str, default: bool = False) -> bool:
+    """Fast path for boolean env var parsing with common conventions.
+    Accepts "1" or "true" (case-insensitive) as True. Returns default if unset.
+    """
+    val = os.getenv(name)
+    if val is None:
+        return default
+    val = val.strip().lower()
+    return val == "1" or val == "true"
+
+
 def get_vllm_port() -> Optional[int]:
     """Get the port from VLLM_PORT environment variable.
 
@@ -948,6 +960,19 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # implement and support a subset of all possible layouts.
     "VLLM_KV_CACHE_LAYOUT":
     lambda: os.getenv("VLLM_KV_CACHE_LAYOUT", None),
+    # If set, allow using hybrid KV cache manager together with chunked local attention.
+    # Default is disabled due to latency regressions; set to 1 to enable.
+    "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE":
+    lambda: getenv_bool("VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE", False),
+    
+    # Force-disable hybrid KV cache manager (overrides compatibility checks).
+    "VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER":
+    lambda: getenv_bool("VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER", False),
+
+    # Disable chunked prefill across the board to reduce latency when needed.
+    "VLLM_DISABLE_CHUNKED_PREFILL":
+    lambda: getenv_bool("VLLM_DISABLE_CHUNKED_PREFILL", False),
+
 
     # Enable checking whether the generated logits contain NaNs,
     # indicating corrupted output. Useful for debugging low level bugs
@@ -997,9 +1022,12 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation with simple caching to avoid repeated parsing
     if name in environment_variables:
-        return environment_variables[name]()
+        value = environment_variables[name]()
+        # cache the resolved value as a module attribute for fast subsequent access
+        setattr(sys.modules[__name__], name, value)
+        return value
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
@@ -1037,10 +1065,8 @@ def compute_hash() -> str:
 
     # summarize environment variables
     def factorize(name: str):
-        if __getattr__(name):
-            factors.append(__getattr__(name))
-        else:
-            factors.append("None")
+        val = __getattr__(name)
+        factors.append(val if val else "None")
 
     # The values of envs may affects the computation graph.
     # TODO(DefTruth): hash all environment variables?
