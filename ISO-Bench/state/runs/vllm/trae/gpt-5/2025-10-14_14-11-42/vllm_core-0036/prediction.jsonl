{"instance_id": "vllm-project__vllm-f168b85", "model_patch": "diff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..0a120c408 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -29,7 +29,7 @@ from vllm.distributed.parallel_state import get_pp_group\n from vllm.forward_context import get_forward_context\n from vllm.model_executor.layers.activation import ReLUSquaredActivation\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -63,26 +63,29 @@ class NemotronHMLP(nn.Module):\n         config: NemotronHConfig,\n         quant_config: Optional[QuantizationConfig] = None,\n         bias: bool = False,\n+        prefix: str = \"\",\n     ) -> None:\n         super().__init__()\n-        self.up_proj = MergedColumnParallelLinear(\n+        self.up_proj = ColumnParallelLinear(\n             input_size=config.hidden_size,\n-            output_sizes=[config.intermediate_size],\n+            output_size=config.intermediate_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"up_proj\"),\n         )\n         self.down_proj = RowParallelLinear(\n             input_size=config.intermediate_size,\n             output_size=config.hidden_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"down_proj\"),\n         )\n         self.act_fn = ReLUSquaredActivation()\n \n     def forward(self, x: torch.Tensor):\n-        x, _ = self.up_proj(x)\n+        x = self.up_proj(x)\n         x = self.act_fn(x)\n-        x, _ = self.down_proj(x)\n+        x = self.down_proj(x)\n         return x\n \n \n@@ -101,7 +104,8 @@ class NemotronHMLPDecoderLayer(nn.Module):\n \n         self.mixer = NemotronHMLP(config,\n                                   quant_config=quant_config,\n-                                  bias=config.mlp_bias)\n+                                  bias=config.mlp_bias,\n+                                  prefix=maybe_prefix(prefix, \"mixer\"))\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -207,12 +211,14 @@ class NemotronHAttention(nn.Module):\n             self.total_num_kv_heads,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"qkv_proj\"),\n         )\n         self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             config.hidden_size,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"o_proj\"),\n         )\n \n         self.attn = Attention(\n@@ -302,6 +308,7 @@ class NemotronHModel(nn.Module):\n             self.vocab_size,\n             config.hidden_size,\n             org_num_embeddings=config.vocab_size,\n+            prefix=maybe_prefix(prefix, \"embed_tokens\"),\n         )\n \n         def get_layer(prefix: str):\n@@ -473,6 +480,7 @@ class NemotronHForCausalLM(nn.Module, HasInnerState, SupportsLoRA, SupportsPP,\n             # We need bigger padding if using lora for kernel\n             # compatibility\n             if not lora_config else lora_config.lora_vocab_padding_size,\n+            prefix=maybe_prefix(prefix, \"lm_head\"),\n         )\n         # Used to track and store by the Mamba cache between steps.\n         self.mamba_cache: Optional[MambaCacheManager] = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
