{
  "changed": [
    "model_patch.diff",
    "vllm/_custom_ops.py",
    "vllm/model_executor/layers/quantization/fp8.py"
  ],
  "allowed": [
    "vllm/_custom_ops.py",
    "vllm/model_executor/layers/quantization/fp8.py"
  ],
  "disallowed": [
    "model_patch.diff"
  ],
  "ok": false
}