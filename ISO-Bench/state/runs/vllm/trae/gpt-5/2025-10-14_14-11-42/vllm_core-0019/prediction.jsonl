{"instance_id": "vllm-project__vllm-ebce310", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 5b5643748..bf4ffaca5 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -189,13 +189,36 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n def scaled_fp8_quant(\n     input: torch.Tensor,\n     scale: Optional[torch.Tensor] = None,\n+    batch_dim_padding: Optional[int] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+    \"\"\"\n+    Quantize input tensor to FP8 and return quantized tensor and scale.\n+\n+    Supports both static and dynamic quantization: if scale is provided, static\n+    scaling is used; otherwise dynamic scaling is computed from input. Optionally\n+    pads the first dimension of the output tensor to at least batch_dim_padding\n+    to enable downstream kernels to select better algorithms on some backends.\n+\n+    Only the leading slice matching the input shape is populated by the\n+    quantization kernels when padding is requested.\n+    \"\"\"\n+    # Allocate output, optionally with padding on the first dimension\n+    if batch_dim_padding is not None and input.size(0) < batch_dim_padding:\n+        padded_shape = list(input.shape)\n+        padded_shape[0] = batch_dim_padding\n+        output = torch.empty(padded_shape,\n+                             dtype=torch.float8_e4m3fn,\n+                             device=input.device)\n+        out_view = output[:input.size(0)]\n+    else:\n+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+        out_view = output\n+\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n-        vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n+        vllm_ops.dynamic_scaled_fp8_quant(out_view, input, scale)\n     else:\n-        vllm_ops.static_scaled_fp8_quant(output, input, scale)\n+        vllm_ops.static_scaled_fp8_quant(out_view, input, scale)\n     return output, scale\n \n \ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex b57e1dde8..efdd84108 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -199,7 +199,7 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                   layer.weight_scale[idx])\n \n                 layer.weight[start:end, :] = per_tensor_quantize(\n-                    weight_dq, layer.weight_scale.max())\n+                    weight_dq, max_w_scale)\n                 start = end\n             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n \n@@ -231,7 +231,10 @@ class Fp8LinearMethod(LinearMethodBase):\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n+        # Enlarge the first dimension slightly to help GEMM algorithm selection on some backends.\n+        pad = 17 if x.size(0) < 17 else None\n+        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale,\n+                                               batch_dim_padding=pad)\n \n         # Fused GEMM_DQ\n         output, _ = torch._scaled_mm(\n@@ -242,24 +245,28 @@ class Fp8LinearMethod(LinearMethodBase):\n             scale_b=layer.weight_scale,\n             bias=bias,\n         )\n+        # Slice back to original batch dim if padding was applied\n+        if pad is not None:\n+            output = output[:x.size(0)]\n \n         return output\n \n \n def all_close_1d(x: torch.Tensor) -> bool:\n     assert len(x.shape) == 1\n-    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))\n+    return torch.allclose(x, x[0].expand_as(x))\n \n \n def per_tensor_quantize(tensor: torch.Tensor,\n                         inv_scale: float) -> torch.Tensor:\n     finfo = torch.finfo(torch.float8_e4m3fn)\n-    qweight = (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max)\n-    return qweight.to(torch.float8_e4m3fn)\n+    q = tensor / inv_scale  # avoid mutating the input tensor\n+    q.clamp_(min=finfo.min, max=finfo.max)\n+    return q.to(torch.float8_e4m3fn)\n \n \n def per_tensor_dequantize(tensor: torch.Tensor,\n                           inv_scale: float) -> torch.Tensor:\n-    fake_qweight = tensor.to(torch.float16)\n-    dq_weight = fake_qweight * inv_scale\n-    return dq_weight\n+    dq = tensor.to(torch.float16)\n+    dq.mul_(inv_scale)\n+    return dq\n", "model_name_or_path": "gpt-5-2025-08-07"}
