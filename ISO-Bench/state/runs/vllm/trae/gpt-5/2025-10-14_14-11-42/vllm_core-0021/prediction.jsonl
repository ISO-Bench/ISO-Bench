{"instance_id": "vllm-project__vllm-edc4fa3", "model_patch": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 1be806bbf..9d4503a1d 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -85,10 +85,6 @@ __global__ void copy_blocks_kernel(int64_t* key_cache_ptrs,\n     int64_t src_offset = src_block_offset + i;\n     int64_t dst_offset = dst_block_offset + i;\n     key_cache[dst_offset] = key_cache[src_offset];\n-  }\n-  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n-    int64_t src_offset = src_block_offset + i;\n-    int64_t dst_offset = dst_block_offset + i;\n     value_cache[dst_offset] = value_cache[src_offset];\n   }\n }\n@@ -267,10 +263,23 @@ void reshape_and_cache(\n         key_cache,  // [num_blocks, num_heads, head_size/x, block_size, x]\n     torch::Tensor&\n         value_cache,  // [num_blocks, num_heads, head_size, block_size]\n-    torch::Tensor& slot_mapping,  // [num_tokens]\n+    torch::Tensor& slot_mapping,  // [num_tokens] or [num_actual_tokens]\n     const std::string& kv_cache_dtype, const double k_scale,\n     const double v_scale) {\n-  int num_tokens = key.size(0);\n+  // NOTE(woosuk): In vLLM V1, key.size(0) can be different from\n+  // slot_mapping.size(0) because of padding for CUDA graphs.\n+  // In vLLM V0, key.size(0) is always equal to slot_mapping.size(0) because\n+  // both include padding.\n+  // In vLLM V1, however, key.size(0) can be larger than slot_mapping.size(0)\n+  // since key includes padding for CUDA graphs, while slot_mapping does not.\n+  // In this case, slot_mapping.size(0) represents the actual number of tokens\n+  // before padding.\n+  // For compatibility with both cases, we use slot_mapping.size(0) as the\n+  // number of tokens.\n+  int num_tokens = slot_mapping.size(0);\n+  if (num_tokens == 0) {\n+    return;\n+  }\n   int num_heads = key.size(1);\n   int head_size = key.size(2);\n   int block_size = key_cache.size(3);\n@@ -280,7 +289,7 @@ void reshape_and_cache(\n   int value_stride = value.stride(0);\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  dim3 block(std::min(num_heads * head_size, 1024));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \n@@ -307,10 +316,23 @@ void reshape_and_cache_flash(\n     torch::Tensor& key_cache,  // [num_blocks, block_size, num_heads, head_size]\n     torch::Tensor&\n         value_cache,  // [num_blocks, block_size, num_heads, head_size]\n-    torch::Tensor& slot_mapping,  // [num_tokens]\n+    torch::Tensor& slot_mapping,  // [num_tokens] or [num_actual_tokens]\n     const std::string& kv_cache_dtype, const double k_scale,\n     const double v_scale) {\n-  int num_tokens = key.size(0);\n+  // NOTE(woosuk): In vLLM V1, key.size(0) can be different from\n+  // slot_mapping.size(0) because of padding for CUDA graphs.\n+  // In vLLM V0, key.size(0) is always equal to slot_mapping.size(0) because\n+  // both include padding.\n+  // In vLLM V1, however, key.size(0) can be larger than slot_mapping.size(0)\n+  // since key includes padding for CUDA graphs, while slot_mapping does not.\n+  // In this case, slot_mapping.size(0) represents the actual number of tokens\n+  // before padding.\n+  // For compatibility with both cases, we use slot_mapping.size(0) as the\n+  // number of tokens.\n+  int num_tokens = slot_mapping.size(0);\n+  if (num_tokens == 0) {\n+    return;\n+  }\n   int num_heads = key.size(1);\n   int head_size = key.size(2);\n   int block_size = key_cache.size(1);\n@@ -321,7 +343,7 @@ void reshape_and_cache_flash(\n   TORCH_CHECK(key_cache.stride(0) == value_cache.stride(0));\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  dim3 block(std::min(num_heads * head_size, 1024));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex d37989055..0f68c1203 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -144,8 +144,8 @@ class FlashAttentionImpl(AttentionImpl):\n         key_cache = kv_cache[0]\n         value_cache = kv_cache[1]\n         torch.ops._C_cache_ops.reshape_and_cache_flash(\n-            key[:num_actual_tokens],\n-            value[:num_actual_tokens],\n+            key,\n+            value,\n             key_cache,\n             value_cache,\n             attn_metadata.slot_mapping,\n@@ -155,11 +155,22 @@ class FlashAttentionImpl(AttentionImpl):\n         )\n \n         # Compute attention and update output up to `num_actual_tokens`.\n+        if num_actual_tokens == 0:\n+            return output\n+\n+        # Avoid slicing overhead when using full range.\n+        if num_actual_tokens == query.size(0):\n+            q_in = query\n+            out_in = output\n+        else:\n+            q_in = query[:num_actual_tokens]\n+            out_in = output[:num_actual_tokens]\n+\n         flash_attn_varlen_func(\n-            q=query[:num_actual_tokens],\n+            q=q_in,\n             k=key_cache,\n             v=value_cache,\n-            out=output[:num_actual_tokens],\n+            out=out_in,\n             cu_seqlens_q=attn_metadata.query_start_loc,\n             max_seqlen_q=attn_metadata.max_query_len,\n             cu_seqlens_k=attn_metadata.seq_start_loc,\n", "model_name_or_path": "gpt-5-2025-08-07"}
