diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff74151..0cfaec59f 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -83,6 +83,7 @@ if TYPE_CHECKING:
     VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False
     VLLM_SKIP_P2P_CHECK: bool = False
     VLLM_DISABLED_KERNELS: list[str] = []
+    VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE: bool = False
     VLLM_USE_V1: bool = True
     VLLM_ROCM_USE_AITER: bool = False
     VLLM_ROCM_USE_AITER_PAGED_ATTN: bool = False
@@ -650,6 +651,11 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_USE_V1":
     lambda: bool(int(os.getenv("VLLM_USE_V1", "1"))),
 
+    # Allow enabling hybrid KV cache manager with chunked local attention.
+    # Disabled by default due to latency regressions.
+    "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE":
+    lambda: bool(int(os.getenv("VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE", "0"))),
+
     # Disable aiter ops unless specifically enabled.
     # Acts as a parent switch to enable the rest of the other operations.
     "VLLM_ROCM_USE_AITER":
@@ -996,10 +1002,23 @@ environment_variables: dict[str, Callable[[], Any]] = {
 # --8<-- [end:env-vars-definition]
 
 
+# Cache for frequently accessed env variables to avoid repeated os.getenv calls
+_ENV_VALUE_CACHE: dict[str, Any] = {}
+# Only cache values that are not expected to change at runtime
+_CACHED_ENV_VARS: set[str] = {
+    "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE",
+    "VLLM_USE_V1",
+}
+
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation of environment variables with caching for hot keys
     if name in environment_variables:
-        return environment_variables[name]()
+        if name in _ENV_VALUE_CACHE:
+            return _ENV_VALUE_CACHE[name]
+        value = environment_variables[name]()
+        if name in _CACHED_ENV_VARS:
+            _ENV_VALUE_CACHE[name] = value
+        return value
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
@@ -1021,6 +1040,8 @@ def set_vllm_use_v1(use_v1: bool):
             "explicitly by the user. Please raise this as a Github "
             "Issue and explicitly set VLLM_USE_V1=0 or 1.")
     os.environ["VLLM_USE_V1"] = "1" if use_v1 else "0"
+    # keep cache consistent
+    _ENV_VALUE_CACHE["VLLM_USE_V1"] = use_v1
 
 
 def compute_hash() -> str:
