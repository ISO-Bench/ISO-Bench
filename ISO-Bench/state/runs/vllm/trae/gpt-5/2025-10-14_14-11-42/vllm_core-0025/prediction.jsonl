{"instance_id": "vllm-project__vllm-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 24ddd35ab..20bf1cb19 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -50,11 +50,12 @@ def decode_tokens(\n     `skip_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n-    if skip_special_tokens is not None:\n-        return tokenizer.decode(token_ids,\n-                                skip_special_tokens=skip_special_tokens)\n+    # Prefer a potential fast-path implementation when no extra options\n+    if skip_special_tokens is None:\n+        decode_method = getattr(tokenizer, \"_decode\", tokenizer.decode)\n+        return decode_method(token_ids)\n \n-    return tokenizer.decode(token_ids)\n+    return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n \n \n def encode_tokens(\n", "model_name_or_path": "gpt-5-2025-08-07"}
