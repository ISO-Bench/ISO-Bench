{"instance_id": "vllm-project__vllm-64172a9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex d07527304..9230654c1 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -65,12 +65,11 @@ class Sampler(nn.Module):\n         if do_min_p:\n             logits = _apply_min_p(logits, sampling_tensors.min_ps)\n \n-        # We use float32 for probabilities and log probabilities.\n-        # Compute the probabilities.\n-        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n         # Compute the log probabilities.\n         # Use log_softmax to ensure numerical stability.\n         logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n+        # Compute the probabilities via exp(logprobs) to avoid an extra softmax pass.\n+        probs = torch.exp(logprobs)\n \n         # Sample the next tokens.\n         sample_results = _sample(probs, logprobs, sampling_metadata,\n@@ -506,7 +505,7 @@ def _sample(\n     #                                   sampling_tensors)\n \n \n-def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n+def _get_ranks(x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     This function calculates the ranks of the chosen tokens in a logprob tensor.\n \n@@ -520,8 +519,11 @@ def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[range(len(x)), indices]\n-    return (x > vals[:, None]).long().sum(1) + 1\n+    # Use tensor-based indexing on the correct device to avoid Python overhead\n+    idx_t = torch.as_tensor(indices, device=x.device, dtype=torch.long)\n+    ar = torch.arange(0, x.size(0), device=x.device, dtype=idx_t.dtype)\n+    vals = x[ar, idx_t]\n+    return torch.count_nonzero(x > vals[:, None], dim=1).add_(1)\n \n \n def _get_logprobs(\n@@ -562,10 +564,9 @@ def _get_logprobs(\n     assert sample_idx == logprobs.size(0)\n \n     # Batched query for logprobs of selected token\n-    batched_logprobs_query_result = logprobs[[\n-        batched_logprobs_query_seq_indices,\n-        batched_logprobs_query_token_indices\n-    ]]\n+    seq_idx_tensor = torch.as_tensor(batched_logprobs_query_seq_indices, device=logprobs.device, dtype=torch.long)\n+    token_idx_tensor = torch.as_tensor(batched_logprobs_query_token_indices, device=logprobs.device, dtype=torch.long)\n+    batched_logprobs_query_result = logprobs[seq_idx_tensor, token_idx_tensor]\n \n     # Batched query for logprobs of topk tokens\n     if largest_num_logprobs > 0:\n@@ -580,8 +581,8 @@ def _get_logprobs(\n     batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n \n     batched_ranks_query_result = _get_ranks(\n-        logprobs[batched_logprobs_query_seq_indices],\n-        batched_logprobs_query_token_indices)\n+        logprobs[seq_idx_tensor],\n+        token_idx_tensor).cpu()\n \n     # Gather results\n     result_prompt_logprobs: List[Optional[PromptLogprobs]] = []\n", "model_name_or_path": "gpt-5-2025-08-07"}
