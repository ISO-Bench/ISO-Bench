{"instance_id": "vllm-project__vllm-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..04c9f8a41 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -455,7 +455,6 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                       is not None)\n         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n-        self.decode_only = True\n \n         # Attention metadata inputs.\n         if self.attn_backend is not None:\n@@ -477,6 +476,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 finished_requests_ids: Optional[List[str]] = None) -> None:\n         self.finished_requests_ids = finished_requests_ids\n \n+        # if the current batch is decode-only.\n+        # will be set to False if there is any non-decode request.\n+        self.decode_only = True\n+\n         # Intermediate data (data in CPU before going to GPU) for\n         # the current sequence group.\n         self.inter_data_list: List[\n@@ -1444,8 +1447,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n                                       dtype=torch.long,\n                                       device=self.device)\n         if self.model_config.uses_mrope:\n-            input_positions = torch.tile(input_positions,\n-                                         (3, 1)).cuda(device=self.device)\n+            input_positions = torch.tile(input_positions, (3, 1))\n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\n         previous_hidden_states = None\n@@ -1750,8 +1752,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                     model_forward_end)\n                 orig_model_forward_time = 0.0\n                 if intermediate_tensors is not None:\n-                    orig_model_forward_time = intermediate_tensors.tensors.get(\n-                        \"model_forward_time\", torch.tensor(0.0)).item()\n+                    t = intermediate_tensors.tensors.get(\"model_forward_time\")\n+                    if t is not None:\n+                        orig_model_forward_time = t.item()\n                 hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                     torch.tensor(model_forward_time + orig_model_forward_time))\n             return hidden_or_intermediate_states\n@@ -1778,8 +1781,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                 model_forward_end)\n             orig_model_forward_time = 0.0\n             if intermediate_tensors is not None:\n-                orig_model_forward_time = intermediate_tensors.tensors.get(\n-                    \"model_forward_time\", torch.tensor(0.0)).item()\n+                t = intermediate_tensors.tensors.get(\"model_forward_time\")\n+                if t is not None:\n+                    orig_model_forward_time = t.item()\n             # If there are multiple workers, we are still tracking the latency\n             # from the start time of the driver worker to the end time of the\n             # driver worker. The model forward time will then end up covering\n", "model_name_or_path": "gpt-5-2025-08-07"}
