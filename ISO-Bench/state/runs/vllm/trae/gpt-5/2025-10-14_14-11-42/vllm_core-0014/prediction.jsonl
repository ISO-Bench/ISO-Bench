{"instance_id": "vllm-project__vllm-3cd91dc", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/test_serial_utils.py b/tests/v1/test_serial_utils.py\nindex bc0e0cbd8..cd31e3e9b 100644\n--- a/tests/v1/test_serial_utils.py\n+++ b/tests/v1/test_serial_utils.py\n@@ -50,7 +50,7 @@ def test_encode_decode():\n         large_non_contig_tensor=torch.rand(1024, 512)[:, 10:20],\n     )\n \n-    encoder = MsgpackEncoder()\n+    encoder = MsgpackEncoder(size_threshold=256)\n     decoder = MsgpackDecoder(MyType)\n \n     encoded = encoder.encode(obj)\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex f80bf878f..bd7e3c7d1 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -605,6 +605,11 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n     lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n \n+    # Threshold (in bytes) below which small numpy/tensor buffers are inlined\n+    # in the V1 msgpack serializer to reduce aux buffer overhead.\n+    \"VLLM_V1_MSGBUF_INLINE_THRESHOLD\":\n+    lambda: int(os.getenv(\"VLLM_V1_MSGBUF_INLINE_THRESHOLD\", \"512\")),\n+\n     # If set, vLLM will disable the MLA attention optimizations.\n     \"VLLM_MLA_DISABLE\":\n     lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\ndiff --git a/vllm/v1/serial_utils.py b/vllm/v1/serial_utils.py\nindex 3af6793fd..6de19586f 100644\n--- a/vllm/v1/serial_utils.py\n+++ b/vllm/v1/serial_utils.py\n@@ -1,5 +1,6 @@\n # SPDX-License-Identifier: Apache-2.0\n \n+import os\n import pickle\n from collections.abc import Sequence\n from inspect import isclass\n@@ -16,7 +17,8 @@ CUSTOM_TYPE_PICKLE = 1\n CUSTOM_TYPE_CLOUDPICKLE = 2\n CUSTOM_TYPE_RAW_VIEW = 3\n \n-# TODO calibrate this size\n+# Default inline threshold for small buffers. Can be overridden via\n+# constructor or the env var VLLM_V1_MSGBUF_INLINE_THRESHOLD.\n MIN_NOCOPY_BUF_SIZE = 512\n \n bytestr = Union[bytes, bytearray, memoryview, zmq.Frame]\n@@ -29,7 +31,14 @@ class MsgpackEncoder:\n     not thread-safe when encoding tensors / numpy arrays.\n     \"\"\"\n \n-    def __init__(self):\n+    def __init__(self, size_threshold: Optional[int] = None):\n+        # Resolve the size threshold once during initialization to avoid repeated\n+        # environment lookups during encoding.\n+        if size_threshold is None:\n+            size_threshold = int(\n+                os.getenv(\"VLLM_V1_MSGBUF_INLINE_THRESHOLD\", MIN_NOCOPY_BUF_SIZE))\n+        self.size_threshold = int(size_threshold)\n+\n         self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)\n         # This is used as a local stash of buffers that we can then access from\n         # our custom `msgspec` hook, `enc_hook`. We don't have a way to\n@@ -38,7 +47,7 @@ class MsgpackEncoder:\n \n     def encode(self, obj: Any) -> Sequence[bytestr]:\n         try:\n-            self.aux_buffers = bufs = [b'']\n+            self.aux_buffers = bufs = [b\"\"]\n             bufs[0] = self.encoder.encode(obj)\n             # This `bufs` list allows us to collect direct pointers to backing\n             # buffers of tensors and np arrays, and return them along with the\n@@ -62,30 +71,48 @@ class MsgpackEncoder:\n             return self._encode_ndarray(obj.numpy())\n \n         # Fall back to pickle for object or void kind ndarrays.\n-        if isinstance(obj, np.ndarray) and obj.dtype.kind not in ('O', 'V'):\n+        if isinstance(obj, np.ndarray) and obj.dtype.kind not in (\"O\", \"V\"):\n             return self._encode_ndarray(obj)\n \n         if isinstance(obj, FunctionType):\n-            # `pickle` is generally faster than cloudpickle, but can have\n-            # problems serializing methods.\n+            # Prefer pickle for top-level functions as it's faster; fall back to\n+            # cloudpickle for methods/closures where pickle may fail.\n+            if getattr(obj, \"__qualname__\", None) == getattr(obj, \"__name__\", None):\n+                try:\n+                    return msgpack.Ext(\n+                        CUSTOM_TYPE_PICKLE,\n+                        pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL),\n+                    )\n+                except Exception:\n+                    pass\n             return msgpack.Ext(CUSTOM_TYPE_CLOUDPICKLE, cloudpickle.dumps(obj))\n \n-        return msgpack.Ext(CUSTOM_TYPE_PICKLE,\n-                           pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))\n+        return msgpack.Ext(\n+            CUSTOM_TYPE_PICKLE,\n+            pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))\n \n     def _encode_ndarray(\n         self, obj: np.ndarray\n     ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n         assert self.aux_buffers is not None\n-        arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()\n-        if not obj.shape or obj.nbytes < MIN_NOCOPY_BUF_SIZE:\n-            # Encode small arrays and scalars inline. Using this extension type\n-            # ensures we can avoid copying when decoding.\n-            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)\n+        bufs = self.aux_buffers  # local alias to avoid attribute lookups in hot path\n+        threshold = self.size_threshold\n+        # Fast path: reuse underlying contiguous buffer when available.\n+        # Use ndarray.flags to avoid creating an intermediate buffer proxy.\n+        if obj.flags.c_contiguous:\n+            arr_data: Union[memoryview, bytes] = memoryview(obj)\n+        else:\n+            # Create a contiguous copy only when necessary.\n+            arr_data = obj.tobytes()\n+        # Inline small arrays and scalars; otherwise append to aux_buffers.\n+        if not obj.shape or obj.nbytes < threshold:\n+            # If we already have a memoryview, avoid wrapping it again.\n+            mv = arr_data if isinstance(arr_data, memoryview) else memoryview(arr_data)\n+            data: Union[int, memoryview] = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, mv)\n         else:\n             # Otherwise encode index of backing buffer to avoid copy.\n-            data = len(self.aux_buffers)\n-            self.aux_buffers.append(arr_data)\n+            data = len(bufs)\n+            bufs.append(arr_data)\n \n         # We serialize the ndarray as a tuple of native types.\n         # The data is either inlined if small, or an index into a list of\n@@ -102,12 +129,12 @@ class MsgpackDecoder:\n \n     def __init__(self, t: Optional[Any] = None):\n         args = () if t is None else (t, )\n-        self.decoder = msgpack.Decoder(*args,\n-                                       ext_hook=self.ext_hook,\n-                                       dec_hook=self.dec_hook)\n+        self.decoder = msgpack.Decoder(\n+            *args, ext_hook=self.ext_hook, dec_hook=self.dec_hook)\n         self.aux_buffers: Sequence[bytestr] = ()\n \n     def decode(self, bufs: Union[bytestr, Sequence[bytestr]]) -> Any:\n+        # Fast path for single-buffer inputs.\n         if isinstance(bufs, (bytes, bytearray, memoryview, zmq.Frame)):\n             # TODO - This check can become `isinstance(bufs, bytestr)`\n             # as of Python 3.10.\n@@ -130,7 +157,8 @@ class MsgpackDecoder:\n \n     def _decode_ndarray(self, arr: Any) -> np.ndarray:\n         dtype, shape, data = arr\n-        buffer = self.aux_buffers[data] if isinstance(data, int) else data\n+        aux = self.aux_buffers\n+        buffer = aux[data] if isinstance(data, int) else data\n         return np.ndarray(buffer=buffer, dtype=np.dtype(dtype), shape=shape)\n \n     def ext_hook(self, code: int, data: memoryview) -> Any:\n", "model_name_or_path": "gpt-5-2025-08-07"}
