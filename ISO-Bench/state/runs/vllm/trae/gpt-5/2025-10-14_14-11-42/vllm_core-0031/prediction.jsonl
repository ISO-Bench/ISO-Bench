{"instance_id": "vllm-project__vllm-7d94577", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 261cc7855..0896ae3a9 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -119,7 +119,8 @@ if TYPE_CHECKING:\n     VLLM_TPU_BUCKET_PADDING_GAP: int = 0\n     VLLM_TPU_MOST_MODEL_LEN: Optional[int] = None\n     VLLM_USE_DEEP_GEMM: bool = False\n-    VLLM_USE_FLASHINFER_MOE: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP8: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP4: bool = False\n     VLLM_XGRAMMAR_CACHE_MB: int = 0\n     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256\n     VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False\n@@ -854,9 +855,13 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_USE_DEEP_GEMM\":\n     lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"0\"))),\n \n+    # Allow use of FlashInfer MoE kernels for fused moe ops.\n+    \"VLLM_USE_FLASHINFER_MOE_FP8\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP8\", \"0\"))),\n+\n     # Allow use of FlashInfer CUTLASS kernels for fused moe ops.\n-    \"VLLM_USE_FLASHINFER_MOE\":\n-    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE\", \"0\"))),\n+    \"VLLM_USE_FLASHINFER_MOE_FP4\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP4\", \"0\"))),\n \n     # Control the cache sized used by the xgrammar compiler. The default\n     # of 512 MB should be enough for roughly 1000 JSON schemas.\ndiff --git a/vllm/model_executor/layers/fused_moe/config.py b/vllm/model_executor/layers/fused_moe/config.py\nindex 9bebb6a65..f711af6bd 100644\n--- a/vllm/model_executor/layers/fused_moe/config.py\n+++ b/vllm/model_executor/layers/fused_moe/config.py\n@@ -191,7 +191,7 @@ class FusedMoEParallelConfig:\n \n     @property\n     def use_flashinfer_cutlass_kernels(self):\n-        return (envs.VLLM_USE_FLASHINFER_MOE\n+        return ((envs.VLLM_USE_FLASHINFER_MOE_FP4 or getattr(envs, \"VLLM_USE_FLASHINFER_MOE\", False))\n                 and has_flashinfer_cutlass_fused_moe())\n \n     @staticmethod\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex aec5d7b25..d4417ecf9 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -950,9 +950,10 @@ def grouped_topk(\n                                    -1).max(dim=-1).values  # [n, n_group]\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,\n                            sorted=False)[1]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n-    score_mask = group_mask.unsqueeze(-1).expand(\n+    n_group = group_scores.size(-1)\n+    mask = (group_idx.unsqueeze(-1) == torch.arange(\n+        n_group, device=group_scores.device)).any(dim=1)  # [n, n_group] bool\n+    score_mask = mask.unsqueeze(-1).expand(\n         num_token, num_expert_group,\n         scores.size(-1) // num_expert_group).reshape(num_token, -1)  # [n, e]\n     tmp_scores = scores.masked_fill(~score_mask.bool(),\n@@ -1145,6 +1146,43 @@ def dispatch_fused_experts_func(inplace: bool) -> Callable[..., torch.Tensor]:\n \n \n # TODO (bnell): replace this with modular op.  Can get rid of inplace/outplace\n+\n+\n+def flashinfer_fused_moe_blockscale_fp8(\n+    hidden_states: torch.Tensor,\n+    w1: torch.Tensor,\n+    w2: torch.Tensor,\n+    topk_weights: torch.Tensor,\n+    topk_ids: torch.Tensor,\n+    inplace: bool = False,\n+    activation: str = \"silu\",\n+    global_num_experts: int = -1,\n+    apply_router_weight_on_input: bool = False,\n+    expert_map: Optional[torch.Tensor] = None,\n+    w1_scale: Optional[torch.Tensor] = None,\n+    w2_scale: Optional[torch.Tensor] = None,\n+    a1_scale: Optional[torch.Tensor] = None,\n+    a2_scale: Optional[torch.Tensor] = None,\n+    block_shape: Optional[list[int]] = None,\n+):\n+    # Thin wrapper to call FlashInfer's FP8 blockscale fused MoE kernel if present.\n+    from vllm.utils.flashinfer import (\n+        flashinfer_fused_moe_blockscale_fp8 as _fi_block_fp8)\n+    return _fi_block_fp8(\n+        hidden_states,\n+        w1,\n+        w2,\n+        topk_weights,\n+        topk_ids,\n+        w1_scale,\n+        w2_scale,\n+        a1_scale,\n+        a2_scale,\n+        expert_map,\n+        activation,\n+        apply_router_weight_on_input,\n+    )\n+\n # torch ops.\n def fused_experts(\n         hidden_states: torch.Tensor,\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex 824dfe15a..b8b33ad66 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -519,13 +519,32 @@ class Fp8MoEMethod(FusedMoEMethodBase):\n                 \"platform.\")\n \n         self.topk_indices_dtype = None\n-        self.fused_experts = functools.partial(  # type: ignore\n-            fused_experts,\n-            use_fp8_w8a8=True,\n-            block_shape=self.quant_config.weight_block_size,\n-            allow_deep_gemm=self.allow_deep_gemm,\n-            allow_cutlass_block_scaled_grouped_gemm=(\n-                self.allow_cutlass_block_scaled_grouped_gemm))\n+        # Prefer FlashInfer FP8 fused MoE when available and enabled via env.\n+        self.fused_experts = None  # type: ignore\n+        if (envs.VLLM_USE_FLASHINFER_MOE_FP8 and self.block_quant\n+                and current_platform.is_cuda()\n+                and current_platform.is_device_capability(100)):\n+            try:\n+                from vllm.utils.flashinfer import has_flashinfer_moe\n+                if has_flashinfer_moe():\n+                    from vllm.model_executor.layers.fused_moe.fused_moe import (\n+                        flashinfer_fused_moe_blockscale_fp8)\n+                    logger.info_once(\n+                        \"Using FlashInfer FP8 blockscale fused MoE kernels for Fp8MoEMethod.\")\n+                    self.fused_experts = functools.partial(\n+                        flashinfer_fused_moe_blockscale_fp8)\n+            except Exception:\n+                # Fallback to default path if flashinfer is unavailable\n+                self.fused_experts = None\n+\n+        if self.fused_experts is None:\n+            self.fused_experts = functools.partial(  # type: ignore\n+                fused_experts,\n+                use_fp8_w8a8=True,\n+                block_shape=self.quant_config.weight_block_size,\n+                allow_deep_gemm=self.allow_deep_gemm,\n+                allow_cutlass_block_scaled_grouped_gemm=(\n+                    self.allow_cutlass_block_scaled_grouped_gemm))\n \n     def create_weights(self, layer: Module, num_experts: int, hidden_size: int,\n                        intermediate_size_per_partition: int,\ndiff --git a/vllm/model_executor/layers/quantization/modelopt.py b/vllm/model_executor/layers/quantization/modelopt.py\nindex 3807899fc..cf0fd8f2f 100644\n--- a/vllm/model_executor/layers/quantization/modelopt.py\n+++ b/vllm/model_executor/layers/quantization/modelopt.py\n@@ -625,7 +625,7 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):\n         round_up_multiple = lambda x, m: (x + m - 1) // m * m\n         M_padded = round_up_multiple(M, 128)\n         K_padded = round_up_multiple(K, 4)\n-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)\n+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)\n         padded_scale[:B, :M, :K] = scale\n         batches, rows, cols = padded_scale.shape\n         assert rows % 128 == 0\n@@ -633,7 +633,7 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):\n         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,\n                                             cols // 4, 4)\n         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))\n-        swizzled_scale = swizzled_scale.contiguous().cuda()\n+        swizzled_scale = swizzled_scale.contiguous().to(scale.device, non_blocking=True)\n         return (swizzled_scale.reshape(M, K)\n                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))\n \n@@ -721,7 +721,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         self.use_marlin = False\n         self.allow_flashinfer_cutlass = False\n \n-        if envs.VLLM_USE_FLASHINFER_MOE:\n+        if envs.VLLM_USE_FLASHINFER_MOE_FP4 or getattr(envs, \"VLLM_USE_FLASHINFER_MOE\", False):\n             if self.cutlass_nvfp4_supported and current_platform.is_cuda() \\\n                and current_platform.is_device_capability(100):\n                 logger.info_once(\n@@ -802,7 +802,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n             # Currently CutlassExpertsFp4 doesn't support DP\n             raise ValueError(\n                 \"CutlassExpertsFp4 doesn't support DP. \"\n-                \"Use flashinfer CUTLASS FusedMoE(VLLM_USE_FLASHINFER_MOE)\"\n+                \"Use flashinfer CUTLASS FusedMoE(VLLM_USE_FLASHINFER_MOE_FP4)\"\n                 \" backend instead.\")\n \n         return experts\n@@ -914,7 +914,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         round_up_multiple = lambda x, m: (x + m - 1) // m * m\n         M_padded = round_up_multiple(M, 128)\n         K_padded = round_up_multiple(K, 4)\n-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)\n+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)\n         padded_scale[:B, :M, :K] = scale\n         batches, rows, cols = padded_scale.shape\n         assert rows % 128 == 0\n@@ -922,7 +922,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,\n                                             cols // 4, 4)\n         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))\n-        swizzled_scale = swizzled_scale.contiguous().cuda()\n+        swizzled_scale = swizzled_scale.contiguous().to(scale.device, non_blocking=True)\n         return (swizzled_scale.reshape(M, K)\n                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))\n \ndiff --git a/vllm/utils/flashinfer.py b/vllm/utils/flashinfer.py\nindex dbd2dc393..0d54a2f10 100644\n--- a/vllm/utils/flashinfer.py\n+++ b/vllm/utils/flashinfer.py\n@@ -1,4 +1,6 @@\n # SPDX-License-Identifier: Apache-2.0\n+\n+\n # SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n \"\"\"Compatibility wrapper for FlashInfer API changes.\n \n@@ -77,6 +79,13 @@ autotune = _lazy_import_wrapper(\n     fallback_fn=lambda *args, **kwargs: contextlib.nullcontext())\n \n \n+# FP8 blockscale support wrappers\n+fp8_swizzle_blockscale = _lazy_import_wrapper(\n+    \"flashinfer\", \"fp8_swizzle_blockscale\")\n+flashinfer_fp8_blockscale_fused_moe = _lazy_import_wrapper(\n+    \"flashinfer.fused_moe\", \"fp8_blockscale_fused_moe\")\n+\n+\n @functools.cache\n def has_flashinfer_cutlass_fused_moe() -> bool:\n     \"\"\"Return ``True`` if FlashInfer CUTLASS fused MoE is available.\"\"\"\n@@ -96,12 +105,63 @@ def has_flashinfer_cutlass_fused_moe() -> bool:\n             return False\n     return True\n \n+@functools.cache\n+def has_flashinfer_fp8_blockscale_fused_moe() -> bool:\n+    \"\"\"Return ``True`` if FlashInfer FP8 blockscale fused MoE is available.\"\"\"\n+    if not has_flashinfer():\n+        return False\n+    required_functions = [\n+        (\"flashinfer.fused_moe\", \"fp8_blockscale_fused_moe\"),\n+        (\"flashinfer\", \"fp8_swizzle_blockscale\"),\n+    ]\n+    for module_name, attr_name in required_functions:\n+        mod = _get_submodule(module_name)\n+        if not mod or not hasattr(mod, attr_name):\n+            return False\n+    return True\n+\n+\n+@functools.cache\n+def has_flashinfer_moe() -> bool:\n+    \"\"\"Return True if any FlashInfer MoE backend is available (CUTLASS FP4 or FP8 blockscale).\"\"\"\n+    if not has_flashinfer():\n+        return False\n+    # CUTLASS FP4 path\n+    if has_flashinfer_cutlass_fused_moe():\n+        return True\n+    # FP8 blockscale path (try multiple potential symbol names)\n+    mod = _get_submodule(\"flashinfer.fused_moe\")\n+    if not mod:\n+        return False\n+    return any(\n+        hasattr(mod, name)\n+        for name in (\"blockscale_fused_moe_fp8\", \"fused_moe_blockscale_fp8\")\n+    )\n+\n+\n+def flashinfer_fused_moe_blockscale_fp8(*args, **kwargs):\n+    \"\"\"Dispatch to FlashInfer FP8 blockscale fused MoE if available.\n+\n+    Tries known symbol names to be resilient to upstream rename.\n+    \"\"\"\n+    mod = _get_submodule(\"flashinfer.fused_moe\") if has_flashinfer() else None\n+    if mod is not None:\n+        for name in (\"blockscale_fused_moe_fp8\", \"fused_moe_blockscale_fp8\"):\n+            fn = getattr(mod, name, None)\n+            if fn is not None:\n+                return fn(*args, **kwargs)\n+    return _missing(*args, **kwargs)\n+\n+\n \n __all__ = [\n     \"has_flashinfer\",\n+    \"has_flashinfer_moe\",\n+    \"flashinfer_fused_moe_blockscale_fp8\",\n+\n     \"has_flashinfer_cutlass_fused_moe\",\n     \"flashinfer_cutlass_fused_moe\",\n     \"fp4_quantize\",\n     \"fp4_swizzle_blockscale\",\n     \"autotune\",\n-]\n+]\n\\ No newline at end of file\n", "model_name_or_path": "gpt-5-2025-08-07"}
