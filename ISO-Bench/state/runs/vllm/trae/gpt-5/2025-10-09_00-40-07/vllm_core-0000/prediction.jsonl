{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..3bcbbe606 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4769,12 +4769,23 @@ class VllmConfig:\n                 # Hybrid KV cache manager is not compatible with KV events.\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n-                self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif \\\n+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\"\n+                    )\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..228d3834a 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -948,6 +948,10 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # implement and support a subset of all possible layouts.\n     \"VLLM_KV_CACHE_LAYOUT\":\n     lambda: os.getenv(\"VLLM_KV_CACHE_LAYOUT\", None),\n+    # If set, allow using hybrid KV cache manager together with chunked local attention.\n+    # Default is disabled due to latency regressions; set to 1 to enable.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: (os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\").strip().lower() in (\"1\", \"true\")),\n \n     # Enable checking whether the generated logits contain NaNs,\n     # indicating corrupted output. Useful for debugging low level bugs\n", "model_name_or_path": "gpt-5-2025-08-07"}
