{"instance_id": "vllm-project__vllm-6dd55af", "model_patch": "diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex e0169f1a4..792f9231a 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -275,9 +275,13 @@ class AsyncLLM(EngineClient):\n                 out = q.get_nowait() if not q.empty() else await q.get()\n \n                 # Coalesce any additional queued outputs\n-                while not q.empty():\n-                    next_out = q.get_nowait()\n-                    if sampling_params.output_kind == RequestOutputKind.DELTA:\n+                is_delta = sampling_params.output_kind == RequestOutputKind.DELTA\n+                while True:\n+                    try:\n+                        next_out = q.get_nowait()\n+                    except asyncio.QueueEmpty:\n+                        break\n+                    if is_delta:\n                         out.add(next_out)\n                     else:\n                         out = next_out\n@@ -315,6 +319,7 @@ class AsyncLLM(EngineClient):\n                     slices = np.array_split(\n                         outputs.outputs,\n                         cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n+                num_slices = len(slices)\n \n                 for i, outputs_slice in enumerate(slices):\n                     # 2) Process EngineCoreOutputs.\n@@ -324,7 +329,7 @@ class AsyncLLM(EngineClient):\n                     assert not processed_outputs.request_outputs\n \n                     # Allow other asyncio tasks to run between chunks\n-                    if i + 1 < len(slices):\n+                    if i + 1 < num_slices:\n                         await asyncio.sleep(0)\n \n                     # 3) Abort any reqs that finished due to stop strings.\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 12df34177..e2c57dfcd 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -24,6 +24,35 @@ class OutputProcessorOutput:\n     reqs_to_abort: list[str]\n \n \n+\n+\n+class RequestOutputCollector:\n+\n+    def __init__(self) -> None:\n+        # Map a queue to a per-request aggregation map\n+        self._queues: dict[asyncio.Queue, dict[str, RequestOutput]] = {}\n+\n+    def add(self, queue: asyncio.Queue, ro: RequestOutput,\n+            output_kind: RequestOutputKind) -> None:\n+        qmap = self._queues.setdefault(queue, {})\n+        prev = qmap.get(ro.request_id)\n+        if prev is None:\n+            qmap[ro.request_id] = ro\n+            return\n+        # Merge behavior depends on output kind\n+        if output_kind == RequestOutputKind.DELTA:\n+            prev.add(ro)\n+        else:\n+            # Replace with the most recent output for non-delta kinds\n+            qmap[ro.request_id] = ro\n+\n+    def flush(self) -> None:\n+        # Push aggregated outputs to their respective queues\n+        for queue, qmap in self._queues.items():\n+            for ro in qmap.values():\n+                queue.put_nowait(ro)\n+        self._queues.clear()\n+\n class RequestState:\n \n     def __init__(\n@@ -56,6 +85,10 @@ class RequestState:\n         self.is_prefilling = True\n         self.queue = queue\n \n+        # Cached flags to avoid repeated enum comparisons in hot paths\n+        self._final_only = (output_kind == RequestOutputKind.FINAL_ONLY)\n+        self._delta = (output_kind == RequestOutputKind.DELTA)\n+\n         self.stats = RequestStateStats(\n             arrival_time=arrival_time) if log_stats else None\n \n@@ -103,7 +136,7 @@ class RequestState:\n     ) -> Optional[RequestOutput]:\n \n         finished = finish_reason is not None\n-        final_only = self.output_kind == RequestOutputKind.FINAL_ONLY\n+        final_only = self._final_only\n \n         if not finished and final_only:\n             # Only the final output is required in FINAL_ONLY mode.\n@@ -153,10 +186,13 @@ class RequestState:\n     ) -> CompletionOutput:\n \n         finished = finish_reason is not None\n-        delta = self.output_kind == RequestOutputKind.DELTA\n+        delta = self._delta\n \n         # Prepare text and token_ids, based on delta mode\n-        text = self.detokenizer.get_next_output_text(finished, delta)\n+        if self.detokenizer.tokenizer is None:\n+            text = \"\"\n+        else:\n+            text = self.detokenizer.get_next_output_text(finished, delta)\n         if not delta:\n             token_ids = self.detokenizer.output_token_ids\n \n@@ -267,6 +303,7 @@ class OutputProcessor:\n \n         request_outputs: list[RequestOutput] = []\n         reqs_to_abort: list[str] = []\n+        collector = RequestOutputCollector()\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n             req_state = self.request_states.get(req_id)\n@@ -286,21 +323,32 @@ class OutputProcessor:\n             req_state.is_prefilling = False\n \n             # 2) Detokenize the token ids into text and perform stop checks.\n-            stop_string = req_state.detokenizer.update(\n-                new_token_ids, finish_reason == FinishReason.STOP)\n+            detok = req_state.detokenizer\n+            if detok.tokenizer is None:\n+                # Fast path: skip detokenization when disabled\n+                detok.token_ids.extend(new_token_ids)\n+                stop_string = None\n+            else:\n+                stop_string = detok.update(new_token_ids,\n+                                           finish_reason == FinishReason.STOP)\n             if stop_string and finish_reason != FinishReason.STOP:\n                 finish_reason = FinishReason.STOP\n                 stop_reason = stop_string\n \n             # 3) Compute sample and prompt logprobs for request, if required.\n-            req_state.logprobs_processor.update_from_output(engine_core_output)\n+            if (engine_core_output.new_logprobs is not None\n+                    or engine_core_output.new_prompt_logprobs_tensors\n+                    is not None):\n+                req_state.logprobs_processor.update_from_output(\n+                    engine_core_output)\n \n             # 4) Create and handle RequestOutput objects.\n             if request_output := req_state.make_request_output(\n                     new_token_ids, finish_reason, stop_reason):\n                 if req_state.queue is not None:\n-                    # AsyncLLM: put into queue for handling by generate().\n-                    req_state.queue.put_nowait(request_output)\n+                    # AsyncLLM: aggregate per-request outputs for this iteration\n+                    collector.add(req_state.queue, request_output,\n+                                   req_state.output_kind)\n                 else:\n                     # LLMEngine: return list of RequestOutputs.\n                     request_outputs.append(request_output)\n@@ -322,6 +370,9 @@ class OutputProcessor:\n                                                  iteration_stats)\n \n         self.lora_states.update_iteration_stats(iteration_stats)\n+        # Flush aggregated outputs to queues (AsyncLLM)\n+        collector.flush()\n+\n \n         return OutputProcessorOutput(\n             request_outputs=request_outputs,\n", "model_name_or_path": "gpt-5-2025-08-07"}
