{"instance_id": "vllm-project__vllm-b0e96aa", "model_patch": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex 79787303a..8ae17fd1a 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -327,15 +327,13 @@ class PyObjectCache:\n         self._obj_builder = obj_builder\n         self._index = 0\n \n-        self._obj_cache = []\n-        for _ in range(128):\n-            self._obj_cache.append(self._obj_builder())\n+        self._obj_cache = [self._obj_builder() for _ in range(128)]\n \n     def _grow_cache(self):\n         # Double the size of the cache\n         num_objs = len(self._obj_cache)\n-        for _ in range(num_objs):\n-            self._obj_cache.append(self._obj_builder())\n+        # Use list comprehension for efficiency\n+        self._obj_cache.extend(self._obj_builder() for _ in range(num_objs))\n \n     def get_object(self):\n         \"\"\"Returns a pre-allocated cached object. If there is not enough\n@@ -412,6 +410,15 @@ async def merge_async_iterators(\n     iterator that yields the item.\n     \"\"\"\n \n+    if not iterators:\n+        return\n+\n+    if len(iterators) == 1:\n+        # Fast-path single iterator case.\n+        async for item in iterators[0]:\n+            yield 0, item\n+        return\n+\n     loop = asyncio.get_running_loop()\n \n     awaits = {_next_task(pair[1], loop): pair for pair in enumerate(iterators)}\n@@ -440,8 +447,9 @@ async def collect_from_async_generator(\n         iterator: AsyncGenerator[T, None]) -> list[T]:\n     \"\"\"Collect all items from an async generator into a list.\"\"\"\n     items = []\n+    append = items.append\n     async for item in iterator:\n-        items.append(item)\n+        append(item)\n     return items\n \n \n@@ -778,6 +786,10 @@ def make_ndarray_with_pad(\n         # Unlike for most functions, map is faster than a genexpr over `len`\n         max_len = max(map(len, x), default=0)\n \n+    # Fast-path: if inputs are already rectangular, avoid padding loop\n+    if all(len(blocktb) == max_len for blocktb in x):\n+        return np.asarray(x, dtype=dtype)\n+\n     padded_x = np.full((len(x), max_len), pad, dtype=dtype)\n     for ind, blocktb in enumerate(x):\n         assert len(blocktb) <= max_len\n@@ -804,9 +816,12 @@ def make_tensor_with_pad(\n     np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE[dtype]\n     padded_x = make_ndarray_with_pad(x, pad, np_dtype, max_len=max_len)\n \n-    tensor = torch.from_numpy(padded_x).to(device)\n+    tensor = torch.from_numpy(padded_x)\n     if pin_memory:\n         tensor = tensor.pin_memory()\n+    if device is not None:\n+        dev = device if isinstance(device, torch.device) else torch.device(device)\n+        tensor = tensor.to(device=dev, non_blocking=(pin_memory and dev.type == \"cuda\"))\n \n     return tensor\n \n@@ -819,9 +834,10 @@ def async_tensor_h2d(\n ) -> torch.Tensor:\n     \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n     t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n-    return t.to(device=target_device, non_blocking=True)\n+    return t.to(device=target_device, non_blocking=pin_memory)\n \n \n+@lru_cache(maxsize=None)\n def get_dtype_size(dtype: torch.dtype) -> int:\n     \"\"\"Get the size of the data type in bytes.\"\"\"\n     return torch.tensor([], dtype=dtype).element_size()\n@@ -1948,8 +1964,8 @@ class MemorySnapshot:\n         self.torch_peak = torch.cuda.memory_stats().get(\n             \"allocated_bytes.all.peak\", 0)\n \n-        self.cuda_memory = torch.cuda.mem_get_info(\n-        )[1] - torch.cuda.mem_get_info()[0]\n+        free, total = torch.cuda.mem_get_info()\n+        self.cuda_memory = total - free\n \n         # torch.cuda.memory_reserved() is how many bytes\n         # PyTorch gets from cuda (by calling cudaMalloc, etc.)\n", "model_name_or_path": "gpt-5-2025-08-07"}
