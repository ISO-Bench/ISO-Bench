{"instance_id": "vllm-project__vllm-beebf47", "model_patch": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed86..fc585ee9e 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -244,6 +244,7 @@ class LLM:\n             engine_args, usage_context=UsageContext.LLM_CLASS)\n \n         self.request_counter = Counter()\n+        self.default_sampling_params: Union[dict[str, Any], None] = None\n \n     @staticmethod\n     def get_engine_class() -> type[LLMEngine]:\n@@ -268,10 +269,11 @@ class LLM:\n             tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n \n     def get_default_sampling_params(self) -> SamplingParams:\n-        diff_sampling_param = (\n-            self.llm_engine.model_config.get_diff_sampling_param())\n-        if diff_sampling_param:\n-            return SamplingParams.from_optional(**diff_sampling_param)\n+        if self.default_sampling_params is None:\n+            self.default_sampling_params = (\n+                self.llm_engine.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            return SamplingParams.from_optional(**self.default_sampling_params)\n         return SamplingParams()\n \n     @overload\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0fc..53e7444e5 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -105,10 +105,10 @@ class OpenAIServingChat(OpenAIServing):\n                                 \"been registered\") from e\n \n         self.enable_prompt_tokens_details = enable_prompt_tokens_details\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\"Overwriting default chat sampling param with: %s\",\n-                        diff_sampling_param)\n+                        self.default_sampling_params)\n \n     async def create_chat_completion(\n         self,\n@@ -211,8 +211,7 @@ class OpenAIServingChat(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n+                default_sampling_params = self.default_sampling_params\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af84f..e3dec37ee 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -51,11 +51,11 @@ class OpenAIServingCompletion(OpenAIServing):\n                          models=models,\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def create_completion(\n         self,\n@@ -120,8 +120,7 @@ class OpenAIServingCompletion(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n+                default_sampling_params = self.default_sampling_params\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a5e..a030ed42e 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -161,11 +161,11 @@ class OpenAIServingTranscription(OpenAIServing):\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n \n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def _preprocess_transcription(\n         self,\n@@ -273,7 +273,7 @@ class OpenAIServingTranscription(OpenAIServing):\n         try:\n             # TODO(rob): subtract len of tokenized prompt.\n             default_max_tokens = self.model_config.max_model_len\n-            default_params = self.model_config.get_diff_sampling_param()\n+            default_params = self.default_sampling_params\n             sampling_params = request.to_sampling_params(\n                 default_max_tokens, default_params)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
