{"instance_id": "vllm-project__vllm-5fc5ce0", "model_patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 45a3b484e..ddd0aca42 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -15,11 +15,30 @@ project(vllm_extensions LANGUAGES CXX)\n \n # CUDA by default, can be overridden by using -DVLLM_TARGET_DEVICE=... (used by setup.py)\n set(VLLM_TARGET_DEVICE \"cuda\" CACHE STRING \"Target device backend for vLLM\")\n+# Default to Release builds for performance unless explicitly specified\n+if(NOT CMAKE_BUILD_TYPE)\n+  set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n+endif()\n+\n \n message(STATUS \"Build type: ${CMAKE_BUILD_TYPE}\")\n message(STATUS \"Target device: ${VLLM_TARGET_DEVICE}\")\n \n include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)\n+# Enable IPO/LTO when supported for better host-side performance\n+include(CheckIPOSupported)\n+check_ipo_supported(RESULT ipo_supported OUTPUT ipo_error)\n+if(ipo_supported)\n+  set(CMAKE_INTERPROCEDURAL_OPTIMIZATION ON)\n+endif()\n+\n+# Set NVCC thread parallelism to number of available cores if not provided\n+include(ProcessorCount)\n+ProcessorCount(NVCC_THREADS_COUNT)\n+if(NOT NVCC_THREADS AND NVCC_THREADS_COUNT GREATER 0)\n+  set(NVCC_THREADS ${NVCC_THREADS_COUNT})\n+endif()\n+\n \n # Suppress potential warnings about unused manually-specified variables\n set(ignoreMe \"${VLLM_PYTHON_PATH}\")\n@@ -158,6 +177,11 @@ endif()\n # The final set of arches is stored in `VLLM_GPU_FLAGS`.\n #\n get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG})\n+# Prefer high optimization for device code in Release-like builds\n+if(CMAKE_BUILD_TYPE MATCHES \"^Release$|^RelWithDebInfo$\")\n+  list(APPEND VLLM_GPU_FLAGS \"-O3\" \"-DNDEBUG\")\n+endif()\n+\n \n #\n # Set nvcc parallelism.\n@@ -522,7 +546,7 @@ else()\n   FetchContent_Declare(\n           vllm-flash-attn\n           GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git\n-          GIT_TAG d886f88165702b3c7e7744502772cd98b06be9e1\n+          GIT_TAG fdf6d72b48aea41f4ae6a89139a453dae554abc8\n           GIT_PROGRESS TRUE\n           # Don't share the vllm-flash-attn build between build types\n           BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn\n", "model_name_or_path": "gpt-5-2025-08-07"}
