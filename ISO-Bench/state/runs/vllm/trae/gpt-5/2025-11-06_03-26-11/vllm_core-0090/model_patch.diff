diff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py
index 4bf5cbbd1..eecb15938 100644
--- a/vllm/executor/ray_gpu_executor.py
+++ b/vllm/executor/ray_gpu_executor.py
@@ -123,6 +123,7 @@ class RayGPUExecutor(DistributedGPUExecutor):
 
         # Create the workers.
         driver_ip = get_ip()
+        workers = []
         for bundle_id, bundle in enumerate(placement_group.bundle_specs):
             if not bundle.get("GPU", 0):
                 continue
@@ -138,20 +139,33 @@ class RayGPUExecutor(DistributedGPUExecutor):
                 scheduling_strategy=scheduling_strategy,
                 **ray_remote_kwargs,
             )(RayWorkerWrapper).remote(vllm_config=self.vllm_config)
+            workers.append(worker)
 
-            if self.use_ray_spmd_worker:
-                self.workers.append(worker)
+        # Fetch worker IPs in batch to avoid per-worker ray.get overhead.
+        worker_ip_refs = [
+            worker.get_node_ip.remote()  # type: ignore[attr-defined]
+            for worker in workers
+        ]
+        workers_ips_all = ray.get(worker_ip_refs)
+
+        if self.use_ray_spmd_worker:
+            self.workers = workers
+        else:
+            driver_index: Optional[int] = None
+            for idx, ip in enumerate(workers_ips_all):
+                if ip == driver_ip:
+                    driver_index = idx
+                    break
+            if driver_index is not None:
+                self.driver_dummy_worker = workers[driver_index]
+                self.driver_worker = RayWorkerWrapper(vllm_config=self.vllm_config)
+                self.workers = [w for i, w in enumerate(workers) if i != driver_index]
+                worker_ips = [
+                    ip for i, ip in enumerate(workers_ips_all) if i != driver_index
+                ]
             else:
-                worker_ip = ray.get(worker.get_node_ip.remote())
-                if worker_ip == driver_ip and self.driver_dummy_worker is None:
-                    # If the worker is on the same node as the driver, we use it
-                    # as the resource holder for the driver process.
-                    self.driver_dummy_worker = worker
-                    self.driver_worker = RayWorkerWrapper(
-                        vllm_config=self.vllm_config)
-                else:
-                    # Else, added to the list of workers.
-                    self.workers.append(worker)
+                self.workers = workers
+                worker_ips = workers_ips_all
 
         logger.debug("workers: %s", self.workers)
         logger.debug("driver_dummy_worker: %s", self.driver_dummy_worker)
@@ -161,41 +175,28 @@ class RayGPUExecutor(DistributedGPUExecutor):
                 "adjusting the Ray placement group or running the driver on a "
                 "GPU node.")
 
-        worker_ips = [
-            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]
-            for worker in self.workers
-        ]
+        # Precompute ip counts and sort workers without additional ray.get calls.
+        if self.use_ray_spmd_worker:
+            # In SPMD mode we haven't filtered out a driver; use all IPs.
+            worker_ips = workers_ips_all
+
         ip_counts: Dict[str, int] = {}
         for ip in worker_ips:
             ip_counts[ip] = ip_counts.get(ip, 0) + 1
 
-        def sort_by_driver_then_worker_ip(worker):
-            """
-            Sort the workers based on 3 properties:
-            1. If the worker is on the same node as the driver (vllm engine),
-                it should be placed first.
-            2. Then, if the worker is on a node with fewer workers, it should
-                be placed first.
-            3. Finally, if the work is on a node with smaller IP address, it
-                should be placed first.
-            """
-            ip = ray.get(worker.get_node_ip.remote())
-            return (ip != driver_ip, ip_counts[ip], ip)
-
-        # After sorting, the workers on the same node will be
-        # close to each other, and the workers on the driver
-        # node will be placed first.
-        self.workers = sorted(self.workers, key=sort_by_driver_then_worker_ip)
+        workers_with_ips = list(zip(self.workers, worker_ips))
+        self.workers = [w for w, _ in sorted(
+            workers_with_ips,
+            key=lambda wi: (wi[1] != driver_ip, ip_counts[wi[1]], wi[1]),
+        )]
 
         # Get the set of GPU IDs used on each node.
-        worker_node_and_gpu_ids = []
-        for worker in [self.driver_dummy_worker] + self.workers:
-            if worker is None:
-                # driver_dummy_worker can be None when using ray spmd worker.
-                continue
-            worker_node_and_gpu_ids.append(
-                ray.get(worker.get_node_and_gpu_ids.remote()) \
-            ) # type: ignore
+        workers_all = [w for w in [self.driver_dummy_worker] + self.workers if w is not None]
+        node_and_gpu_id_refs = [
+            w.get_node_and_gpu_ids.remote()  # type: ignore[attr-defined]
+            for w in workers_all
+        ]
+        worker_node_and_gpu_ids = ray.get(node_and_gpu_id_refs)
 
         node_workers = defaultdict(list)  # node id -> list of worker ranks
         node_gpus = defaultdict(list)  # node id -> list of gpu ids
