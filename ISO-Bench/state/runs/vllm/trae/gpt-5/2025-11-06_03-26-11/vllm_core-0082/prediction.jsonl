{"instance_id": "vllm-project__vllm-f508e03", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4ea1..b47adee61 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -501,9 +501,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                             and self.sliding_window is None\n                             and inter_data.is_prompt)\n         inter_data.prefix_cache_hit = prefix_cache_hit\n-        if self.chunked_prefill_enabled and prefix_cache_hit:\n-            raise RuntimeError(\n-                \"chunked prefill cannot be used with prefix caching now.\")\n+        # Allow chunked prefill together with prefix cache hit.\n \n         # If prefix cache is hit, advance context length to bypass\n         # hit blocks. Accordingly, input tokens, position and query length\n@@ -1197,8 +1195,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n+        input_tokens = torch.empty(max_batch_size, dtype=torch.long, device=self.device)\n+        input_positions = torch.empty(max_batch_size, dtype=torch.long, device=self.device)\n \n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\n", "model_name_or_path": "gpt-5-2025-08-07"}
