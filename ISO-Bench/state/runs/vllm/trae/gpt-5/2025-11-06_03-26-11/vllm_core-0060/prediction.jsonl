{"instance_id": "vllm-project__vllm-7206ce4", "model_patch": "diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..288b72381 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -1,5 +1,6 @@\n import asyncio\n import atexit\n+import gc\n import importlib\n import inspect\n import multiprocessing\n@@ -104,6 +105,10 @@ async def lifespan(app: FastAPI):\n             task.add_done_callback(_running_tasks.remove)\n         else:\n             task = None\n+\n+        # Mark the startup heap as static so that it's ignored by GC to reduce pause times.\n+        gc.collect()\n+        gc.freeze()\n         try:\n             yield\n         finally:\ndiff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..693314e03 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -19,6 +19,13 @@ from vllm.utils import random_uuid, resolve_obj_by_qualname\n \n logger = init_logger(__name__)\n \n+\n+# Fast path for integer epoch time used in default_factory\n+_time = time.time\n+\n+def _now_int() -> int:\n+    return int(_time())\n+\n # torch is mocked during docs generation,\n # so we have to provide the values as literals\n _MOCK_LONG_INFO = Namespace(min=-9223372036854775808, max=9223372036854775807)\n@@ -73,7 +80,7 @@ class ErrorResponse(OpenAIBaseModel):\n class ModelPermission(OpenAIBaseModel):\n     id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\n     object: str = \"model_permission\"\n-    created: int = Field(default_factory=lambda: int(time.time()))\n+    created: int = Field(default_factory=_now_int)\n     allow_create_engine: bool = False\n     allow_sampling: bool = True\n     allow_logprobs: bool = True\n@@ -88,7 +95,7 @@ class ModelPermission(OpenAIBaseModel):\n class ModelCard(OpenAIBaseModel):\n     id: str\n     object: str = \"model\"\n-    created: int = Field(default_factory=lambda: int(time.time()))\n+    created: int = Field(default_factory=_now_int)\n     owned_by: str = \"vllm\"\n     root: Optional[str] = None\n     parent: Optional[str] = None\n@@ -170,10 +177,12 @@ def get_logits_processors(processors: Optional[LogitsProcessors],\n                           pattern: Optional[str]) -> Optional[List[Any]]:\n     if processors and pattern:\n         logits_processors = []\n+        compiled = re.compile(pattern)\n+        match = compiled.match\n         for processor in processors:\n             qualname = processor if isinstance(processor,\n                                                str) else processor.qualname\n-            if not re.match(pattern, qualname):\n+            if not match(qualname):\n                 raise ValueError(\n                     f\"Logits processor '{qualname}' is not allowed by this \"\n                     \"server. See --logits-processor-pattern engine argument \"\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e4411..e50a1da7d 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -205,10 +205,13 @@ class AsyncLLM(EngineClient):\n \n             # The output_handler task pushes items into the queue.\n             # This task pulls from the queue and yields to caller.\n+            get_nowait = q.get_nowait\n+            get = q.get\n+            qsize = q.qsize\n             while True:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                out = get_nowait() if qsize() > 0 else await get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\n@@ -229,24 +232,26 @@ class AsyncLLM(EngineClient):\n         \"\"\"Background loop: pulls from EngineCore and pushes to AsyncStreams.\"\"\"\n \n         try:\n+            get_output_async = self.engine_core.get_output_async\n+            process_outputs = self.output_processor.process_outputs\n+            abort_requests_async = self.engine_core.abort_requests_async\n+            log_stats = self._log_stats\n             while True:\n                 # 1) Pull EngineCoreOutputs from the EngineCore.\n-                outputs = await self.engine_core.get_output_async()\n+                outputs = await get_output_async()\n \n                 # 2) Process EngineCoreOutputs.\n-                processed_outputs = self.output_processor.process_outputs(\n-                    outputs.outputs)\n+                processed_outputs = process_outputs(outputs.outputs)\n                 # NOTE: RequestOutputs are pushed to their queues.\n                 assert len(processed_outputs.request_outputs) == 0\n \n                 # 3) Abort any reqs that finished due to stop strings.\n-                await self.engine_core.abort_requests_async(\n-                    processed_outputs.reqs_to_abort)\n+                await abort_requests_async(processed_outputs.reqs_to_abort)\n \n                 # 4) Logging.\n                 # TODO(rob): make into a coroutine and launch it in\n                 # background thread once we add Prometheus.\n-                self._log_stats(\n+                log_stats(\n                     scheduler_stats=outputs.scheduler_stats,\n                     iteration_stats=processed_outputs.iteration_stats,\n                 )\ndiff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py\nindex 19b89003c..12d546ebe 100644\n--- a/vllm/v1/engine/core_client.py\n+++ b/vllm/v1/engine/core_client.py\n@@ -111,7 +111,7 @@ class InprocClient(EngineCoreClient):\n         self.engine_core.add_request(request)\n \n     def abort_requests(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             self.engine_core.abort_requests(request_ids)\n \n     def shutdown(self) -> None:\n@@ -231,7 +231,7 @@ class SyncMPClient(MPClient):\n         self._send_input(EngineCoreRequestType.ADD, request)\n \n     def abort_requests(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             self._send_input(EngineCoreRequestType.ABORT, request_ids)\n \n     def profile(self, is_start: bool = True) -> None:\n@@ -273,7 +273,7 @@ class AsyncMPClient(MPClient):\n         await self._send_input(EngineCoreRequestType.ADD, request)\n \n     async def abort_requests_async(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             await self._send_input(EngineCoreRequestType.ABORT, request_ids)\n \n     async def profile_async(self, is_start: bool = True) -> None:\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..b1f35de96 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -21,6 +21,17 @@ class OutputProcessorOutput:\n \n class RequestState:\n \n+    __slots__ = (\n+        \"request_id\",\n+        \"prompt\",\n+        \"prompt_token_ids\",\n+        \"prompt_len\",\n+        \"detokenizer\",\n+        \"is_prefilling\",\n+        \"queue\",\n+    )\n+\n+\n     def __init__(\n         self,\n         request_id: str,\n@@ -134,17 +145,19 @@ class OutputProcessor:\n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n         iteration_stats = IterationStats(self.log_stats)\n+        request_states = self.request_states\n+        iteration_stats_update = iteration_stats.update_from_output\n+        make_request_output = self._make_request_output\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n-            req_state = self.request_states.get(req_id)\n+            req_state = request_states.get(req_id)\n             if req_state is None:\n                 # Ignore output for already-aborted request.\n                 continue\n \n             # 1) Compute stats for this iteration.\n-            iteration_stats.update_from_output(engine_core_output,\n-                                               req_state.is_prefilling,\n-                                               req_state.prompt_len)\n+            iteration_stats_update(engine_core_output, req_state.is_prefilling,\n+                                   req_state.prompt_len)\n             req_state.is_prefilling = False\n \n             # 2) Detokenize the token ids into text.\n@@ -152,8 +165,8 @@ class OutputProcessor:\n                 engine_core_output)\n \n             # 3) Create and handle RequestOutput objects.\n-            if request_output := self._make_request_output(\n-                    req_state, detokenizer_output):\n+            if request_output := make_request_output(req_state,\n+                                                     detokenizer_output):\n                 if req_state.queue is not None:\n                     # AsyncLLM: put into queue for handling by generate().\n                     req_state.queue.put_nowait(request_output)\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..436bd04cf 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -15,6 +15,28 @@ if TYPE_CHECKING:\n \n class Request:\n \n+    __slots__ = (\n+        \"request_id\",\n+        \"sampling_params\",\n+        \"eos_token_id\",\n+        \"metrics\",\n+        \"lora_request\",\n+        \"status\",\n+        \"stop_reason\",\n+        \"max_tokens\",\n+        \"prompt\",\n+        \"prompt_token_ids\",\n+        \"num_prompt_tokens\",\n+        \"_output_token_ids\",\n+        \"_all_token_ids\",\n+        \"num_computed_tokens\",\n+        \"mm_positions\",\n+        \"mm_inputs\",\n+        \"mm_hashes\",\n+        \"_kv_block_hashes\",\n+    )\n+\n+\n     def __init__(\n         self,\n         request_id: str,\n", "model_name_or_path": "gpt-5-2025-08-07"}
