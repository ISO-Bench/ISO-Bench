diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index 6bf5d5130..c7d3d8538 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
 
     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)
       list(APPEND GPU_FLAGS "-DENABLE_FP8_E5M2")
+      list(REMOVE_ITEM GPU_FLAGS
+        "-D__CUDA_NO_HALF_OPERATORS__"
+        "-D__CUDA_NO_HALF_CONVERSIONS__"
+        "-D__CUDA_NO_BFLOAT16_CONVERSIONS__"
+        "-D__CUDA_NO_HALF2_OPERATORS__")
     endif()
 
   elseif(${GPU_LANG} STREQUAL "HIP")
diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu
index 6d34d014c..8057732f9 100644
--- a/csrc/layernorm_kernels.cu
+++ b/csrc/layernorm_kernels.cu
@@ -4,6 +4,15 @@
 
 #include "dispatch_utils.h"
 #include "reduction_utils.cuh"
+#include "cuda_compat.h"
+#ifndef USE_ROCM
+  #include <cuda_bf16.h>
+  #include <cuda_fp16.h>
+#else
+  #include <hip/hip_bf16.h>
+  #include <hip/hip_fp16.h>
+#endif
+
 
 namespace vllm {
 
@@ -18,9 +27,10 @@ __global__ void rms_norm_kernel(
   const int hidden_size) {
   __shared__ float s_variance;
   float variance = 0.0f;
+  const int base = blockIdx.x * hidden_size;
 
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    const float x = (float) input[blockIdx.x * hidden_size + idx];
+    const float x = (float) VLLM_LDG(input + base + idx);
     variance += x * x;
   }
   variance = blockReduceSum<float>(variance);
@@ -30,8 +40,9 @@ __global__ void rms_norm_kernel(
   __syncthreads();
 
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) input[blockIdx.x * hidden_size + idx];
-    out[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];
+    float x = (float) VLLM_LDG(input + base + idx);
+    const scalar_t w = VLLM_LDG(weight + idx);
+    out[base + idx] = ((scalar_t) (x * s_variance)) * w;
   }
 }
 
@@ -46,12 +57,13 @@ __global__ void fused_add_rms_norm_kernel(
   const int hidden_size) {
   __shared__ float s_variance;
   float variance = 0.0f;
+  const int base = blockIdx.x * hidden_size;
 
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) input[blockIdx.x * hidden_size + idx];
-    x += (float) residual[blockIdx.x * hidden_size + idx];
+    float x = (float) VLLM_LDG(input + base + idx);
+    x += (float) VLLM_LDG(residual + base + idx);
     variance += x * x;
-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;
+    residual[base + idx] = (scalar_t) x;
   }
   variance = blockReduceSum<float>(variance);
   if (threadIdx.x == 0) {
@@ -60,8 +72,9 @@ __global__ void fused_add_rms_norm_kernel(
   __syncthreads();
 
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) residual[blockIdx.x * hidden_size + idx];
-    input[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];
+    float x = (float) VLLM_LDG(residual + base + idx);
+    const scalar_t w = VLLM_LDG(weight + idx);
+    input[base + idx] = ((scalar_t) (x * s_variance)) * w;
   }
 }
 
@@ -76,7 +89,8 @@ void rms_norm(
   int num_tokens = input.numel() / hidden_size;
 
   dim3 grid(num_tokens);
-  dim3 block(std::min(hidden_size, 1024));
+  int threads = std::min(1024, ((hidden_size + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);
+  dim3 block(threads);
   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
   VLLM_DISPATCH_FLOATING_TYPES(
@@ -102,7 +116,8 @@ void fused_add_rms_norm(
   int num_tokens = input.numel() / hidden_size;
 
   dim3 grid(num_tokens);
-  dim3 block(std::min(hidden_size, 1024));
+  int threads = std::min(1024, ((hidden_size + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);
+  dim3 block(threads);
   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));
   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();
   VLLM_DISPATCH_FLOATING_TYPES(
diff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh
index c25464e86..e23f80d37 100644
--- a/csrc/reduction_utils.cuh
+++ b/csrc/reduction_utils.cuh
@@ -40,12 +40,11 @@ __inline__ __device__ constexpr int _calculateWidShift(int warp_size) {
 /* Calculate the sum of all elements in a block */
 template<typename T>
 __inline__ __device__ T blockReduceSum(T val) {
-  static __shared__ T shared[WARP_SIZE];
-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);
-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);
-  int lane = threadIdx.x & LANE_MASK;
-  int wid = threadIdx.x >> WID_SHIFT;
+  __shared__ T shared[WARP_SIZE];
+  int lane = threadIdx.x & (WARP_SIZE - 1);
+  int wid = threadIdx.x / WARP_SIZE;
 
+  // Intra-warp reduce
   val = warpReduceSum<T>(val);
 
   if (lane == 0)
@@ -53,11 +52,12 @@ __inline__ __device__ T blockReduceSum(T val) {
 
   __syncthreads();
 
-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
-  // blockDim.x is not divided by 32
-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);
-  val = warpReduceSum<T>(val);
-  return val;
+  // Only the first warp reduces across warps
+  T total = (threadIdx.x < ((blockDim.x + WARP_SIZE - 1) / WARP_SIZE)) ? shared[lane] : static_cast<T>(0);
+  if (threadIdx.x < WARP_SIZE) {
+    total = warpReduceSum<T>(total);
+  }
+  return total;
 }
 
 } // namespace vllm
