{"instance_id": "vllm-project__vllm-51c31bc", "model_patch": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex 6bf5d5130..c7d3d8538 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+      list(REMOVE_ITEM GPU_FLAGS\n+        \"-D__CUDA_NO_HALF_OPERATORS__\"\n+        \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+        \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\"\n+        \"-D__CUDA_NO_HALF2_OPERATORS__\")\n     endif()\n \n   elseif(${GPU_LANG} STREQUAL \"HIP\")\ndiff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex 6d34d014c..8057732f9 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -4,6 +4,15 @@\n \n #include \"dispatch_utils.h\"\n #include \"reduction_utils.cuh\"\n+#include \"cuda_compat.h\"\n+#ifndef USE_ROCM\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n+#else\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n+#endif\n+\n \n namespace vllm {\n \n@@ -18,9 +27,10 @@ __global__ void rms_norm_kernel(\n   const int hidden_size) {\n   __shared__ float s_variance;\n   float variance = 0.0f;\n+  const int base = blockIdx.x * hidden_size;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float) input[blockIdx.x * hidden_size + idx];\n+    const float x = (float) VLLM_LDG(input + base + idx);\n     variance += x * x;\n   }\n   variance = blockReduceSum<float>(variance);\n@@ -30,8 +40,9 @@ __global__ void rms_norm_kernel(\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    out[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n+    float x = (float) VLLM_LDG(input + base + idx);\n+    const scalar_t w = VLLM_LDG(weight + idx);\n+    out[base + idx] = ((scalar_t) (x * s_variance)) * w;\n   }\n }\n \n@@ -46,12 +57,13 @@ __global__ void fused_add_rms_norm_kernel(\n   const int hidden_size) {\n   __shared__ float s_variance;\n   float variance = 0.0f;\n+  const int base = blockIdx.x * hidden_size;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    x += (float) residual[blockIdx.x * hidden_size + idx];\n+    float x = (float) VLLM_LDG(input + base + idx);\n+    x += (float) VLLM_LDG(residual + base + idx);\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n+    residual[base + idx] = (scalar_t) x;\n   }\n   variance = blockReduceSum<float>(variance);\n   if (threadIdx.x == 0) {\n@@ -60,8 +72,9 @@ __global__ void fused_add_rms_norm_kernel(\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) residual[blockIdx.x * hidden_size + idx];\n-    input[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n+    float x = (float) VLLM_LDG(residual + base + idx);\n+    const scalar_t w = VLLM_LDG(weight + idx);\n+    input[base + idx] = ((scalar_t) (x * s_variance)) * w;\n   }\n }\n \n@@ -76,7 +89,8 @@ void rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  int threads = std::min(1024, ((hidden_size + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n+  dim3 block(threads);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(\n@@ -102,7 +116,8 @@ void fused_add_rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  int threads = std::min(1024, ((hidden_size + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE);\n+  dim3 block(threads);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(\ndiff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nindex c25464e86..e23f80d37 100644\n--- a/csrc/reduction_utils.cuh\n+++ b/csrc/reduction_utils.cuh\n@@ -40,12 +40,11 @@ __inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n /* Calculate the sum of all elements in a block */\n template<typename T>\n __inline__ __device__ T blockReduceSum(T val) {\n-  static __shared__ T shared[WARP_SIZE];\n-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n-  int lane = threadIdx.x & LANE_MASK;\n-  int wid = threadIdx.x >> WID_SHIFT;\n+  __shared__ T shared[WARP_SIZE];\n+  int lane = threadIdx.x & (WARP_SIZE - 1);\n+  int wid = threadIdx.x / WARP_SIZE;\n \n+  // Intra-warp reduce\n   val = warpReduceSum<T>(val);\n \n   if (lane == 0)\n@@ -53,11 +52,12 @@ __inline__ __device__ T blockReduceSum(T val) {\n \n   __syncthreads();\n \n-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n-  // blockDim.x is not divided by 32\n-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n-  val = warpReduceSum<T>(val);\n-  return val;\n+  // Only the first warp reduces across warps\n+  T total = (threadIdx.x < ((blockDim.x + WARP_SIZE - 1) / WARP_SIZE)) ? shared[lane] : static_cast<T>(0);\n+  if (threadIdx.x < WARP_SIZE) {\n+    total = warpReduceSum<T>(total);\n+  }\n+  return total;\n }\n \n } // namespace vllm\n", "model_name_or_path": "gpt-5-2025-08-07"}
