{"instance_id": "vllm-project__vllm-9bde5ba", "model_patch": "diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex 43f30f710..f2beb091d 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -67,11 +67,11 @@ class BlockPool:\n         Returns:\n             The cached block if it exists, or None.\n         \"\"\"\n-        if block_hash in self.cached_block_hash_to_block:\n-            first_block_id = list(\n-                self.cached_block_hash_to_block[block_hash].keys())[0]\n-            return self.cached_block_hash_to_block[block_hash][first_block_id]\n-        return None\n+        cached_blocks = self.cached_block_hash_to_block.get(block_hash)\n+        if not cached_blocks:\n+            return None\n+        first_block_id = next(iter(cached_blocks))\n+        return cached_blocks[first_block_id]\n \n     def cache_full_blocks(\n         self,\n@@ -102,11 +102,11 @@ class BlockPool:\n             block_size: Number of tokens in each block.\n             hash_fn: The hash function to use for block hashes.\n         \"\"\"\n-        if num_cached_blocks == num_full_blocks:\n-            return\n+        cdict = self.cached_block_hash_to_block\n         new_full_blocks = blocks[num_cached_blocks:num_full_blocks]\n         assert len(block_hashes) >= num_cached_blocks\n         new_block_hashes = block_hashes[num_cached_blocks:]\n+        new_block_hashes_len = len(new_block_hashes)\n \n         # Update the new blocks with the block hashes through the chain.\n         if num_cached_blocks == 0:\n@@ -116,10 +116,11 @@ class BlockPool:\n             assert prev_block.block_hash is not None\n             prev_block_hash_value = prev_block.block_hash.hash_value\n \n+        all_token_ids = request.all_token_ids\n         for i, blk in enumerate(new_full_blocks):\n             assert blk.block_hash is None\n \n-            if i < len(new_block_hashes):\n+            if i < new_block_hashes_len:\n                 # The block hash may already be computed in\n                 # \"get_computed_blocks\" if the tokens are not generated by\n                 # this request (either the prompt tokens or the previously\n@@ -132,8 +133,7 @@ class BlockPool:\n                 blk_idx = num_cached_blocks + i\n                 start_token_idx = blk_idx * block_size\n                 end_token_idx = (blk_idx + 1) * block_size\n-                block_tokens = request.all_token_ids[\n-                    start_token_idx:end_token_idx]\n+                block_tokens = all_token_ids[start_token_idx:end_token_idx]\n                 assert len(block_tokens) == block_size, (\n                     f\"Expected {block_size} tokens, got \"\n                     f\"{len(block_tokens)} at {blk_idx}th block for request \"\n@@ -152,7 +152,7 @@ class BlockPool:\n \n             # Update and added the full block to the cache.\n             blk.block_hash = block_hash\n-            self.cached_block_hash_to_block[block_hash][blk.block_id] = blk\n+            cdict[block_hash][blk.block_id] = blk\n             prev_block_hash_value = block_hash.hash_value\n \n     def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:\n@@ -171,19 +171,19 @@ class BlockPool:\n                 f\"Cannot get {num_blocks} free blocks from the pool\")\n \n         ret: list[KVCacheBlock] = []\n-        idx = 0\n-        while idx < num_blocks:\n+        free_q = self.free_block_queue\n+        maybe_evict = self._maybe_evict_cached_block\n+        for _ in range(num_blocks):\n             # First allocate blocks.\n-            curr_block = self.free_block_queue.popleft()\n+            curr_block = free_q.popleft()\n             assert curr_block.ref_cnt == 0\n \n             # If the block is cached, evict it.\n             if self.enable_caching:\n-                self._maybe_evict_cached_block(curr_block)\n+                maybe_evict(curr_block)\n \n             curr_block.incr_ref()\n             ret.append(curr_block)\n-            idx += 1\n \n         return ret\n \n@@ -199,15 +199,18 @@ class BlockPool:\n             True if the block is evicted, False otherwise.\n         \"\"\"\n         block_hash = block.block_hash\n-        if block_hash and block_hash in self.cached_block_hash_to_block:\n-            block.reset_hash()\n-            del self.cached_block_hash_to_block[block_hash][block.block_id]\n+        if not block_hash:\n+            return False\n+        cached_blocks = self.cached_block_hash_to_block.get(block_hash)\n+        if not cached_blocks:\n+            return False\n+        block.reset_hash()\n+        cached_blocks.pop(block.block_id, None)\n \n-            if len(self.cached_block_hash_to_block[block_hash]) == 0:\n-                del self.cached_block_hash_to_block[block_hash]\n+        if not cached_blocks:\n+            self.cached_block_hash_to_block.pop(block_hash, None)\n \n-            return True\n-        return False\n+        return True\n \n     def touch(self, blocks: list[KVCacheBlock]) -> None:\n         \"\"\"Touch a block increases its reference count by 1, and may remove\n", "model_name_or_path": "gpt-5-2025-08-07"}
