{"instance_id": "vllm-project__vllm-b56b6ca", "model_patch": "diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nnew file mode 100644\nindex 000000000..34f8fc0e6\n--- /dev/null\n+++ b/cacheflow/models/attention.py\n@@ -0,0 +1,35 @@\n+from __future__ import annotations\n+\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+from torch.nn import functional as F\n+\n+\n+class OPTCacheFlowAttention(nn.Module):\n+    \"\"\"\n+    Efficient causal self-attention using PyTorch scaled_dot_product_attention.\n+    Avoids explicit attention mask construction and redundant tensor initialization.\n+    \"\"\"\n+\n+    def __init__(self, embed_dim: int, num_heads: int) -> None:\n+        super().__init__()\n+        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.head_dim = embed_dim // num_heads\n+\n+    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n+        # Shapes: (B, T, C)\n+        B, T, _ = q.shape\n+        # Reshape to (B, nH, T, H)\n+        q = q.reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n+        k = k.reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n+        v = v.reshape(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        # Use SDPA with causal flag to leverage optimized kernels (flash/mem-efficient when available)\n+        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True)\n+        # (B, nH, T, H) -> (B, T, C)\n+        out = attn.transpose(1, 2).reshape(B, T, self.embed_dim)\n+        return out\ndiff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py\nindex 3a340317a..5dae3d7c9 100644\n--- a/cacheflow/models/opt.py\n+++ b/cacheflow/models/opt.py\n@@ -3,6 +3,7 @@ import torch\n from torch import nn\n from transformers import OPTConfig\n from transformers import PreTrainedModel\n+from .attention import OPTCacheFlowAttention\n \n \n class OPTLearnedPositionalEmbedding(nn.Embedding):\n@@ -35,13 +36,13 @@ class OPTAttention(nn.Module):\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.attn = OPTCacheFlowAttention(embed_dim, num_heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        q = self.q_proj(hidden_states) * self.scaling\n+        q = self.q_proj(hidden_states)\n         k = self.k_proj(hidden_states)\n         v = self.v_proj(hidden_states)\n-        # TODO\n-        attn_output = None\n+        attn_output = self.attn(q, k, v)\n         output = self.out_proj(attn_output)\n         return output\n \n@@ -54,17 +55,17 @@ class OPTDecoderLayer(nn.Module):\n         self.self_attn = OPTAttention(\n             embed_dim=self.embed_dim,\n             num_heads=config.num_attention_heads,\n-            bias=config.enable_bias,\n+            bias=getattr(config, \"enable_bias\", True),\n         )\n         self.do_layer_norm_before = config.do_layer_norm_before\n-        assert config.activation_function == 'relu'\n-        self.activation_fn = nn.ReLU()\n+        assert getattr(config, 'activation_function', 'relu') == 'relu'\n+        self.activation_fn = nn.ReLU(inplace=True)\n \n         self.self_attn_layer_norm = nn.LayerNorm(\n-            self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n-        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=config.enable_bias)\n-        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n-        self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n+            self.embed_dim, elementwise_affine=getattr(config, 'layer_norm_elementwise_affine', True))\n+        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=getattr(config, 'enable_bias', True))\n+        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=getattr(config, 'enable_bias', True))\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=getattr(config, 'layer_norm_elementwise_affine', True))\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # Self Attention\n@@ -129,9 +130,9 @@ class OPTDecoder(OPTPreTrainedModel):\n         # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n         # with checkpoints that have been fine-tuned before transformers v4.20.1\n         # see https://github.com/facebookresearch/metaseq/pull/164\n-        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n+        if config.do_layer_norm_before and not getattr(config, '_remove_final_layer_norm', False):\n             self.final_layer_norm = nn.LayerNorm(\n-                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine\n+                config.hidden_size, elementwise_affine=getattr(config, 'layer_norm_elementwise_affine', True)\n             )\n         else:\n             self.final_layer_norm = None\n@@ -148,7 +149,6 @@ class OPTDecoder(OPTPreTrainedModel):\n     ) -> torch.Tensor:\n         inputs_embeds = self.embed_tokens(input_ids)\n         pos_embeds = self.embed_positions(positions)\n-        pos_embeds = None\n         if self.project_in is not None:\n             inputs_embeds = self.project_in(inputs_embeds)\n         hidden_states = inputs_embeds + pos_embeds\n@@ -198,5 +198,5 @@ class OPTForCausalLM(OPTPreTrainedModel):\n         positions: torch.LongTensor,\n     ) -> torch.Tensor:\n         hidden_states = self.model.decoder(input_ids, positions)\n-        logits = self.lm_head(hidden_states).contiguous()\n+        logits = self.lm_head(hidden_states)\n         return logits\n", "model_name_or_path": "gpt-5-2025-08-07"}
