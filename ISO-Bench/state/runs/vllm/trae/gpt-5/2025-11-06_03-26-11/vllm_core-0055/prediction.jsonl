{"instance_id": "vllm-project__vllm-51f8aa9", "model_patch": "diff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex 49e63c231..24d00cd3b 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -337,10 +337,19 @@ class BlockTable:\n         This is required for the scheduler to determine whether a sequence can\n         continue generation, or if it must be preempted.\n         \"\"\"\n-\n-        all_token_ids = token_ids + [-1] * num_lookahead_slots\n-        token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n-        return len(token_blocks)\n+        # Number of slots to account for (new tokens + lookahead slots)\n+        total_new_slots = len(token_ids) + num_lookahead_slots\n+\n+        # The first touched block is always the current (possibly partially\n+        # filled) block. Even when total_new_slots is 0, we still count touching\n+        # the current block for consistency with previous behavior.\n+        first_chunk_capacity = self._block_size - (self._num_full_slots %\n+                                                   self._block_size)\n+        remaining = total_new_slots - first_chunk_capacity\n+        if remaining <= 0:\n+            return 1\n+        # Additional blocks needed beyond the first touched block.\n+        return 1 + cdiv(remaining, self._block_size)\n \n     def _chunk_token_blocks_for_append(\n             self, token_ids: List[int]) -> List[List[int]]:\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex f272e23ee..2987dda4e 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -501,7 +501,24 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        \"\"\"Mark blocks as computed, used in prefix caching.\n+\n+        This operation is designed to be lightweight and incremental. It only\n+        updates internal tracking state for active blocks; blocks that have\n+        already been moved to the evictor are considered computed implicitly.\n+        \"\"\"\n+        if not block_ids:\n+            return\n+        for block_id in block_ids:\n+            if self._block_tracker[block_id].active:\n+                self._block_tracker[block_id].computed = True\n+            elif block_id in self.evictor:\n+                # Already tracked by evictor as a computed, cached block\n+                continue\n+            else:\n+                # Invalid block_id for this allocator\n+                raise ValueError(\n+                    \"Mark block as computed which is not belonged to GPU\")\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 87508a116..6be55de19 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -7,6 +7,10 @@ from vllm.logger import init_logger\n from vllm.utils import is_hip\n \n logger = init_logger(__name__)\n+# Cache for imported modules and model classes to avoid repeated imports\n+_IMPORTED_MODULES: Dict[str, object] = {}\n+_MODEL_CLASS_CACHE: Dict[str, Optional[Type[nn.Module]]] = {}\n+\n \n # Architecture -> (module, class).\n _GENERATION_MODELS = {\n@@ -114,10 +118,20 @@ class ModelRegistry:\n                     \"Model architecture %s is partially supported by ROCm: %s\",\n                     model_arch, _ROCM_PARTIALLY_SUPPORTED_MODELS[model_arch])\n \n+        # Fast path: return from cache if available\n+        if model_arch in _MODEL_CLASS_CACHE:\n+            return _MODEL_CLASS_CACHE[model_arch]\n+\n         module_name, model_cls_name = _MODELS[model_arch]\n-        module = importlib.import_module(\n-            f\"vllm.model_executor.models.{module_name}\")\n-        return getattr(module, model_cls_name, None)\n+        # Import module with caching\n+        module = _IMPORTED_MODULES.get(module_name)\n+        if module is None:\n+            module = importlib.import_module(\n+                f\"vllm.model_executor.models.{module_name}\")\n+            _IMPORTED_MODULES[module_name] = module\n+        model_cls = getattr(module, model_cls_name, None)\n+        _MODEL_CLASS_CACHE[model_arch] = model_cls\n+        return model_cls\n \n     @staticmethod\n     def get_supported_archs() -> List[str]:\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex f3025a68d..4ecc861ae 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -520,7 +520,7 @@ def create_kv_caches_with_random(\n     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n \n     scale = head_size**-0.5\n-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n+    x = 16 // get_dtype_size(torch_dtype)\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n     key_caches: List[torch.Tensor] = []\n     for _ in range(num_layers):\n", "model_name_or_path": "gpt-5-2025-08-07"}
