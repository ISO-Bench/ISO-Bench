{"instance_id": "vllm-project__vllm-802329d", "model_patch": "diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9dcf..51efbfe20 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -37,7 +37,7 @@ from vllm.model_executor.layers.rotary_embedding import get_rope\n from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n \n from .llama import LlamaForCausalLM, LlamaMLP, LlamaModel\n-from .utils import (AutoWeightsLoader, extract_layer_index,\n+from .utils import (AutoWeightsLoader, extract_layer_index, fast_topk,\n                     is_pp_missing_parameter)\n \n \n@@ -50,7 +50,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores, router_indices = torch.topk(gating_output, topk, dim=-1)\n+        router_scores, router_indices = fast_topk(gating_output, topk, dim=-1)\n         router_scores = torch.sigmoid(router_scores.float()).to(\n             hidden_states.dtype)\n         return (router_scores, router_indices.to(torch.int32))\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434f3..8893f00db 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -654,7 +654,7 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):\n     ) -> IntermediateTensors:\n         return IntermediateTensors({\n             key:\n-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)\n+            torch.empty((batch_size, hidden_size), dtype=dtype, device=device)\n             for key in keys\n         })\n \n@@ -695,6 +695,18 @@ def extract_layer_index(layer_name: str) -> int:\n     return int_vals[0]\n \n \n+\n+def fast_topk(x: torch.Tensor, k: int, dim: int = -1) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Optimized top-k for the common k==1 case used by some MoE routers.\n+    Falls back to torch.topk for k>1.\n+    \"\"\"\n+    if k == 1:\n+        values, indices = torch.max(x, dim=dim, keepdim=True)\n+        return values, indices\n+    return torch.topk(x, k, dim=dim)\n+\n+\n def cast_overflow_tensors(\n     tensors: torch.Tensor,\n     offset: float = 1000,\n", "model_name_or_path": "gpt-5-2025-08-07"}
