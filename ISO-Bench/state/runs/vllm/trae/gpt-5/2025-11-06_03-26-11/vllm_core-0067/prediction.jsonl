{"instance_id": "vllm-project__vllm-f67e9e9", "model_patch": "diff --git a/tests/core/block/e2e/test_correctness.py b/tests/core/block/e2e/test_correctness.py\nindex e9b537ed5..22f71e89b 100644\n--- a/tests/core/block/e2e/test_correctness.py\n+++ b/tests/core/block/e2e/test_correctness.py\n@@ -109,7 +109,7 @@ def test_block_manager_with_preemption(baseline_llm_generator,\n             \"num_gpu_blocks_override\": 2 * (8 + 1),\n         },\n         {\n-            \"block_size\": 8,\n+            \"block_size\": 16,\n \n             # Allow only 2 sequences of ~128 tokens in worst case.\n             # Note 16 = 128/block_size\n@@ -195,15 +195,15 @@ def test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n     ])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\",\n                          [{\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 2,\n                              \"max_num_seqs\": 2,\n                          }, {\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 3,\n                              \"max_num_seqs\": 2,\n                          }, {\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 256,\n                              \"max_num_seqs\": 10,\n                          }])\ndiff --git a/vllm/attention/ops/prefix_prefill.py b/vllm/attention/ops/prefix_prefill.py\nindex e0478c2ae..699006a2f 100644\n--- a/vllm/attention/ops/prefix_prefill.py\n+++ b/vllm/attention/ops/prefix_prefill.py\n@@ -145,8 +145,7 @@ if triton.__version__ >= \"2.1.0\":\n             else:\n                 k = k_load\n \n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)  # [M,N]\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+            qk = tl.dot(q, k, input_precision=IN_PRECISION)\n             qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n                           float(\"-inf\"))\n             qk *= sm_scale\n@@ -219,8 +218,7 @@ if triton.__version__ >= \"2.1.0\":\n                         ((start_n + offs_n[None, :]) < cur_batch_query_len),\n                         other=0.0)\n \n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+            qk = tl.dot(q, k, input_precision=IN_PRECISION)\n             qk *= sm_scale\n             # apply causal mask\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n@@ -363,8 +361,7 @@ if triton.__version__ >= \"2.1.0\":\n             k = tl.load(K_cache + off_k,\n                         mask=(start_n + offs_n[None, :]) < cur_batch_ctx_len,\n                         other=0.0)\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk += tl.dot(q, k)\n+            qk = tl.dot(q, k)\n             qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n                           float(\"-inf\"))\n             qk *= sm_scale\n@@ -414,8 +411,7 @@ if triton.__version__ >= \"2.1.0\":\n                         < cur_batch_seq_len - cur_batch_ctx_len,\n                         other=0.0)\n \n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk += tl.dot(q, k)\n+            qk = tl.dot(q, k)\n             qk *= sm_scale\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n                           float(\"-inf\"))\n@@ -582,8 +578,7 @@ if triton.__version__ >= \"2.1.0\":\n             else:\n                 k = k_load\n \n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+            qk = tl.dot(q, k, input_precision=IN_PRECISION)\n             qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n                           float(\"-inf\"))\n             qk *= sm_scale\n@@ -657,8 +652,7 @@ if triton.__version__ >= \"2.1.0\":\n                          < cur_batch_seq_len - cur_batch_ctx_len),\n                         other=0.0)\n \n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision='ieee')\n+            qk = tl.dot(q, k, input_precision='ieee')\n             qk *= sm_scale\n             qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n                           float(\"-inf\"))\n", "model_name_or_path": "gpt-5-2025-08-07"}
