{"instance_id": "vllm-project__vllm-6368e77", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..3a43a77b5 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -81,20 +81,21 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n+    cts = tokenizer.convert_tokens_to_string\n+    added_vocab = tokenizer.get_added_vocab()\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else None\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if skip_special_tokens and token in all_special_tokens:  # type: ignore[arg-type]\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in added_vocab:\n             if current_sub_text:\n-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-                sub_texts.append(sub_text)\n+                sub_texts.append(cts(current_sub_text))\n                 current_sub_text = []\n             sub_texts.append(token)\n         else:\n             current_sub_text.append(token)\n     if current_sub_text:\n-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-        sub_texts.append(sub_text)\n+        sub_texts.append(cts(current_sub_text))\n     return \" \".join(sub_texts)\n \n \n@@ -129,16 +130,23 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n-        prefix_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:read_offset])\n+    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n+        prefix_slice = output_tokens[prefix_offset:read_offset]\n+        if prefix_slice:\n+            prefix_text = tokenizer.convert_tokens_to_string(prefix_slice)\n+        else:\n+            prefix_text = \"\"\n         new_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:])\n     else:\n-        prefix_text = _convert_tokens_to_string_with_added_encoders(\n-            tokenizer,\n-            output_tokens[prefix_offset:read_offset],\n-            skip_special_tokens=skip_special_tokens)\n+        prefix_slice = output_tokens[prefix_offset:read_offset]\n+        if prefix_slice:\n+            prefix_text = _convert_tokens_to_string_with_added_encoders(\n+                tokenizer,\n+                prefix_slice,\n+                skip_special_tokens=skip_special_tokens)\n+        else:\n+            prefix_text = \"\"\n         new_text = _convert_tokens_to_string_with_added_encoders(\n             tokenizer,\n             output_tokens[prefix_offset:],\n", "model_name_or_path": "gpt-5-2025-08-07"}
