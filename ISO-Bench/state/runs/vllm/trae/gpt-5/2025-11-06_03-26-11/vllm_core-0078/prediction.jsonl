{"instance_id": "vllm-project__vllm-cc466a3", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 1f19d2053..da0e29c57 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -320,7 +320,7 @@ def _random_sample(\n         seq_group has do_sample=False, tuple contains ([], [])\n     \"\"\"\n     # Find the maximum best_of value of the prompt phase requests.\n-    random_samples = random_samples.cpu()\n+    random_samples = random_samples.to('cpu')\n     sample_idx = 0\n     results: SampleResultType = []\n     for seq_group in selected_seq_groups:\n@@ -721,7 +721,7 @@ def _get_logprobs(\n     next_token_ids: List[int] = []\n     # The largest requested number of logprobs. We find logprobs as many as the\n     # largest num logprobs in this API.\n-    largest_num_logprobs = 1\n+    largest_num_logprobs = 0\n \n     # Select indices to compute logprob from, ranks of token ids, and the top\n     # k token ids from logprobs.\n@@ -730,8 +730,7 @@ def _get_logprobs(\n         sampling_params = seq_group.sampling_params\n \n         # Update indices and tokens for prompt logprobs.\n-        if (seq_group.is_prompt\n-                and sampling_params.prompt_logprobs is not None):\n+        if seq_group.is_prompt and sampling_params.prompt_logprobs is not None:\n             largest_num_logprobs = max(largest_num_logprobs,\n                                        sampling_params.prompt_logprobs)\n             next_prompt_tokens = _get_next_prompt_tokens(seq_group)\n@@ -763,15 +762,12 @@ def _get_logprobs(\n \n     query_indices_gpu = torch.tensor(query_indices, device=logprobs.device)\n     next_token_ids_gpu = torch.tensor(next_token_ids, device=logprobs.device)\n+    logprobs_selected = logprobs.index_select(0, query_indices_gpu)\n \n-    # (num_selected_query_tokens, num_logprobs). Note that query_indices can\n-    # contain duplicates if beam search is enabled.\n-    selected_logprobs = logprobs[[\n-        query_indices_gpu,\n-        next_token_ids_gpu,\n-    ]]\n+    # (num_selected_query_tokens,)\n+    selected_logprobs = logprobs_selected.gather(1, next_token_ids_gpu.unsqueeze(1)).squeeze(1)\n     ranks = _get_ranks(\n-        logprobs[query_indices_gpu],\n+        logprobs_selected,\n         next_token_ids_gpu,\n     )\n     assert selected_logprobs.shape[0] == ranks.shape[0]\n@@ -779,16 +775,17 @@ def _get_logprobs(\n     # Logprobs of topk tokens for a batch of sequence groups.\n     # (num_query_tokens_across_batch).\n     if largest_num_logprobs > 0:\n-        top_logprobs, top_token_ids = torch.topk(logprobs,\n+        top_logprobs, top_token_ids = torch.topk(logprobs_selected,\n                                                  largest_num_logprobs,\n                                                  dim=-1)\n-        top_logprobs = top_logprobs.cpu()\n-        top_token_ids = top_token_ids.cpu()\n     else:\n         top_logprobs, top_token_ids = None, None\n \n-    selected_logprobs = selected_logprobs.cpu()\n-    ranks = ranks.cpu()\n+    selected_logprobs = selected_logprobs.to('cpu')\n+    ranks = ranks.to('cpu')\n+    if top_logprobs is not None and top_token_ids is not None:\n+        top_logprobs = top_logprobs.to('cpu')\n+        top_token_ids = top_token_ids.to('cpu')\n \n     # Find prompt/sample logprobs.\n     prompt_logprobs_per_seq_group: List[Optional[PromptLogprobs]] = []\n@@ -828,7 +825,7 @@ def _get_prompt_logprob_if_needed(\n \n     # Find prompt logprobs\n     prompt_logprobs: Optional[PromptLogprobs] = None\n-    if (is_prompt and sampling_params.prompt_logprobs is not None):\n+    if is_prompt and sampling_params.prompt_logprobs is not None:\n         prompt_logprobs = []\n         num_logprobs = sampling_params.prompt_logprobs\n         next_prompt_tokens = _get_next_prompt_tokens(seq_group)\n", "model_name_or_path": "gpt-5-2025-08-07"}
