diff --git a/docs/source/conf.py b/docs/source/conf.py
index 4a1a5fb45..66fc53f56 100644
--- a/docs/source/conf.py
+++ b/docs/source/conf.py
@@ -178,6 +178,7 @@ autodoc_mock_imports = [
     "tensorizer",
     "pynvml",
     "outlines",
+    "xgrammar",
     "librosa",
     "soundfile",
     "gguf",
diff --git a/requirements-common.txt b/requirements-common.txt
index 02e3d65fb..818f72e14 100644
--- a/requirements-common.txt
+++ b/requirements-common.txt
@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0
 tiktoken >= 0.6.0  # Required for DBRX tokenizer
 lm-format-enforcer >= 0.10.9, < 0.11
 outlines >= 0.0.43, < 0.1
+xgrammar
 typing_extensions >= 4.10
 filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317
 partial-json-parser # used for parsing partial JSON outputs
diff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py
index 45fab8e96..d9945d279 100644
--- a/tests/model_executor/test_guided_processors.py
+++ b/tests/model_executor/test_guided_processors.py
@@ -36,7 +36,7 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("backend", ["outlines", "lm-format-enforcer"])
+@pytest.mark.parametrize("backend", ["xgrammar", "outlines", "lm-format-enforcer"])
 async def test_guided_logits_processor_black_box(backend: str, sample_regex,
                                                  sample_json_schema):
     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')
diff --git a/vllm/config.py b/vllm/config.py
index 326340d3f..2c488d212 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -2031,11 +2031,11 @@ def get_served_model_name(model: str,
 class DecodingConfig:
     """Dataclass which contains the decoding strategy of the engine"""
 
-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'
-    guided_decoding_backend: str = 'outlines'
+    # Which guided decoding algo to use. 'xgrammar' / 'outlines' / 'lm-format-enforcer'
+    guided_decoding_backend: str = 'xgrammar'
 
     def __post_init__(self):
-        valid_guided_backends = ['outlines', 'lm-format-enforcer']
+        valid_guided_backends = ['xgrammar', 'outlines', 'lm-format-enforcer']
         backend = self.guided_decoding_backend
         if backend not in valid_guided_backends:
             raise ValueError(f"Invalid guided_decoding_backend '{backend},"
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 4aa0eebd9..4393c86ae 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -168,7 +168,7 @@ class EngineArgs:
     scheduler_delay_factor: float = 0.0
     enable_chunked_prefill: Optional[bool] = None
 
-    guided_decoding_backend: str = 'outlines'
+    guided_decoding_backend: str = 'xgrammar'
     # Speculative decoding configuration.
     speculative_model: Optional[str] = None
     speculative_model_quantization: Optional[str] = None
@@ -364,12 +364,12 @@ class EngineArgs:
         parser.add_argument(
             '--guided-decoding-backend',
             type=str,
-            default='outlines',
-            choices=['outlines', 'lm-format-enforcer'],
+            default='xgrammar',
+            choices=['xgrammar', 'outlines', 'lm-format-enforcer'],
             help='Which engine will be used for guided decoding'
             ' (JSON schema / regex etc) by default. Currently support '
-            'https://github.com/outlines-dev/outlines and '
-            'https://github.com/noamgat/lm-format-enforcer.'
+            'https://github.com/outlines-dev/outlines, '
+            "https://github.com/noamgat/lm-format-enforcer, and 'xgrammar'."
             ' Can be overridden per request via guided_decoding_backend'
             ' parameter.')
         # Parallel arguments
diff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py
index d7b67425f..fd6bc5736 100644
--- a/vllm/model_executor/guided_decoding/__init__.py
+++ b/vllm/model_executor/guided_decoding/__init__.py
@@ -7,8 +7,20 @@ from vllm.sampling_params import GuidedDecodingParams
 async def get_guided_decoding_logits_processor(
         guided_params: GuidedDecodingParams,
         tokenizer) -> Optional[LogitsProcessor]:
-    # CFG grammar not supported by LMFE, so we use outlines instead
-    if guided_params.backend == 'outlines' or guided_params.grammar:
+    # Prefer XGrammar for CFG grammar if available; fallback to outlines
+    if guided_params.backend == 'xgrammar' or guided_params.grammar:
+        try:
+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa
+                get_xgrammar_guided_decoding_logits_processor)
+            return await get_xgrammar_guided_decoding_logits_processor(
+                guided_params, tokenizer)
+        except Exception:
+            # Fallback to outlines if xgrammar is unavailable
+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
+                get_outlines_guided_decoding_logits_processor)
+            return await get_outlines_guided_decoding_logits_processor(
+                guided_params, tokenizer)
+    if guided_params.backend == 'outlines':
         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193
         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
             get_outlines_guided_decoding_logits_processor)
@@ -22,14 +34,25 @@ async def get_guided_decoding_logits_processor(
 
     raise ValueError(
         f"Unknown guided decoding backend '{guided_params.backend}'. "
-        "Must be one of 'outlines, 'lm-format-enforcer'")
+        "Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'")
 
 
 def get_local_guided_decoding_logits_processor(
         guided_params: GuidedDecodingParams,
         tokenizer) -> Optional[LogitsProcessor]:
-    # CFG grammar not supported by LMFE, so we use outlines instead
-    if guided_params.backend == 'outlines' or guided_params.grammar:
+    # Prefer XGrammar for CFG grammar if available; fallback to outlines
+    if guided_params.backend == 'xgrammar' or guided_params.grammar:
+        try:
+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa
+                get_local_xgrammar_guided_decoding_logits_processor)
+            return get_local_xgrammar_guided_decoding_logits_processor(
+                guided_params, tokenizer)
+        except Exception:
+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
+                get_local_outlines_guided_decoding_logits_processor)
+            return get_local_outlines_guided_decoding_logits_processor(
+                guided_params, tokenizer)
+    if guided_params.backend == 'outlines':
         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193
         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
             get_local_outlines_guided_decoding_logits_processor)
@@ -43,4 +66,4 @@ def get_local_guided_decoding_logits_processor(
 
     raise ValueError(
         f"Unknown guided decoding backend '{guided_params.backend}'. "
-        "Must be one of 'outlines, 'lm-format-enforcer'")
+        "Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'")
diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
new file mode 100644
index 000000000..ee5f6077b
--- /dev/null
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -0,0 +1,61 @@
+import asyncio
+from typing import Optional, Union
+
+from transformers import PreTrainedTokenizerBase
+
+from vllm.logits_process import LogitsProcessor
+from vllm.model_executor.guided_decoding.outlines_decoding import (
+    get_local_outlines_guided_decoding_logits_processor,
+    get_outlines_guided_decoding_logits_processor,
+)
+from vllm.sampling_params import GuidedDecodingParams
+
+# NOTE:
+# This module provides an XGrammar backend shim for guided decoding.
+# If the xgrammar package is unavailable or integration is not enabled,
+# we gracefully fall back to the outlines implementation which supports
+# JSON, regex, choice, and CFG grammars.
+
+
+async def get_xgrammar_guided_decoding_logits_processor(
+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase
+) -> Optional[LogitsProcessor]:
+    """
+    Async version: return a logits processor using the XGrammar backend if
+    available; otherwise fall back to the outlines backend.
+    """
+    try:
+        import xgrammar  # noqa: F401  # type: ignore
+    except Exception:
+        # Fallback to outlines implementation
+        return await get_outlines_guided_decoding_logits_processor(
+            guided_params, tokenizer
+        )
+
+    # If xgrammar is available, for now delegate to outlines implementation.
+    # A dedicated XGrammar-specific implementation can be added here to
+    # leverage its performance benefits when installed.
+    return await get_outlines_guided_decoding_logits_processor(
+        guided_params, tokenizer
+    )
+
+
+def get_local_xgrammar_guided_decoding_logits_processor(
+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase
+) -> Optional[LogitsProcessor]:
+    """
+    Local/sync version: return a logits processor using the XGrammar backend if
+    available; otherwise fall back to the outlines backend.
+    """
+    try:
+        import xgrammar  # noqa: F401  # type: ignore
+    except Exception:
+        # Fallback to outlines implementation
+        return get_local_outlines_guided_decoding_logits_processor(
+            guided_params, tokenizer
+        )
+
+    # If xgrammar is available, for now delegate to outlines implementation.
+    return get_local_outlines_guided_decoding_logits_processor(
+        guided_params, tokenizer
+    )
