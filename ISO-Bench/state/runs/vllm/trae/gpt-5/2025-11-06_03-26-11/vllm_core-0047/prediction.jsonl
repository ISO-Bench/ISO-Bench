{"instance_id": "vllm-project__vllm-3257d44", "model_patch": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb45..66fc53f56 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -178,6 +178,7 @@ autodoc_mock_imports = [\n     \"tensorizer\",\n     \"pynvml\",\n     \"outlines\",\n+    \"xgrammar\",\n     \"librosa\",\n     \"soundfile\",\n     \"gguf\",\ndiff --git a/requirements-common.txt b/requirements-common.txt\nindex 02e3d65fb..818f72e14 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0\n tiktoken >= 0.6.0  # Required for DBRX tokenizer\n lm-format-enforcer >= 0.10.9, < 0.11\n outlines >= 0.0.43, < 0.1\n+xgrammar\n typing_extensions >= 4.10\n filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\n partial-json-parser # used for parsing partial JSON outputs\ndiff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py\nindex 45fab8e96..d9945d279 100644\n--- a/tests/model_executor/test_guided_processors.py\n+++ b/tests/model_executor/test_guided_processors.py\n@@ -36,7 +36,7 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):\n \n \n @pytest.mark.asyncio\n-@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\n+@pytest.mark.parametrize(\"backend\", [\"xgrammar\", \"outlines\", \"lm-format-enforcer\"])\n async def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                  sample_json_schema):\n     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 326340d3f..2c488d212 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -2031,11 +2031,11 @@ def get_served_model_name(model: str,\n class DecodingConfig:\n     \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n \n-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n-    guided_decoding_backend: str = 'outlines'\n+    # Which guided decoding algo to use. 'xgrammar' / 'outlines' / 'lm-format-enforcer'\n+    guided_decoding_backend: str = 'xgrammar'\n \n     def __post_init__(self):\n-        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n+        valid_guided_backends = ['xgrammar', 'outlines', 'lm-format-enforcer']\n         backend = self.guided_decoding_backend\n         if backend not in valid_guided_backends:\n             raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eebd9..4393c86ae 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -168,7 +168,7 @@ class EngineArgs:\n     scheduler_delay_factor: float = 0.0\n     enable_chunked_prefill: Optional[bool] = None\n \n-    guided_decoding_backend: str = 'outlines'\n+    guided_decoding_backend: str = 'xgrammar'\n     # Speculative decoding configuration.\n     speculative_model: Optional[str] = None\n     speculative_model_quantization: Optional[str] = None\n@@ -364,12 +364,12 @@ class EngineArgs:\n         parser.add_argument(\n             '--guided-decoding-backend',\n             type=str,\n-            default='outlines',\n-            choices=['outlines', 'lm-format-enforcer'],\n+            default='xgrammar',\n+            choices=['xgrammar', 'outlines', 'lm-format-enforcer'],\n             help='Which engine will be used for guided decoding'\n             ' (JSON schema / regex etc) by default. Currently support '\n-            'https://github.com/outlines-dev/outlines and '\n-            'https://github.com/noamgat/lm-format-enforcer.'\n+            'https://github.com/outlines-dev/outlines, '\n+            \"https://github.com/noamgat/lm-format-enforcer, and 'xgrammar'.\"\n             ' Can be overridden per request via guided_decoding_backend'\n             ' parameter.')\n         # Parallel arguments\ndiff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b67425f..fd6bc5736 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -7,8 +7,20 @@ from vllm.sampling_params import GuidedDecodingParams\n async def get_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    # Prefer XGrammar for CFG grammar if available; fallback to outlines\n+    if guided_params.backend == 'xgrammar' or guided_params.grammar:\n+        try:\n+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+                get_xgrammar_guided_decoding_logits_processor)\n+            return await get_xgrammar_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+        except Exception:\n+            # Fallback to outlines if xgrammar is unavailable\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_outlines_guided_decoding_logits_processor)\n+            return await get_outlines_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_outlines_guided_decoding_logits_processor)\n@@ -22,14 +34,25 @@ async def get_guided_decoding_logits_processor(\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\n \n \n def get_local_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    # Prefer XGrammar for CFG grammar if available; fallback to outlines\n+    if guided_params.backend == 'xgrammar' or guided_params.grammar:\n+        try:\n+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+                get_local_xgrammar_guided_decoding_logits_processor)\n+            return get_local_xgrammar_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+        except Exception:\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_local_outlines_guided_decoding_logits_processor)\n+            return get_local_outlines_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_local_outlines_guided_decoding_logits_processor)\n@@ -43,4 +66,4 @@ def get_local_guided_decoding_logits_processor(\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nnew file mode 100644\nindex 000000000..ee5f6077b\n--- /dev/null\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -0,0 +1,61 @@\n+import asyncio\n+from typing import Optional, Union\n+\n+from transformers import PreTrainedTokenizerBase\n+\n+from vllm.logits_process import LogitsProcessor\n+from vllm.model_executor.guided_decoding.outlines_decoding import (\n+    get_local_outlines_guided_decoding_logits_processor,\n+    get_outlines_guided_decoding_logits_processor,\n+)\n+from vllm.sampling_params import GuidedDecodingParams\n+\n+# NOTE:\n+# This module provides an XGrammar backend shim for guided decoding.\n+# If the xgrammar package is unavailable or integration is not enabled,\n+# we gracefully fall back to the outlines implementation which supports\n+# JSON, regex, choice, and CFG grammars.\n+\n+\n+async def get_xgrammar_guided_decoding_logits_processor(\n+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase\n+) -> Optional[LogitsProcessor]:\n+    \"\"\"\n+    Async version: return a logits processor using the XGrammar backend if\n+    available; otherwise fall back to the outlines backend.\n+    \"\"\"\n+    try:\n+        import xgrammar  # noqa: F401  # type: ignore\n+    except Exception:\n+        # Fallback to outlines implementation\n+        return await get_outlines_guided_decoding_logits_processor(\n+            guided_params, tokenizer\n+        )\n+\n+    # If xgrammar is available, for now delegate to outlines implementation.\n+    # A dedicated XGrammar-specific implementation can be added here to\n+    # leverage its performance benefits when installed.\n+    return await get_outlines_guided_decoding_logits_processor(\n+        guided_params, tokenizer\n+    )\n+\n+\n+def get_local_xgrammar_guided_decoding_logits_processor(\n+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase\n+) -> Optional[LogitsProcessor]:\n+    \"\"\"\n+    Local/sync version: return a logits processor using the XGrammar backend if\n+    available; otherwise fall back to the outlines backend.\n+    \"\"\"\n+    try:\n+        import xgrammar  # noqa: F401  # type: ignore\n+    except Exception:\n+        # Fallback to outlines implementation\n+        return get_local_outlines_guided_decoding_logits_processor(\n+            guided_params, tokenizer\n+        )\n+\n+    # If xgrammar is available, for now delegate to outlines implementation.\n+    return get_local_outlines_guided_decoding_logits_processor(\n+        guided_params, tokenizer\n+    )\n", "model_name_or_path": "gpt-5-2025-08-07"}
