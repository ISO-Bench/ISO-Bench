{"instance_id": "vllm-project__vllm-a869bac", "model_patch": "diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 5d8b3f423..eaaa48cdd 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -89,18 +89,18 @@ class TopKTopPSampler(nn.Module):\n         p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n         \"\"\"More optimized implementation for top-k and top-p sampling.\"\"\"\n-        probs = logits.softmax(dim=-1, dtype=torch.float32)\n         if k is None and p is None:\n             # We prefer `random_sample` over `flashinfer_sample` when sorting is\n             # not needed. This is because `random_sample` does not require\n             # CPU-GPU synchronization while `flashinfer_sample` does.\n+            probs = logits.softmax(dim=-1, dtype=torch.float32)\n             return random_sample(probs, generators)\n         if generators:\n             logger.warning(\"FlashInfer 0.2.3+ does not support \"\n                            \"per-request generators. Falling back to \"\n                            \"PyTorch-native implementation.\")\n             return self.forward_native(logits, generators, k, p)\n-        return flashinfer_sample(probs, k, p, generators)\n+        return flashinfer_sample(logits, k, p, generators)\n \n     def forward_tpu(\n         self,\n@@ -200,7 +200,7 @@ def apply_top_k_top_p(\n         logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    logits.scatter_(dim=-1, index=logits_idx, src=logits_sort)\n     return logits\n \n \n@@ -254,12 +254,12 @@ def random_sample(\n \n \n def flashinfer_sample(\n-    probs: torch.Tensor,\n+    logits: torch.Tensor,\n     k: Optional[torch.Tensor],\n     p: Optional[torch.Tensor],\n     generators: dict[int, torch.Generator],\n ) -> torch.Tensor:\n-    \"\"\"Sample from the probabilities using FlashInfer.\n+    \"\"\"Sample from the logits using FlashInfer.\n \n     Statistically, this function is equivalent to the `random_sample` function.\n     However, this function is faster because it avoids sorting the logits tensor\n@@ -275,17 +275,45 @@ def flashinfer_sample(\n     \"\"\"\n     assert not (k is None and p is None)\n \n+    # Prefer logits-based FlashInfer APIs when available to avoid redundant\n+    # softmax on the host side. Fallback to *_from_probs if necessary.\n+    fi = flashinfer.sampling\n+\n+    def _to_probs(x: torch.Tensor) -> torch.Tensor:\n+        return x.softmax(dim=-1, dtype=torch.float32)\n+\n     if k is None:\n         # Top-p only.\n-        next_token_ids = flashinfer.sampling.top_p_sampling_from_probs(\n-            probs, p, deterministic=True)\n+        if hasattr(fi, 'top_p_sampling_from_logits'):\n+            next_token_ids = fi.top_p_sampling_from_logits(\n+                logits, p, deterministic=True)\n+        elif hasattr(fi, 'top_p_sampling'):\n+            next_token_ids = fi.top_p_sampling(\n+                logits, p, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_p_sampling_from_probs(\n+                _to_probs(logits), p, deterministic=True)\n     elif p is None:\n         # Top-k only.\n-        next_token_ids = flashinfer.sampling.top_k_sampling_from_probs(\n-            probs, k, deterministic=True)\n+        if hasattr(fi, 'top_k_sampling_from_logits'):\n+            next_token_ids = fi.top_k_sampling_from_logits(\n+                logits, k, deterministic=True)\n+        elif hasattr(fi, 'top_k_sampling'):\n+            next_token_ids = fi.top_k_sampling(\n+                logits, k, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_k_sampling_from_probs(\n+                _to_probs(logits), k, deterministic=True)\n     else:\n         # Both top-k and top-p.\n-        next_token_ids = (flashinfer.sampling.top_k_top_p_sampling_from_probs(\n-            probs, k, p, deterministic=True))\n+        if hasattr(fi, 'top_k_top_p_sampling_from_logits'):\n+            next_token_ids = fi.top_k_top_p_sampling_from_logits(\n+                logits, k, p, deterministic=True)\n+        elif hasattr(fi, 'top_k_top_p_sampling'):\n+            next_token_ids = fi.top_k_top_p_sampling(\n+                logits, k, p, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_k_top_p_sampling_from_probs(\n+                _to_probs(logits), k, p, deterministic=True)\n \n     return next_token_ids.view(-1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
