{"instance_id": "vllm-project__vllm-e642ec9", "model_patch": "diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py\nindex 8184b0732..2dca25b78 100644\n--- a/vllm/attention/backends/mla/common.py\n+++ b/vllm/attention/backends/mla/common.py\n@@ -961,10 +961,8 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):\n             chunk_seq_lens = (chunk_ends - context_chunk_starts).clamp(min=0)\n             _context_chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                 torch.int32)\n-            zero = torch.zeros(num_chunks, dtype=torch.int32, device=device)\\\n-                .unsqueeze(-1)\n-            context_chunk_cu_seq_lens = \\\n-                torch.cat([zero, _context_chunk_cu_seq_lens], dim=1)\n+            context_chunk_cu_seq_lens = torch.nn.functional.pad(\n+                _context_chunk_cu_seq_lens, (1, 0), value=0)\n             context_chunk_max_seq_lens = \\\n                 chunk_seq_lens.max(dim=1).values.tolist()\n             context_chunk_seq_tot = chunk_seq_lens.sum(dim=1).tolist()\n@@ -1307,8 +1305,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n                 seq_starts=prefill_metadata.context_chunk_starts[i],\n             )\n \n-            kv_c_normed = workspace[:toks]\\\n-                [..., :self.kv_lora_rank].unsqueeze(1)\n+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]\n             k_pe = workspace[:toks]\\\n                 [..., self.kv_lora_rank:].unsqueeze(1)\n \ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex c98262eea..5c0e9f5cb 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,14 +532,11 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=torch.nn.functional.pad(\n+                        _chunk_cu_seq_lens, (1, 0), value=0),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -873,8 +870,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 seq_starts=prefill_metadata.chunked_context.starts[i],\n             )\n \n-            kv_c_normed = workspace[:toks]\\\n-                [..., :self.kv_lora_rank].unsqueeze(1)\n+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]\n             k_pe = workspace[:toks]\\\n                 [..., self.kv_lora_rank:].unsqueeze(1)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
