diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py
index 8184b0732..2dca25b78 100644
--- a/vllm/attention/backends/mla/common.py
+++ b/vllm/attention/backends/mla/common.py
@@ -961,10 +961,8 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):
             chunk_seq_lens = (chunk_ends - context_chunk_starts).clamp(min=0)
             _context_chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(
                 torch.int32)
-            zero = torch.zeros(num_chunks, dtype=torch.int32, device=device)\
-                .unsqueeze(-1)
-            context_chunk_cu_seq_lens = \
-                torch.cat([zero, _context_chunk_cu_seq_lens], dim=1)
+            context_chunk_cu_seq_lens = torch.nn.functional.pad(
+                _context_chunk_cu_seq_lens, (1, 0), value=0)
             context_chunk_max_seq_lens = \
                 chunk_seq_lens.max(dim=1).values.tolist()
             context_chunk_seq_tot = chunk_seq_lens.sum(dim=1).tolist()
@@ -1307,8 +1305,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):
                 seq_starts=prefill_metadata.context_chunk_starts[i],
             )
 
-            kv_c_normed = workspace[:toks]\
-                [..., :self.kv_lora_rank].unsqueeze(1)
+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]
             k_pe = workspace[:toks]\
                 [..., self.kv_lora_rank:].unsqueeze(1)
 
diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
index c98262eea..5c0e9f5cb 100644
--- a/vllm/v1/attention/backends/mla/common.py
+++ b/vllm/v1/attention/backends/mla/common.py
@@ -532,14 +532,11 @@ class MLACommonMetadataBuilder(Generic[M]):
                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)
                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(
                     torch.int32)
-                zero = torch.zeros(num_chunks,
-                                   dtype=torch.int32,
-                                   device=device).unsqueeze(-1)
 
                 chunked_context_metadata = \
                     MLACommonPrefillMetadata.ChunkedContextMetadata(
-                    cu_seq_lens=torch.cat(
-                        [zero, _chunk_cu_seq_lens], dim=1),
+                    cu_seq_lens=torch.nn.functional.pad(
+                        _chunk_cu_seq_lens, (1, 0), value=0),
                     starts=chunk_starts,
                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),
                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),
@@ -873,8 +870,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):
                 seq_starts=prefill_metadata.chunked_context.starts[i],
             )
 
-            kv_c_normed = workspace[:toks]\
-                [..., :self.kv_lora_rank].unsqueeze(1)
+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]
             k_pe = workspace[:toks]\
                 [..., self.kv_lora_rank:].unsqueeze(1)
 
