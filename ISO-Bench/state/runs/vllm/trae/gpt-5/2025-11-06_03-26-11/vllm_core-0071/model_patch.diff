diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py
new file mode 100644
index 000000000..a0a77bd81
--- /dev/null
+++ b/benchmark/benchmark_latency.py
@@ -0,0 +1,55 @@
+import argparse
+import time
+from typing import List
+
+import torch
+
+from cacheflow.master.simple_frontend import SimpleFrontend
+from cacheflow.master.server import (Server, add_server_arguments,
+                                     initialize_ray_cluster)
+from cacheflow.sampling_params import SamplingParams
+from cacheflow.utils import get_gpu_memory, get_cpu_memory
+
+
+def main(args: argparse.Namespace):
+    assert args.pipeline_parallel_size == 1, (
+        'Pipeline parallelism is not supported yet.')
+
+    (num_nodes, num_devices_per_node, distributed_init_method,
+     all_stage_devices) = initialize_ray_cluster(address='local',
+                                                 num_gpus=args.tensor_parallel_size)
+
+    server = Server(args.model,
+                    tensor_parallel_size=args.tensor_parallel_size,
+                    pipeline_parallel_size=args.pipeline_parallel_size,
+                    seed=args.seed,
+                    dtype=args.dtype,
+                    worker_use_ray=True)
+
+    frontend = SimpleFrontend(server)
+
+    prompts: List[str] = ["Hello world!"] * args.num_prompts
+    params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=args.max_tokens)
+
+    # Warmup
+    frontend.generate(prompts[:2], params)
+
+    t0 = time.time()
+    frontend.generate(prompts, params)
+    t1 = time.time()
+    print({
+        "num_prompts": args.num_prompts,
+        "max_tokens": args.max_tokens,
+        "latency_s": round(t1 - t0, 3),
+    })
+
+
+if __name__ == '__main__':
+    parser = argparse.ArgumentParser()
+    add_server_arguments(parser)
+    parser.add_argument('--num-prompts', type=int, default=8)
+    parser.add_argument('--max-tokens', type=int, default=8)
+    parser.add_argument('--dtype', type=str, default='float16')
+    parser.add_argument('--seed', type=int, default=0)
+    args = parser.parse_args()
+    main(args)
diff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py
index 978ca04e6..63c647002 100644
--- a/cacheflow/parallel_utils/tensor_parallel/layers.py
+++ b/cacheflow/parallel_utils/tensor_parallel/layers.py
@@ -198,8 +198,8 @@ class VocabParallelEmbedding(torch.nn.Module):
             # Build the mask.
             input_mask = (input_ < self.vocab_start_index) | \
                          (input_ >= self.vocab_end_index)
-            # Mask the input.
-            masked_input = input_.clone() - self.vocab_start_index
+            # Mask the input without redundant clone.
+            masked_input = input_ - self.vocab_start_index
             masked_input[input_mask] = 0
         else:
             masked_input = input_
@@ -210,7 +210,7 @@ class VocabParallelEmbedding(torch.nn.Module):
                                       self.sparse)
         # Mask the output embedding.
         if self.tensor_model_parallel_size > 1:
-            output_parallel[input_mask, :] = 0.0
+            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0.0)
         # Reduce across all the model parallel GPUs.
         output = reduce_from_tensor_model_parallel_region(output_parallel)
         return output
@@ -270,7 +270,9 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):
             total_input = all_gather_buffer
         else:
             total_input = input
-        grad_input = grad_output.matmul(weight)
+        # Preallocate grad_input buffer and compute in-place to reduce allocations
+        grad_input = get_global_memory_buffer().get_tensor(list(input.size()), input.dtype, "mpu_grad_input")
+        torch.matmul(grad_output, weight, out=grad_input)
 
         if ctx.sequence_parallel:
             handle.wait()
@@ -279,7 +281,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):
         grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1],
                                        grad_output.shape[2])
         total_input = total_input.view(total_input.shape[0] * total_input.shape[1],
-				       total_input.shape[2])
+                                       total_input.shape[2])
 
         if ctx.async_grad_allreduce:
             # Asynchronous all-reduce
@@ -291,9 +293,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):
         if ctx.sequence_parallel:
             assert not ctx.async_grad_allreduce
             dim_size = list(input.size())
-            sub_grad_input = torch.empty(dim_size, dtype=input.dtype,
-                                         device=torch.cuda.current_device(),
-                                         requires_grad=False)
+            sub_grad_input = get_global_memory_buffer().get_tensor(dim_size, input.dtype, "mpu_sub_grad_input")
             # reduce_scatter
             handle = torch.distributed._reduce_scatter_base(sub_grad_input, grad_input,
                                                             group=get_tensor_model_parallel_group(),
