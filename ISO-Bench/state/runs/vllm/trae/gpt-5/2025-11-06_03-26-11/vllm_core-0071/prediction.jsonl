{"instance_id": "vllm-project__vllm-7a7929a", "model_patch": "diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\nnew file mode 100644\nindex 000000000..a0a77bd81\n--- /dev/null\n+++ b/benchmark/benchmark_latency.py\n@@ -0,0 +1,55 @@\n+import argparse\n+import time\n+from typing import List\n+\n+import torch\n+\n+from cacheflow.master.simple_frontend import SimpleFrontend\n+from cacheflow.master.server import (Server, add_server_arguments,\n+                                     initialize_ray_cluster)\n+from cacheflow.sampling_params import SamplingParams\n+from cacheflow.utils import get_gpu_memory, get_cpu_memory\n+\n+\n+def main(args: argparse.Namespace):\n+    assert args.pipeline_parallel_size == 1, (\n+        'Pipeline parallelism is not supported yet.')\n+\n+    (num_nodes, num_devices_per_node, distributed_init_method,\n+     all_stage_devices) = initialize_ray_cluster(address='local',\n+                                                 num_gpus=args.tensor_parallel_size)\n+\n+    server = Server(args.model,\n+                    tensor_parallel_size=args.tensor_parallel_size,\n+                    pipeline_parallel_size=args.pipeline_parallel_size,\n+                    seed=args.seed,\n+                    dtype=args.dtype,\n+                    worker_use_ray=True)\n+\n+    frontend = SimpleFrontend(server)\n+\n+    prompts: List[str] = [\"Hello world!\"] * args.num_prompts\n+    params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=args.max_tokens)\n+\n+    # Warmup\n+    frontend.generate(prompts[:2], params)\n+\n+    t0 = time.time()\n+    frontend.generate(prompts, params)\n+    t1 = time.time()\n+    print({\n+        \"num_prompts\": args.num_prompts,\n+        \"max_tokens\": args.max_tokens,\n+        \"latency_s\": round(t1 - t0, 3),\n+    })\n+\n+\n+if __name__ == '__main__':\n+    parser = argparse.ArgumentParser()\n+    add_server_arguments(parser)\n+    parser.add_argument('--num-prompts', type=int, default=8)\n+    parser.add_argument('--max-tokens', type=int, default=8)\n+    parser.add_argument('--dtype', type=str, default='float16')\n+    parser.add_argument('--seed', type=int, default=0)\n+    args = parser.parse_args()\n+    main(args)\ndiff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py\nindex 978ca04e6..63c647002 100644\n--- a/cacheflow/parallel_utils/tensor_parallel/layers.py\n+++ b/cacheflow/parallel_utils/tensor_parallel/layers.py\n@@ -198,8 +198,8 @@ class VocabParallelEmbedding(torch.nn.Module):\n             # Build the mask.\n             input_mask = (input_ < self.vocab_start_index) | \\\n                          (input_ >= self.vocab_end_index)\n-            # Mask the input.\n-            masked_input = input_.clone() - self.vocab_start_index\n+            # Mask the input without redundant clone.\n+            masked_input = input_ - self.vocab_start_index\n             masked_input[input_mask] = 0\n         else:\n             masked_input = input_\n@@ -210,7 +210,7 @@ class VocabParallelEmbedding(torch.nn.Module):\n                                       self.sparse)\n         # Mask the output embedding.\n         if self.tensor_model_parallel_size > 1:\n-            output_parallel[input_mask, :] = 0.0\n+            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0.0)\n         # Reduce across all the model parallel GPUs.\n         output = reduce_from_tensor_model_parallel_region(output_parallel)\n         return output\n@@ -270,7 +270,9 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n             total_input = all_gather_buffer\n         else:\n             total_input = input\n-        grad_input = grad_output.matmul(weight)\n+        # Preallocate grad_input buffer and compute in-place to reduce allocations\n+        grad_input = get_global_memory_buffer().get_tensor(list(input.size()), input.dtype, \"mpu_grad_input\")\n+        torch.matmul(grad_output, weight, out=grad_input)\n \n         if ctx.sequence_parallel:\n             handle.wait()\n@@ -279,7 +281,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n         grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1],\n                                        grad_output.shape[2])\n         total_input = total_input.view(total_input.shape[0] * total_input.shape[1],\n-\t\t\t\t       total_input.shape[2])\n+                                       total_input.shape[2])\n \n         if ctx.async_grad_allreduce:\n             # Asynchronous all-reduce\n@@ -291,9 +293,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n         if ctx.sequence_parallel:\n             assert not ctx.async_grad_allreduce\n             dim_size = list(input.size())\n-            sub_grad_input = torch.empty(dim_size, dtype=input.dtype,\n-                                         device=torch.cuda.current_device(),\n-                                         requires_grad=False)\n+            sub_grad_input = get_global_memory_buffer().get_tensor(dim_size, input.dtype, \"mpu_sub_grad_input\")\n             # reduce_scatter\n             handle = torch.distributed._reduce_scatter_base(sub_grad_input, grad_input,\n                                                             group=get_tensor_model_parallel_group(),\n", "model_name_or_path": "gpt-5-2025-08-07"}
