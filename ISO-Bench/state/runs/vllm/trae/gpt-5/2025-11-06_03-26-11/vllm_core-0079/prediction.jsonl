{"instance_id": "vllm-project__vllm-c34eeec", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 64c2dac52..39833c3f2 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -161,8 +161,10 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm import _custom_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != query.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                       dtype=query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n@@ -184,8 +186,10 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm._ipex_ops import ipex_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != positions.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n+                                                       dtype=query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n@@ -290,8 +294,10 @@ class RotaryEmbedding(CustomOp):\n         if offsets is not None:\n             positions = positions + offsets\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != query.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                       dtype=query.dtype)\n \n         positions = positions.flatten()\n         num_tokens = positions.shape[0]\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b55854de..b9a4df689 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -200,7 +200,6 @@ from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n import torch\n from compressed_tensors.quantization import QuantizationStrategy\n \n-from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionBackend, AttentionLayer,\n                                               AttentionMetadata,\n@@ -222,8 +221,6 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n     apply_fp8_linear_generic, current_platform_fp8_dtype, is_fp8)\n from vllm.model_executor.layers.quantization.utils.quant_utils import (\n     scaled_quantize)\n-from vllm.model_executor.layers.rotary_embedding import (\n-    DeepseekScalingRotaryEmbedding, RotaryEmbedding)\n from vllm.utils import cdiv, round_down\n \n try:\n@@ -237,9 +234,13 @@ if TYPE_CHECKING:\n     from vllm.v1.worker.gpu_input_batch import InputBatch\n     from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n \n+    from vllm.model_executor.layers.rotary_embedding import (\n+        DeepseekScalingRotaryEmbedding, RotaryEmbedding)\n+\n logger = init_logger(__name__)\n \n \n+\n class MLACommonBackend(AttentionBackend):\n \n     accept_output_buffer: bool = True\n@@ -532,14 +533,11 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=torch.nn.functional.pad(\n+                        _chunk_cu_seq_lens, (1, 0, 0, 0)),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -605,7 +603,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         qk_rope_head_dim: int,\n         qk_head_dim: int,\n         v_head_dim: int,\n-        rotary_emb: RotaryEmbedding,\n+        rotary_emb: \"RotaryEmbedding\",\n         # q_proj should be q_b_proj if q_lora_rank is not None, but from an\n         # attention backend perspective we rely on the layer to pass in the\n         # correct matrix\n@@ -627,6 +625,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         self.v_head_dim = v_head_dim\n \n         self.rotary_emb = rotary_emb\n+        from vllm.model_executor.layers.rotary_embedding import DeepseekScalingRotaryEmbedding\n         self.use_yarn_rope = isinstance(rotary_emb,\n                                         DeepseekScalingRotaryEmbedding)\n         self.q_proj = q_proj\n@@ -864,6 +863,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         for i in range(iters):\n             toks = prefill_metadata.chunked_context.seq_tot[i]\n \n+            from vllm import _custom_ops as ops\n             ops.gather_cache(\n                 src_cache=kv_c_and_k_pe_cache,\n                 dst=workspace,\n@@ -1054,6 +1054,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n \n         # write the latent and rope to kv cache\n         if kv_cache.numel() > 0:\n+            from vllm import _custom_ops as ops\n             ops.concat_and_cache_mla(\n                 k_c_normed,\n                 k_pe.squeeze(1),\n", "model_name_or_path": "gpt-5-2025-08-07"}
