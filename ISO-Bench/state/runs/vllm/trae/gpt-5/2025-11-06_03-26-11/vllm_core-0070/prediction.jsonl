{"instance_id": "vllm-project__vllm-8bb43b9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\nindex 61247e930..cc34415cc 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         else:\n             b_type = quant_config.quant_dtype\n \n-        b_a1 = torch.zeros(\n+        b_a1 = torch.empty(\n             (num_local_experts, self.max_num_tokens, hidden_dim),\n             dtype=b_type,\n             device=a1.device)\ndiff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py\nindex d0d8c7d6f..e6baf4c58 100644\n--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py\n+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py\n@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n         \"\"\"\n \n         a1 = hidden_states\n-        output = a1 if inplace else torch.zeros_like(a1)\n+        output = a1 if inplace else torch.empty_like(a1)\n \n         local_num_experts = w1.size(0)\n         if global_num_experts == -1:\ndiff --git a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\nindex 9a5315b8b..785b284a9 100644\n--- a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n+++ b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n@@ -115,11 +115,11 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):\n         K = fused_expert_output.size(-1)\n \n         if output is None:\n-            output = torch.zeros((num_tokens, K),\n+            output = torch.empty((num_tokens, K),\n                                  device=fused_expert_output.device,\n                                  dtype=fused_expert_output.dtype)\n-        else:\n-            output.fill_(0)\n+\n+        initialized = torch.zeros((num_tokens,), dtype=torch.bool, device=fused_expert_output.device)\n \n         assert output.size() == (num_tokens, K), (\n             f\"Expected output size {(num_tokens, K)}, but got {output.size()}\")\n@@ -132,8 +132,20 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):\n             topks = torch.any(matching_tokens, dim=1).flatten()\n             rows = torch.count_nonzero(topks)\n             rhs = fused_expert_output[expert_id - first_expert, :rows, :]\n-            if not apply_router_weight_on_input:\n-                rhs.mul_(topk_weights[matching_tokens].view(rhs.size(0), 1))\n-            output[topks] = output[topks] + rhs\n+            if rows > 0:\n+                idx = torch.nonzero(topks, as_tuple=False).squeeze(1)\n+                new_rows_mask = ~initialized[idx]\n+                if not apply_router_weight_on_input:\n+                    rhs.mul_(topk_weights[matching_tokens].view(rhs.size(0), 1))\n+                if new_rows_mask.any():\n+                    output[idx[new_rows_mask]] = rhs[new_rows_mask]\n+                old_rows_mask = ~new_rows_mask\n+                if old_rows_mask.any():\n+                    output[idx[old_rows_mask]] = output[idx[old_rows_mask]] + rhs[old_rows_mask]\n+                initialized[idx] = True\n+\n+        # Zero rows that received no contributions from local experts\n+        if (~initialized).any():\n+            output[~initialized] = 0\n \n         return output\n", "model_name_or_path": "gpt-5-2025-08-07"}
