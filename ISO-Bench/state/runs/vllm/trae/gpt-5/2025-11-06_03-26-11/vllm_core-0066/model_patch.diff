diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index da09bb70b..892d6ee44 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -106,7 +106,7 @@ class TritonMLAState(AttentionState):
         self._graph_block_tables = torch.from_numpy(
             self.runner.graph_block_tables).to(device=self.runner.device)
 
-        self._positions = torch.zeros((max_batch_size, ),
+        self._positions = torch.empty((max_batch_size, ),
                                       dtype=torch.long,
                                       device=self.runner.device)
 
@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         B = q_nope.shape[0]
 
         q = torch.cat([q_nope, q_pe], dim=-1)
-        o = torch.zeros(B,
+        o = torch.empty(B,
                         self.num_heads,
                         self.kv_lora_rank,
                         dtype=q.dtype,
diff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py
index 83055d600..53765a7b9 100644
--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py
@@ -56,7 +56,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,
     new_shape_perm = list(w_q_perm.shape)
     new_shape_perm[-1] *= pack_factor
 
-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)
     for i in range(pack_factor):
         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask
 
@@ -105,9 +105,7 @@ def permute_rows(q_w: torch.Tensor,
     orig_device = q_w.device
     k_size, _ = q_w.shape
 
-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)
-    for i in range(k_size):
-        g_idx[i] = i // group_size
+    g_idx = (torch.arange(k_size, dtype=torch.int32) // group_size)
 
     # Simulate act_order by doing a random permutation on K
     rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)
