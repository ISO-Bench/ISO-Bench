{"instance_id": "vllm-project__vllm-3a1e648", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a40b..0f56f708a 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -345,8 +345,8 @@ def estimate_rejection_sampling_pdf(\n                                             num_samples, k)\n \n     # Bonus tokens not used but required.\n-    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n-                                  device=DEVICE).repeat(num_samples, 1)\n+    bonus_token_ids = torch.zeros((num_samples, 1), dtype=torch.int64,\n+                                  device=DEVICE)\n \n     sampling_metadata = create_sampling_metadata(all_greedy=False)\n     output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62e9..6b7910c46 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -248,13 +248,12 @@ class RejectionSampler(nn.Module):\n         if sampling_metadata.all_greedy:\n             return logits\n         assert sampling_metadata.temperature is not None\n-        # We should optimize the following code as\n-        # it will cause CPU -> GPU synchronization.\n-        temperature = torch.repeat_interleave(\n-            sampling_metadata.temperature,\n-            torch.tensor(sample_lens,\n-                         device=sampling_metadata.temperature.device))\n-        temperature = temperature.unsqueeze(dim=1)\n+        temps = sampling_metadata.temperature\n+        # Move temps to logits device if needed\n+        if temps.device != logits.device:\n+            temps = temps.to(logits.device)\n+        counts = torch.as_tensor(sample_lens, device=temps.device)\n+        temperature = torch.repeat_interleave(temps, counts).unsqueeze(1)\n         logits = logits / temperature\n         return logits.softmax(dim=-1, dtype=torch.float32)\n \n@@ -292,9 +291,7 @@ def _convert_2d_probs(\n         [num_total_tokens, vocab_size] -> \n             [batch_size, max_spec_len + 1, vocab_size]\n     \"\"\"\n-    cumulative_lens = torch.cumsum(torch.tensor(sample_lens,\n-                                                device=probs.device),\n-                                   dim=0)\n+    cumulative_lens = torch.as_tensor(sample_lens, device=probs.device).cumsum(0)\n     split_indices = cumulative_lens[:-1].tolist()  # Exclude last index\n \n     # Split into chunks without loops\ndiff --git a/vllm/v1/spec_decode/utils.py b/vllm/v1/spec_decode/utils.py\nindex 584140136..045a6cc84 100644\n--- a/vllm/v1/spec_decode/utils.py\n+++ b/vllm/v1/spec_decode/utils.py\n@@ -1,9 +1,9 @@\n # SPDX-License-Identifier: Apache-2.0\n from vllm.v1.sample.ops.topk_topp_sampler import random_sample  # noqa\n-from vllm.v1.worker.gpu_input_batch import InputBatch\n+from typing import Any\n \n \n-def is_spec_decode_supported(req_id: str, input_batch: InputBatch) -> bool:\n+def is_spec_decode_supported(req_id: str, input_batch: Any) -> bool:\n     if req_id in input_batch.top_k_reqs or req_id in input_batch.top_p_reqs:\n         # Spec decode doesn't support top_p/top_k sampling.\n         return False\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 66015382b..927c60780 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -221,7 +221,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -234,27 +234,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
