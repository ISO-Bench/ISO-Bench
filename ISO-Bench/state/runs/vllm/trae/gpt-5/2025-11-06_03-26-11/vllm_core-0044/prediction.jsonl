{"instance_id": "vllm-project__vllm-0fca3cd", "model_patch": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 4feea786f..d2259ecac 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -60,11 +60,12 @@ steps:\n   mirror_hardwares: [amd]\n   commands:\n     # install aws cli for llava_example.py\n-    - pip install awscli\n+    - pip install awscli tensorizer\n     - python3 offline_inference.py\n     - python3 offline_inference_with_prefix.py\n     - python3 llm_engine_example.py\n     - python3 llava_example.py\n+    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors\n \n - label: Kernels Test %N\n   command: pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT\ndiff --git a/examples/tensorize_vllm_model.py b/examples/tensorize_vllm_model.py\nindex e2456168d..aa9cff677 100644\n--- a/examples/tensorize_vllm_model.py\n+++ b/examples/tensorize_vllm_model.py\n@@ -1,6 +1,7 @@\n import argparse\n import dataclasses\n import os\n+import json\n import time\n import uuid\n from functools import partial\n@@ -188,6 +189,7 @@ def serialize():\n     model = (engine.model_executor.driver_worker.\n              model_runner.model)\n \n+    make_model_contiguous(model)\n     encryption_params = EncryptionParams.random() if keyfile else None\n     if keyfile:\n         with _write_stream(keyfile) as stream:\n@@ -211,7 +213,7 @@ def deserialize():\n         model = model_class(config)\n \n     before_mem = get_mem_usage()\n-    start = time.time()\n+    start = time.perf_counter()\n \n     if keyfile:\n         with _read_stream(keyfile) as stream:\n@@ -223,7 +225,7 @@ def deserialize():\n     with (_read_stream(model_path)) as stream, TensorDeserializer(\n             stream, **tensorizer_args.deserializer_params) as deserializer:\n         deserializer.load_into_module(model)\n-        end = time.time()\n+        end = time.perf_counter()\n \n     # Brag about how fast we are.\n     total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 91cc8f3be..68d8a074d 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -145,7 +145,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n \n     # S3 access information, used for tensorizer to load model from S3\n     \"S3_ACCESS_KEY_ID\":\n-    lambda: os.environ.get(\"S3_ACCESS_KEY\", None),\n+    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n     \"S3_SECRET_ACCESS_KEY\":\n     lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n     \"S3_ENDPOINT_URL\":\ndiff --git a/vllm/model_executor/model_loader/tensorizer.py b/vllm/model_executor/model_loader/tensorizer.py\nindex 219a2a392..a7701e869 100644\n--- a/vllm/model_executor/model_loader/tensorizer.py\n+++ b/vllm/model_executor/model_loader/tensorizer.py\n@@ -293,7 +293,7 @@ class TensorizerAgent:\n                                          dtype=child.weight.dtype,\n                                          device=child.weight.device)\n                 new_weight[:child.weight.shape[0]].copy_(child.weight.data)\n-                new_weight[child.weight.shape[0]:].fill_(0)\n+                new_weight[child.weight.shape[0]:].zero_()\n                 child.weight.data = new_weight\n \n     def _check_tensors_on_meta_device(self):\n@@ -337,7 +337,7 @@ class TensorizerAgent:\n         duration = end - start\n         per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n         after_mem = get_mem_usage()\n-        deserializer.close()\n+\n         logger.info(\"Deserialized %s in %0.2fs, %s/s\", total_bytes_str,\n                     end - start, per_second)\n         logger.info(\"Memory usage before: %s\", before_mem)\n", "model_name_or_path": "gpt-5-2025-08-07"}
