{"instance_id": "vllm-project__vllm-20478c4", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex 44ac05a14..9e42ad8f9 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -4,7 +4,7 @@ from typing import List\n \n from vllm import SamplingParams\n from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager import (BlockAllocator, BlockSpaceManager,\n+from vllm.core.block_manager import (UncachedBlockAllocator, BlockSpaceManager,\n                                      AllocStatus)\n from vllm.utils import Device\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus, Logprob\n@@ -15,7 +15,7 @@ from .utils import create_dummy_prompt\n def test_block_allocator_allocate():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     num_free = num_cpu_blocks\n@@ -24,7 +24,7 @@ def test_block_allocator_allocate():\n         block = cpu_allocator.allocate()\n         num_free -= 1\n \n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n     with pytest.raises(ValueError):\n@@ -34,14 +34,14 @@ def test_block_allocator_allocate():\n def test_block_allocator_free():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     blocks: List[PhysicalTokenBlock] = []\n     for _ in range(num_cpu_blocks):\n         block = cpu_allocator.allocate()\n         blocks.append(block)\n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n \n     # Free all allocated cpu blocks.\n     num_free = 0\n@@ -49,7 +49,7 @@ def test_block_allocator_free():\n     for block in blocks:\n         cpu_allocator.free(block)\n         num_free += 1\n-        assert block.block_hash in cpu_allocator.evictor\n+        assert block in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n         with pytest.raises(ValueError):\ndiff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nindex 8b089a565..bd9143bf2 100644\n--- a/vllm/core/block_manager.py\n+++ b/vllm/core/block_manager.py\n@@ -10,6 +10,72 @@ from vllm.utils import Device\n from vllm.core.evictor import Evictor, EvictionPolicy, make_evictor\n \n \n+\n+\n+class UncachedBlockAllocator:\n+    \"\"\"A lightweight allocator for when prefix caching is disabled.\n+\n+    Uses a simple free list of PhysicalTokenBlock objects to avoid the\n+    overhead of maintaining hash-indexed structures and eviction policies.\n+    \"\"\"\n+\n+    def __init__(self, device: Device, block_size: int, num_blocks: int) -> None:\n+        self.device = device\n+        self.block_size = block_size\n+        self.num_blocks = num_blocks\n+        self.current_num_blocks = 0\n+        self.free_blocks: List[PhysicalTokenBlock] = []\n+        self._default_hash_ctr = count()\n+\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        # Reuse freed blocks if available.\n+        if self.free_blocks:\n+            block = self.free_blocks.pop()\n+            assert block.ref_count == 0\n+            # Update block metadata to reflect new usage\n+            block.block_hash = (next(self._default_hash_ctr)\n+                                if block_hash is None else block_hash)\n+            block.num_hashed_tokens = num_hashed_tokens\n+            block.computed = False\n+            block.ref_count = 1\n+            return block\n+\n+        # Allocate a new block if capacity allows\n+        if self.current_num_blocks < self.num_blocks:\n+            new_block = PhysicalTokenBlock(device=self.device,\n+                                           block_number=self.current_num_blocks,\n+                                           block_size=self.block_size,\n+                                           block_hash=(next(self._default_hash_ctr)\n+                                                       if block_hash is None else block_hash),\n+                                           num_hashed_tokens=num_hashed_tokens)\n+            self.current_num_blocks += 1\n+            new_block.ref_count = 1\n+            return new_block\n+\n+        # Out of capacity and no reusable blocks\n+        raise ValueError(\"No usable cache memory left\")\n+\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        if block.ref_count == 0:\n+            raise ValueError(f\"Double free! {block} is already freed.\")\n+        block.ref_count -= 1\n+        if block.ref_count == 0:\n+            block.computed = False\n+            self.free_blocks.append(block)\n+\n+    def contains_block(self, block_hash: int) -> bool:\n+        # No hashing/caching in uncached mode\n+        return False\n+\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock) -> None:\n+        # Simply update the hash on the block\n+        block.block_hash = block_hash\n+\n+    def get_num_free_blocks(self) -> int:\n+        return (self.num_blocks - self.current_num_blocks + len(self.free_blocks))\n+\n class BlockAllocator:\n     \"\"\"Manages free physical token blocks for a device.\n \n@@ -154,14 +220,23 @@ class BlockSpaceManager:\n         self.enable_caching = enable_caching\n \n         self.watermark_blocks = int(watermark * num_gpu_blocks)\n-        self.gpu_allocator = BlockAllocator(Device.GPU,\n-                                            block_size,\n-                                            num_gpu_blocks,\n-                                            enable_caching=enable_caching)\n-        self.cpu_allocator = BlockAllocator(Device.CPU,\n-                                            block_size,\n-                                            num_cpu_blocks,\n-                                            enable_caching=enable_caching)\n+        if enable_caching:\n+            self.gpu_allocator = BlockAllocator(Device.GPU,\n+                                                block_size,\n+                                                num_gpu_blocks,\n+                                                enable_caching=enable_caching)\n+            self.cpu_allocator = BlockAllocator(Device.CPU,\n+                                                block_size,\n+                                                num_cpu_blocks,\n+                                                enable_caching=enable_caching)\n+        else:\n+            # Use a lightweight allocator when caching is disabled for better perf\n+            self.gpu_allocator = UncachedBlockAllocator(Device.GPU,\n+                                                        block_size,\n+                                                        num_gpu_blocks)\n+            self.cpu_allocator = UncachedBlockAllocator(Device.CPU,\n+                                                        block_size,\n+                                                        num_cpu_blocks)\n         # Mapping: seq_id -> BlockTable.\n         self.block_tables: Dict[int, BlockTable] = {}\n \ndiff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex 1d81f5a97..802d5cc34 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -70,31 +70,19 @@ class LRUEvictor(Evictor):\n         if len(free_blocks) == 0:\n             raise ValueError(\"No usable cache memory left\")\n \n-        # Find lowest timestamp\n-        lowest_timestamp = free_blocks[0].last_accessed\n-        for block in free_blocks:\n-            if block.last_accessed < lowest_timestamp:\n-                lowest_timestamp = block.last_accessed\n-\n-        # Find all blocks with the lowest timestamp\n-        least_recent: List[PhysicalTokenBlock] = []\n+        # Single-pass selection of eviction candidate: lowest timestamp, then max num_hashed_tokens\n+        evicted_block: Optional[PhysicalTokenBlock] = None\n+        lowest_timestamp = None\n+        highest_num_hashed_tokens = -1\n         for block in free_blocks:\n-            if block.last_accessed == lowest_timestamp:\n-                least_recent.append(block)\n-\n-        # Find highest prefix count per block\n-        highest_num_hashed_tokens = 0\n-        for block in least_recent:\n-            if block.num_hashed_tokens > highest_num_hashed_tokens:\n+            ts = block.last_accessed\n+            if lowest_timestamp is None or ts < lowest_timestamp:\n+                lowest_timestamp = ts\n+                highest_num_hashed_tokens = block.num_hashed_tokens\n+                evicted_block = block\n+            elif ts == lowest_timestamp and block.num_hashed_tokens > highest_num_hashed_tokens:\n                 highest_num_hashed_tokens = block.num_hashed_tokens\n-\n-        evicted_block: Optional[PhysicalTokenBlock] = None\n-\n-        # Find the first block with the lowest timestamp\n-        for block in least_recent:\n-            if block.num_hashed_tokens == highest_num_hashed_tokens:\n                 evicted_block = block\n-                break\n \n         assert evicted_block is not None\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
