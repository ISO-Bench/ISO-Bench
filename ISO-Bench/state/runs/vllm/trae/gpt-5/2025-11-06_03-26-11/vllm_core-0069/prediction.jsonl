{"instance_id": "vllm-project__vllm-cf2f084", "model_patch": "diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 442173939..62440d001 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -41,7 +41,7 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n         if prev_tokens is None:\n             prev_tokens = new_tokens\n         else:\n-            prev_tokens += new_tokens\n+            prev_tokens.extend(new_tokens)\n     return decoded_text\n \n \ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 724782841..d0eab1cc4 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -718,12 +718,18 @@ class LLMEngine:\n                          all_input_ids: List[int]) -> None:\n         if not logprobs:\n             return\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+        special_ids = tokenizer.all_special_ids\n         for token_id, sample_logprob in logprobs.items():\n             if (sample_logprob.decoded_token is None and token_id != -1):\n+                # Fast-path skip if this is a special token and we are skipping them.\n+                if prms.skip_special_tokens and token_id in special_ids:\n+                    sample_logprob.decoded_token = \"\"\n+                    continue\n                 all_input_ids_with_logprob = all_input_ids[:-1] + [token_id]\n                 (_, new_text, prefix_offset,\n                  read_offset) = detokenize_incrementally(\n-                     self.get_tokenizer_for_seq(seq),\n+                     tokenizer,\n                      all_input_ids=all_input_ids_with_logprob,\n                      prev_tokens=seq.tokens,\n                      prefix_offset=seq.prefix_offset,\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19a8..04cb1f753 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -137,21 +137,24 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    # tokenizer.all_special_tokens is already a set via get_cached_tokenizer.\n+    all_special_tokens = tokenizer.all_special_tokens\n+    added_vocab = tokenizer.get_added_vocab()\n+    append_sub_text = sub_texts.append\n+    convert_sub = tokenizer.convert_tokens_to_string\n+\n     for token in output_tokens:\n         if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.get_added_vocab():\n+        if token in added_vocab:\n             if current_sub_text:\n-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-                sub_texts.append(sub_text)\n+                sub_texts.append(convert_sub(current_sub_text))\n                 current_sub_text = []\n-            sub_texts.append(token)\n+            append_sub_text(token)\n         else:\n             current_sub_text.append(token)\n     if current_sub_text:\n-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-        sub_texts.append(sub_text)\n+        sub_texts.append(convert_sub(current_sub_text))\n     if spaces_between_special_tokens:\n         return \" \".join(sub_texts)\n     else:\n@@ -171,6 +174,12 @@ def detokenize_incrementally(\n     spaces_between_special_tokens: bool = True,\n ) -> Tuple[List[str], str, int, int]:\n     new_token_id = all_input_ids[-1]\n+    # Fast path when we already have previous tokens and the new token is a\n+    # special token to be skipped: avoid any string conversion work.\n+    if prev_tokens is not None and skip_special_tokens and (\n+            new_token_id in tokenizer.all_special_ids):\n+        return [], \"\", prefix_offset, read_offset\n+\n     # This is the first iteration for this sequence\n     if prev_tokens is None:\n         new_tokens = tokenizer.convert_ids_to_tokens(\n@@ -195,10 +204,9 @@ def detokenize_incrementally(\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n     if tokenizer.is_fast or not tokenizer.get_added_vocab():\n-        prefix_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:read_offset])\n-        new_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:])\n+        convert = tokenizer.convert_tokens_to_string\n+        prefix_text = convert(output_tokens[prefix_offset:read_offset])\n+        new_text = convert(output_tokens[prefix_offset:])\n     else:\n         prefix_text = _convert_tokens_to_string_with_added_encoders(\n             tokenizer,\n", "model_name_or_path": "gpt-5-2025-08-07"}
