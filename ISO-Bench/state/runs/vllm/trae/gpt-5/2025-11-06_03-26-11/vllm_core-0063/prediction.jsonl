{"instance_id": "vllm-project__vllm-2f38518", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e79145300..423f70ae9 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -59,23 +59,24 @@ class InputBatch:\n         # Find a way to reduce the CPU memory usage.\n         # This buffer is not directly transferred to the GPU, so it does not\n         # need to be pinned.\n-        self.token_ids_cpu_tensor = torch.zeros(\n+        self.token_ids_cpu_tensor = torch.empty(\n             (max_num_reqs, max_model_len),\n             device=\"cpu\",\n             dtype=torch.int32,\n             pin_memory=False,\n         )\n         self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n+        self.num_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n         self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros(\n+        self.block_table = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=self.device,\n             dtype=torch.int32,\n         )\n-        self.block_table_cpu_tensor = torch.zeros(\n+        self.block_table_cpu_tensor = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\n             dtype=torch.int32,\n@@ -189,6 +190,7 @@ class InputBatch:\n         end_idx = start_idx + len(request.output_token_ids)\n         self.token_ids_cpu[req_index,\n                            start_idx:end_idx] = request.output_token_ids\n+        self.num_tokens[req_index] = request.num_tokens\n \n         self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens\n         num_blocks = len(request.block_ids)\n@@ -290,14 +292,17 @@ class InputBatch:\n             self.req_ids[last_req_index] = None\n             self.req_id_to_index[req_id] = empty_index\n \n-            # TODO(woosuk): Optimize the copy of token_ids_cpu and\n-            # block_table_cpu.\n-            self.token_ids_cpu[empty_index] = self.token_ids_cpu[\n-                last_req_index]\n+            # Optimize: Copy only the valid token range.\n+            num_tokens = self.num_tokens[last_req_index]\n+            if num_tokens > 0:\n+                self.token_ids_cpu[empty_index, :num_tokens] = (\n+                    self.token_ids_cpu[last_req_index, :num_tokens])\n+            self.num_tokens[empty_index] = num_tokens\n             self.num_prompt_tokens[empty_index] = \\\n                 self.num_prompt_tokens[last_req_index]\n             self.num_computed_tokens_cpu[\n                 empty_index] = self.num_computed_tokens_cpu[last_req_index]\n+            # NOTE(woosuk): block_table_cpu copy may be heavy; copying full row.\n             self.block_table_cpu[empty_index] = self.block_table_cpu[\n                 last_req_index]\n             self.temperature_cpu[empty_index] = self.temperature_cpu[\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54e8..a1b8a683f 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -143,27 +143,27 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                              dtype=torch.int32,\n                                              device=\"cpu\",\n                                              pin_memory=self.pin_memory)\n@@ -644,6 +644,7 @@ class GPUModelRunner:\n                 # Append the sampled token to the output token ids.\n                 token_id = sampled_token_ids[i]\n                 self.input_batch.token_ids_cpu[i, seq_len] = token_id\n+                self.input_batch.num_tokens[i] = seq_len + 1\n                 req_state.output_token_ids.append(token_id)\n             else:\n                 # Ignore the sampled token from the partial request.\n", "model_name_or_path": "gpt-5-2025-08-07"}
