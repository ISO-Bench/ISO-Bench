{"instance_id": "vllm-project__vllm-acaea3b", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 5b19e3f35..3519cbad9 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -467,16 +467,10 @@ class MambaMixer2(CustomOp):\n \n             initial_states = None\n \n-            if has_initial_states is not None and torch.any(\n-                    has_initial_states):\n-\n-                # vectorized ssm_state zero init\n-                batched_zero_init_func = torch.vmap(\n-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n-                batched_zero_init_func(\n-                    mamba_cache_params.\n-                    state_indices_tensor[~has_initial_states].unsqueeze(\n-                        dim=-1), )\n+            if has_initial_states is not None and any(has_initial_states):\n+\n+                for idx in mamba_cache_params.state_indices_tensor[~has_initial_states]:\n+                    mamba_cache_params.ssm_state[idx].zero_()\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -505,12 +499,9 @@ class MambaMixer2(CustomOp):\n             # limitation which doesn't allow use of `item()`\n             # Note: the lambda capture can happen where ssm_state is initialized\n             #       instead of here\n-            batched_copy = torch.vmap(\n-                lambda idx, source_state: mamba_cache_params.ssm_state[\n-                    idx].copy_(source_state))\n-            batched_copy(\n-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n-                varlen_state)\n+            for idx, source_state in zip(\n+                mamba_cache_params.state_indices_tensor, varlen_state):\n+                mamba_cache_params.ssm_state[idx].copy_(source_state)\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n@@ -518,7 +509,7 @@ class MambaMixer2(CustomOp):\n \n             n_groups = self.n_groups // self.tp_size\n             A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n+                -1, self.head_dim, self.ssm_state_size)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n             D = self.D[:, None, ...].expand(-1, self.head_dim)\n", "model_name_or_path": "gpt-5-2025-08-07"}
