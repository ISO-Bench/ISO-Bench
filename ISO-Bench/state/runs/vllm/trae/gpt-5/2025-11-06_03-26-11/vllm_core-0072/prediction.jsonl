{"instance_id": "vllm-project__vllm-3336814", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 48cdebee9..a26257b04 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -161,12 +161,8 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm import _custom_ops as ops\n \n-        # __setattr__ in nn.Module (called by `self.cos_sin_cache = ...`)\n-        # is expensive, so avoid calling it if possible\n-        if self.cos_sin_cache.device != query.device or \\\n-            self.cos_sin_cache.dtype != query.dtype:\n-            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                       dtype=query.dtype)\n+        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                   dtype=query.dtype)\n \n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex f3fff585b..e43aba2c0 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -199,6 +199,8 @@ from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n \n import torch\n from compressed_tensors.quantization import QuantizationStrategy\n+import torch.nn.functional as F\n+\n \n from vllm import _custom_ops as ops\n from vllm import envs\n@@ -223,7 +225,6 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n from vllm.model_executor.layers.quantization.utils.quant_utils import (\n     scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n-from vllm.platforms import current_platform\n from vllm.utils import cdiv, round_down\n \n try:\n@@ -532,14 +533,10 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=F.pad(_chunk_cu_seq_lens, (1, 0, 0, 0), mode='constant', value=0),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -628,14 +625,6 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n \n         self.rotary_emb = rotary_emb\n \n-        if current_platform.is_cuda():\n-            # Hack for V1 for now to avoid torch library overhead (since we are\n-            # already inside an attention custom op), pull out the forward\n-            # method from the rotary embedding and call it directly (and avoid\n-            # calling forward_native, when we can call forward_cuda)\n-            # TODO(lucas): we should probably find a cleaner way to do this\n-            self.rotary_emb = rotary_emb.forward_cuda\n-\n         self.q_proj = q_proj\n         self.kv_b_proj = kv_b_proj\n         self.o_proj = o_proj\n", "model_name_or_path": "gpt-5-2025-08-07"}
