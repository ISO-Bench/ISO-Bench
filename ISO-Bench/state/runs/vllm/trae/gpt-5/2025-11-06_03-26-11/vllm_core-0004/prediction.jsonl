{"instance_id": "vllm-project__vllm-f1c8520", "model_patch": "diff --git a/Dockerfile b/Dockerfile\nindex f41753aeb..0549c0ee3 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -41,13 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads\n \n RUN python3 setup.py build_ext --inplace\n \n-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\n-RUN apt-get install -y git && \\\n-    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n-    cd megablocks && \\\n-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n \n # image to run unit testing suite\n FROM dev AS test\n@@ -87,10 +80,5 @@ RUN --mount=type=cache,target=/root/.cache/pip \\\n \n COPY vllm vllm\n COPY --from=build /workspace/vllm/*.so /workspace/vllm/\n-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/\n-RUN --mount=type=cache,target=/root/.cache/pip \\\n-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n-\n ENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n \ndiff --git a/README.md b/README.md\nindex 84cadee48..e4b3b5026 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get\n ```bash\n pip install vllm\n ```\n-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n-```bash\n-pip install megablocks\n-```\n \n ## Getting Started\n \ndiff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst\nindex e21cdd65d..f9663fa07 100644\n--- a/docs/source/models/supported_models.rst\n+++ b/docs/source/models/supported_models.rst\n@@ -76,7 +76,7 @@ Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-pr\n .. note::\n     Currently, the ROCm version of vLLM does not support Mixtral.\n     Additionally, it only supports Mistral for context lengths up to 4096.\n-\n+    Mixtral models require additional optional dependencies (megablocks and stanford-stk) to be installed; see the README for details.\n .. tip::\n     The easiest way to check if your model is supported is to run the program below:\n \ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bafa73c7..9e88f0d19 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -12,6 +12,9 @@ logger = init_logger(__name__)\n \n _GB = 1 << 30\n \n+_SUPPORTED_LOAD_FORMATS = (\"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\")\n+_ROCM_NOT_SUPPORTED_LOAD_FORMATS = (\"safetensors\",)\n+\n \n class ModelConfig:\n     \"\"\"Configuration for the model.\n@@ -98,22 +101,20 @@ class ModelConfig:\n \n     def _verify_load_format(self) -> None:\n         load_format = self.load_format.lower()\n-        supported_load_format = [\n-            \"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"\n-        ]\n-        rocm_not_supported_load_format = [\"safetensors\"]\n+        supported_load_format = _SUPPORTED_LOAD_FORMATS\n+        rocm_not_supported_load_format = _ROCM_NOT_SUPPORTED_LOAD_FORMATS\n         if load_format not in supported_load_format:\n             raise ValueError(\n                 f\"Unknown load format: {self.load_format}. Must be one of \"\n                 \"'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.\")\n         if is_hip():\n-            if load_format in [\"safetensors\"]:\n+            if load_format in rocm_not_supported_load_format:\n                 rocm_supported_load_format = [\n                     f for f in supported_load_format\n                     if (f not in rocm_not_supported_load_format)\n                 ]\n                 raise ValueError(\n-                    f\"load format \\'{load_format}\\' is not supported in ROCm. \"\n+                    f\"load format '{load_format}' is not supported in ROCm. \"\n                     f\"Supported load format are \"\n                     f\"{rocm_supported_load_format}\")\n             # Force ROCm to load from pt weights if nothing specific is set\ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 5596884f3..9e0e4adfa 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -1,4 +1,5 @@\n import importlib\n+\n from typing import List, Optional, Type\n \n import torch.nn as nn\n@@ -39,6 +40,8 @@ _MODELS = {\n }\n \n # Models not supported by ROCm.\n+_SUPPORTED_ARCHS = list(_MODELS.keys())\n+\n _ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n \n # Models partially supported by ROCm.\n@@ -72,7 +75,7 @@ class ModelRegistry:\n \n     @staticmethod\n     def get_supported_archs() -> List[str]:\n-        return list(_MODELS.keys())\n+        return _SUPPORTED_ARCHS\n \n \n __all__ = [\ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 8e0a094c7..2879c77f0 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -340,9 +340,10 @@ class BlockSparseMoE(nn.Module):\n         all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)\n         # weights, selected_experts: (sequence_length, top-k)\n         weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)\n-        weights /= weights.sum(dim=-1, keepdim=True)\n-        weights = weights.flatten().to(x.dtype)\n-        selected_experts = selected_experts.flatten()\n+        denom = weights.sum(dim=-1, keepdim=True)\n+        weights.mul_(denom.reciprocal())\n+        weights = weights.to(x.dtype).reshape(-1)\n+        selected_experts = selected_experts.reshape(-1)\n \n         (indices, bin_ids, bins, padded_bins,\n          _) = self.indices_and_padded_bins(selected_experts)\n@@ -477,9 +478,8 @@ class MixtralForCausalLM(nn.Module):\n         hidden_states = self.tok_embeddings(input_ids)\n \n         # forward\n-        for i in range(len(self.layers)):\n+        for i, layer in enumerate(self.layers):\n             cache_event = None if cache_events is None else cache_events[i]\n-            layer = self.layers[i]\n             hidden_states = layer(\n                 positions,\n                 hidden_states,\n", "model_name_or_path": "gpt-5-2025-08-07"}
