diff --git a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
index 7016ff34c..bedfc5276 100644
--- a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
+++ b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py
@@ -31,7 +31,12 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         self.handle = None
 
         # From https://github.com/deepseek-ai/DeepEP/blob/9fe9021f29c9083cd1808ab36b740208524d9f63/deep_ep/buffer.py#L164
-        self.available_rank_configs = [2, 4, 8, 16, 24, 32, 64, 128, 144, 160]
+        self.available_rank_configs = (2, 4, 8, 16, 24, 32, 64, 128, 144, 160)
+        # Cache DeepEP configs once per instance to avoid repeated lookups.
+        self._dispatch_config = (deep_ep.Buffer.get_dispatch_config(dp_size)
+                                 if dp_size in self.available_rank_configs else None)
+        self._combine_config = (deep_ep.Buffer.get_combine_config(dp_size)
+                                if dp_size in self.available_rank_configs else None)
 
     def num_dispatchers(self) -> int:
         return self.num_dispatchers_
@@ -47,14 +52,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         return torch.int64
 
     def _get_dispatch_config(self) -> Optional[deep_ep.Config]:
-        if self.dp_size not in self.available_rank_configs:
-            return None
-        return deep_ep.Buffer.get_dispatch_config(self.dp_size)
+        return self._dispatch_config
 
     def _get_combine_config(self) -> Optional[deep_ep.Config]:
-        if self.dp_size not in self.available_rank_configs:
-            return None
-        return deep_ep.Buffer.get_combine_config(self.dp_size)
+        return self._combine_config
 
     def _do_dispatch(self, tokens: torch.Tensor,
                      token_scales: Optional[torch.Tensor],
@@ -112,10 +113,15 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         # DeepEP's topk_ids output refers to the local experts directly. Offset
         # the topk_ids to move it back to the global experts space so it aligns
         # with existing vLLM interfaces.
-        expert_topk_ids = torch.where(
-            expert_topk_ids == -1,
-            num_experts - 1 if self.rank_expert_offset == 0 else 0,
-            expert_topk_ids + self.rank_expert_offset)
+        invalid_mask = (expert_topk_ids == -1)
+        # Offset only valid local expert ids to global space.
+        if self.rank_expert_offset != 0:
+            expert_topk_ids[~invalid_mask] += self.rank_expert_offset
+        # Set tokens not in this rank to a valid global expert id so that
+        # expert_map can later remap them to -1 safely.
+        expert_topk_ids[invalid_mask] = (
+            num_experts - 1 if self.rank_expert_offset == 0 else 0
+        )
 
         # Makes a GPU-CPU copy.
         # TODO (varun): Maybe it is better to re-compute the expert_num_tokens
@@ -142,14 +148,16 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
             # TODO: this only works for topK=1, will need to update for topK>1
             assert topk == 1, (
                 "apply_router_weight_on_input is only implemented for topk=1")
-            a1 = a1 * topk_weights.to(a1.dtype)
+            w = topk_weights if topk_weights.dtype == a1.dtype else topk_weights.to(a1.dtype)
+            a1 = a1 * w
 
-        if quant_config.per_act_token_quant:
+        if quant_config.is_block_quantized:
+            # Quant and Dispatch
             a1q, a1q_scale = moe_kernel_quantize_input(
                 a1,
                 a1_scale,
                 quant_dtype=quant_config.quant_dtype,
-                per_act_token_quant=True,
+                per_act_token_quant=quant_config.per_act_token_quant,
                 block_shape=quant_config.block_shape,
             )
             if a1q_scale is not None and a1q_scale.numel() == 1:
@@ -162,8 +170,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
                  rank_topk_weights=topk_weights,
                  num_experts=num_experts)
         else:
-            # DeepEP kernels only support dispatching per-token-quant
-            # quantization. dispatch in bfloat16.
+            # Dispatch and Quant
+            # DeepEP kernels only support dispatching block-quantized
+            # activation scales.
+            # Dispatch in bfloat16
             (expert_x, _, expert_tokens_meta, expert_topk_ids,
              expert_topk_weights) = self._do_dispatch(
                  tokens=a1,
