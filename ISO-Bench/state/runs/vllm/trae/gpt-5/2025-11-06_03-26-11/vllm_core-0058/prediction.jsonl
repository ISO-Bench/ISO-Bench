{"instance_id": "vllm-project__vllm-d666466", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\nindex 7016ff34c..bedfc5276 100644\n--- a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n+++ b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n@@ -31,7 +31,12 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         self.handle = None\n \n         # From https://github.com/deepseek-ai/DeepEP/blob/9fe9021f29c9083cd1808ab36b740208524d9f63/deep_ep/buffer.py#L164\n-        self.available_rank_configs = [2, 4, 8, 16, 24, 32, 64, 128, 144, 160]\n+        self.available_rank_configs = (2, 4, 8, 16, 24, 32, 64, 128, 144, 160)\n+        # Cache DeepEP configs once per instance to avoid repeated lookups.\n+        self._dispatch_config = (deep_ep.Buffer.get_dispatch_config(dp_size)\n+                                 if dp_size in self.available_rank_configs else None)\n+        self._combine_config = (deep_ep.Buffer.get_combine_config(dp_size)\n+                                if dp_size in self.available_rank_configs else None)\n \n     def num_dispatchers(self) -> int:\n         return self.num_dispatchers_\n@@ -47,14 +52,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         return torch.int64\n \n     def _get_dispatch_config(self) -> Optional[deep_ep.Config]:\n-        if self.dp_size not in self.available_rank_configs:\n-            return None\n-        return deep_ep.Buffer.get_dispatch_config(self.dp_size)\n+        return self._dispatch_config\n \n     def _get_combine_config(self) -> Optional[deep_ep.Config]:\n-        if self.dp_size not in self.available_rank_configs:\n-            return None\n-        return deep_ep.Buffer.get_combine_config(self.dp_size)\n+        return self._combine_config\n \n     def _do_dispatch(self, tokens: torch.Tensor,\n                      token_scales: Optional[torch.Tensor],\n@@ -112,10 +113,15 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # DeepEP's topk_ids output refers to the local experts directly. Offset\n         # the topk_ids to move it back to the global experts space so it aligns\n         # with existing vLLM interfaces.\n-        expert_topk_ids = torch.where(\n-            expert_topk_ids == -1,\n-            num_experts - 1 if self.rank_expert_offset == 0 else 0,\n-            expert_topk_ids + self.rank_expert_offset)\n+        invalid_mask = (expert_topk_ids == -1)\n+        # Offset only valid local expert ids to global space.\n+        if self.rank_expert_offset != 0:\n+            expert_topk_ids[~invalid_mask] += self.rank_expert_offset\n+        # Set tokens not in this rank to a valid global expert id so that\n+        # expert_map can later remap them to -1 safely.\n+        expert_topk_ids[invalid_mask] = (\n+            num_experts - 1 if self.rank_expert_offset == 0 else 0\n+        )\n \n         # Makes a GPU-CPU copy.\n         # TODO (varun): Maybe it is better to re-compute the expert_num_tokens\n@@ -142,14 +148,16 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n             # TODO: this only works for topK=1, will need to update for topK>1\n             assert topk == 1, (\n                 \"apply_router_weight_on_input is only implemented for topk=1\")\n-            a1 = a1 * topk_weights.to(a1.dtype)\n+            w = topk_weights if topk_weights.dtype == a1.dtype else topk_weights.to(a1.dtype)\n+            a1 = a1 * w\n \n-        if quant_config.per_act_token_quant:\n+        if quant_config.is_block_quantized:\n+            # Quant and Dispatch\n             a1q, a1q_scale = moe_kernel_quantize_input(\n                 a1,\n                 a1_scale,\n                 quant_dtype=quant_config.quant_dtype,\n-                per_act_token_quant=True,\n+                per_act_token_quant=quant_config.per_act_token_quant,\n                 block_shape=quant_config.block_shape,\n             )\n             if a1q_scale is not None and a1q_scale.numel() == 1:\n@@ -162,8 +170,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n                  rank_topk_weights=topk_weights,\n                  num_experts=num_experts)\n         else:\n-            # DeepEP kernels only support dispatching per-token-quant\n-            # quantization. dispatch in bfloat16.\n+            # Dispatch and Quant\n+            # DeepEP kernels only support dispatching block-quantized\n+            # activation scales.\n+            # Dispatch in bfloat16\n             (expert_x, _, expert_tokens_meta, expert_topk_ids,\n              expert_topk_weights) = self._do_dispatch(\n                  tokens=a1,\n", "model_name_or_path": "gpt-5-2025-08-07"}
