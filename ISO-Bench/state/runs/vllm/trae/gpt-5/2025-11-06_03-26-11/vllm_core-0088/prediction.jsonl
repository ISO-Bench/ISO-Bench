{"instance_id": "vllm-project__vllm-88faa46", "model_patch": "diff --git a/benchmarks/kernels/benchmark_reshape_and_cache_flash.py b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\nnew file mode 100644\nindex 000000000..e062d2016\n--- /dev/null\n+++ b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\n@@ -0,0 +1,113 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n+from __future__ import annotations\n+\n+import argparse\n+import math\n+import time\n+\n+import torch\n+\n+from vllm import _custom_ops as ops\n+from vllm.logger import init_logger\n+from vllm.platforms import current_platform\n+from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE\n+\n+logger = init_logger(__name__)\n+\n+\n+def _to_dtype(dtype_str: str) -> torch.dtype:\n+    if dtype_str in STR_DTYPE_TO_TORCH_DTYPE:\n+        return STR_DTYPE_TO_TORCH_DTYPE[dtype_str]\n+    raise ValueError(f\"Unsupported dtype: {dtype_str}\")\n+\n+\n+@torch.inference_mode()\n+def run_benchmark(\n+    num_tokens: int,\n+    num_heads: int,\n+    head_size: int,\n+    block_size: int,\n+    model_dtype: str,\n+    kv_cache_dtype: str,\n+    iters: int,\n+    warmup: int,\n+    seed: int | None,\n+) -> float:\n+    assert torch.cuda.is_available(), \"CUDA required for this benchmark\"\n+    device = torch.device(\"cuda\")\n+    current_platform.seed_everything(seed)\n+\n+    dtype = _to_dtype(model_dtype)\n+\n+    # Inputs\n+    key = torch.empty((num_tokens, num_heads, head_size), device=device,\n+                      dtype=dtype)\n+    key.normal_(mean=0.0, std=1.0)\n+    value = torch.empty_like(key).normal_(mean=0.0, std=1.0)\n+\n+    # Slot mapping: sequential\n+    slot_mapping = torch.arange(num_tokens, device=device, dtype=torch.long)\n+\n+    # Caches with flash layout [num_blocks, block_size, num_heads, head_size]\n+    num_blocks = math.ceil(num_tokens / block_size)\n+    cache_dtype = STR_DTYPE_TO_TORCH_DTYPE.get(kv_cache_dtype, dtype)\n+    key_cache = torch.empty((num_blocks, block_size, num_heads, head_size),\n+                            device=device, dtype=cache_dtype)\n+    value_cache = torch.empty_like(key_cache)\n+\n+    # Scales (needed for fp8 path; ignored for others)\n+    k_scale = torch.ones((), device=device, dtype=torch.float32)\n+    v_scale = torch.ones((), device=device, dtype=torch.float32)\n+\n+    # Warmup\n+    for _ in range(warmup):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                    slot_mapping, kv_cache_dtype, k_scale,\n+                                    v_scale)\n+    torch.cuda.synchronize()\n+\n+    # Timing\n+    start = time.perf_counter()\n+    for _ in range(iters):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                    slot_mapping, kv_cache_dtype, k_scale,\n+                                    v_scale)\n+    torch.cuda.synchronize()\n+    end = time.perf_counter()\n+\n+    return (end - start) * 1000.0 / iters\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser(\n+        description=\"Benchmark reshape_and_cache_flash kernel\")\n+    parser.add_argument(\"--num-tokens\", type=int, default=2048)\n+    parser.add_argument(\"--num-heads\", type=int, default=16)\n+    parser.add_argument(\"--head-size\", type=int, default=128)\n+    parser.add_argument(\"--block-size\", type=int, default=16)\n+    parser.add_argument(\"--model-dtype\", type=str, default=\"half\",\n+                        choices=[\"half\", \"bfloat16\", \"float\"])  # inputs\n+    parser.add_argument(\"--kv-cache-dtype\",\n+                        type=str,\n+                        default=\"auto\",\n+                        choices=[\"auto\", \"half\", \"bfloat16\", \"float\",\n+                                 \"fp8\"])\n+    parser.add_argument(\"--iters\", type=int, default=20)\n+    parser.add_argument(\"--warmup\", type=int, default=3)\n+    parser.add_argument(\"--seed\", type=int, default=0)\n+\n+    args = parser.parse_args()\n+\n+    if not torch.cuda.is_available():\n+        logger.warning(\"CUDA not available; skipping.\")\n+        return\n+\n+    ms = run_benchmark(args.num_tokens, args.num_heads, args.head_size,\n+                       args.block_size, args.model_dtype, args.kv_cache_dtype,\n+                       args.iters, args.warmup, args.seed)\n+    print(f\"reshape_and_cache_flash: {ms:.3f} ms/iter\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 88559c8fe..022c9506e 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -88,12 +88,10 @@ __global__ void copy_blocks_kernel(int64_t* key_cache_ptrs,\n   for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n     int64_t src_offset = src_block_offset + i;\n     int64_t dst_offset = dst_block_offset + i;\n-    key_cache[dst_offset] = key_cache[src_offset];\n-  }\n-  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n-    int64_t src_offset = src_block_offset + i;\n-    int64_t dst_offset = dst_block_offset + i;\n-    value_cache[dst_offset] = value_cache[src_offset];\n+    scalar_t tmp_k = key_cache[src_offset];\n+    scalar_t tmp_v = value_cache[src_offset];\n+    key_cache[dst_offset] = tmp_k;\n+    value_cache[dst_offset] = tmp_v;\n   }\n }\n \n@@ -282,25 +280,33 @@ __global__ void reshape_and_cache_flash_kernel(\n   }\n   const int64_t block_idx = slot_idx / block_size;\n   const int64_t block_offset = slot_idx % block_size;\n-  const int n = num_heads * head_size;\n-  for (int i = threadIdx.x; i < n; i += blockDim.x) {\n-    const int64_t src_key_idx = token_idx * key_stride + i;\n-    const int64_t src_value_idx = token_idx * value_stride + i;\n-    const int head_idx = i / head_size;\n-    const int head_offset = i % head_size;\n-    const int64_t tgt_key_value_idx = block_idx * block_stride +\n-                                      block_offset * page_stride +\n-                                      head_idx * head_stride + head_offset;\n-    scalar_t tgt_key = key[src_key_idx];\n-    scalar_t tgt_value = value[src_value_idx];\n-    if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {\n-      key_cache[tgt_key_value_idx] = tgt_key;\n-      value_cache[tgt_key_value_idx] = tgt_value;\n-    } else {\n-      key_cache[tgt_key_value_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, *k_scale);\n-      value_cache[tgt_key_value_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, *v_scale);\n+\n+  // Precompute base pointers to reduce index math in the inner loop.\n+  const scalar_t* __restrict__ key_base = key + token_idx * key_stride;\n+  const scalar_t* __restrict__ value_base = value + token_idx * value_stride;\n+  cache_t* __restrict__ key_cache_base =\n+      key_cache + block_idx * block_stride + block_offset * page_stride;\n+  cache_t* __restrict__ value_cache_base =\n+      value_cache + block_idx * block_stride + block_offset * page_stride;\n+\n+  // Iterate over heads outside and head_size inside to avoid expensive\n+  // div/mod operations on each iteration.\n+  for (int head_idx = 0; head_idx < num_heads; ++head_idx) {\n+    const int64_t src_head_off = static_cast<int64_t>(head_idx) * head_size;\n+    const int64_t tgt_head_off = static_cast<int64_t>(head_idx) * head_stride;\n+    for (int ho = threadIdx.x; ho < head_size; ho += blockDim.x) {\n+      const scalar_t k_val = key_base[src_head_off + ho];\n+      const scalar_t v_val = value_base[src_head_off + ho];\n+      const int64_t tgt_idx = tgt_head_off + static_cast<int64_t>(ho);\n+      if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {\n+        key_cache_base[tgt_idx] = k_val;\n+        value_cache_base[tgt_idx] = v_val;\n+      } else {\n+        key_cache_base[tgt_idx] =\n+            fp8::scaled_convert<cache_t, scalar_t, kv_dt>(k_val, *k_scale);\n+        value_cache_base[tgt_idx] =\n+            fp8::scaled_convert<cache_t, scalar_t, kv_dt>(v_val, *v_scale);\n+      }\n     }\n   }\n }\n@@ -441,7 +447,7 @@ void reshape_and_cache_flash(\n   TORCH_CHECK(key_cache.stride(0) == value_cache.stride(0));\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  dim3 block(std::min(head_size, 512));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
