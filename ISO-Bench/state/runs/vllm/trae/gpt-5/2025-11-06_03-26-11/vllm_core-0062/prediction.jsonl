{"instance_id": "vllm-project__vllm-4a18fd1", "model_patch": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..f30cdd3b0 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,8 +90,8 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n-        x = self.act_fn(gate_up)\n+        x, _ = self.gate_up_proj(x)\n+        x = self.act_fn(x)\n         x, _ = self.down_proj(x)\n         return x\n \n@@ -134,6 +134,7 @@ class LlamaAttention(nn.Module):\n         self.q_size = self.num_heads * self.head_dim\n         self.kv_size = self.num_kv_heads * self.head_dim\n         self.scaling = self.head_dim**-0.5\n+        self._split_sizes = [self.q_size, self.kv_size, self.kv_size]\n         self.rope_theta = rope_theta\n         self.max_position_embeddings = max_position_embeddings\n \n@@ -184,11 +185,11 @@ class LlamaAttention(nn.Module):\n         attn_metadata: AttentionMetadata,\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        q, k, v = qkv.split(self._split_sizes, dim=-1)\n         q, k = self.rotary_emb(positions, q, k)\n         attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n-        output, _ = self.o_proj(attn_output)\n-        return output\n+        attn_output, _ = self.o_proj(attn_output)\n+        return attn_output\n \n \n class LlamaDecoderLayer(nn.Module):\n", "model_name_or_path": "gpt-5-2025-08-07"}
