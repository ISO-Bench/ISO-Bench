{"instance_id": "vllm-project__vllm-388596c", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a75..c931b6933 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -258,7 +258,7 @@ def scaled_fp8_quant(\n     else:\n         output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e3d..96a09e349 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -204,7 +204,7 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                   layer.weight_scale[idx])\n \n                 layer.weight[start:end, :] = per_tensor_quantize(\n-                    weight_dq, layer.weight_scale.max())\n+                    weight_dq, max_w_scale)\n                 start = end\n             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n \n@@ -240,18 +240,39 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                layer.act_scale,\n                                                batch_dim_padding=17)\n \n-        # Fused GEMM_DQ -- note we padded the input above because\n-        # torch._scaled_mm is more performant for matrices with\n-        # batch dimension > 16. Note that this could change\n-        # in the future.\n-        output, _ = torch._scaled_mm(\n-            qinput,\n-            layer.weight,\n-            out_dtype=x.dtype,\n-            scale_a=x_scale,\n-            scale_b=layer.weight_scale,\n-            bias=bias,\n+        # Try optimized CUTLASS kernel when supported; fallback to torch._scaled_mm\n+        use_cutlass = (\n+            hasattr(ops, 'cutlass_scaled_mm_dq') and\n+            isinstance(layer.weight, torch.Tensor) and\n+            layer.weight.is_cuda and\n+            layer.weight.shape[0] % 16 == 0 and layer.weight.shape[1] % 16 == 0 and\n+            (x.dtype is torch.bfloat16 or x.dtype is torch.float16)\n         )\n+        output = None\n+        if use_cutlass:\n+            try:\n+                output = ops.cutlass_scaled_mm_dq(\n+                    qinput,\n+                    layer.weight,\n+                    x_scale,\n+                    layer.weight_scale,\n+                    out_dtype=x.dtype,\n+                )\n+                if bias is not None:\n+                    output = output + bias\n+            except Exception:\n+                output = None\n+\n+        if output is None:\n+            # Fused GEMM_DQ via PyTorch fallback\n+            output, _ = torch._scaled_mm(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+                bias=bias,\n+            )\n \n         return torch.narrow(output, 0, 0, x.shape[0])\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
