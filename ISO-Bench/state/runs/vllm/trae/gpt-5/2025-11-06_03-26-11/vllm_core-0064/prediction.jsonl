{"instance_id": "vllm-project__vllm-25373b6", "model_patch": "diff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py\nindex 00c1a2911..a7fffe34e 100644\n--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py\n+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py\n@@ -163,7 +163,7 @@ def generate_continuous_batched_examples(example_lens_by_batch,\n \n         # get the metadata\n         cu_seqlens = torch.tensor((0, ) + spec, device=device).cumsum(dim=0)\n-        seq_idx = torch.zeros(cu_seqlens[-1],\n+        seq_idx = torch.empty(cu_seqlens[-1],\n                               dtype=torch.int32,\n                               device=cu_seqlens.device)\n         for i, (srt, end) in enumerate(zip(\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 36edac237..e1fa2ccb7 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -583,7 +583,11 @@ class MambaMixer2(MambaBase, CustomOp):\n                                                                1]\n                                  if has_prefill else None)\n \n-        ssd_output_list = []\n+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim\n+        ssd_output_combined = torch.empty(num_actual_tokens,\n+                                          feature_dim,\n+                                          device=hidden_states_B_C.device,\n+                                          dtype=hidden_states_B_C.dtype)\n \n         # Process prefill requests\n         if has_prefill:\n@@ -653,7 +657,13 @@ class MambaMixer2(MambaBase, CustomOp):\n             ssm_state[state_indices_tensor_p] = varlen_state\n \n             # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n+            prefill_flat = scan_output.view(num_prefill_tokens, -1)\n+            if envs.VLLM_USE_V1:\n+                start = num_decodes\n+            else:\n+                start = 0\n+            ssd_output_combined[start:start + num_prefill_tokens].copy_(\n+                prefill_flat)\n \n         # Process decode requests\n         if has_decode:\n@@ -699,18 +709,16 @@ class MambaMixer2(MambaBase, CustomOp):\n                 state_batch_indices=state_indices_tensor_d,\n             )\n \n+            decode_flat = hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n+                                         self.head_dim)\n             if envs.VLLM_USE_V1:\n-                ssd_output_list.insert(\n-                    0,\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n+                ssd_output_combined[:num_decodes].copy_(decode_flat)\n             else:\n-                ssd_output_list.append(\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n+                ssd_output_combined[num_prefill_tokens:num_prefill_tokens +\n+                                    num_decodes].copy_(decode_flat)\n \n         # Merge prefill and decode outputs before passing to gated MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n+        hidden_states = ssd_output_combined\n \n         # 4. gated MLP\n         # GatedRMSNorm internally applying SiLU to the gate\ndiff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py\nindex a4ded2b7a..27dcb4c11 100644\n--- a/vllm/model_executor/models/phi4flash.py\n+++ b/vllm/model_executor/models/phi4flash.py\n@@ -129,16 +129,16 @@ class SambaYAttention(nn.Module):\n \n         self.lambda_init = self.lambda_init_fn(layer_idx)\n         self.lambda_q1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_q2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.subln = nn.RMSNorm(2 * self.head_dim,\n                                 eps=1e-5,\ndiff --git a/vllm/model_executor/models/plamo2.py b/vllm/model_executor/models/plamo2.py\nindex 9bc577cfe..787ae1668 100644\n--- a/vllm/model_executor/models/plamo2.py\n+++ b/vllm/model_executor/models/plamo2.py\n@@ -257,7 +257,12 @@ class Plamo2MambaMixer(nn.Module):\n         query_start_loc_p = (attn_metadata.query_start_loc[:num_prefills + 1]\n                              if has_prefill else None)\n \n-        ssd_output_list = []\n+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim\n+        num_actual_tokens = num_prefill_tokens + num_decodes\n+        ssd_output_combined = torch.empty(num_actual_tokens,\n+                                          feature_dim,\n+                                          device=hidden_states.device,\n+                                          dtype=hidden_states.dtype)\n \n         # Process prefill requests\n         if has_prefill:\n@@ -319,7 +324,8 @@ class Plamo2MambaMixer(nn.Module):\n             mamba_cache_params.ssm_state[state_indices_tensor_p] = varlen_state\n \n             # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n+            prefill_flat = scan_output.view(num_prefill_tokens, -1)\n+            ssd_output_combined[:num_prefill_tokens].copy_(prefill_flat)\n \n         # Process decode requests\n         if has_decode:\n@@ -363,13 +369,14 @@ class Plamo2MambaMixer(nn.Module):\n                 dt_softplus=True,\n                 state_batch_indices=state_indices_tensor_d,\n             )\n-            assert self.num_heads % self.tp_size == 0\n-            ssd_output_list.append(\n-                hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                     self.head_dim))\n+            ssd_output_combined[num_prefill_tokens:num_prefill_tokens +\n+                                num_decodes].copy_(\n+                                    hidden_states_d.view(-1, (self.num_heads //\n+                                                             self.tp_size) *\n+                                                         self.head_dim))\n \n         # Merge prefill and decode outputs before passing to MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n+        hidden_states = ssd_output_combined\n \n         # 4. Final linear projection\n         out = self.out_proj(hidden_states)\n", "model_name_or_path": "gpt-5-2025-08-07"}
