{
  "task": "vLLM core performance",
  "description": "Run vLLM performance checks with Dockerfile-based env",
  "constraints": [
    "No public API breakage",
    "All TestPack checks must pass"
  ],
  "target_files": [
    "vllm/entrypoints/openai/api_server.py",
    "vllm/entrypoints/openai/protocol.py",
    "vllm/envs.py",
    "vllm/v1/engine/async_llm.py",
    "vllm/v1/engine/core_client.py",
    "vllm/v1/engine/output_processor.py",
    "vllm/v1/request.py"
  ],
  "success": {
    "primary_metric": "functional_match",
    "rules": [
      "Do not modify tests or metrics harness",
      "Preserve external behavior; optimize internals only"
    ]
  },
  "commits": {
    "pre": "7206ce4ce112ed117796a59045c968a6d353f691",
    "human": "aea94362c9bdd08ed2b346701bdc09d278e85f66"
  }
}