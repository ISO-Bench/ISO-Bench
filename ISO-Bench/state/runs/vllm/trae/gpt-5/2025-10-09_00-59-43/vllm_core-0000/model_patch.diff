diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/config.py b/vllm/config.py
index 6bfe94b76..2a405b66c 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -4717,6 +4717,23 @@ class VllmConfig:
 
         disable_chunked_prefill_reasons: list[str] = []
 
+        # Allow users to force-disable the hybrid KV cache manager regardless of
+        # detected compatibility, which can improve latency in some scenarios.
+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:
+            self.scheduler_config.disable_hybrid_kv_cache_manager = True
+
+        # Optional escape hatch to disable chunked prefill entirely via env var.
+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:
+            logger.info("Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.")
+            self.scheduler_config.chunked_prefill_enabled = False
+            self.scheduler_config.long_prefill_token_threshold = 0
+            self.scheduler_config.max_num_batched_tokens = max(
+                self.scheduler_config.max_model_len,
+                DEFAULT_MAX_NUM_BATCHED_TOKENS)
+            if self.cache_config is not None:
+                self.cache_config.enable_prefix_caching = False
+
+
         if self.model_config and self.model_config.pooler_config:
             pooling_type = self.model_config.pooler_config.pooling_type
             if pooling_type is None or pooling_type.lower() != "last":
@@ -4769,12 +4786,23 @@ class VllmConfig:
                 # Hybrid KV cache manager is not compatible with KV events.
                 self.scheduler_config.disable_hybrid_kv_cache_manager = True
             if self.model_config is not None and \
-                self.model_config.attention_chunk_size is not None and \
-                self.speculative_config is not None and \
-                self.speculative_config.use_eagle():
-                # Hybrid KV cache manager is not yet supported with chunked
-                # local attention + eagle.
-                self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                self.model_config.attention_chunk_size is not None:
+                if self.speculative_config is not None and \
+                    self.speculative_config.use_eagle():
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention + eagle.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                elif \
+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:
+                    logger.warning_once(
+                        "There is a latency regression when using chunked local"
+                        " attention with the hybrid KV cache manager. Disabling"
+                        " it, by default. To enable it, set the environment "
+                        "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1."
+                    )
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
 
     def update_sizes_for_sequence_parallelism(self,
                                               possible_sizes: list) -> list:
diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff74151..51a04b625 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -948,6 +948,19 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # implement and support a subset of all possible layouts.
     "VLLM_KV_CACHE_LAYOUT":
     lambda: os.getenv("VLLM_KV_CACHE_LAYOUT", None),
+    # If set, allow using hybrid KV cache manager together with chunked local attention.
+    # Default is disabled due to latency regressions; set to 1 to enable.
+    "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE":
+    lambda: (os.getenv("VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE", "0").strip().lower() in ("1", "true")),
+    
+    # Force-disable hybrid KV cache manager (overrides compatibility checks).
+    "VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER":
+    lambda: bool(int(os.getenv("VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER", "0"))),
+
+    # Disable chunked prefill across the board to reduce latency when needed.
+    "VLLM_DISABLE_CHUNKED_PREFILL":
+    lambda: bool(int(os.getenv("VLLM_DISABLE_CHUNKED_PREFILL", "0"))),
+
 
     # Enable checking whether the generated logits contain NaNs,
     # indicating corrupted output. Useful for debugging low level bugs
