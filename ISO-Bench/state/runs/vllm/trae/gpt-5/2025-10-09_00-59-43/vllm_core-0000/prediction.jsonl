{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..2a405b66c 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4717,6 +4717,23 @@ class VllmConfig:\n \n         disable_chunked_prefill_reasons: list[str] = []\n \n+        # Allow users to force-disable the hybrid KV cache manager regardless of\n+        # detected compatibility, which can improve latency in some scenarios.\n+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:\n+            self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+\n+        # Optional escape hatch to disable chunked prefill entirely via env var.\n+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:\n+            logger.info(\"Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.\")\n+            self.scheduler_config.chunked_prefill_enabled = False\n+            self.scheduler_config.long_prefill_token_threshold = 0\n+            self.scheduler_config.max_num_batched_tokens = max(\n+                self.scheduler_config.max_model_len,\n+                DEFAULT_MAX_NUM_BATCHED_TOKENS)\n+            if self.cache_config is not None:\n+                self.cache_config.enable_prefix_caching = False\n+\n+\n         if self.model_config and self.model_config.pooler_config:\n             pooling_type = self.model_config.pooler_config.pooling_type\n             if pooling_type is None or pooling_type.lower() != \"last\":\n@@ -4769,12 +4786,23 @@ class VllmConfig:\n                 # Hybrid KV cache manager is not compatible with KV events.\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n-                self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif \\\n+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning_once(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\"\n+                    )\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..51a04b625 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -948,6 +948,19 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # implement and support a subset of all possible layouts.\n     \"VLLM_KV_CACHE_LAYOUT\":\n     lambda: os.getenv(\"VLLM_KV_CACHE_LAYOUT\", None),\n+    # If set, allow using hybrid KV cache manager together with chunked local attention.\n+    # Default is disabled due to latency regressions; set to 1 to enable.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: (os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\").strip().lower() in (\"1\", \"true\")),\n+    \n+    # Force-disable hybrid KV cache manager (overrides compatibility checks).\n+    \"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\":\n+    lambda: bool(int(os.getenv(\"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\", \"0\"))),\n+\n+    # Disable chunked prefill across the board to reduce latency when needed.\n+    \"VLLM_DISABLE_CHUNKED_PREFILL\":\n+    lambda: bool(int(os.getenv(\"VLLM_DISABLE_CHUNKED_PREFILL\", \"0\"))),\n+\n \n     # Enable checking whether the generated logits contain NaNs,\n     # indicating corrupted output. Useful for debugging low level bugs\n", "model_name_or_path": "gpt-5-2025-08-07"}
