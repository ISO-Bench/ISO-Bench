{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..fad372e7d 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -2489,6 +2489,10 @@ class SchedulerConfig:\n         if self.max_num_seqs is None:\n             self.max_num_seqs = 128\n \n+        if self.enable_chunked_prefill is None:\n+            # Allow environment-based default to avoid overhead unless explicitly enabled.\n+            self.enable_chunked_prefill = envs.VLLM_ENABLE_CHUNKED_PREFILL\n+\n         if self.max_num_batched_tokens is None:\n             if self.enable_chunked_prefill:\n                 if self.num_scheduler_steps > 1:\n@@ -2532,15 +2536,16 @@ class SchedulerConfig:\n         self.encoder_cache_size = self.max_num_batched_tokens\n \n         if self.enable_chunked_prefill:\n-            logger.info(\n+            logger.info_once(\n                 \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                 self.max_num_batched_tokens)\n \n         self.chunked_prefill_enabled = self.enable_chunked_prefill\n         if self.max_num_partial_prefills > 1:\n             if self.long_prefill_token_threshold == 0:\n-                self.long_prefill_token_threshold = int(self.max_model_len *\n-                                                        0.04)\n+                self.long_prefill_token_threshold = int(\n+                    self.max_model_len * envs.VLLM_LONG_PREFILL_THRESHOLD_FRACTION\n+                )\n \n             logger.info(\n                 \"Concurrent partial prefills enabled with \"\n@@ -4711,12 +4716,34 @@ class VllmConfig:\n \n         if self.compilation_config.full_cuda_graph and \\\n             not self.model_config.disable_cascade_attn:\n-            logger.info(\"full_cuda_graph is not supported with \"\n+            logger.info_once(\"full_cuda_graph is not supported with \"\n                         \"cascade attention. Disabling cascade attention.\")\n             self.model_config.disable_cascade_attn = True\n \n         disable_chunked_prefill_reasons: list[str] = []\n \n+        # Allow users to force-disable the hybrid KV cache manager regardless of\n+        # detected compatibility, which can improve latency in some scenarios.\n+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:\n+            self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+\n+        # Optional escape hatch to disable chunked prefill entirely via env var.\n+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:\n+            logger.info_once(\"Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.\")\n+            self.scheduler_config.chunked_prefill_enabled = False\n+            self.scheduler_config.long_prefill_token_threshold = 0\n+            self.scheduler_config.max_num_batched_tokens = max(\n+                self.scheduler_config.max_model_len,\n+                DEFAULT_MAX_NUM_BATCHED_TOKENS)\n+            if self.cache_config is not None:\n+                self.cache_config.enable_prefix_caching = False\n+\n+\n+        # Optional escape hatch to disable chunked multimodal input scheduling.\n+        if envs.VLLM_DISABLE_CHUNKED_MM_INPUT:\n+            self.scheduler_config.disable_chunked_mm_input = True\n+\n+\n         if self.model_config and self.model_config.pooler_config:\n             pooling_type = self.model_config.pooler_config.pooling_type\n             if pooling_type is None or pooling_type.lower() != \"last\":\n@@ -4726,7 +4753,7 @@ class VllmConfig:\n \n         if disable_chunked_prefill_reasons:\n             for reason in disable_chunked_prefill_reasons:\n-                logger.info(reason)\n+                logger.info_once(reason)\n             self.scheduler_config.chunked_prefill_enabled = False\n             self.scheduler_config.long_prefill_token_threshold = 0\n             self.scheduler_config.max_num_batched_tokens = max(\n@@ -4739,13 +4766,13 @@ class VllmConfig:\n         if (self.kv_events_config is not None\n                 and self.kv_events_config.enable_kv_cache_events\n                 and not self.cache_config.enable_prefix_caching):\n-            logger.warning(\n+            logger.warning_once(\n                 \"KV cache events are on, but prefix caching is not enabled.\"\n                 \"Use --enable-prefix-caching to enable.\")\n         if (self.kv_events_config is not None\n                 and self.kv_events_config.publisher != \"null\"\n                 and not self.kv_events_config.enable_kv_cache_events):\n-            logger.warning(\"KV cache events are disabled,\"\n+            logger.warning_once(\"KV cache events are disabled,\"\n                            \"but the scheduler is configured to publish them.\"\n                            \"Modify KVEventsConfig.enable_kv_cache_events\"\n                            \"to True to enable.\")\n@@ -4770,11 +4797,30 @@ class VllmConfig:\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n                 self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                envs.VLLM_DISABLE_CHUNKED_LOCAL_ATTN:\n+                logger.info_once(\n+                    \"Disabling chunked local attention due to \"\n+                    \"VLLM_DISABLE_CHUNKED_LOCAL_ATTN=1.\")\n+                self.model_config.attention_chunk_size = None\n+\n+            if self.model_config is not None and \\\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif \\\n+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning_once(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\"\n+                    )\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..a17f57815 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -7,6 +7,10 @@ import sys\n import tempfile\n from typing import TYPE_CHECKING, Any, Callable, Optional\n \n+\n+# Cache for compute_hash to avoid repeated re-hashing on hot paths\n+_ENV_HASH_CACHE: Optional[str] = None\n+\n if TYPE_CHECKING:\n     VLLM_HOST_IP: str = \"\"\n     VLLM_PORT: Optional[int] = None\n@@ -165,6 +169,30 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:\n     return int(value)\n \n \n+\n+def getenv_bool(name: str, default: bool = False) -> bool:\n+    \"\"\"Fast path for boolean env var parsing with common conventions.\n+    Accepts \"1\" or \"true\" (case-insensitive) as True. Returns default if unset.\n+    \"\"\"\n+    val = os.getenv(name)\n+    if val is None:\n+        return default\n+    val = val.strip().lower()\n+    return val == \"1\" or val == \"true\"\n+\n+\n+\n+def getenv_float(name: str, default: float) -> float:\n+    \"\"\"Parse float env var; return default on unset or invalid.\"\"\"\n+    val = os.getenv(name)\n+    if val is None:\n+        return default\n+    try:\n+        return float(val)\n+    except ValueError:\n+        return default\n+\n+\n def get_vllm_port() -> Optional[int]:\n     \"\"\"Get the port from VLLM_PORT environment variable.\n \n@@ -285,7 +313,7 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # If true, will load models from ModelScope instead of Hugging Face Hub.\n     # note that the value is true or false, not numbers\n     \"VLLM_USE_MODELSCOPE\":\n-    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n+    lambda: getenv_bool(\"VLLM_USE_MODELSCOPE\", False),\n \n     # Interval in seconds to log a warning message when the ring buffer is full\n     \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n@@ -308,15 +336,12 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n     # flag to control if vllm should use triton flash attention\n     \"VLLM_USE_TRITON_FLASH_ATTN\":\n-    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n-             (\"true\", \"1\")),\n+    lambda: getenv_bool(\"VLLM_USE_TRITON_FLASH_ATTN\", True),\n \n     # Use separate prefill and decode kernels for V1 attention instead of\n     # the unified triton kernel.\n     \"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\":\n-    lambda:\n-    (os.getenv(\"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\", \"False\").lower() in\n-     (\"true\", \"1\")),\n+    lambda: getenv_bool(\"VLLM_V1_USE_PREFILL_DECODE_ATTENTION\", False),\n \n     # Force vllm to use a specific flash-attention version (2 or 3), only valid\n     # when using the flash-attention backend.\n@@ -332,7 +357,7 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # In torch <= 2.7 we ignore this flag; in torch >= 2.8 this is\n     # enabled by default.\n     \"VLLM_USE_STANDALONE_COMPILE\":\n-    lambda: os.environ.get(\"VLLM_USE_STANDALONE_COMPILE\", \"1\") == \"1\",\n+    lambda: getenv_bool(\"VLLM_USE_STANDALONE_COMPILE\", True),\n \n     # local rank of the process in the distributed setting, used to determine\n     # the GPU device id\n@@ -353,8 +378,7 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n     # Whether to log responses from API Server for debugging\n     \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\":\n-    lambda: os.environ.get(\"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\", \"False\"\n-                           ).lower() == \"true\",\n+    lambda: getenv_bool(\"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\", False),\n \n     # S3 access information, used for tensorizer to load model from S3\n     \"S3_ACCESS_KEY_ID\":\n@@ -948,6 +972,40 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # implement and support a subset of all possible layouts.\n     \"VLLM_KV_CACHE_LAYOUT\":\n     lambda: os.getenv(\"VLLM_KV_CACHE_LAYOUT\", None),\n+    # If set, allow using hybrid KV cache manager together with chunked local attention.\n+    # Default is disabled due to latency regressions; set to 1 to enable.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: getenv_bool(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", False),\n+    \n+    # Force-disable hybrid KV cache manager (overrides compatibility checks).\n+    \"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER\", False),\n+\n+\n+    # Enable chunked prefill by default. When false (default), reduces CPU overhead.\n+    \"VLLM_ENABLE_CHUNKED_PREFILL\":\n+    lambda: getenv_bool(\"VLLM_ENABLE_CHUNKED_PREFILL\", False),\n+\n+    # Fraction of max_model_len used to classify 'long' prompts for partial prefills.\n+    \"VLLM_LONG_PREFILL_THRESHOLD_FRACTION\":\n+    lambda: getenv_float(\"VLLM_LONG_PREFILL_THRESHOLD_FRACTION\", 0.04),\n+\n+    # Disable chunked prefill across the board to reduce latency when needed.\n+    \"VLLM_DISABLE_CHUNKED_PREFILL\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_CHUNKED_PREFILL\", False),\n+    \n+    # Disable chunked local attention entirely. Can reduce latency on some\n+    # models with hybrid KV cache or when chunking offers little benefit.\n+    \"VLLM_DISABLE_CHUNKED_LOCAL_ATTN\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_CHUNKED_LOCAL_ATTN\", False),\n+\n+    # Disable chunked multimodal input scheduling. When set, MM items are scheduled\n+    # as a whole to reduce scheduling overhead in some scenarios.\n+    \"VLLM_DISABLE_CHUNKED_MM_INPUT\":\n+    lambda: getenv_bool(\"VLLM_DISABLE_CHUNKED_MM_INPUT\", False),\n+\n+\n+\n \n     # Enable checking whether the generated logits contain NaNs,\n     # indicating corrupted output. Useful for debugging low level bugs\n@@ -997,9 +1055,12 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n \n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation with simple caching to avoid repeated parsing\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        value = environment_variables[name]()\n+        # cache the resolved value as a module attribute for fast subsequent access\n+        setattr(sys.modules[__name__], name, value)\n+        return value\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n@@ -1033,14 +1094,17 @@ def compute_hash() -> str:\n     affect the choice of different kernels or attention backends should\n     also be included in the factors list.\n     \"\"\"\n+    # Fast path: return cached value if available.\n+    global _ENV_HASH_CACHE\n+    if _ENV_HASH_CACHE is not None:\n+        return _ENV_HASH_CACHE\n+\n     factors: list[Any] = []\n \n     # summarize environment variables\n     def factorize(name: str):\n-        if __getattr__(name):\n-            factors.append(__getattr__(name))\n-        else:\n-            factors.append(\"None\")\n+        val = __getattr__(name)\n+        factors.append(val if val else \"None\")\n \n     # The values of envs may affects the computation graph.\n     # TODO(DefTruth): hash all environment variables?\n@@ -1063,4 +1127,5 @@ def compute_hash() -> str:\n     hash_str = hashlib.md5(str(factors).encode(),\n                            usedforsecurity=False).hexdigest()\n \n+    _ENV_HASH_CACHE = hash_str\n     return hash_str\n", "model_name_or_path": "gpt-5-2025-08-07"}
