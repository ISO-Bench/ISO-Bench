{"instance_id": "vllm-project__vllm-5b8a1fd", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 36e5e1774..1c8683dd4 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -63,14 +63,36 @@ class NGramWorker(NonLLMProposerWorkerBase):\n         has_spec_out = False\n         token_id_list: List[Optional[torch.Tensor]] = []\n         token_prob_list: List[Optional[torch.Tensor]] = []\n+        has_cuda = (isinstance(self.device, torch.device)\n+                     and self.device.type == \"cuda\"\n+                     and torch.cuda.is_available())\n+        # Precompute arange for index generation on CPU, and lazily for GPU.\n+        arange_cpu = torch.arange(sample_len)\n+        need_gpu_arange = has_cuda and any(\n+            next(iter(sg.seq_data.values())).get_len() >= 3072\n+            for sg in execute_model_req.seq_group_metadata_list)\n+        arange_gpu = (torch.arange(sample_len, device=self.device)\n+                      if need_gpu_arange else None)\n+        vocab_size = self.vocab_size\n+\n         for idx, seq_group_metadata in enumerate(\n                 execute_model_req.seq_group_metadata_list):\n             seq_data = next(iter(seq_group_metadata.seq_data.values()))\n \n+            seq_len = seq_data.get_len()\n+            # For short sequences, do n-gram matching on CPU to reduce GPU sync overhead.\n+            cur_device = 'cpu' if seq_len < 3072 else self.device\n+            if seq_len <= 1 or (seq_len - 1) < self.ngram_prompt_lookup_min:\n+                token_id_list.append(None)\n+                token_prob_list.append(None)\n+                continue\n+\n             input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                         dtype=torch.long,\n-                                        device=self.device)\n-            input_length = seq_data.get_len()\n+                                        device=cur_device)\n+            input_length = seq_len\n+            arange_idx = (arange_cpu if cur_device == 'cpu' else\n+                           (arange_gpu if arange_gpu is not None else torch.arange(sample_len, device=self.device)))\n \n             for ngram_size in range(\n                     min(self.ngram_prompt_lookup_max, input_length - 1),\n@@ -97,16 +119,15 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 first_match = matches.max(dim=-1)\n                 if first_match.values.item():\n                     proposal_start_idx = first_match.indices.add_(ngram_size)\n-                    spec_indices = (\n-                        proposal_start_idx).repeat(sample_len) + torch.arange(\n-                            sample_len, device=self.device)\n+                    # Use broadcasting with a precomputed arange to avoid repeat() allocation.\n+                    spec_indices = proposal_start_idx + arange_idx\n                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n                     res = input_ids.gather(dim=-1, index=spec_indices)\n                     token_id_list.append(res)\n                     token_prob_list.append(\n                         torch.nn.functional.one_hot(\n                             res,\n-                            num_classes=self.vocab_size).to(torch.float32))\n+                            num_classes=vocab_size).to(torch.float32))\n                     has_spec_out = True\n                     break\n             else:\n@@ -125,9 +146,7 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                     SamplerOutput(\n                         outputs=None,\n                         sampled_token_probs=token_prob_list[idx],\n-                        logprobs=torch.zeros((sample_len, self.vocab_size),\n-                                             dtype=torch.float32,\n-                                             device=self.device),\n+                        logprobs=token_prob_list[idx].new_zeros((sample_len, vocab_size)),\n                         sampled_token_ids=token_id_list[idx],\n                     ))\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
