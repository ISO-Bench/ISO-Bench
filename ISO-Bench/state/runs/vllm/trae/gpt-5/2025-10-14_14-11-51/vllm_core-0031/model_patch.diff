diff --git a/vllm/envs.py b/vllm/envs.py
index 261cc7855..0896ae3a9 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -119,7 +119,8 @@ if TYPE_CHECKING:
     VLLM_TPU_BUCKET_PADDING_GAP: int = 0
     VLLM_TPU_MOST_MODEL_LEN: Optional[int] = None
     VLLM_USE_DEEP_GEMM: bool = False
-    VLLM_USE_FLASHINFER_MOE: bool = False
+    VLLM_USE_FLASHINFER_MOE_FP8: bool = False
+    VLLM_USE_FLASHINFER_MOE_FP4: bool = False
     VLLM_XGRAMMAR_CACHE_MB: int = 0
     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256
     VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False
@@ -854,9 +855,13 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_USE_DEEP_GEMM":
     lambda: bool(int(os.getenv("VLLM_USE_DEEP_GEMM", "0"))),
 
+    # Allow use of FlashInfer MoE kernels for fused moe ops.
+    "VLLM_USE_FLASHINFER_MOE_FP8":
+    lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE_FP8", "0"))),
+
     # Allow use of FlashInfer CUTLASS kernels for fused moe ops.
-    "VLLM_USE_FLASHINFER_MOE":
-    lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE", "0"))),
+    "VLLM_USE_FLASHINFER_MOE_FP4":
+    lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE_FP4", "0"))),
 
     # Control the cache sized used by the xgrammar compiler. The default
     # of 512 MB should be enough for roughly 1000 JSON schemas.
diff --git a/vllm/model_executor/layers/fused_moe/config.py b/vllm/model_executor/layers/fused_moe/config.py
index 9bebb6a65..f711af6bd 100644
--- a/vllm/model_executor/layers/fused_moe/config.py
+++ b/vllm/model_executor/layers/fused_moe/config.py
@@ -191,7 +191,7 @@ class FusedMoEParallelConfig:
 
     @property
     def use_flashinfer_cutlass_kernels(self):
-        return (envs.VLLM_USE_FLASHINFER_MOE
+        return ((envs.VLLM_USE_FLASHINFER_MOE_FP4 or getattr(envs, "VLLM_USE_FLASHINFER_MOE", False))
                 and has_flashinfer_cutlass_fused_moe())
 
     @staticmethod
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index aec5d7b25..d4417ecf9 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -950,9 +950,10 @@ def grouped_topk(
                                    -1).max(dim=-1).values  # [n, n_group]
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,
                            sorted=False)[1]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
-    score_mask = group_mask.unsqueeze(-1).expand(
+    n_group = group_scores.size(-1)
+    mask = (group_idx.unsqueeze(-1) == torch.arange(
+        n_group, device=group_scores.device)).any(dim=1)  # [n, n_group] bool
+    score_mask = mask.unsqueeze(-1).expand(
         num_token, num_expert_group,
         scores.size(-1) // num_expert_group).reshape(num_token, -1)  # [n, e]
     tmp_scores = scores.masked_fill(~score_mask.bool(),
@@ -1145,6 +1146,43 @@ def dispatch_fused_experts_func(inplace: bool) -> Callable[..., torch.Tensor]:
 
 
 # TODO (bnell): replace this with modular op.  Can get rid of inplace/outplace
+
+
+def flashinfer_fused_moe_blockscale_fp8(
+    hidden_states: torch.Tensor,
+    w1: torch.Tensor,
+    w2: torch.Tensor,
+    topk_weights: torch.Tensor,
+    topk_ids: torch.Tensor,
+    inplace: bool = False,
+    activation: str = "silu",
+    global_num_experts: int = -1,
+    apply_router_weight_on_input: bool = False,
+    expert_map: Optional[torch.Tensor] = None,
+    w1_scale: Optional[torch.Tensor] = None,
+    w2_scale: Optional[torch.Tensor] = None,
+    a1_scale: Optional[torch.Tensor] = None,
+    a2_scale: Optional[torch.Tensor] = None,
+    block_shape: Optional[list[int]] = None,
+):
+    # Thin wrapper to call FlashInfer's FP8 blockscale fused MoE kernel if present.
+    from vllm.utils.flashinfer import (
+        flashinfer_fused_moe_blockscale_fp8 as _fi_block_fp8)
+    return _fi_block_fp8(
+        hidden_states,
+        w1,
+        w2,
+        topk_weights,
+        topk_ids,
+        w1_scale,
+        w2_scale,
+        a1_scale,
+        a2_scale,
+        expert_map,
+        activation,
+        apply_router_weight_on_input,
+    )
+
 # torch ops.
 def fused_experts(
         hidden_states: torch.Tensor,
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 824dfe15a..b8b33ad66 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -519,13 +519,32 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                 "platform.")
 
         self.topk_indices_dtype = None
-        self.fused_experts = functools.partial(  # type: ignore
-            fused_experts,
-            use_fp8_w8a8=True,
-            block_shape=self.quant_config.weight_block_size,
-            allow_deep_gemm=self.allow_deep_gemm,
-            allow_cutlass_block_scaled_grouped_gemm=(
-                self.allow_cutlass_block_scaled_grouped_gemm))
+        # Prefer FlashInfer FP8 fused MoE when available and enabled via env.
+        self.fused_experts = None  # type: ignore
+        if (envs.VLLM_USE_FLASHINFER_MOE_FP8 and self.block_quant
+                and current_platform.is_cuda()
+                and current_platform.is_device_capability(100)):
+            try:
+                from vllm.utils.flashinfer import has_flashinfer_moe
+                if has_flashinfer_moe():
+                    from vllm.model_executor.layers.fused_moe.fused_moe import (
+                        flashinfer_fused_moe_blockscale_fp8)
+                    logger.info_once(
+                        "Using FlashInfer FP8 blockscale fused MoE kernels for Fp8MoEMethod.")
+                    self.fused_experts = functools.partial(
+                        flashinfer_fused_moe_blockscale_fp8)
+            except Exception:
+                # Fallback to default path if flashinfer is unavailable
+                self.fused_experts = None
+
+        if self.fused_experts is None:
+            self.fused_experts = functools.partial(  # type: ignore
+                fused_experts,
+                use_fp8_w8a8=True,
+                block_shape=self.quant_config.weight_block_size,
+                allow_deep_gemm=self.allow_deep_gemm,
+                allow_cutlass_block_scaled_grouped_gemm=(
+                    self.allow_cutlass_block_scaled_grouped_gemm))
 
     def create_weights(self, layer: Module, num_experts: int, hidden_size: int,
                        intermediate_size_per_partition: int,
diff --git a/vllm/model_executor/layers/quantization/modelopt.py b/vllm/model_executor/layers/quantization/modelopt.py
index 3807899fc..cf0fd8f2f 100644
--- a/vllm/model_executor/layers/quantization/modelopt.py
+++ b/vllm/model_executor/layers/quantization/modelopt.py
@@ -625,7 +625,7 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):
         round_up_multiple = lambda x, m: (x + m - 1) // m * m
         M_padded = round_up_multiple(M, 128)
         K_padded = round_up_multiple(K, 4)
-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)
         padded_scale[:B, :M, :K] = scale
         batches, rows, cols = padded_scale.shape
         assert rows % 128 == 0
@@ -633,7 +633,7 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):
         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,
                                             cols // 4, 4)
         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
-        swizzled_scale = swizzled_scale.contiguous().cuda()
+        swizzled_scale = swizzled_scale.contiguous().to(scale.device, non_blocking=True)
         return (swizzled_scale.reshape(M, K)
                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))
 
@@ -721,7 +721,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
         self.use_marlin = False
         self.allow_flashinfer_cutlass = False
 
-        if envs.VLLM_USE_FLASHINFER_MOE:
+        if envs.VLLM_USE_FLASHINFER_MOE_FP4 or getattr(envs, "VLLM_USE_FLASHINFER_MOE", False):
             if self.cutlass_nvfp4_supported and current_platform.is_cuda() \
                and current_platform.is_device_capability(100):
                 logger.info_once(
@@ -802,7 +802,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
             # Currently CutlassExpertsFp4 doesn't support DP
             raise ValueError(
                 "CutlassExpertsFp4 doesn't support DP. "
-                "Use flashinfer CUTLASS FusedMoE(VLLM_USE_FLASHINFER_MOE)"
+                "Use flashinfer CUTLASS FusedMoE(VLLM_USE_FLASHINFER_MOE_FP4)"
                 " backend instead.")
 
         return experts
@@ -914,7 +914,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
         round_up_multiple = lambda x, m: (x + m - 1) // m * m
         M_padded = round_up_multiple(M, 128)
         K_padded = round_up_multiple(K, 4)
-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)
         padded_scale[:B, :M, :K] = scale
         batches, rows, cols = padded_scale.shape
         assert rows % 128 == 0
@@ -922,7 +922,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,
                                             cols // 4, 4)
         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
-        swizzled_scale = swizzled_scale.contiguous().cuda()
+        swizzled_scale = swizzled_scale.contiguous().to(scale.device, non_blocking=True)
         return (swizzled_scale.reshape(M, K)
                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))
 
diff --git a/vllm/utils/flashinfer.py b/vllm/utils/flashinfer.py
index dbd2dc393..3c30d2b2b 100644
--- a/vllm/utils/flashinfer.py
+++ b/vllm/utils/flashinfer.py
@@ -1,4 +1,36 @@
 # SPDX-License-Identifier: Apache-2.0
+
+@functools.cache
+def has_flashinfer_moe() -> bool:
+    """Return True if any FlashInfer MoE backend is available (CUTLASS FP4 or FP8 blockscale)."""
+    if not has_flashinfer():
+        return False
+    # CUTLASS FP4 path
+    if has_flashinfer_cutlass_fused_moe():
+        return True
+    # FP8 blockscale path (try multiple potential symbol names)
+    mod = _get_submodule("flashinfer.fused_moe")
+    if not mod:
+        return False
+    return any(
+        hasattr(mod, name)
+        for name in ("blockscale_fused_moe_fp8", "fused_moe_blockscale_fp8")
+    )
+
+
+def flashinfer_fused_moe_blockscale_fp8(*args, **kwargs):
+    """Dispatch to FlashInfer FP8 blockscale fused MoE if available.
+
+    Tries known symbol names to be resilient to upstream rename.
+    """
+    mod = _get_submodule("flashinfer.fused_moe") if has_flashinfer() else None
+    if mod is not None:
+        for name in ("blockscale_fused_moe_fp8", "fused_moe_blockscale_fp8"):
+            fn = getattr(mod, name, None)
+            if fn is not None:
+                return fn(*args, **kwargs)
+    return _missing(*args, **kwargs)
+
 # SPDX-FileCopyrightText: Copyright contributors to the vLLM project
 """Compatibility wrapper for FlashInfer API changes.
 
@@ -99,9 +131,12 @@ def has_flashinfer_cutlass_fused_moe() -> bool:
 
 __all__ = [
     "has_flashinfer",
+    "has_flashinfer_moe",
+    "flashinfer_fused_moe_blockscale_fp8",
+
     "has_flashinfer_cutlass_fused_moe",
     "flashinfer_cutlass_fused_moe",
     "fp4_quantize",
     "fp4_swizzle_blockscale",
     "autotune",
-]
+]
\ No newline at end of file
