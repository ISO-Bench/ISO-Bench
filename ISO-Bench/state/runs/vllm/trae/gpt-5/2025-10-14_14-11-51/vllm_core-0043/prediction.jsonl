{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..0cfaec59f 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -83,6 +83,7 @@ if TYPE_CHECKING:\n     VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n     VLLM_SKIP_P2P_CHECK: bool = False\n     VLLM_DISABLED_KERNELS: list[str] = []\n+    VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE: bool = False\n     VLLM_USE_V1: bool = True\n     VLLM_ROCM_USE_AITER: bool = False\n     VLLM_ROCM_USE_AITER_PAGED_ATTN: bool = False\n@@ -650,6 +651,11 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_USE_V1\":\n     lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"1\"))),\n \n+    # Allow enabling hybrid KV cache manager with chunked local attention.\n+    # Disabled by default due to latency regressions.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: bool(int(os.getenv(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\"))),\n+\n     # Disable aiter ops unless specifically enabled.\n     # Acts as a parent switch to enable the rest of the other operations.\n     \"VLLM_ROCM_USE_AITER\":\n@@ -996,10 +1002,23 @@ environment_variables: dict[str, Callable[[], Any]] = {\n # --8<-- [end:env-vars-definition]\n \n \n+# Cache for frequently accessed env variables to avoid repeated os.getenv calls\n+_ENV_VALUE_CACHE: dict[str, Any] = {}\n+# Only cache values that are not expected to change at runtime\n+_CACHED_ENV_VARS: set[str] = {\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\",\n+    \"VLLM_USE_V1\",\n+}\n+\n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation of environment variables with caching for hot keys\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        if name in _ENV_VALUE_CACHE:\n+            return _ENV_VALUE_CACHE[name]\n+        value = environment_variables[name]()\n+        if name in _CACHED_ENV_VARS:\n+            _ENV_VALUE_CACHE[name] = value\n+        return value\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n@@ -1021,6 +1040,8 @@ def set_vllm_use_v1(use_v1: bool):\n             \"explicitly by the user. Please raise this as a Github \"\n             \"Issue and explicitly set VLLM_USE_V1=0 or 1.\")\n     os.environ[\"VLLM_USE_V1\"] = \"1\" if use_v1 else \"0\"\n+    # keep cache consistent\n+    _ENV_VALUE_CACHE[\"VLLM_USE_V1\"] = use_v1\n \n \n def compute_hash() -> str:\n", "model_name_or_path": "gpt-5-2025-08-07"}
