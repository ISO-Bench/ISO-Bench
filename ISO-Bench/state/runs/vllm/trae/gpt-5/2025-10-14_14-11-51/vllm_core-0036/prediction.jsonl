{"instance_id": "vllm-project__vllm-f168b85", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..75a4ed7a0 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -29,7 +29,7 @@ from vllm.distributed.parallel_state import get_pp_group\n from vllm.forward_context import get_forward_context\n from vllm.model_executor.layers.activation import ReLUSquaredActivation\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -63,26 +63,31 @@ class NemotronHMLP(nn.Module):\n         config: NemotronHConfig,\n         quant_config: Optional[QuantizationConfig] = None,\n         bias: bool = False,\n+        prefix: str = \"\",\n     ) -> None:\n         super().__init__()\n-        self.up_proj = MergedColumnParallelLinear(\n+        self.up_proj = ColumnParallelLinear(\n             input_size=config.hidden_size,\n-            output_sizes=[config.intermediate_size],\n+            output_size=config.intermediate_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"up_proj\"),\n+            return_bias=False,\n         )\n         self.down_proj = RowParallelLinear(\n             input_size=config.intermediate_size,\n             output_size=config.hidden_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"down_proj\"),\n+            return_bias=False,\n         )\n         self.act_fn = ReLUSquaredActivation()\n \n     def forward(self, x: torch.Tensor):\n-        x, _ = self.up_proj(x)\n+        x = self.up_proj(x)\n         x = self.act_fn(x)\n-        x, _ = self.down_proj(x)\n+        x = self.down_proj(x)\n         return x\n \n \n@@ -101,7 +106,8 @@ class NemotronHMLPDecoderLayer(nn.Module):\n \n         self.mixer = NemotronHMLP(config,\n                                   quant_config=quant_config,\n-                                  bias=config.mlp_bias)\n+                                  bias=config.mlp_bias,\n+                                  prefix=maybe_prefix(prefix, \"mixer\"))\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -198,6 +204,7 @@ class NemotronHAttention(nn.Module):\n         self.head_dim = config.hidden_size // self.total_num_heads\n         self.q_size = self.num_heads * self.head_dim\n         self.kv_size = self.num_kv_heads * self.head_dim\n+        self._qkv_split_sizes = (self.q_size, self.kv_size, self.kv_size)\n         self.scaling = self.head_dim**-0.5\n \n         self.qkv_proj = QKVParallelLinear(\n@@ -207,12 +214,16 @@ class NemotronHAttention(nn.Module):\n             self.total_num_kv_heads,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"qkv_proj\"),\n+            return_bias=False,\n         )\n         self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             config.hidden_size,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=maybe_prefix(prefix, \"o_proj\"),\n+            return_bias=False,\n         )\n \n         self.attn = Attention(\n@@ -229,10 +240,10 @@ class NemotronHAttention(nn.Module):\n         hidden_states: torch.Tensor,\n         **kwargs,\n     ) -> torch.Tensor:\n-        qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        qkv = self.qkv_proj(hidden_states)\n+        q, k, v = qkv.split(self._qkv_split_sizes, dim=-1)\n         attn_output = self.attn(q, k, v)\n-        output, _ = self.o_proj(attn_output)\n+        output = self.o_proj(attn_output)\n         return output\n \n \n@@ -302,6 +313,7 @@ class NemotronHModel(nn.Module):\n             self.vocab_size,\n             config.hidden_size,\n             org_num_embeddings=config.vocab_size,\n+            prefix=maybe_prefix(prefix, \"embed_tokens\"),\n         )\n \n         def get_layer(prefix: str):\n@@ -473,6 +485,7 @@ class NemotronHForCausalLM(nn.Module, HasInnerState, SupportsLoRA, SupportsPP,\n             # We need bigger padding if using lora for kernel\n             # compatibility\n             if not lora_config else lora_config.lora_vocab_padding_size,\n+            prefix=maybe_prefix(prefix, \"lm_head\"),\n         )\n         # Used to track and store by the Mamba cache between steps.\n         self.mamba_cache: Optional[MambaCacheManager] = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
