{
  "changed": [
    "model_patch.diff",
    "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py"
  ],
  "allowed": [
    "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py"
  ],
  "disallowed": [
    "model_patch.diff"
  ],
  "ok": false
}