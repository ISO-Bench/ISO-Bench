{"instance_id": "vllm-project__vllm-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 24ddd35ab..2a3026815 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -50,11 +50,12 @@ def decode_tokens(\n     `skip_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n-    if skip_special_tokens is not None:\n-        return tokenizer.decode(token_ids,\n-                                skip_special_tokens=skip_special_tokens)\n+    # Prefer a potential fast-path implementation when no extra options\n+    if skip_special_tokens is None:\n+        decode_method = getattr(tokenizer, \"_decode\", tokenizer.decode)\n+        return decode_method(token_ids)\n \n-    return tokenizer.decode(token_ids)\n+    return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n \n \n def encode_tokens(\n@@ -73,6 +74,11 @@ def encode_tokens(\n     settings.\n     \"\"\"\n \n+    # Fast path: no options provided, prefer potential optimized implementation\n+    if truncation is None and max_length is None and add_special_tokens is None:\n+        encode_method = getattr(tokenizer, \"_encode\", tokenizer.encode)\n+        return encode_method(text)\n+\n     kw_args: dict[str, Any] = {}\n     if max_length is not None:\n         kw_args[\"max_length\"] = max_length\n@@ -213,7 +219,7 @@ def get_tokenizer(\n         tokenizer_name = Path(tokenizer_name).parent\n \n     # if tokenizer is from official mistral org\n-    is_from_mistral_org = str(tokenizer_name).split(\"/\")[0] == \"mistralai\"\n+    is_from_mistral_org = str(tokenizer_name).startswith(\"mistralai/\")\n     if is_from_mistral_org and tokenizer_mode != \"mistral\":\n         warnings.warn(\n             'It is strongly recommended to run mistral models with '\n", "model_name_or_path": "gpt-5-2025-08-07"}
