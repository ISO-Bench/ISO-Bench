{"instance_id": "vllm-project__vllm-bd43973", "model_patch": "diff --git a/Dockerfile.cpu b/Dockerfile.cpu\nindex 403a1cd03..1bfd5609e 100644\n--- a/Dockerfile.cpu\n+++ b/Dockerfile.cpu\n@@ -3,11 +3,17 @@\n FROM ubuntu:22.04 AS cpu-test-1\n \n RUN apt-get update  -y \\\n-    && apt-get install -y git wget vim numactl gcc-12 g++-12 python3 python3-pip \\\n+    && apt-get install -y git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 \\\n     && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n \n+# Use tcmalloc for improved CPU memory allocation performance\n+RUN echo 'export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD' >> ~/.bashrc\n+\n RUN pip install --upgrade pip \\\n     && pip install wheel packaging ninja \"setuptools>=49.4.0\" numpy\n+# Optional: install Intel Extension for PyTorch for optimized CPU kernels\n+RUN pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.3.100%2Bgit0eb3473-cp310-cp310-linux_x86_64.whl || true\n+\n \n FROM cpu-test-1 AS build\n \n@@ -21,6 +27,6 @@ RUN VLLM_TARGET_DEVICE=cpu python3 setup.py install\n \n WORKDIR /workspace/\n \n-RUN ln -s /workspace/vllm/tests  && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n+RUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n \n CMD [\"/bin/bash\"]\ndiff --git a/README.md b/README.md\nindex 57374d279..e7824924e 100644\n--- a/README.md\n+++ b/README.md\n@@ -89,6 +89,8 @@ Visit our [documentation](https://vllm.readthedocs.io/en/latest/) to learn more.\n - [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n - [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n \n+  - CPU users: see the CPU installation guide and performance tips for best results: https://vllm.readthedocs.io/en/latest/getting_started/cpu-installation.html\n+\n ## Contributing\n \n We welcome and value any contributions and collaborations.\ndiff --git a/docs/source/getting_started/cpu-installation.rst b/docs/source/getting_started/cpu-installation.rst\nindex 5270253ca..0699be7d2 100644\n--- a/docs/source/getting_started/cpu-installation.rst\n+++ b/docs/source/getting_started/cpu-installation.rst\n@@ -85,3 +85,21 @@ Performance tips\n \n \n \n+- Use a high-performance memory allocator on CPU. On Ubuntu, install `libtcmalloc-minimal4` and export it via LD_PRELOAD, e.g.::\n+\n+    export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD\n+\n+- Consider installing Intel Extension for PyTorch (IPEX) for optimized CPU kernels. Example (PyTorch 2.3, CPU wheels):\n+\n+  .. code-block:: console\n+\n+      $ pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.3.100%2Bgit0eb3473-cp310-cp310-linux_x86_64.whl\n+\n+- Tune OpenMP environment variables to match your hardware and workload. For example::\n+\n+    export OMP_NUM_THREADS=<num-cores>\n+    export KMP_AFFINITY=granularity=fine,compact,1,0\n+\n+- Pin CPU cores and memory locality when running on multi-socket NUMA systems (e.g., using `numactl` or Docker `--cpuset-cpus/--cpuset-mems`).\n+\n+\ndiff --git a/requirements-cpu.txt b/requirements-cpu.txt\nindex b739642d8..347b2b30e 100644\n--- a/requirements-cpu.txt\n+++ b/requirements-cpu.txt\n@@ -2,5 +2,5 @@\n -r requirements-common.txt\n \n # Dependencies for x86_64 CPUs\n-torch == 2.3.0+cpu\n-triton >= 2.2.0  # FIXME(woosuk): This is a hack to avoid import error.\n\\ No newline at end of file\n+torch==2.3.0+cpu\n+triton>=2.2.0  # FIXME(woosuk): This is a hack to avoid import error.\n\\ No newline at end of file\ndiff --git a/vllm/attention/backends/torch_sdpa.py b/vllm/attention/backends/torch_sdpa.py\nindex 9b50adec5..9bbf98049 100644\n--- a/vllm/attention/backends/torch_sdpa.py\n+++ b/vllm/attention/backends/torch_sdpa.py\n@@ -11,6 +11,12 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n from vllm.attention.ops.paged_attn import (PagedAttention,\n                                            PagedAttentionMetadata)\n \n+# Optional optimized SDPA path for CPU (e.g., IPEX or contiguous fastpath)\n+try:\n+    from vllm.attention.ops.ipex_attn import sdpa_optimized as _sdpa_opt\n+except Exception:  # pragma: no cover - optional optimization\n+    _sdpa_opt = None\n+\n \n class TorchSDPABackend(AttentionBackend):\n \n@@ -176,11 +182,11 @@ class TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n                     if self.alibi_slopes is not None:\n                         att_masks = _make_alibi_bias(\n                             self.alibi_slopes, query.dtype,\n-                            attn_metadata.seq_lens)  # type: ignore\n+                            attn_metadata.seq_lens, query.device)  # type: ignore\n                     elif self.sliding_window is not None:\n                         att_masks = _make_sliding_window_bias(\n                             attn_metadata.seq_lens, self.sliding_window,\n-                            query.dtype)  # type: ignore\n+                            query.dtype, query.device)  # type: ignore\n                     else:\n                         att_masks = [None] * len(attn_metadata.seq_lens)\n                     attn_metadata.attn_bias = att_masks\n@@ -192,11 +198,13 @@ class TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n                 start = 0\n                 output = torch.empty(\n                     (num_tokens, self.num_heads, self.head_size),\n-                    dtype=query.dtype)\n+                    dtype=query.dtype,\n+                    device=query.device)\n+                sdpa_fn = _sdpa_opt if _sdpa_opt is not None else scaled_dot_product_attention\n                 for seq_len, mask in zip(attn_metadata.seq_lens,\n                                          attn_metadata.attn_bias):\n                     end = start + seq_len\n-                    sub_out = scaled_dot_product_attention(\n+                    sub_out = sdpa_fn(\n                         query[:, start:end, :],\n                         key[:, start:end, :],\n                         value[:, start:end, :],\n@@ -235,23 +243,25 @@ def _make_alibi_bias(\n     alibi_slopes: torch.Tensor,\n     dtype: torch.dtype,\n     seq_lens: List[int],\n+    device: Optional[torch.device] = None,\n ) -> List[torch.Tensor]:\n+    if device is None:\n+        device = alibi_slopes.device\n     attn_biases = []\n+    # prevent dtype/device conversion each loop\n+    slopes = alibi_slopes.to(dtype=dtype, device=device)\n     for seq_len in seq_lens:\n-        bias = torch.arange(seq_len, dtype=dtype)\n-        # NOTE(zhuohan): HF uses\n-        #     `bias = bias[None, :].repeat(seq_len, 1)`\n-        # here. We find that both biases give the same results, but\n-        # the bias below more accurately follows the original ALiBi\n-        # paper.\n-        bias = bias[None, :] - bias[:, None]\n-\n-        num_heads = alibi_slopes.shape[0]\n-        bias = bias[None, :].repeat((num_heads, 1, 1))\n-        bias.mul_(alibi_slopes[:, None, None])\n-        inf_mask = torch.empty(\n+        line = torch.arange(seq_len, dtype=dtype, device=device)\n+        # bias[i, j] = j - i\n+        bias = line[None, :] - line[:, None]\n+        # Broadcast multiply instead of repeat\n+        bias = bias.unsqueeze(0) * slopes[:, None, None]\n+        inf_mask = torch.full(\n             (1, seq_len, seq_len),\n-            dtype=bias.dtype).fill_(-torch.inf).triu_(diagonal=1)\n+            fill_value=-torch.inf,\n+            dtype=dtype,\n+            device=device,\n+        ).triu_(diagonal=1)\n         attn_biases.append((bias + inf_mask).to(dtype))\n \n     return attn_biases\n@@ -264,10 +274,9 @@ def _make_sliding_window_bias(\n ) -> List[torch.Tensor]:\n     attn_biases = []\n     for seq_len in seq_lens:\n-        tensor = torch.full(\n+        tensor = torch.ones(\n             (1, seq_len, seq_len),\n             dtype=dtype,\n-            fill_value=1,\n         )\n         shift = 0\n         mask = torch.tril(tensor, diagonal=shift).to(dtype)  # type: ignore\ndiff --git a/vllm/attention/ops/ipex_attn.py b/vllm/attention/ops/ipex_attn.py\nnew file mode 100644\nindex 000000000..60f7532f1\n--- /dev/null\n+++ b/vllm/attention/ops/ipex_attn.py\n@@ -0,0 +1,42 @@\n+\"\"\"\n+Optional CPU-optimized attention ops.\n+\n+This module provides a thin wrapper around torch.nn.functional.scaled_dot_product_attention\n+that ensures inputs are contiguous and leverages any available CPU optimizations\n+(e.g., IPEX) when installed. If IPEX is not available, this simply falls back to\n+PyTorch's implementation with minimal overhead.\n+\"\"\"\n+from typing import Optional\n+\n+import torch\n+from torch.nn.functional import scaled_dot_product_attention as _sdpa\n+\n+try:  # optional dependency\n+    import intel_extension_for_pytorch as ipex  # noqa: F401\n+    _HAS_IPEX = True\n+except Exception:  # pragma: no cover\n+    _HAS_IPEX = False\n+\n+\n+def _maybe_contiguous(x: torch.Tensor) -> torch.Tensor:\n+    return x if x.is_contiguous() else x.contiguous()\n+\n+\n+def sdpa_optimized(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attn_mask: Optional[torch.Tensor] = None,\n+    dropout_p: float = 0.0,\n+    is_causal: bool = False,\n+    scale: Optional[float] = None,\n+) -> torch.Tensor:\n+    # Ensure layouts are friendly for kernels\n+    q = _maybe_contiguous(query)\n+    k = _maybe_contiguous(key)\n+    v = _maybe_contiguous(value)\n+\n+    # If IPEX is present and running on CPU, the underlying kernels may be optimized.\n+    # We don't need special handling here; enabling IPEX generally patches PyTorch ops.\n+    return _sdpa(q, k, v, attn_mask=attn_mask, dropout_p=dropout_p,\n+                 is_causal=is_causal, scale=scale)\n", "model_name_or_path": "gpt-5-2025-08-07"}
