{"instance_id": "vllm-project__vllm-029c71d", "model_patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..94d1a4d6d 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -708,6 +708,42 @@ class TestPrefixCachingBlockAllocator:\n                                                token_ids=token_ids)\n         assert allocator.get_prefix_cache_hit_rate() > 0.99\n \n+    # Test case for marking cache hit blocks as computed right after\n+    # a batch of prefill sequences are scheduled.\n+    @staticmethod\n+    def test_touch_block():\n+        block_size = 16\n+        common_blocks = 4\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+\n+        common_token_ids = list(range(block_size * common_blocks))\n+\n+        # Mimic the behavior of allocating the same block chain\n+        # (i.e., common prefix) for a batch of 3 different prefill sequences.\n+        first_chain = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+            block_size=block_size,\n+            token_ids=common_token_ids,\n+            allocator=allocator,\n+        )\n+        # Record the block ids from the first allocation\n+        first_block_ids = [block.block_id for block in first_chain]\n+\n+        # Allocate two more chains sharing the same prefix\n+        for _ in range(2):\n+            _ = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+                block_size=block_size,\n+                token_ids=common_token_ids,\n+                allocator=allocator,\n+            )\n+\n+        # The blocks from the first chain should be touched (tracked) but\n+        # not computed yet.\n+        for bid in first_block_ids:\n+            assert allocator._block_tracker[bid].active\n+            assert allocator._block_tracker[bid].computed is False\n+\n+\n     @staticmethod\n     def create_immutable_chain(\n         block_size: int,\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..0955dc16c 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -94,6 +94,12 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             block_pool=self._block_pool,  # Share block pool here\n         )\n \n+        # Precompute absolute->physical id mapping for faster lookups\n+        self._abs_to_phys_map = {\n+            abs_id: idx\n+            for idx, abs_id in enumerate(sorted(self._hashless_allocator.all_block_ids))\n+        }\n+\n         # Evitor used to maintain how we want to handle those computed blocks\n         # if we find memory pressure is high.\n         self.evictor: Evictor = make_evictor(eviction_policy)\n@@ -401,7 +407,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         Returns:\n             int: The rzero-offset block id on certain device.\n         \"\"\"\n-        return sorted(self.all_block_ids).index(absolute_id)\n+        return self._abs_to_phys_map[absolute_id]\n \n     @property\n     def all_block_ids(self) -> FrozenSet[int]:\n@@ -490,13 +496,16 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         return self._cow_tracker.clear_cows()\n \n     def mark_blocks_as_accessed(self, block_ids: List[int],\n-                                now: float) -> None:\n+                                 now: float) -> None:\n         \"\"\"Mark blocks as accessed, used in prefix caching.\n \n         If the block is added into evictor, we need to update corresponding\n         info in evictor's metadata.\n         \"\"\"\n \n+        if not block_ids:\n+            return\n+\n         for block_id in block_ids:\n             if self._block_tracker[block_id].active:\n                 self._block_tracker[block_id].last_accessed = now\n@@ -507,7 +516,16 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        # Fast path: nothing to mark\n+        if not block_ids:\n+            return\n+\n+        # Mark provided block ids as computed in the tracker. This is used by\n+        # prefix caching to avoid redundant scans in later scheduler passes.\n+        for block_id in block_ids:\n+            tracker = self._block_tracker.get(block_id)\n+            if tracker is not None and tracker.active:\n+                tracker.computed = True\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\n@@ -543,6 +561,8 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             block_id = block_ids[i]\n             if self.block_is_computed(block_id):\n                 ret.append(block_id)\n+            else:\n+                break\n         return ret\n \n     def get_common_computed_block_ids(\n@@ -840,7 +860,7 @@ class PrefixCachingBlock(Block):\n         - int: The computed hash value for the block.\n         \"\"\"\n         assert (prev_block_hash is None) == is_first_block\n-        return hash((is_first_block, prev_block_hash, *cur_block_token_ids))\n+        return hash((is_first_block, prev_block_hash, tuple(cur_block_token_ids)))\n \n \n class ComputedBlocksTracker:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..0bf4bb9dd 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,21 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+-        # The only need for mark block as computed is for prefix caching,\n+-        # while currently we could determine whether one block is computed\n+-        # or not by check whether it has content hash.\n+-        # So this function is useless for block_v2.\n+-        pass\n++        # Mark full blocks in scheduled sequences as computed to\n++        # accelerate subsequent prefix-caching checks.\n++        if not self.enable_caching:\n++            return\n++\n++        for seq in seq_group.get_seqs():\n++            block_ids = self.block_tables[seq.seq_id].physical_block_ids\n++            # Exclude the last block (may be partial) to avoid caching a full seq\n++            if len(block_ids) > 1:\n++                self.block_allocator.mark_blocks_as_computed(block_ids[:-1])\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
