{"instance_id": "vllm-project__vllm-0e74d79", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..9d6c476e9 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -455,7 +455,6 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                       is not None)\n         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n-        self.decode_only = True\n \n         # Attention metadata inputs.\n         if self.attn_backend is not None:\n@@ -477,6 +476,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 finished_requests_ids: Optional[List[str]] = None) -> None:\n         self.finished_requests_ids = finished_requests_ids\n \n+        # if the current batch is decode-only.\n+        # will be set to False if there is any non-decode request.\n+        self.decode_only = True\n+\n         # Intermediate data (data in CPU before going to GPU) for\n         # the current sequence group.\n         self.inter_data_list: List[\n@@ -1322,7 +1325,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n             # multiplying the list, to avoid Dynamo from treating them as\n             # tensor aliasing.\n             kv_caches = [\n-                torch.tensor([], dtype=torch.float32, device=self.device)\n+                torch.empty((0,), dtype=torch.float32, device=self.device)\n                 for _ in range(num_layers)\n             ]\n             finished_requests_ids = [seq.request_id for seq in seqs]\n@@ -1440,12 +1443,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         input_tokens = torch.zeros(max_batch_size,\n                                    dtype=torch.long,\n                                    device=self.device)\n-        input_positions = torch.zeros(max_batch_size,\n-                                      dtype=torch.long,\n-                                      device=self.device)\n         if self.model_config.uses_mrope:\n-            input_positions = torch.tile(input_positions,\n-                                         (3, 1)).cuda(device=self.device)\n+            input_positions = torch.zeros(1, dtype=torch.long, device=self.device).expand(3, max_batch_size)\n+        else:\n+            input_positions = torch.zeros(1, dtype=torch.long, device=self.device).expand(max_batch_size)\n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\n         previous_hidden_states = None\n@@ -1568,10 +1569,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         \"\"\"\n         # During the decode phase encoder_input_ids and encoder_positions are\n         # unset. Do the same thing for graph capture.\n-        capture_inputs[\"encoder_input_ids\"] = torch.tensor([],\n+        capture_inputs[\"encoder_input_ids\"] = torch.empty((0,),\n                                                            dtype=torch.long,\n                                                            device=self.device)\n-        capture_inputs[\"encoder_positions\"] = torch.tensor([],\n+        capture_inputs[\"encoder_positions\"] = torch.empty((0,),\n                                                            dtype=torch.long,\n                                                            device=self.device)\n \n@@ -1750,8 +1751,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                     model_forward_end)\n                 orig_model_forward_time = 0.0\n                 if intermediate_tensors is not None:\n-                    orig_model_forward_time = intermediate_tensors.tensors.get(\n-                        \"model_forward_time\", torch.tensor(0.0)).item()\n+                    t = intermediate_tensors.tensors.get(\"model_forward_time\")\n+                    if t is not None:\n+                        orig_model_forward_time = t.item()\n                 hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                     torch.tensor(model_forward_time + orig_model_forward_time))\n             return hidden_or_intermediate_states\n@@ -1778,8 +1780,9 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n                 model_forward_end)\n             orig_model_forward_time = 0.0\n             if intermediate_tensors is not None:\n-                orig_model_forward_time = intermediate_tensors.tensors.get(\n-                    \"model_forward_time\", torch.tensor(0.0)).item()\n+                t = intermediate_tensors.tensors.get(\"model_forward_time\")\n+                if t is not None:\n+                    orig_model_forward_time = t.item()\n             # If there are multiple workers, we are still tracking the latency\n             # from the start time of the driver worker to the end time of the\n             # driver worker. The model forward time will then end up covering\n", "model_name_or_path": "gpt-5-2025-08-07"}
