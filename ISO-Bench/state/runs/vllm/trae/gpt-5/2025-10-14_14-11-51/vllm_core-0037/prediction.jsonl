{"instance_id": "vllm-project__vllm-51e971d", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 13746cef2..3e58ea914 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -39,46 +39,41 @@ PromptLogprobs = List[Optional[Dict[int, Logprob]]]\n SampleLogprobs = List[Dict[int, Logprob]]\n \n \n-class SequenceStatus(enum.Enum):\n+class SequenceStatus(enum.IntEnum):\n     \"\"\"Status of a sequence.\"\"\"\n-    WAITING = enum.auto()\n-    RUNNING = enum.auto()\n-    SWAPPED = enum.auto()\n-    FINISHED_STOPPED = enum.auto()\n-    FINISHED_LENGTH_CAPPED = enum.auto()\n-    FINISHED_ABORTED = enum.auto()\n-    FINISHED_IGNORED = enum.auto()\n+    WAITING = 0\n+    RUNNING = 1\n+    SWAPPED = 2\n+    # Note: anything after SWAPPED (2) will be considered\n+    # as a finished status.\n+    FINISHED_STOPPED = 3\n+    FINISHED_LENGTH_CAPPED = 4\n+    FINISHED_ABORTED = 5\n+    FINISHED_IGNORED = 6\n \n     @staticmethod\n     def is_finished(status: \"SequenceStatus\") -> bool:\n-        return status in [\n-            SequenceStatus.FINISHED_STOPPED,\n-            SequenceStatus.FINISHED_LENGTH_CAPPED,\n-            SequenceStatus.FINISHED_ABORTED,\n-            SequenceStatus.FINISHED_IGNORED,\n-        ]\n+        return status > SequenceStatus.SWAPPED\n \n     @staticmethod\n     def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n-        if status == SequenceStatus.FINISHED_STOPPED:\n-            finish_reason = \"stop\"\n-        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n-            finish_reason = \"length\"\n-        elif status == SequenceStatus.FINISHED_ABORTED:\n-            finish_reason = \"abort\"\n-        elif status == SequenceStatus.FINISHED_IGNORED:\n-            # The ignored sequences are the sequences whose prompt lengths\n-            # are longer than the model's length cap. Therefore, the stop\n-            # reason should also be \"length\" as in OpenAI API.\n-            finish_reason = \"length\"\n-        else:\n-            finish_reason = None\n-        return finish_reason\n+        return _FINISH_REASON_MAP.get(status)\n+\n \n+# Map finished statuses to OpenAI-compatible finish reasons.\n+_FINISH_REASON_MAP = {\n+    SequenceStatus.FINISHED_STOPPED: \"stop\",\n+    SequenceStatus.FINISHED_LENGTH_CAPPED: \"length\",\n+    SequenceStatus.FINISHED_ABORTED: \"abort\",\n+    # The ignored sequences are the sequences whose prompt lengths are\n+    # longer than the model's length cap. Therefore, the stop reason should\n+    # also be \"length\" as in OpenAI API.\n+    SequenceStatus.FINISHED_IGNORED: \"length\",\n+}\n \n-class SequenceStage(enum.Enum):\n-    PREFILL = enum.auto()\n-    DECODE = enum.auto()\n+class SequenceStage(enum.IntEnum):\n+    PREFILL = 0\n+    DECODE = 1\n \n \n @dataclass\n@@ -186,9 +181,7 @@ class SequenceData:\n         return self.get_len() - self.get_num_computed_tokens()\n \n     def get_last_token_id(self) -> int:\n-        if not self.output_token_ids:\n-            return self.prompt_token_ids[-1]\n-        return self.output_token_ids[-1]\n+        return (self.output_token_ids or self.prompt_token_ids)[-1]\n \n     def get_prompt_token_ids(self) -> List[int]:\n         return self.prompt_token_ids\n@@ -247,7 +240,7 @@ class Sequence:\n \n     @property\n     def n_blocks(self) -> int:\n-        return math.ceil(self.get_len() / self.block_size)\n+        return (self.get_len() + self.block_size - 1) // self.block_size\n \n     @property\n     def prompt(self) -> Optional[str]:\n@@ -525,11 +518,11 @@ class SequenceGroup:\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n     def get_num_uncomputed_tokens(self) -> int:\n-        num_uncomputed_tokens = 0\n-        for seq in self.get_seqs():\n-            if not seq.is_finished():\n-                num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n-        return num_uncomputed_tokens\n+        return sum(\n+            seq.data.get_num_uncomputed_tokens()\n+            for seq in self.seqs_dict.values()\n+            if not seq.is_finished()\n+        )\n \n     def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:\n         # Optimization. We don't need to call get_seqs if we don't need to\n@@ -561,11 +554,11 @@ class SequenceGroup:\n         del self.seqs_dict[seq_id]\n \n     def is_finished(self) -> bool:\n-        return all(seq.is_finished() for seq in self.get_seqs())\n+        return all(seq.is_finished() for seq in self.seqs_dict.values())\n \n     def is_prefill(self) -> bool:\n         # Every sequence should be in the same stage.\n-        return self.get_seqs()[0].is_prefill()\n+        return next(iter(self.seqs_dict.values())).is_prefill()\n \n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\n@@ -857,7 +850,8 @@ class HiddenStates:\n         seq_ids = get_all_seq_ids(seq_group_metadata_list)\n         if seq_ids != self.seq_ids:\n             # Batch contents changed - prune removed sequences.\n-            index = [self.seq_ids.index(seq_id) for seq_id in seq_ids]\n+            idx_map = {sid: i for i, sid in enumerate(self.seq_ids)}\n+            index = [idx_map[sid] for sid in seq_ids]\n             self.hidden_states = self.hidden_states[index]\n             self.seq_ids = seq_ids\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
