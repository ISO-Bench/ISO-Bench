{"instance_id": "vllm-project__vllm-5c04bb8", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex 5904ad1f1..8caf9dfc5 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -198,7 +198,7 @@ class Qwen2_5_VisionMLP(nn.Module):\n def all_gather_interleave(local_tensor, hidden_size: int, tp_size: int):\n     \"\"\"All-gather the input tensor interleavely across model parallel group.\"\"\"\n     import torch.distributed as dist\n-    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(tp_size)]\n+    gathered_tensors = [torch.empty_like(local_tensor) for _ in range(tp_size)]\n     dist.all_gather(gathered_tensors,\n                     local_tensor,\n                     group=parallel_state.get_tp_group().device_group)\n@@ -478,8 +478,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         super().__init__()\n         self.dim = dim\n         self.theta = theta\n-        inv_freq = 1.0 / (theta\n-                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        inv_freq = 1.0 / (theta**(\n+            torch.arange(0, dim, 2, dtype=torch.float, device='cpu') / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self._seq_len_cached = 0\n         self._freqs_cached = None\n@@ -488,9 +488,7 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         if seqlen > self._seq_len_cached:\n             seqlen *= 2\n             self._seq_len_cached = seqlen\n-            self.inv_freq = 1.0 / (self.theta**(torch.arange(\n-                0, self.dim, 2, dtype=torch.float, device=self.inv_freq.device)\n-                                                / self.dim))\n+\n             seq = torch.arange(seqlen,\n                                device=self.inv_freq.device,\n                                dtype=self.inv_freq.dtype)\n@@ -570,8 +568,8 @@ class Qwen2_5_VisionTransformer(nn.Module):\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         pos_ids = []\n         for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+            hpos_ids = torch.arange(h, device=self.device).unsqueeze(1).expand(-1, w)\n+            wpos_ids = torch.arange(w, device=self.device).unsqueeze(0).expand(h, -1)\n             hpos_ids = hpos_ids.reshape(\n                 h // self.spatial_merge_size,\n                 self.spatial_merge_size,\n@@ -587,7 +585,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n@@ -620,8 +618,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             index_padded = index_padded.reshape(-1)\n             index_new = index_padded[index_padded != -100]\n             window_index.append(index_new + window_index_id)\n-            cu_seqlens_tmp = seqlens.cumsum(\n-                0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n+            cu_seqlens_tmp = seqlens.cumsum(0, dtype=torch.int32) * self.spatial_merge_unit + cu_window_seqlens[-1]\n             cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n             window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n         window_index = torch.cat(window_index, dim=0)\n", "model_name_or_path": "gpt-5-2025-08-07"}
