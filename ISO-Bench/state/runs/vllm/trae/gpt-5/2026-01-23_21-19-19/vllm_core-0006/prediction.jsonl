{"instance_id": "vllm-project__vllm-bc8a8ce", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e8e764d\n--- /dev/null\n+++ b/model_patch.diff\n@@ -0,0 +1,46 @@\n+diff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py\n+index 3a4c259..347a851 100644\n+--- a/vllm/v1/sample/logits_processor.py\n++++ b/vllm/v1/sample/logits_processor.py\n+@@ -246,6 +246,8 @@ class MinPLogitsProcessor(LogitsProcessor):\n+             self.min_p_device = self.min_p_cpu_tensor\n+         # Current slice of the device tensor\n+         self.min_p: torch.Tensor = self.min_p_device[:0]\n++        # Cached log(min_p) slice for efficient masking\n++        self.log_min_p: torch.Tensor = self.min_p_device[:0]\n+ \n+     def is_argmax_invariant(self) -> bool:\n+         \"\"\"Min-p never impacts greedy sampling\"\"\"\n+@@ -294,23 +296,20 @@ class MinPLogitsProcessor(LogitsProcessor):\n+                 self.min_p.copy_(self.min_p_cpu_tensor[:size],\n+                                  non_blocking=True)\n+             self.min_p.unsqueeze_(1)\n++            # Cache log(min_p) for use during apply; sequences with min_p==0\n++            # will produce -inf which results in no masking for that row.\n++            self.log_min_p = self.min_p.log()\n+ \n+     def apply(self, logits: torch.Tensor) -> torch.Tensor:\n+         if not self.min_p_count:\n+             return logits\n+ \n+-        # Convert logits to probability distribution\n+-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n+-        # Calculate maximum probabilities per sequence\n+-        max_probabilities = torch.amax(probability_values,\n+-                                       dim=-1,\n+-                                       keepdim=True)\n+-        # Adjust min_p\n+-        adjusted_min_p = max_probabilities.mul_(self.min_p)\n+-        # Identify valid tokens using threshold comparison\n+-        invalid_token_mask = probability_values < adjusted_min_p\n+-        # Apply mask using boolean indexing\n+-        logits[invalid_token_mask] = -float('inf')\n++        # Identify tokens that fall below the relative threshold in logit space:\n++        # p_i < min_p * p_max  <=>  (l_i - l_max) < log(min_p)\n++        max_logits = torch.amax(logits, dim=-1, keepdim=True)\n++        delta = logits - max_logits\n++        invalid_token_mask = delta < self.log_min_p\n++        logits.masked_fill_(invalid_token_mask, -float('inf'))\n+         return logits\n+ \n+ \ndiff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py\nindex 347a851..5d9cfbe 100644\n--- a/vllm/v1/sample/logits_processor.py\n+++ b/vllm/v1/sample/logits_processor.py\n@@ -304,12 +304,10 @@ class MinPLogitsProcessor(LogitsProcessor):\n         if not self.min_p_count:\n             return logits\n \n-        # Identify tokens that fall below the relative threshold in logit space:\n-        # p_i < min_p * p_max  <=>  (l_i - l_max) < log(min_p)\n+        # p_i < min_p * p_max  <=>  l_i < l_max + log(min_p)\n         max_logits = torch.amax(logits, dim=-1, keepdim=True)\n-        delta = logits - max_logits\n-        invalid_token_mask = delta < self.log_min_p\n-        logits.masked_fill_(invalid_token_mask, -float('inf'))\n+        threshold_logits = max_logits + self.log_min_p\n+        logits.masked_fill_(logits < threshold_logits, -float('inf'))\n         return logits\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
