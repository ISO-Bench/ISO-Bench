diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py
index dee7052..7479c85 100644
--- a/vllm/transformers_utils/tokenizer.py
+++ b/vllm/transformers_utils/tokenizer.py
@@ -102,12 +102,12 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:
     tokenizer_len = len(tokenizer)
 
     # Compute max token id without scanning the entire vocab (O(n)).
-    max_token_id = tokenizer_len - 1
+    max_token_id = tokenizer_len - 1 if tokenizer_len > 0 else -1
     # Some tokenizers (e.g., QwenTokenizer) may include special tokens in
     # `vocab_size` but not in `get_vocab()`. Prefer the greater value when
     # available.
-    with contextlib.suppress(NotImplementedError, AttributeError, TypeError):
-        max_token_id = max(max_token_id, tokenizer.vocab_size - 1)
+    with contextlib.suppress(NotImplementedError, AttributeError, TypeError, ValueError):
+        max_token_id = max(max_token_id, int(tokenizer.vocab_size) - 1)
 
     class CachedTokenizer(tokenizer.__class__):  # type: ignore
 
