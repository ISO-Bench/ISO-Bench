{"instance_id": "vllm-project__vllm-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex dee7052..7479c85 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -102,12 +102,12 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n     tokenizer_len = len(tokenizer)\n \n     # Compute max token id without scanning the entire vocab (O(n)).\n-    max_token_id = tokenizer_len - 1\n+    max_token_id = tokenizer_len - 1 if tokenizer_len > 0 else -1\n     # Some tokenizers (e.g., QwenTokenizer) may include special tokens in\n     # `vocab_size` but not in `get_vocab()`. Prefer the greater value when\n     # available.\n-    with contextlib.suppress(NotImplementedError, AttributeError, TypeError):\n-        max_token_id = max(max_token_id, tokenizer.vocab_size - 1)\n+    with contextlib.suppress(NotImplementedError, AttributeError, TypeError, ValueError):\n+        max_token_id = max(max_token_id, int(tokenizer.vocab_size) - 1)\n \n     class CachedTokenizer(tokenizer.__class__):  # type: ignore\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
