diff --git a/model_patch.diff b/model_patch.diff
index b4f153f..e69de29 100644
--- a/model_patch.diff
+++ b/model_patch.diff
@@ -1,55 +0,0 @@
-diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
-index 0026031..0c79d45 100644
---- a/vllm/model_executor/layers/fused_moe/fused_moe.py
-+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
-@@ -634,29 +634,28 @@ def moe_align_block_size(
-     num_tokens_post_pad = torch.empty((1),
-                                       dtype=torch.int32,
-                                       device=topk_ids.device)
--    if num_experts >= 224:
--        if envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON or num_experts != 256:
--            moe_align_block_size_triton(
--                topk_ids,
--                num_experts,
--                block_size,
--                sorted_ids,
--                expert_ids,
--                num_tokens_post_pad,
--            )
--        else:
--            # Currently requires num_experts=256
--            ops.sgl_moe_align_block_size(
--                topk_ids,
--                num_experts,
--                block_size,
--                sorted_ids,
--                expert_ids,
--                num_tokens_post_pad,
--            )
-+    if num_experts == 256 and not envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON:
-+        # Prefer the specialized CUDA kernel for DeepSeek V3 layout when available.
-+        ops.sgl_moe_align_block_size(
-+            topk_ids,
-+            num_experts,
-+            block_size,
-+            sorted_ids,
-+            expert_ids,
-+            num_tokens_post_pad,
-+        )
-     else:
--        ops.moe_align_block_size(topk_ids, num_experts, block_size, sorted_ids,
--                                 expert_ids, num_tokens_post_pad)
-+        # Default to the Triton implementation for broader expert sizes.
-+        # This avoids extra dispatch/launch overheads in the CUDA path and
-+        # shows better performance for common configurations (e.g., E<=224).
-+        moe_align_block_size_triton(
-+            topk_ids,
-+            num_experts,
-+            block_size,
-+            sorted_ids,
-+            expert_ids,
-+            num_tokens_post_pad,
-+        )
-     if expert_map is not None:
-         expert_ids = expert_map[expert_ids]
- 
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index 90da94a..f7dd65a 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -471,6 +471,10 @@ def moe_align_block_size_stage2(
 ):
     pid = tl.program_id(0)
 
+    # Initialize the prefix row (row 0) for this column to zero to avoid host-side
+    # tensor zeroing.
+    tl.store(tokens_cnts_ptr + pid, 0)
+
     last_cnt = 0
     for i in range(1, num_experts + 1):
         token_cnt = tl.load(tokens_cnts_ptr + i * num_experts + pid)
