{"instance_id": "vllm-project__vllm-2b04c20", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nindex b4f153f..e69de29 100644\n--- a/model_patch.diff\n+++ b/model_patch.diff\n@@ -1,55 +0,0 @@\n-diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\n-index 0026031..0c79d45 100644\n---- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n-+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n-@@ -634,29 +634,28 @@ def moe_align_block_size(\n-     num_tokens_post_pad = torch.empty((1),\n-                                       dtype=torch.int32,\n-                                       device=topk_ids.device)\n--    if num_experts >= 224:\n--        if envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON or num_experts != 256:\n--            moe_align_block_size_triton(\n--                topk_ids,\n--                num_experts,\n--                block_size,\n--                sorted_ids,\n--                expert_ids,\n--                num_tokens_post_pad,\n--            )\n--        else:\n--            # Currently requires num_experts=256\n--            ops.sgl_moe_align_block_size(\n--                topk_ids,\n--                num_experts,\n--                block_size,\n--                sorted_ids,\n--                expert_ids,\n--                num_tokens_post_pad,\n--            )\n-+    if num_experts == 256 and not envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON:\n-+        # Prefer the specialized CUDA kernel for DeepSeek V3 layout when available.\n-+        ops.sgl_moe_align_block_size(\n-+            topk_ids,\n-+            num_experts,\n-+            block_size,\n-+            sorted_ids,\n-+            expert_ids,\n-+            num_tokens_post_pad,\n-+        )\n-     else:\n--        ops.moe_align_block_size(topk_ids, num_experts, block_size, sorted_ids,\n--                                 expert_ids, num_tokens_post_pad)\n-+        # Default to the Triton implementation for broader expert sizes.\n-+        # This avoids extra dispatch/launch overheads in the CUDA path and\n-+        # shows better performance for common configurations (e.g., E<=224).\n-+        moe_align_block_size_triton(\n-+            topk_ids,\n-+            num_experts,\n-+            block_size,\n-+            sorted_ids,\n-+            expert_ids,\n-+            num_tokens_post_pad,\n-+        )\n-     if expert_map is not None:\n-         expert_ids = expert_map[expert_ids]\n- \ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 90da94a..f7dd65a 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -471,6 +471,10 @@ def moe_align_block_size_stage2(\n ):\n     pid = tl.program_id(0)\n \n+    # Initialize the prefix row (row 0) for this column to zero to avoid host-side\n+    # tensor zeroing.\n+    tl.store(tokens_cnts_ptr + pid, 0)\n+\n     last_cnt = 0\n     for i in range(1, num_experts + 1):\n         token_cnt = tl.load(tokens_cnts_ptr + i * num_experts + pid)\n", "model_name_or_path": "gpt-5-2025-08-07"}
