diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/config.py b/vllm/config.py
index 6bfe94b76..3cdc604d3 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -4711,12 +4711,29 @@ class VllmConfig:
 
         if self.compilation_config.full_cuda_graph and \
             not self.model_config.disable_cascade_attn:
-            logger.info("full_cuda_graph is not supported with "
+            logger.info_once("full_cuda_graph is not supported with "
                         "cascade attention. Disabling cascade attention.")
             self.model_config.disable_cascade_attn = True
 
         disable_chunked_prefill_reasons: list[str] = []
 
+        # Allow users to force-disable the hybrid KV cache manager regardless of
+        # detected compatibility, which can improve latency in some scenarios.
+        if envs.VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER:
+            self.scheduler_config.disable_hybrid_kv_cache_manager = True
+
+        # Optional escape hatch to disable chunked prefill entirely via env var.
+        if envs.VLLM_DISABLE_CHUNKED_PREFILL:
+            logger.info_once("Disabling chunked prefill due to VLLM_DISABLE_CHUNKED_PREFILL=1.")
+            self.scheduler_config.chunked_prefill_enabled = False
+            self.scheduler_config.long_prefill_token_threshold = 0
+            self.scheduler_config.max_num_batched_tokens = max(
+                self.scheduler_config.max_model_len,
+                DEFAULT_MAX_NUM_BATCHED_TOKENS)
+            if self.cache_config is not None:
+                self.cache_config.enable_prefix_caching = False
+
+
         if self.model_config and self.model_config.pooler_config:
             pooling_type = self.model_config.pooler_config.pooling_type
             if pooling_type is None or pooling_type.lower() != "last":
@@ -4726,7 +4743,7 @@ class VllmConfig:
 
         if disable_chunked_prefill_reasons:
             for reason in disable_chunked_prefill_reasons:
-                logger.info(reason)
+                logger.info_once(reason)
             self.scheduler_config.chunked_prefill_enabled = False
             self.scheduler_config.long_prefill_token_threshold = 0
             self.scheduler_config.max_num_batched_tokens = max(
@@ -4739,13 +4756,13 @@ class VllmConfig:
         if (self.kv_events_config is not None
                 and self.kv_events_config.enable_kv_cache_events
                 and not self.cache_config.enable_prefix_caching):
-            logger.warning(
+            logger.warning_once(
                 "KV cache events are on, but prefix caching is not enabled."
                 "Use --enable-prefix-caching to enable.")
         if (self.kv_events_config is not None
                 and self.kv_events_config.publisher != "null"
                 and not self.kv_events_config.enable_kv_cache_events):
-            logger.warning("KV cache events are disabled,"
+            logger.warning_once("KV cache events are disabled,"
                            "but the scheduler is configured to publish them."
                            "Modify KVEventsConfig.enable_kv_cache_events"
                            "to True to enable.")
@@ -4770,11 +4787,30 @@ class VllmConfig:
                 self.scheduler_config.disable_hybrid_kv_cache_manager = True
             if self.model_config is not None and \
                 self.model_config.attention_chunk_size is not None and \
-                self.speculative_config is not None and \
-                self.speculative_config.use_eagle():
-                # Hybrid KV cache manager is not yet supported with chunked
-                # local attention + eagle.
-                self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                envs.VLLM_DISABLE_CHUNKED_LOCAL_ATTN:
+                logger.info_once(
+                    "Disabling chunked local attention due to "
+                    "VLLM_DISABLE_CHUNKED_LOCAL_ATTN=1.")
+                self.model_config.attention_chunk_size = None
+
+            if self.model_config is not None and \
+                self.model_config.attention_chunk_size is not None:
+                if self.speculative_config is not None and \
+                    self.speculative_config.use_eagle():
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention + eagle.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
+                elif \
+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:
+                    logger.warning_once(
+                        "There is a latency regression when using chunked local"
+                        " attention with the hybrid KV cache manager. Disabling"
+                        " it, by default. To enable it, set the environment "
+                        "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1."
+                    )
+                    # Hybrid KV cache manager is not yet supported with chunked
+                    # local attention.
+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True
 
     def update_sizes_for_sequence_parallelism(self,
                                               possible_sizes: list) -> list:
diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff74151..794aec2ac 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -165,6 +165,18 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:
     return int(value)
 
 
+
+def getenv_bool(name: str, default: bool = False) -> bool:
+    """Fast path for boolean env var parsing with common conventions.
+    Accepts "1" or "true" (case-insensitive) as True. Returns default if unset.
+    """
+    val = os.getenv(name)
+    if val is None:
+        return default
+    val = val.strip().lower()
+    return val == "1" or val == "true"
+
+
 def get_vllm_port() -> Optional[int]:
     """Get the port from VLLM_PORT environment variable.
 
@@ -948,6 +960,25 @@ environment_variables: dict[str, Callable[[], Any]] = {
     # implement and support a subset of all possible layouts.
     "VLLM_KV_CACHE_LAYOUT":
     lambda: os.getenv("VLLM_KV_CACHE_LAYOUT", None),
+    # If set, allow using hybrid KV cache manager together with chunked local attention.
+    # Default is disabled due to latency regressions; set to 1 to enable.
+    "VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE":
+    lambda: getenv_bool("VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE", False),
+    
+    # Force-disable hybrid KV cache manager (overrides compatibility checks).
+    "VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER":
+    lambda: getenv_bool("VLLM_DISABLE_HYBRID_KV_CACHE_MANAGER", False),
+
+    # Disable chunked prefill across the board to reduce latency when needed.
+    "VLLM_DISABLE_CHUNKED_PREFILL":
+    lambda: getenv_bool("VLLM_DISABLE_CHUNKED_PREFILL", False),
+    
+    # Disable chunked local attention entirely. Can reduce latency on some
+    # models with hybrid KV cache or when chunking offers little benefit.
+    "VLLM_DISABLE_CHUNKED_LOCAL_ATTN":
+    lambda: getenv_bool("VLLM_DISABLE_CHUNKED_LOCAL_ATTN", False),
+
+
 
     # Enable checking whether the generated logits contain NaNs,
     # indicating corrupted output. Useful for debugging low level bugs
@@ -997,9 +1028,12 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation with simple caching to avoid repeated parsing
     if name in environment_variables:
-        return environment_variables[name]()
+        value = environment_variables[name]()
+        # cache the resolved value as a module attribute for fast subsequent access
+        setattr(sys.modules[__name__], name, value)
+        return value
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
@@ -1037,10 +1071,8 @@ def compute_hash() -> str:
 
     # summarize environment variables
     def factorize(name: str):
-        if __getattr__(name):
-            factors.append(__getattr__(name))
-        else:
-            factors.append("None")
+        val = __getattr__(name)
+        factors.append(val if val else "None")
 
     # The values of envs may affects the computation graph.
     # TODO(DefTruth): hash all environment variables?
