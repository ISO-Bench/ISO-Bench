{"instance_id": "vllm-project__vllm-b0e96aa", "model_patch": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex 79787303a..d38c81d0d 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -327,15 +327,13 @@ class PyObjectCache:\n         self._obj_builder = obj_builder\n         self._index = 0\n \n-        self._obj_cache = []\n-        for _ in range(128):\n-            self._obj_cache.append(self._obj_builder())\n+        self._obj_cache = [self._obj_builder() for _ in range(128)]\n \n     def _grow_cache(self):\n         # Double the size of the cache\n         num_objs = len(self._obj_cache)\n-        for _ in range(num_objs):\n-            self._obj_cache.append(self._obj_builder())\n+    # Use list comprehension for efficiency\n+    self._obj_cache.extend(self._obj_builder() for _ in range(num_objs))\n \n     def get_object(self):\n         \"\"\"Returns a pre-allocated cached object. If there is not enough\n@@ -412,7 +410,12 @@ async def merge_async_iterators(\n     iterator that yields the item.\n     \"\"\"\n \n-    loop = asyncio.get_running_loop()\n+    if len(iterators) == 1:\n+        # Fast-path single iterator case.\n+        async for item in iterators[0]:\n+            yield 0, item\n+        return\n+\n \n     awaits = {_next_task(pair[1], loop): pair for pair in enumerate(iterators)}\n     try:\n", "model_name_or_path": "gpt-5-2025-08-07"}
