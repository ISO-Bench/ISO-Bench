{"instance_id": "vllm-project__vllm-2b04c20", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 00260313e..d4b048eff 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -24,7 +24,7 @@ logger = init_logger(__name__)\n def write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token,\n                           token_mask, BLOCK_SIZE_M, BLOCK_SIZE_N,\n                           compute_type):\n-    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=compute_type)\n+    accumulator = tl.empty((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=compute_type)\n     offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n     c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n         None, :]\n@@ -534,10 +534,12 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n+    tokens_cnts = torch.empty((num_experts + 1, num_experts), dtype=torch.int32, device='cuda')\n+\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device='cuda')\n+,\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n     tokens_per_thread = ceil_div(numel, num_experts)\n@@ -628,7 +630,8 @@ def moe_align_block_size(\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     # Expert ids must be zeroed out to prevent index out of bounds error while\n     # mapping global expert ids to local expert ids in expert parallelism.\n-    expert_ids = torch.zeros((max_num_m_blocks, ),\n+    expert_ids = torch.empty((max_num_m_blocks,), dtype=torch.int32, device='cuda')\n+,\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n     num_tokens_post_pad = torch.empty((1),\n@@ -994,7 +997,8 @@ def grouped_topk(hidden_states: torch.Tensor,\n                                    -1).max(dim=-1).values  # [n, n_group]\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,\n                            sorted=False)[1]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    group_mask = torch.empty_like(group_scores)\n+\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = group_mask.unsqueeze(-1).expand(\n         num_token, num_expert_group,\n", "model_name_or_path": "gpt-5-2025-08-07"}
