diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index 00260313e..8da8f8ee1 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -534,10 +534,12 @@ def moe_align_block_size_triton(
 ) -> None:
     numel = topk_ids.numel()
     grid = (num_experts, )
-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),
+    tokens_cnts = torch.empty((num_experts + 1, num_experts), dtype=torch.int32, device='cuda')
+
                               dtype=torch.int32,
                               device=topk_ids.device)
-    cumsum = torch.zeros((num_experts + 1, ),
+    cumsum = torch.empty((num_experts + 1,), dtype=torch.int32, device='cuda')
+,
                          dtype=torch.int32,
                          device=topk_ids.device)
     tokens_per_thread = ceil_div(numel, num_experts)
@@ -628,7 +630,8 @@ def moe_align_block_size(
     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)
     # Expert ids must be zeroed out to prevent index out of bounds error while
     # mapping global expert ids to local expert ids in expert parallelism.
-    expert_ids = torch.zeros((max_num_m_blocks, ),
+    expert_ids = torch.empty((max_num_m_blocks,), dtype=torch.int32, device='cuda')
+,
                              dtype=torch.int32,
                              device=topk_ids.device)
     num_tokens_post_pad = torch.empty((1),
@@ -994,7 +997,8 @@ def grouped_topk(hidden_states: torch.Tensor,
                                    -1).max(dim=-1).values  # [n, n_group]
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,
                            sorted=False)[1]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
+    group_mask = torch.empty_like(group_scores)
+
     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
     score_mask = group_mask.unsqueeze(-1).expand(
         num_token, num_expert_group,
