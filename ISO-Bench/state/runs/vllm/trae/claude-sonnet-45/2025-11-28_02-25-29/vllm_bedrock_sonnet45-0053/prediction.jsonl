{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-8c1e77f", "model_patch": "diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 5f8535eaa..b4f7f6529 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -135,6 +135,13 @@ class FlashAttentionImpl(AttentionImpl):\n         assert k_scale == 1.0 and v_scale == 1.0, (\n             \"key/v_scale is not supported in FlashAttention.\")\n \n+        # Reshape the query, key, and value tensors.\n+        # NOTE(woosuk): We do this outside the custom op to minimize the CPU\n+        # overheads from the non-CUDA-graph regions.\n+        query = query.view(-1, self.num_heads, self.head_size)\n+        key = key.view(-1, self.num_kv_heads, self.head_size)\n+        value = value.view(-1, self.num_kv_heads, self.head_size)\n+\n         output = torch.empty_like(query)\n         torch.ops.vllm.unified_v1_flash_attention(\n             output,\n@@ -153,7 +160,7 @@ class FlashAttentionImpl(AttentionImpl):\n             self.alibi_slopes,\n             self.logits_soft_cap,\n         )\n-        return output\n+        return output.view(-1, self.num_heads * self.head_size)\n \n \n def unified_v1_flash_attention(\n@@ -184,10 +191,8 @@ def unified_v1_flash_attention(\n     attn_metadata: FlashAttentionMetadata = current_metadata\n     num_actual_tokens = attn_metadata.num_actual_tokens\n \n-    # Reshape the query, key, and value tensors.\n-    query = query.view(-1, num_heads, head_size)\n-    key = key.view(-1, num_kv_heads, head_size)\n-    value = value.view(-1, num_kv_heads, head_size)\n+    # NOTE(woosuk): The query, key, and value tensors are already reshaped\n+    # in the forward method to minimize the CPU overheads.\n \n     # Reshape the input keys and values and store them in the cache.\n     key_cache = kv_cache[0]\n", "model_name_or_path": "gpt-5-2025-08-07"}
