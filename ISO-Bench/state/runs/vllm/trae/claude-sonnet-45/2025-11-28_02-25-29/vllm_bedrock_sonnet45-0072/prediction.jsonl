{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-802329d", "model_patch": "diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9dcf..51efbfe20 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -37,7 +37,7 @@ from vllm.model_executor.layers.rotary_embedding import get_rope\n from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n \n from .llama import LlamaForCausalLM, LlamaMLP, LlamaModel\n-from .utils import (AutoWeightsLoader, extract_layer_index,\n+from .utils import (AutoWeightsLoader, extract_layer_index, fast_topk,\n                     is_pp_missing_parameter)\n \n \n@@ -50,7 +50,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores, router_indices = torch.topk(gating_output, topk, dim=-1)\n+        router_scores, router_indices = fast_topk(gating_output, topk, dim=-1)\n         router_scores = torch.sigmoid(router_scores.float()).to(\n             hidden_states.dtype)\n         return (router_scores, router_indices.to(torch.int32))\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434f3..ae8637f06 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -703,3 +703,39 @@ def cast_overflow_tensors(\n         clamp_value = torch.finfo(tensors.dtype).max - offset\n         tensors = torch.clamp(tensors, min=-clamp_value, max=clamp_value)\n     return tensors\n+\n+\n+def fast_topk(\n+    input: torch.Tensor,\n+    k: int,\n+    dim: int = -1,\n+    largest: bool = True,\n+    sorted: bool = True,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Optimized topk implementation for k=1 case.\n+    \n+    For k=1, we can use argmax/argmin which is faster than the general topk.\n+    For k>1, falls back to torch.topk.\n+    \n+    Args:\n+        input: Input tensor\n+        k: Number of top elements to return\n+        dim: Dimension along which to find topk\n+        largest: If True, return largest k elements, else smallest k\n+        sorted: If True, return sorted results (only relevant for k>1)\n+    \n+    Returns:\n+        Tuple of (values, indices) matching torch.topk output\n+    \"\"\"\n+    if k == 1:\n+        # Optimized path for k=1\n+        if largest:\n+            indices = torch.argmax(input, dim=dim, keepdim=True)\n+        else:\n+            indices = torch.argmin(input, dim=dim, keepdim=True)\n+        values = torch.gather(input, dim, indices)\n+        return values, indices\n+    else:\n+        # Fall back to standard topk for k>1\n+        return torch.topk(input, k, dim=dim, largest=largest, sorted=sorted)\n", "model_name_or_path": "gpt-5-2025-08-07"}
