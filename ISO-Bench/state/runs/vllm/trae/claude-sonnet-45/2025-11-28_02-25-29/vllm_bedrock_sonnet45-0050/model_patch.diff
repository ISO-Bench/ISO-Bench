diff --git a/docs/source/conf.py b/docs/source/conf.py
index 4a1a5fb45..66fc53f56 100644
--- a/docs/source/conf.py
+++ b/docs/source/conf.py
@@ -178,6 +178,7 @@ autodoc_mock_imports = [
     "tensorizer",
     "pynvml",
     "outlines",
+    "xgrammar",
     "librosa",
     "soundfile",
     "gguf",
diff --git a/requirements-common.txt b/requirements-common.txt
index 02e3d65fb..818f72e14 100644
--- a/requirements-common.txt
+++ b/requirements-common.txt
@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0
 tiktoken >= 0.6.0  # Required for DBRX tokenizer
 lm-format-enforcer >= 0.10.9, < 0.11
 outlines >= 0.0.43, < 0.1
+xgrammar
 typing_extensions >= 4.10
 filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317
 partial-json-parser # used for parsing partial JSON outputs
diff --git a/tests/entrypoints/llm/test_guided_generate.py b/tests/entrypoints/llm/test_guided_generate.py
index 67c79415f..13ce09d75 100644
--- a/tests/entrypoints/llm/test_guided_generate.py
+++ b/tests/entrypoints/llm/test_guided_generate.py
@@ -159,3 +159,28 @@ def test_validation_against_both_guided_decoding_options(sample_regex, llm):
                      sampling_params=sampling_params,
                      use_tqdm=True,
                      guided_options_request=dict(guided_regex=sample_regex))
+
+
+@pytest.mark.skip_global_cleanup
+def test_guided_regex_with_xgrammar_backend(sample_regex, llm):
+    sampling_params = SamplingParams(
+        temperature=0.8,
+        top_p=0.95,
+        guided_decoding=GuidedDecodingParams(regex=sample_regex,
+                                             backend='xgrammar'))
+    outputs = llm.generate(prompts=[
+        f"Give an example IPv4 address with this regex: {sample_regex}"
+    ] * 2,
+                           sampling_params=sampling_params,
+                           use_tqdm=True)
+
+    assert outputs is not None
+    for output in outputs:
+        assert output is not None
+        assert isinstance(output, RequestOutput)
+        prompt = output.prompt
+        generated_text = output.outputs[0].text
+        print(generated_text)
+        assert generated_text is not None
+        assert re.fullmatch(sample_regex, generated_text) is not None
+        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
diff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py
index 45fab8e96..d9945d279 100644
--- a/tests/model_executor/test_guided_processors.py
+++ b/tests/model_executor/test_guided_processors.py
@@ -36,7 +36,7 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):
 
 
 @pytest.mark.asyncio
-@pytest.mark.parametrize("backend", ["outlines", "lm-format-enforcer"])
+@pytest.mark.parametrize("backend", ["xgrammar", "outlines", "lm-format-enforcer"])
 async def test_guided_logits_processor_black_box(backend: str, sample_regex,
                                                  sample_json_schema):
     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')
diff --git a/vllm/config.py b/vllm/config.py
index 326340d3f..2c488d212 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -2031,11 +2031,11 @@ def get_served_model_name(model: str,
 class DecodingConfig:
     """Dataclass which contains the decoding strategy of the engine"""
 
-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'
-    guided_decoding_backend: str = 'outlines'
+    # Which guided decoding algo to use. 'xgrammar' / 'outlines' / 'lm-format-enforcer'
+    guided_decoding_backend: str = 'xgrammar'
 
     def __post_init__(self):
-        valid_guided_backends = ['outlines', 'lm-format-enforcer']
+        valid_guided_backends = ['xgrammar', 'outlines', 'lm-format-enforcer']
         backend = self.guided_decoding_backend
         if backend not in valid_guided_backends:
             raise ValueError(f"Invalid guided_decoding_backend '{backend},"
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 4aa0eebd9..6e8d9f8aa 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -168,7 +168,7 @@ class EngineArgs:
     scheduler_delay_factor: float = 0.0
     enable_chunked_prefill: Optional[bool] = None
 
-    guided_decoding_backend: str = 'outlines'
+    guided_decoding_backend: str = 'xgrammar'
     # Speculative decoding configuration.
     speculative_model: Optional[str] = None
     speculative_model_quantization: Optional[str] = None
@@ -364,10 +364,11 @@ class EngineArgs:
         parser.add_argument(
             '--guided-decoding-backend',
             type=str,
-            default='outlines',
-            choices=['outlines', 'lm-format-enforcer'],
+            default='xgrammar',
+            choices=['xgrammar', 'outlines', 'lm-format-enforcer'],
             help='Which engine will be used for guided decoding'
             ' (JSON schema / regex etc) by default. Currently support '
+            'https://github.com/guidance-ai/xgrammar, '
             'https://github.com/outlines-dev/outlines and '
             'https://github.com/noamgat/lm-format-enforcer.'
             ' Can be overridden per request via guided_decoding_backend'
diff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py
index d7b67425f..82a877f06 100644
--- a/vllm/model_executor/guided_decoding/__init__.py
+++ b/vllm/model_executor/guided_decoding/__init__.py
@@ -4,43 +4,80 @@ from vllm.logits_process import LogitsProcessor
 from vllm.sampling_params import GuidedDecodingParams
 
 
+def _get_guided_decoding_backend(guided_params: GuidedDecodingParams) -> str:
+    """Determine the guided decoding backend to use."""
+    # If backend is explicitly specified, use it
+    if guided_params.backend:
+        return guided_params.backend
+    
+    # Default to xgrammar if available
+    try:
+        import xgrammar  # noqa: F401
+        return 'xgrammar'
+    except ImportError:
+        # Fall back to outlines if xgrammar is not available
+        return 'outlines'
+
+
 async def get_guided_decoding_logits_processor(
         guided_params: GuidedDecodingParams,
         tokenizer) -> Optional[LogitsProcessor]:
-    # CFG grammar not supported by LMFE, so we use outlines instead
-    if guided_params.backend == 'outlines' or guided_params.grammar:
+    backend = _get_guided_decoding_backend(guided_params)
+    
+    # XGrammar backend (default)
+    if backend == 'xgrammar':
+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa
+            get_xgrammar_guided_decoding_logits_processor)
+        return await get_xgrammar_guided_decoding_logits_processor(
+            guided_params, tokenizer)
+    
+    # Outlines backend
+    if backend == 'outlines':
         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193
         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
             get_outlines_guided_decoding_logits_processor)
         return await get_outlines_guided_decoding_logits_processor(
             guided_params, tokenizer)
-    if guided_params.backend == 'lm-format-enforcer':
+    
+    # LM Format Enforcer backend
+    if backend == 'lm-format-enforcer':
         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa
             get_local_lm_format_enforcer_guided_decoding_logits_processor)
         return get_local_lm_format_enforcer_guided_decoding_logits_processor(
             guided_params, tokenizer)
 
     raise ValueError(
-        f"Unknown guided decoding backend '{guided_params.backend}'. "
-        "Must be one of 'outlines, 'lm-format-enforcer'")
+        f"Unknown guided decoding backend '{backend}'. "
+        "Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'")
 
 
 def get_local_guided_decoding_logits_processor(
         guided_params: GuidedDecodingParams,
         tokenizer) -> Optional[LogitsProcessor]:
-    # CFG grammar not supported by LMFE, so we use outlines instead
-    if guided_params.backend == 'outlines' or guided_params.grammar:
+    backend = _get_guided_decoding_backend(guided_params)
+    
+    # XGrammar backend (default)
+    if backend == 'xgrammar':
+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa
+            get_local_xgrammar_guided_decoding_logits_processor)
+        return get_local_xgrammar_guided_decoding_logits_processor(
+            guided_params, tokenizer)
+    
+    # Outlines backend
+    if backend == 'outlines':
         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193
         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa
             get_local_outlines_guided_decoding_logits_processor)
         return get_local_outlines_guided_decoding_logits_processor(
             guided_params, tokenizer)
-    if guided_params.backend == 'lm-format-enforcer':
+    
+    # LM Format Enforcer backend
+    if backend == 'lm-format-enforcer':
         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa
             get_local_lm_format_enforcer_guided_decoding_logits_processor)
         return get_local_lm_format_enforcer_guided_decoding_logits_processor(
             guided_params, tokenizer)
 
     raise ValueError(
-        f"Unknown guided decoding backend '{guided_params.backend}'. "
-        "Must be one of 'outlines, 'lm-format-enforcer'")
+        f"Unknown guided decoding backend '{backend}'. "
+        "Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'")
diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
new file mode 100644
index 000000000..7d2f2bad1
--- /dev/null
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -0,0 +1,95 @@
+"""XGrammar guided decoding implementation for vLLM."""
+import json
+from typing import Optional
+
+import torch
+from transformers import PreTrainedTokenizerBase
+
+from vllm.logits_process import LogitsProcessor
+from vllm.sampling_params import GuidedDecodingParams
+
+try:
+    import xgrammar as xgr
+    XGRAMMAR_AVAILABLE = True
+except ImportError:
+    XGRAMMAR_AVAILABLE = False
+    xgr = None
+
+
+class XGrammarLogitsProcessor(LogitsProcessor):
+    """Logits processor using XGrammar for guided decoding."""
+
+    def __init__(self, xgr_matcher, tokenizer_data):
+        self.xgr_matcher = xgr_matcher
+        self.tokenizer_data = tokenizer_data
+
+    def __call__(self, token_ids: list[int],
+                 logits: torch.Tensor) -> torch.Tensor:
+        """Apply XGrammar constraints to logits."""
+        # Get the bitmask from XGrammar
+        bitmask = self.xgr_matcher.get_next_token_bitmask(token_ids)
+        
+        # Apply the bitmask to logits
+        # Set logits to -inf for tokens not in the bitmask
+        vocab_size = logits.shape[-1]
+        for token_id in range(vocab_size):
+            if not self._is_token_allowed(bitmask, token_id):
+                logits[token_id] = float('-inf')
+        
+        return logits
+
+    def _is_token_allowed(self, bitmask, token_id: int) -> bool:
+        """Check if a token is allowed by the bitmask."""
+        byte_idx = token_id // 8
+        bit_idx = token_id % 8
+        if byte_idx < len(bitmask):
+            return bool(bitmask[byte_idx] & (1 << bit_idx))
+        return False
+
+
+def get_local_xgrammar_guided_decoding_logits_processor(
+        guided_params: GuidedDecodingParams,
+        tokenizer: PreTrainedTokenizerBase) -> Optional[LogitsProcessor]:
+    """Get XGrammar logits processor for guided decoding."""
+    if not XGRAMMAR_AVAILABLE:
+        raise ImportError(
+            "XGrammar is not installed. Please install it with: "
+            "pip install xgrammar")
+
+    # Convert tokenizer to XGrammar format
+    tokenizer_info = xgr.TokenizerInfo.from_huggingface(tokenizer)
+    
+    # Create the appropriate grammar based on the guided params
+    if guided_params.json:
+        if isinstance(guided_params.json, dict):
+            json_schema = json.dumps(guided_params.json)
+        else:
+            json_schema = guided_params.json
+        grammar = xgr.Grammar.from_json_schema(json_schema)
+    elif guided_params.regex:
+        grammar = xgr.Grammar.from_regex(guided_params.regex)
+    elif guided_params.choice:
+        # Create a regex that matches any of the choices
+        choices_str = "|".join(f"({choice})" for choice in guided_params.choice)
+        grammar = xgr.Grammar.from_regex(choices_str)
+    elif guided_params.grammar:
+        grammar = xgr.Grammar.from_ebnf(guided_params.grammar)
+    elif guided_params.json_object:
+        # For json_object, use a generic JSON schema
+        grammar = xgr.Grammar.builtin_json_grammar()
+    else:
+        return None
+
+    # Create the matcher
+    matcher = xgr.GrammarMatcher(grammar, tokenizer_info)
+    
+    return XGrammarLogitsProcessor(matcher, tokenizer_info)
+
+
+async def get_xgrammar_guided_decoding_logits_processor(
+        guided_params: GuidedDecodingParams,
+        tokenizer: PreTrainedTokenizerBase) -> Optional[LogitsProcessor]:
+    """Async version of get_xgrammar_guided_decoding_logits_processor."""
+    # XGrammar operations are CPU-bound, so we can just call the sync version
+    return get_local_xgrammar_guided_decoding_logits_processor(
+        guided_params, tokenizer)
