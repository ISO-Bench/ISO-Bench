{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3257d44", "model_patch": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb45..66fc53f56 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -178,6 +178,7 @@ autodoc_mock_imports = [\n     \"tensorizer\",\n     \"pynvml\",\n     \"outlines\",\n+    \"xgrammar\",\n     \"librosa\",\n     \"soundfile\",\n     \"gguf\",\ndiff --git a/requirements-common.txt b/requirements-common.txt\nindex 02e3d65fb..818f72e14 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0\n tiktoken >= 0.6.0  # Required for DBRX tokenizer\n lm-format-enforcer >= 0.10.9, < 0.11\n outlines >= 0.0.43, < 0.1\n+xgrammar\n typing_extensions >= 4.10\n filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\n partial-json-parser # used for parsing partial JSON outputs\ndiff --git a/tests/entrypoints/llm/test_guided_generate.py b/tests/entrypoints/llm/test_guided_generate.py\nindex 67c79415f..13ce09d75 100644\n--- a/tests/entrypoints/llm/test_guided_generate.py\n+++ b/tests/entrypoints/llm/test_guided_generate.py\n@@ -159,3 +159,28 @@ def test_validation_against_both_guided_decoding_options(sample_regex, llm):\n                      sampling_params=sampling_params,\n                      use_tqdm=True,\n                      guided_options_request=dict(guided_regex=sample_regex))\n+\n+\n+@pytest.mark.skip_global_cleanup\n+def test_guided_regex_with_xgrammar_backend(sample_regex, llm):\n+    sampling_params = SamplingParams(\n+        temperature=0.8,\n+        top_p=0.95,\n+        guided_decoding=GuidedDecodingParams(regex=sample_regex,\n+                                             backend='xgrammar'))\n+    outputs = llm.generate(prompts=[\n+        f\"Give an example IPv4 address with this regex: {sample_regex}\"\n+    ] * 2,\n+                           sampling_params=sampling_params,\n+                           use_tqdm=True)\n+\n+    assert outputs is not None\n+    for output in outputs:\n+        assert output is not None\n+        assert isinstance(output, RequestOutput)\n+        prompt = output.prompt\n+        generated_text = output.outputs[0].text\n+        print(generated_text)\n+        assert generated_text is not None\n+        assert re.fullmatch(sample_regex, generated_text) is not None\n+        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\ndiff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py\nindex 45fab8e96..d9945d279 100644\n--- a/tests/model_executor/test_guided_processors.py\n+++ b/tests/model_executor/test_guided_processors.py\n@@ -36,7 +36,7 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):\n \n \n @pytest.mark.asyncio\n-@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\n+@pytest.mark.parametrize(\"backend\", [\"xgrammar\", \"outlines\", \"lm-format-enforcer\"])\n async def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                  sample_json_schema):\n     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 326340d3f..2c488d212 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -2031,11 +2031,11 @@ def get_served_model_name(model: str,\n class DecodingConfig:\n     \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n \n-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n-    guided_decoding_backend: str = 'outlines'\n+    # Which guided decoding algo to use. 'xgrammar' / 'outlines' / 'lm-format-enforcer'\n+    guided_decoding_backend: str = 'xgrammar'\n \n     def __post_init__(self):\n-        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n+        valid_guided_backends = ['xgrammar', 'outlines', 'lm-format-enforcer']\n         backend = self.guided_decoding_backend\n         if backend not in valid_guided_backends:\n             raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eebd9..6e8d9f8aa 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -168,7 +168,7 @@ class EngineArgs:\n     scheduler_delay_factor: float = 0.0\n     enable_chunked_prefill: Optional[bool] = None\n \n-    guided_decoding_backend: str = 'outlines'\n+    guided_decoding_backend: str = 'xgrammar'\n     # Speculative decoding configuration.\n     speculative_model: Optional[str] = None\n     speculative_model_quantization: Optional[str] = None\n@@ -364,10 +364,11 @@ class EngineArgs:\n         parser.add_argument(\n             '--guided-decoding-backend',\n             type=str,\n-            default='outlines',\n-            choices=['outlines', 'lm-format-enforcer'],\n+            default='xgrammar',\n+            choices=['xgrammar', 'outlines', 'lm-format-enforcer'],\n             help='Which engine will be used for guided decoding'\n             ' (JSON schema / regex etc) by default. Currently support '\n+            'https://github.com/guidance-ai/xgrammar, '\n             'https://github.com/outlines-dev/outlines and '\n             'https://github.com/noamgat/lm-format-enforcer.'\n             ' Can be overridden per request via guided_decoding_backend'\ndiff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b67425f..82a877f06 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -4,43 +4,80 @@ from vllm.logits_process import LogitsProcessor\n from vllm.sampling_params import GuidedDecodingParams\n \n \n+def _get_guided_decoding_backend(guided_params: GuidedDecodingParams) -> str:\n+    \"\"\"Determine the guided decoding backend to use.\"\"\"\n+    # If backend is explicitly specified, use it\n+    if guided_params.backend:\n+        return guided_params.backend\n+    \n+    # Default to xgrammar if available\n+    try:\n+        import xgrammar  # noqa: F401\n+        return 'xgrammar'\n+    except ImportError:\n+        # Fall back to outlines if xgrammar is not available\n+        return 'outlines'\n+\n+\n async def get_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    backend = _get_guided_decoding_backend(guided_params)\n+    \n+    # XGrammar backend (default)\n+    if backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_xgrammar_guided_decoding_logits_processor)\n+        return await get_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer)\n+    \n+    # Outlines backend\n+    if backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_outlines_guided_decoding_logits_processor)\n         return await get_outlines_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n-    if guided_params.backend == 'lm-format-enforcer':\n+    \n+    # LM Format Enforcer backend\n+    if backend == 'lm-format-enforcer':\n         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n \n     raise ValueError(\n-        f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        f\"Unknown guided decoding backend '{backend}'. \"\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\n \n \n def get_local_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    backend = _get_guided_decoding_backend(guided_params)\n+    \n+    # XGrammar backend (default)\n+    if backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_local_xgrammar_guided_decoding_logits_processor)\n+        return get_local_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer)\n+    \n+    # Outlines backend\n+    if backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_local_outlines_guided_decoding_logits_processor)\n         return get_local_outlines_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n-    if guided_params.backend == 'lm-format-enforcer':\n+    \n+    # LM Format Enforcer backend\n+    if backend == 'lm-format-enforcer':\n         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n \n     raise ValueError(\n-        f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        f\"Unknown guided decoding backend '{backend}'. \"\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nnew file mode 100644\nindex 000000000..7d2f2bad1\n--- /dev/null\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -0,0 +1,95 @@\n+\"\"\"XGrammar guided decoding implementation for vLLM.\"\"\"\n+import json\n+from typing import Optional\n+\n+import torch\n+from transformers import PreTrainedTokenizerBase\n+\n+from vllm.logits_process import LogitsProcessor\n+from vllm.sampling_params import GuidedDecodingParams\n+\n+try:\n+    import xgrammar as xgr\n+    XGRAMMAR_AVAILABLE = True\n+except ImportError:\n+    XGRAMMAR_AVAILABLE = False\n+    xgr = None\n+\n+\n+class XGrammarLogitsProcessor(LogitsProcessor):\n+    \"\"\"Logits processor using XGrammar for guided decoding.\"\"\"\n+\n+    def __init__(self, xgr_matcher, tokenizer_data):\n+        self.xgr_matcher = xgr_matcher\n+        self.tokenizer_data = tokenizer_data\n+\n+    def __call__(self, token_ids: list[int],\n+                 logits: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Apply XGrammar constraints to logits.\"\"\"\n+        # Get the bitmask from XGrammar\n+        bitmask = self.xgr_matcher.get_next_token_bitmask(token_ids)\n+        \n+        # Apply the bitmask to logits\n+        # Set logits to -inf for tokens not in the bitmask\n+        vocab_size = logits.shape[-1]\n+        for token_id in range(vocab_size):\n+            if not self._is_token_allowed(bitmask, token_id):\n+                logits[token_id] = float('-inf')\n+        \n+        return logits\n+\n+    def _is_token_allowed(self, bitmask, token_id: int) -> bool:\n+        \"\"\"Check if a token is allowed by the bitmask.\"\"\"\n+        byte_idx = token_id // 8\n+        bit_idx = token_id % 8\n+        if byte_idx < len(bitmask):\n+            return bool(bitmask[byte_idx] & (1 << bit_idx))\n+        return False\n+\n+\n+def get_local_xgrammar_guided_decoding_logits_processor(\n+        guided_params: GuidedDecodingParams,\n+        tokenizer: PreTrainedTokenizerBase) -> Optional[LogitsProcessor]:\n+    \"\"\"Get XGrammar logits processor for guided decoding.\"\"\"\n+    if not XGRAMMAR_AVAILABLE:\n+        raise ImportError(\n+            \"XGrammar is not installed. Please install it with: \"\n+            \"pip install xgrammar\")\n+\n+    # Convert tokenizer to XGrammar format\n+    tokenizer_info = xgr.TokenizerInfo.from_huggingface(tokenizer)\n+    \n+    # Create the appropriate grammar based on the guided params\n+    if guided_params.json:\n+        if isinstance(guided_params.json, dict):\n+            json_schema = json.dumps(guided_params.json)\n+        else:\n+            json_schema = guided_params.json\n+        grammar = xgr.Grammar.from_json_schema(json_schema)\n+    elif guided_params.regex:\n+        grammar = xgr.Grammar.from_regex(guided_params.regex)\n+    elif guided_params.choice:\n+        # Create a regex that matches any of the choices\n+        choices_str = \"|\".join(f\"({choice})\" for choice in guided_params.choice)\n+        grammar = xgr.Grammar.from_regex(choices_str)\n+    elif guided_params.grammar:\n+        grammar = xgr.Grammar.from_ebnf(guided_params.grammar)\n+    elif guided_params.json_object:\n+        # For json_object, use a generic JSON schema\n+        grammar = xgr.Grammar.builtin_json_grammar()\n+    else:\n+        return None\n+\n+    # Create the matcher\n+    matcher = xgr.GrammarMatcher(grammar, tokenizer_info)\n+    \n+    return XGrammarLogitsProcessor(matcher, tokenizer_info)\n+\n+\n+async def get_xgrammar_guided_decoding_logits_processor(\n+        guided_params: GuidedDecodingParams,\n+        tokenizer: PreTrainedTokenizerBase) -> Optional[LogitsProcessor]:\n+    \"\"\"Async version of get_xgrammar_guided_decoding_logits_processor.\"\"\"\n+    # XGrammar operations are CPU-bound, so we can just call the sync version\n+    return get_local_xgrammar_guided_decoding_logits_processor(\n+        guided_params, tokenizer)\n", "model_name_or_path": "gpt-5-2025-08-07"}
