{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 24ddd35ab..ded7667e7 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -50,11 +50,12 @@ def decode_tokens(\n     `skip_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n+    decode_method = getattr(tokenizer, \"_decode\", tokenizer.decode)\n     if skip_special_tokens is not None:\n-        return tokenizer.decode(token_ids,\n-                                skip_special_tokens=skip_special_tokens)\n+        return decode_method(token_ids,\n+                             skip_special_tokens=skip_special_tokens)\n \n-    return tokenizer.decode(token_ids)\n+    return decode_method(token_ids)\n \n \n def encode_tokens(\n@@ -72,7 +73,12 @@ def encode_tokens(\n     `add_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n-\n+    encode_method = getattr(tokenizer, \"_encode\", tokenizer.encode)\n+    \n+    # Fast path: if no optional arguments, avoid dict construction\n+    if max_length is None and truncation is None and add_special_tokens is None:\n+        return encode_method(text)\n+    \n     kw_args: dict[str, Any] = {}\n     if max_length is not None:\n         kw_args[\"max_length\"] = max_length\n@@ -83,7 +89,7 @@ def encode_tokens(\n     if add_special_tokens is not None:\n         kw_args[\"add_special_tokens\"] = add_special_tokens\n \n-    return tokenizer.encode(text, **kw_args)\n+    return encode_method(text, **kw_args)\n \n \n def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n", "model_name_or_path": "gpt-5-2025-08-07"}
