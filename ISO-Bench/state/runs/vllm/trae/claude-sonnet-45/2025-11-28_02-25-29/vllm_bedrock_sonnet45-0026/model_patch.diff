diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py
index 24ddd35ab..ded7667e7 100644
--- a/vllm/transformers_utils/tokenizer.py
+++ b/vllm/transformers_utils/tokenizer.py
@@ -50,11 +50,12 @@ def decode_tokens(
     `skip_special_tokens=None` means to use the backend's default
     settings.
     """
+    decode_method = getattr(tokenizer, "_decode", tokenizer.decode)
     if skip_special_tokens is not None:
-        return tokenizer.decode(token_ids,
-                                skip_special_tokens=skip_special_tokens)
+        return decode_method(token_ids,
+                             skip_special_tokens=skip_special_tokens)
 
-    return tokenizer.decode(token_ids)
+    return decode_method(token_ids)
 
 
 def encode_tokens(
@@ -72,7 +73,12 @@ def encode_tokens(
     `add_special_tokens=None` means to use the backend's default
     settings.
     """
-
+    encode_method = getattr(tokenizer, "_encode", tokenizer.encode)
+    
+    # Fast path: if no optional arguments, avoid dict construction
+    if max_length is None and truncation is None and add_special_tokens is None:
+        return encode_method(text)
+    
     kw_args: dict[str, Any] = {}
     if max_length is not None:
         kw_args["max_length"] = max_length
@@ -83,7 +89,7 @@ def encode_tokens(
     if add_special_tokens is not None:
         kw_args["add_special_tokens"] = add_special_tokens
 
-    return tokenizer.encode(text, **kw_args)
+    return encode_method(text, **kw_args)
 
 
 def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:
