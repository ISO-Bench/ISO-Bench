diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py
index c6c8a6034..25f8574b8 100644
--- a/vllm/attention/backends/mla/utils.py
+++ b/vllm/attention/backends/mla/utils.py
@@ -1,17 +1,25 @@
 from abc import abstractmethod
 from dataclasses import dataclass
-from typing import Any, Dict, Generic, List, Optional
+from typing import Any, Dict, Generic, List, Optional, Tuple
 
 import torch
+from compressed_tensors.quantization import QuantizationStrategy
 
 from vllm import _custom_ops as ops
 from vllm import envs
 from vllm.attention.backends.abstract import (AttentionLayer,
                                               AttentionMetadata,
                                               MLAAttentionImpl, T)
-from vllm.distributed import get_tensor_model_parallel_world_size
+from vllm.distributed import (get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
 from vllm.model_executor.layers.linear import (ColumnParallelLinear,
-                                               RowParallelLinear)
+                                               LinearBase, RowParallelLinear,
+                                               UnquantizedLinearMethod)
+from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501
+    CompressedTensorsLinearMethod)
+from vllm.model_executor.layers.quantization.compressed_tensors.schemes import (
+    CompressedTensorsW8A8Fp8)
+from vllm.model_executor.layers.quantization.fp8 import Fp8LinearMethod
 from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding
 from vllm.vllm_flash_attn import flash_attn_varlen_func
 
diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index da09bb70b..892d6ee44 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -106,7 +106,7 @@ class TritonMLAState(AttentionState):
         self._graph_block_tables = torch.from_numpy(
             self.runner.graph_block_tables).to(device=self.runner.device)
 
-        self._positions = torch.zeros((max_batch_size, ),
+        self._positions = torch.empty((max_batch_size, ),
                                       dtype=torch.long,
                                       device=self.runner.device)
 
@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         B = q_nope.shape[0]
 
         q = torch.cat([q_nope, q_pe], dim=-1)
-        o = torch.zeros(B,
+        o = torch.empty(B,
                         self.num_heads,
                         self.kv_lora_rank,
                         dtype=q.dtype,
diff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py
index 83055d600..27ea2d9e0 100644
--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py
@@ -35,7 +35,8 @@ def pack_quantized_values_into_int32(w_q: torch.Tensor,
     assert w_q_perm.shape[-1] % pack_factor == 0
     new_shape_perm[-1] //= pack_factor
 
-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res.zero_()
     for i in range(pack_factor):
         res |= (w_q_perm[..., i::pack_factor] & mask) << wtype.size_bits * i
 
@@ -56,7 +57,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,
     new_shape_perm = list(w_q_perm.shape)
     new_shape_perm[-1] *= pack_factor
 
-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)
     for i in range(pack_factor):
         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask
 
@@ -105,9 +106,7 @@ def permute_rows(q_w: torch.Tensor,
     orig_device = q_w.device
     k_size, _ = q_w.shape
 
-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)
-    for i in range(k_size):
-        g_idx[i] = i // group_size
+    g_idx = torch.arange(k_size, dtype=torch.int32) // group_size
 
     # Simulate act_order by doing a random permutation on K
     rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)
diff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py
index 0b44f0d06..0526d028c 100644
--- a/vllm/model_executor/models/deepseek_v3.py
+++ b/vllm/model_executor/models/deepseek_v3.py
@@ -556,11 +556,11 @@ class DeepseekV3ForCausalLM(nn.Module, SupportsPP):
             device: torch.device) -> IntermediateTensors:
         return IntermediateTensors({
             "hidden_states":
-            torch.zeros((batch_size, self.config.hidden_size),
+            torch.empty((batch_size, self.config.hidden_size),
                         dtype=dtype,
                         device=device),
             "residual":
-            torch.zeros((batch_size, self.config.hidden_size),
+            torch.empty((batch_size, self.config.hidden_size),
                         dtype=dtype,
                         device=device),
         })
