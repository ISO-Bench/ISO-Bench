{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3e1c76c", "model_patch": "diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py\nindex c6c8a6034..25f8574b8 100644\n--- a/vllm/attention/backends/mla/utils.py\n+++ b/vllm/attention/backends/mla/utils.py\n@@ -1,17 +1,25 @@\n from abc import abstractmethod\n from dataclasses import dataclass\n-from typing import Any, Dict, Generic, List, Optional\n+from typing import Any, Dict, Generic, List, Optional, Tuple\n \n import torch\n+from compressed_tensors.quantization import QuantizationStrategy\n \n from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionLayer,\n                                               AttentionMetadata,\n                                               MLAAttentionImpl, T)\n-from vllm.distributed import get_tensor_model_parallel_world_size\n+from vllm.distributed import (get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n-                                               RowParallelLinear)\n+                                               LinearBase, RowParallelLinear,\n+                                               UnquantizedLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501\n+    CompressedTensorsLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.schemes import (\n+    CompressedTensorsW8A8Fp8)\n+from vllm.model_executor.layers.quantization.fp8 import Fp8LinearMethod\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n from vllm.vllm_flash_attn import flash_attn_varlen_func\n \ndiff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex da09bb70b..892d6ee44 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -106,7 +106,7 @@ class TritonMLAState(AttentionState):\n         self._graph_block_tables = torch.from_numpy(\n             self.runner.graph_block_tables).to(device=self.runner.device)\n \n-        self._positions = torch.zeros((max_batch_size, ),\n+        self._positions = torch.empty((max_batch_size, ),\n                                       dtype=torch.long,\n                                       device=self.runner.device)\n \n@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):\n         B = q_nope.shape[0]\n \n         q = torch.cat([q_nope, q_pe], dim=-1)\n-        o = torch.zeros(B,\n+        o = torch.empty(B,\n                         self.num_heads,\n                         self.kv_lora_rank,\n                         dtype=q.dtype,\ndiff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 83055d600..27ea2d9e0 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -35,7 +35,8 @@ def pack_quantized_values_into_int32(w_q: torch.Tensor,\n     assert w_q_perm.shape[-1] % pack_factor == 0\n     new_shape_perm[-1] //= pack_factor\n \n-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)\n+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)\n+    res.zero_()\n     for i in range(pack_factor):\n         res |= (w_q_perm[..., i::pack_factor] & mask) << wtype.size_bits * i\n \n@@ -56,7 +57,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,\n     new_shape_perm = list(w_q_perm.shape)\n     new_shape_perm[-1] *= pack_factor\n \n-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)\n+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)\n     for i in range(pack_factor):\n         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask\n \n@@ -105,9 +106,7 @@ def permute_rows(q_w: torch.Tensor,\n     orig_device = q_w.device\n     k_size, _ = q_w.shape\n \n-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)\n-    for i in range(k_size):\n-        g_idx[i] = i // group_size\n+    g_idx = torch.arange(k_size, dtype=torch.int32) // group_size\n \n     # Simulate act_order by doing a random permutation on K\n     rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)\ndiff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py\nindex 0b44f0d06..0526d028c 100644\n--- a/vllm/model_executor/models/deepseek_v3.py\n+++ b/vllm/model_executor/models/deepseek_v3.py\n@@ -556,11 +556,11 @@ class DeepseekV3ForCausalLM(nn.Module, SupportsPP):\n             device: torch.device) -> IntermediateTensors:\n         return IntermediateTensors({\n             \"hidden_states\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n             \"residual\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n         })\n", "model_name_or_path": "gpt-5-2025-08-07"}
