{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3a1e648", "model_patch": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a40b..329371041 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -8,18 +8,19 @@ import torch.nn.functional as F\n from vllm.v1.sample.metadata import SamplingMetadata\n from vllm.v1.sample.rejection_sampler import INVALID_TOKEN_ID, RejectionSampler\n \n-DEVICE = \"cpu\"\n+DEVICE = \"cuda\"\n \n \n @pytest.fixture\n-def sampler():\n+def rejection_sampler():\n     return RejectionSampler()\n \n \n-def create_logits_tensor(token_ids: list[list[int]],\n+def create_logits_tensor(output_token_ids: list[list[int]],\n                          vocab_size: int = 100) -> torch.Tensor:\n     \"\"\"Helper function to create logits tensor that \n        will produce desired token ids on argmax\"\"\"\n+    token_ids = [tokens[:-1] for tokens in output_token_ids]\n     num_total_tokens = sum(len(tokens) for tokens in token_ids)\n     logits = torch.full((num_total_tokens, vocab_size), -100.0, device=DEVICE)\n     start_loc = 0\n@@ -61,7 +62,7 @@ def create_sampling_metadata(\n \n \n ########################### Tests for Greedy Sampling ###################\n-def test_perfect_match(sampler):\n+def test_perfect_match(rejection_sampler):\n     \"\"\"Test when output tokens perfectly match speculated tokens\"\"\"\n     spec_tokens = [[1, 2, 3]]\n     output_tokens = [[1, 2, 3, 4]]  # 4 is the bonus token\n@@ -71,14 +72,14 @@ def test_perfect_match(sampler):\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[1, 2, 3, 4]],\n                             dtype=torch.int,\n                             device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_early_mismatch(sampler):\n+def test_early_mismatch(rejection_sampler):\n     \"\"\"Test when there's an early mismatch in tokens\"\"\"\n     spec_tokens = [[1, 2, 3]]\n     output_tokens = [[1, 5, 3, 4]]  # Mismatch at position 1\n@@ -88,14 +89,14 @@ def test_early_mismatch(sampler):\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[1, 5, INVALID_TOKEN_ID, INVALID_TOKEN_ID]],\n                             dtype=torch.int,\n                             device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_multiple_sequences(sampler):\n+def test_multiple_sequences(rejection_sampler):\n     \"\"\"Test handling multiple sequences of speculated tokens\"\"\"\n     spec_tokens = [[1, 2], [3]]\n     output_tokens = [[1, 2, 5], [3,\n@@ -106,14 +107,14 @@ def test_multiple_sequences(sampler):\n     bonus_token_tensor = torch.tensor(\n         [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[1, 2, 5], [3, 4, INVALID_TOKEN_ID]],\n                             dtype=torch.int,\n                             device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_single_token_sequence(sampler):\n+def test_single_token_sequence(rejection_sampler):\n     \"\"\"Test handling sequences with single token\"\"\"\n     spec_tokens = [[1]]\n     output_tokens = [[1, 2]]  # Single token with bonus token 2\n@@ -123,12 +124,12 @@ def test_single_token_sequence(sampler):\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[1, 2]], dtype=torch.int, device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_empty_sequence(sampler):\n+def test_empty_sequence(rejection_sampler):\n     \"\"\"Test handling empty sequence of speculated tokens\"\"\"\n     spec_tokens: list[list[int]] = [[]]\n     output_tokens = [[5]]  # Just the bonus token\n@@ -138,12 +139,12 @@ def test_empty_sequence(sampler):\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[5]], dtype=torch.int, device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_multiple_mismatches(sampler):\n+def test_multiple_mismatches(rejection_sampler):\n     \"\"\"Test handling multiple sequences with mismatches\"\"\"\n     spec_tokens = [[1, 2, 3], [4, 5, 6]]\n     output_tokens = [[1, 2, 7, 6], [4, 8, 6,\n@@ -154,7 +155,7 @@ def test_multiple_mismatches(sampler):\n     bonus_token_tensor = torch.tensor(\n         [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected = torch.tensor([[1, 2, 7, INVALID_TOKEN_ID],\n                              [4, 8, INVALID_TOKEN_ID, INVALID_TOKEN_ID]],\n                             dtype=torch.int,\n@@ -170,14 +171,14 @@ def test_multiple_mismatches(sampler):\n         ([[1, 2], [3, 4]], [[1, 5, 6], [3, 4, 7]],\n          [[1, 5, INVALID_TOKEN_ID], [3, 4, 7]]),  # Mixed matches\n     ])\n-def test_parametrized_cases(sampler, spec_tokens, output_tokens, expected):\n+def test_parametrized_cases(rejection_sampler, spec_tokens, output_tokens, expected):\n     \"\"\"Parametrized test for various matching scenarios\"\"\"\n     metadata = create_sampling_metadata(all_greedy=True)\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([tokens[-1] for tokens in output_tokens],\n                                       device=logits.device)\n \n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    output = rejection_sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n     expected_tensor = torch.tensor(expected,\n                                    dtype=torch.int,\n                                    device=logits.device)\n@@ -190,7 +191,7 @@ def test_parametrized_cases(sampler, spec_tokens, output_tokens, expected):\n @pytest.mark.parametrize(\"batch_size\", [1, 4, 8])\n @pytest.mark.parametrize(\"frac_seeded\", [0.0, 0.5])\n @pytest.mark.parametrize(\"n_rep\", [20])\n-def test_deterministic_when_seeded(sampler, k: int, vocab_size: int,\n+def test_deterministic_when_seeded(rejection_sampler, k: int, vocab_size: int,\n                                    batch_size: int, frac_seeded: float,\n                                    n_rep: int):\n     draft_probs = torch.rand(batch_size, k, vocab_size, dtype=torch.float32)\n@@ -217,7 +218,7 @@ def test_deterministic_when_seeded(sampler, k: int, vocab_size: int,\n \n         sampling_metadata = create_sampling_metadata(all_greedy=False,\n                                                      generators=seeded_seqs)\n-        rep_result = sampler(draft_token_ids.tolist(), draft_probs,\n+        rep_result = rejection_sampler(draft_token_ids.tolist(), draft_probs,\n                              bonus_token_ids, target_probs, sampling_metadata)\n \n         results.append(rep_result)\n@@ -329,7 +330,7 @@ def estimate_rejection_sampling_pdf(\n     Returns:\n         Estimated probability distribution of the output tokens.\n     \"\"\"\n-    sampler = RejectionSampler()\n+    rejection_sampler = RejectionSampler()\n     # Repeat draft probs num_samples times.\n     draft_probs = draft_probs.reshape(1, 1,\n                                       vocab_size).repeat(num_samples, k, 1)\n@@ -345,11 +346,11 @@ def estimate_rejection_sampling_pdf(\n                                             num_samples, k)\n \n     # Bonus tokens not used but required.\n-    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n-                                  device=DEVICE).repeat(num_samples, 1)\n+    bonus_token_ids = torch.empty((num_samples, 1), dtype=torch.int64,\n+                                  device=DEVICE)\n \n     sampling_metadata = create_sampling_metadata(all_greedy=False)\n-    output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\n+    output_token_ids = rejection_sampler(draft_token_ids.tolist(), draft_probs,\n                                bonus_token_ids, target_probs,\n                                sampling_metadata)\n     output_token_ids = output_token_ids[:, :-1].flatten()\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex bf214f314..b2937462a 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -35,7 +35,6 @@ if TYPE_CHECKING:\n     VLLM_TRACE_FUNCTION: int = 0\n     VLLM_ATTENTION_BACKEND: Optional[str] = None\n     VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None\n-    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False\n     VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False\n     VLLM_PP_LAYER_PARTITION: Optional[str] = None\n     VLLM_CPU_KVCACHE_SPACE: int = 0\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 66015382b..b00f6988d 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -191,10 +191,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -212,16 +212,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -234,27 +234,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
