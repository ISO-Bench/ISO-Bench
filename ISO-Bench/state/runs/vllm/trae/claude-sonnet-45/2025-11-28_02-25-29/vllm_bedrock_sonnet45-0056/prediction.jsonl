{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-beebf47", "model_patch": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed86..fc585ee9e 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -244,6 +244,7 @@ class LLM:\n             engine_args, usage_context=UsageContext.LLM_CLASS)\n \n         self.request_counter = Counter()\n+        self.default_sampling_params: Union[dict[str, Any], None] = None\n \n     @staticmethod\n     def get_engine_class() -> type[LLMEngine]:\n@@ -268,10 +269,11 @@ class LLM:\n             tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n \n     def get_default_sampling_params(self) -> SamplingParams:\n-        diff_sampling_param = (\n-            self.llm_engine.model_config.get_diff_sampling_param())\n-        if diff_sampling_param:\n-            return SamplingParams.from_optional(**diff_sampling_param)\n+        if self.default_sampling_params is None:\n+            self.default_sampling_params = (\n+                self.llm_engine.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            return SamplingParams.from_optional(**self.default_sampling_params)\n         return SamplingParams()\n \n     @overload\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0fc..76d3f68dd 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -105,10 +105,10 @@ class OpenAIServingChat(OpenAIServing):\n                                 \"been registered\") from e\n \n         self.enable_prompt_tokens_details = enable_prompt_tokens_details\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\"Overwriting default chat sampling param with: %s\",\n-                        diff_sampling_param)\n+                        self.default_sampling_params)\n \n     async def create_chat_completion(\n         self,\n@@ -211,16 +211,14 @@ class OpenAIServingChat(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n-                        default_max_tokens, default_sampling_params)\n+                        default_max_tokens, self.default_sampling_params)\n                 else:\n                     sampling_params = request.to_sampling_params(\n                         default_max_tokens,\n                         self.model_config.logits_processor_pattern,\n-                        default_sampling_params)\n+                        self.default_sampling_params)\n \n                 self._log_inputs(request_id,\n                                  request_prompts[i],\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af84f..dbf2bed0d 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -51,11 +51,11 @@ class OpenAIServingCompletion(OpenAIServing):\n                          models=models,\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def create_completion(\n         self,\n@@ -120,16 +120,14 @@ class OpenAIServingCompletion(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n-                        default_max_tokens, default_sampling_params)\n+                        default_max_tokens, self.default_sampling_params)\n                 else:\n                     sampling_params = request.to_sampling_params(\n                         default_max_tokens,\n                         self.model_config.logits_processor_pattern,\n-                        default_sampling_params)\n+                        self.default_sampling_params)\n \n                 request_id_item = f\"{request_id}-{i}\"\n \ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a5e..84067e520 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -161,11 +161,11 @@ class OpenAIServingTranscription(OpenAIServing):\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n \n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def _preprocess_transcription(\n         self,\n@@ -273,9 +273,8 @@ class OpenAIServingTranscription(OpenAIServing):\n         try:\n             # TODO(rob): subtract len of tokenized prompt.\n             default_max_tokens = self.model_config.max_model_len\n-            default_params = self.model_config.get_diff_sampling_param()\n             sampling_params = request.to_sampling_params(\n-                default_max_tokens, default_params)\n+                default_max_tokens, self.default_sampling_params)\n \n             self._log_inputs(\n                 request_id,\n", "model_name_or_path": "gpt-5-2025-08-07"}
