{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5c04bb8", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex 5904ad1f1..e3797b13c 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -25,7 +25,7 @@\n # limitations under the License.\n \"\"\"Inference-only Qwen2.5-VL model compatible with HuggingFace weights.\"\"\"\n from collections.abc import Iterable, Mapping\n-from functools import partial\n+from functools import lru_cache, partial\n from typing import Callable, Literal, Optional, TypedDict, Union\n \n import torch\n@@ -478,8 +478,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         super().__init__()\n         self.dim = dim\n         self.theta = theta\n-        inv_freq = 1.0 / (theta\n-                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        inv_freq = 1.0 / (theta**(\n+            torch.arange(0, dim, 2, dtype=torch.float, device='cpu') / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self._seq_len_cached = 0\n         self._freqs_cached = None\n@@ -567,25 +567,33 @@ class Qwen2_5_VisionTransformer(nn.Module):\n     def device(self) -> torch.device:\n         return self.patch_embed.proj.weight.device\n \n+    @lru_cache(maxsize=32)\n+    def _get_pos_ids_for_grid(self, t: int, h: int, w: int,\n+                              device: torch.device) -> torch.Tensor:\n+        \"\"\"Cached computation of position IDs for a given grid size.\"\"\"\n+        hpos_ids = torch.arange(h, device=device).unsqueeze(1).expand(-1, w)\n+        wpos_ids = torch.arange(w, device=device).unsqueeze(0).expand(h, -1)\n+        hpos_ids = hpos_ids.reshape(\n+            h // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+            w // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+        ).permute(0, 2, 1, 3).flatten()\n+        wpos_ids = wpos_ids.reshape(\n+            h // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+            w // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+        ).permute(0, 2, 1, 3).flatten()\n+        return torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1)\n+\n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n+        device = grid_thw.device\n         pos_ids = []\n         for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n-            hpos_ids = hpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            wpos_ids = wpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n             pos_ids.append(\n-                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n+                self._get_pos_ids_for_grid(t.item(), h.item(), w.item(),\n+                                           device))\n         pos_ids = torch.cat(pos_ids, dim=0)\n         max_grid_size = grid_thw[:, 1:].max()\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n", "model_name_or_path": "gpt-5-2025-08-07"}
