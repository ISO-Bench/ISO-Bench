{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-a4d577b", "model_patch": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 8bc33e841..3e810e525 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -26,17 +26,13 @@ def create_logits_tensor(token_ids: List[int],\n def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n     batch_size = len(spec_tokens)\n     return SamplingMetadata(\n-        temperature=0.0,\n+        temperature=torch.tensor([]),\n         all_greedy=True,\n         all_random=False,\n-        rejection_sampling=True,\n         spec_token_ids=spec_tokens,\n         top_p=None,\n         top_k=None,\n-        no_top_p=False,\n-        no_top_k=False,\n         min_p=torch.empty(batch_size, ),\n-        no_min_p=True,\n         generators={},\n         max_num_logprobs=0,\n         no_penalties=False,\n@@ -45,8 +41,7 @@ def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n         presence_penalties=torch.tensor([]),\n         repetition_penalties=torch.tensor([]),\n         output_token_ids=[],\n-        min_tokens=[],\n-        stop_token_ids=[],\n+        min_tokens={},\n         logit_bias=[None] * batch_size,\n     )\n \ndiff --git a/tests/v1/sample/test_sampler.py b/tests/v1/sample/test_sampler.py\nindex a4bd651f8..6b7d0a2ce 100644\n--- a/tests/v1/sample/test_sampler.py\n+++ b/tests/v1/sample/test_sampler.py\n@@ -74,28 +74,23 @@ def _create_default_sampling_metadata(\n                               size=np.random.randint(\n                                   1, MAX_NUM_PROMPT_TOKENS)).tolist())\n     fake_sampling_metadata = SamplingMetadata(\n-        temperature=torch.full((batch_size, ), 0.0),\n+        temperature=torch.tensor([]),\n         all_greedy=True,\n         all_random=False,\n-        rejection_sampling=False,\n         top_p=torch.empty(batch_size, ),\n         top_k=torch.empty(batch_size, ),\n-        no_top_p=True,\n-        no_top_k=True,\n         min_p=torch.empty(batch_size, ),\n-        no_min_p=True,\n         generators={},\n         max_num_logprobs=0,\n         prompt_token_ids=_create_prompt_tokens_tensor(prompt_token_ids,\n                                                       vocab_size, device),\n         output_token_ids=output_token_ids,\n         spec_token_ids=[],\n-        frequency_penalties=_create_penalty_tensor(batch_size, 0.0, device),\n-        presence_penalties=_create_penalty_tensor(batch_size, 0.0, device),\n-        repetition_penalties=_create_penalty_tensor(batch_size, 1.0, device),\n+        frequency_penalties=torch.tensor([]),\n+        presence_penalties=torch.tensor([]),\n+        repetition_penalties=torch.tensor([]),\n         no_penalties=True,\n-        min_tokens=[],\n-        stop_token_ids=[],\n+        min_tokens={},\n         logit_bias=[None] * batch_size,\n     )\n     return fake_sampling_metadata\ndiff --git a/vllm/v1/sample/metadata.py b/vllm/v1/sample/metadata.py\nindex ea64181c0..2ae6dd594 100644\n--- a/vllm/v1/sample/metadata.py\n+++ b/vllm/v1/sample/metadata.py\n@@ -12,15 +12,11 @@ class SamplingMetadata:\n     temperature: torch.Tensor\n     all_greedy: bool\n     all_random: bool\n-    rejection_sampling: bool\n     spec_token_ids: List[List[int]]\n \n     top_p: torch.Tensor\n     top_k: torch.Tensor\n-    no_top_p: bool\n-    no_top_k: bool\n     min_p: torch.Tensor\n-    no_min_p: bool\n \n     generators: Dict[int, torch.Generator]\n \n@@ -34,7 +30,13 @@ class SamplingMetadata:\n     repetition_penalties: torch.Tensor\n \n     output_token_ids: List[List[int]]\n-    min_tokens: List[int]\n-    stop_token_ids: List[Set[int]]\n+    min_tokens: Dict[int, int]\n \n     logit_bias: List[Optional[Dict[int, float]]]\n+    \n+    # Optional fields with defaults\n+    rejection_sampling: bool = False\n+    no_top_p: bool = True\n+    no_top_k: bool = True\n+    no_min_p: bool = True\n+    stop_token_ids: Optional[List[Set[int]]] = None\ndiff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex cb7411a44..6bfe8d19f 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -171,10 +171,8 @@ class InputBatch:\n                 self.repetition_penalties_cpu_tensor.numpy()\n         self.repetition_penalties_reqs: Set[str] = set()\n \n-        self.min_tokens: List[int] = [0] * max_num_reqs\n-        self.stop_token_ids: List[Set[int]] = [\n-            set() for _ in range(max_num_reqs)\n-        ]\n+        self.min_tokens: Dict[int, int] = {}\n+        self.stop_token_ids: Dict[int, Set[int]] = {}\n         self.prompt_token_ids: Optional[torch.Tensor] = None\n \n         # lora related\n@@ -300,6 +298,8 @@ class InputBatch:\n         self.generators.pop(req_index, None)\n         self.num_logprobs.pop(req_id, None)\n         self.num_prompt_logprobs.pop(req_id, None)\n+        self.min_tokens.pop(req_index, None)\n+        self.stop_token_ids.pop(req_index, None)\n \n         # LoRA\n         lora_id = self.request_lora_mapping[req_index]\n@@ -327,6 +327,8 @@ class InputBatch:\n         self.generators.clear()\n         self.num_logprobs.clear()\n         self.num_prompt_logprobs.clear()\n+        self.min_tokens.clear()\n+        self.stop_token_ids.clear()\n         self.request_lora_mapping.fill(0)\n         self.lora_id_to_lora_request.clear()\n         self.lora_id_to_request_ids.clear()\n@@ -379,9 +381,10 @@ class InputBatch:\n             self.repetition_penalties_cpu[\n                 empty_index] = self.repetition_penalties_cpu[last_req_index]\n             self.min_p_cpu[empty_index] = self.min_p_cpu[last_req_index]\n-            self.min_tokens[empty_index] = self.min_tokens[last_req_index]\n-            self.stop_token_ids[empty_index] = self.stop_token_ids[\n-                last_req_index]\n+            if last_req_index in self.min_tokens:\n+                self.min_tokens[empty_index] = self.min_tokens.pop(last_req_index)\n+            if last_req_index in self.stop_token_ids:\n+                self.stop_token_ids[empty_index] = self.stop_token_ids.pop(last_req_index)\n             generator = self.generators.pop(last_req_index, None)\n             if generator is not None:\n                 self.generators[empty_index] = generator\n@@ -469,8 +472,8 @@ class InputBatch:\n             repetition_penalties=self.repetition_penalties[:self.num_reqs],\n             output_token_ids=output_token_ids,\n             spec_token_ids=spec_token_ids,\n-            min_tokens=self.min_tokens[:self.num_reqs],\n-            stop_token_ids=self.stop_token_ids[:self.num_reqs],\n+            min_tokens=self.min_tokens,\n+            stop_token_ids=self.stop_token_ids,\n             no_penalties=self.no_penalties,\n             logit_bias=self.logit_bias[:self.num_reqs],\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
