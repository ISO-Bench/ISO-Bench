{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-2f38518", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e79145300..594a43c20 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -66,8 +66,9 @@ class InputBatch:\n             pin_memory=False,\n         )\n         self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n-        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n+        self.num_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n         self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n+        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n         self.block_table = torch.zeros(\n@@ -189,6 +190,7 @@ class InputBatch:\n         end_idx = start_idx + len(request.output_token_ids)\n         self.token_ids_cpu[req_index,\n                            start_idx:end_idx] = request.output_token_ids\n+        self.num_tokens[req_index] = request.num_tokens\n \n         self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens\n         num_blocks = len(request.block_ids)\n@@ -290,10 +292,11 @@ class InputBatch:\n             self.req_ids[last_req_index] = None\n             self.req_id_to_index[req_id] = empty_index\n \n-            # TODO(woosuk): Optimize the copy of token_ids_cpu and\n-            # block_table_cpu.\n-            self.token_ids_cpu[empty_index] = self.token_ids_cpu[\n-                last_req_index]\n+            # Optimize the copy of token_ids_cpu by only copying the used tokens.\n+            num_tokens = self.num_tokens[last_req_index]\n+            self.token_ids_cpu[empty_index, :num_tokens] = self.token_ids_cpu[\n+                last_req_index, :num_tokens]\n+            self.num_tokens[empty_index] = num_tokens\n             self.num_prompt_tokens[empty_index] = \\\n                 self.num_prompt_tokens[last_req_index]\n             self.num_computed_tokens_cpu[\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54e8..eb83c9a86 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -143,27 +143,27 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                              dtype=torch.int32,\n                                              device=\"cpu\",\n                                              pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
