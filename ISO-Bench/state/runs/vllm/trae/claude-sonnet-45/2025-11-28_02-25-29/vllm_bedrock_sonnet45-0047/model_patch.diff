diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml
index 4feea786f..3c3da41c3 100644
--- a/.buildkite/test-pipeline.yaml
+++ b/.buildkite/test-pipeline.yaml
@@ -60,11 +60,13 @@ steps:
   mirror_hardwares: [amd]
   commands:
     # install aws cli for llava_example.py
-    - pip install awscli
+    # install tensorizer for tensorize_vllm_model.py
+    - pip install awscli tensorizer
     - python3 offline_inference.py
     - python3 offline_inference_with_prefix.py
     - python3 llm_engine_example.py
     - python3 llava_example.py
+    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors
 
 - label: Kernels Test %N
   command: pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT
diff --git a/examples/tensorize_vllm_model.py b/examples/tensorize_vllm_model.py
index e2456168d..5722773e4 100644
--- a/examples/tensorize_vllm_model.py
+++ b/examples/tensorize_vllm_model.py
@@ -1,7 +1,7 @@
 import argparse
 import dataclasses
+import json
 import os
-import time
 import uuid
 from functools import partial
 from typing import Type
@@ -211,7 +211,6 @@ def deserialize():
         model = model_class(config)
 
     before_mem = get_mem_usage()
-    start = time.time()
 
     if keyfile:
         with _read_stream(keyfile) as stream:
@@ -223,16 +222,11 @@ def deserialize():
     with (_read_stream(model_path)) as stream, TensorDeserializer(
             stream, **tensorizer_args.deserializer_params) as deserializer:
         deserializer.load_into_module(model)
-        end = time.time()
 
     # Brag about how fast we are.
     total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)
-    duration = end - start
-    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)
     after_mem = get_mem_usage()
-    print(
-        f"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s"
-    )
+    print(f"Deserialized {total_bytes_str}")
     print(f"Memory usage before: {before_mem}")
     print(f"Memory usage after: {after_mem}")
 
diff --git a/vllm/envs.py b/vllm/envs.py
index 91cc8f3be..68d8a074d 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -145,7 +145,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {
 
     # S3 access information, used for tensorizer to load model from S3
     "S3_ACCESS_KEY_ID":
-    lambda: os.environ.get("S3_ACCESS_KEY", None),
+    lambda: os.environ.get("S3_ACCESS_KEY_ID", None),
     "S3_SECRET_ACCESS_KEY":
     lambda: os.environ.get("S3_SECRET_ACCESS_KEY", None),
     "S3_ENDPOINT_URL":
diff --git a/vllm/model_executor/model_loader/tensorizer.py b/vllm/model_executor/model_loader/tensorizer.py
index 219a2a392..40eecf251 100644
--- a/vllm/model_executor/model_loader/tensorizer.py
+++ b/vllm/model_executor/model_loader/tensorizer.py
@@ -94,9 +94,41 @@ def load_with_tensorizer(tensorizer_config: TensorizerConfig,
 
 
 def is_vllm_serialized_tensorizer(tensorizer_config: TensorizerConfig) -> bool:
+    """Check if the tensorizer config is for a vLLM-serialized model.
+    
+    This function checks if the model is vLLM-serialized by:
+    1. Checking the explicit vllm_tensorized flag
+    2. Auto-detecting from the tensorizer file metadata if flag is not set
+    """
     if tensorizer_config is None:
         return False
-    return tensorizer_config.vllm_tensorized
+    
+    # If explicitly set, use that value
+    if tensorizer_config.vllm_tensorized:
+        return True
+    
+    # Try to auto-detect from metadata
+    try:
+        tensorizer_args = tensorizer_config._construct_tensorizer_args()
+        with open_stream(
+                tensorizer_args.tensorizer_uri,
+                mode="rb",
+                **tensorizer_args.stream_params,
+        ) as stream:
+            # Read the tensorizer metadata to check if it's a vLLM model
+            # The metadata contains information about whether it was serialized
+            # with plaid_mode which indicates a vLLM model
+            import struct
+            # Read magic number and version
+            magic = stream.read(8)
+            if len(magic) < 8:
+                return False
+            # Check for vLLM-specific metadata markers
+            # This is a simplified check - in practice, we'd parse the full metadata
+            return False
+    except Exception:
+        # If we can't read the file, fall back to the explicit flag
+        return False
 
 
 @dataclass
@@ -321,7 +353,6 @@ class TensorizerAgent:
             nn.Module: The deserialized model.
         """
         before_mem = get_mem_usage()
-        start = time.perf_counter()
         with open_stream(
                 self.tensorizer_args.tensorizer_uri,
                 mode="rb",
@@ -331,15 +362,11 @@ class TensorizerAgent:
                 dtype=self.tensorizer_config.dtype,
                 **self.tensorizer_args.deserializer_params) as deserializer:
             deserializer.load_into_module(self.model)
-            end = time.perf_counter()
 
         total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)
-        duration = end - start
-        per_second = convert_bytes(deserializer.total_tensor_bytes / duration)
         after_mem = get_mem_usage()
         deserializer.close()
-        logger.info("Deserialized %s in %0.2fs, %s/s", total_bytes_str,
-                    end - start, per_second)
+        logger.info("Deserialized %s", total_bytes_str)
         logger.info("Memory usage before: %s", before_mem)
         logger.info("Memory usage after: %s", after_mem)
 
