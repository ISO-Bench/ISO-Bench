{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5b8a1fd", "model_patch": "diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 36e5e1774..0d1b34e5d 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -67,9 +67,16 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 execute_model_req.seq_group_metadata_list):\n             seq_data = next(iter(seq_group_metadata.seq_data.values()))\n \n+            seq_len = seq_data.get_len()\n+            # When seq_len is less than 3072 (3K), we use CPU to perform\n+            # the ngram match. Otherwise, we use the device specified in\n+            # the model config (normally GPU). 3072 is a rough threshold\n+            # based on profiling on H100, and it can be adjusted based\n+            # on the actual performance on different hardware.\n+            cur_device = \"cpu\" if seq_len < 3072 else self.device\n             input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                         dtype=torch.long,\n-                                        device=self.device)\n+                                        device=cur_device)\n             input_length = seq_data.get_len()\n \n             for ngram_size in range(\n@@ -91,17 +98,17 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 # first_match includes \"values\" (bool), indicating whether\n                 # the match is found, and \"indices\", indicating the index\n                 # of the first match.\n-                # Note that \"first_match.values.item()\" triggers GPU-CPU\n-                # sync so it is a bit inefficient, but we have not found\n-                # a better way to do this.\n                 first_match = matches.max(dim=-1)\n                 if first_match.values.item():\n                     proposal_start_idx = first_match.indices.add_(ngram_size)\n                     spec_indices = (\n                         proposal_start_idx).repeat(sample_len) + torch.arange(\n-                            sample_len, device=self.device)\n+                            sample_len, device=cur_device)\n                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n                     res = input_ids.gather(dim=-1, index=spec_indices)\n+                    # Move result back to self.device if needed\n+                    if cur_device != self.device:\n+                        res = res.to(self.device)\n                     token_id_list.append(res)\n                     token_prob_list.append(\n                         torch.nn.functional.one_hot(\n@@ -125,7 +132,7 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                     SamplerOutput(\n                         outputs=None,\n                         sampled_token_probs=token_prob_list[idx],\n-                        logprobs=torch.zeros((sample_len, self.vocab_size),\n+                        logprobs=torch.empty((sample_len, self.vocab_size),\n                                              dtype=torch.float32,\n                                              device=self.device),\n                         sampled_token_ids=token_id_list[idx],\n", "model_name_or_path": "gpt-5-2025-08-07"}
