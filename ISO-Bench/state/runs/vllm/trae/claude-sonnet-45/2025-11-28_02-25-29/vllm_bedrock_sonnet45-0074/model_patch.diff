diff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
index 61247e930..de71313a2 100644
--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
@@ -523,7 +523,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         num_tokens, hidden_dim = a1.size()
         topk = topk_ids.size(1)
 
-        tokens_per_expert = torch.zeros(num_experts,
+        tokens_per_expert = torch.empty(num_experts,
                                         dtype=torch.int,
                                         device=a1.device)
 
@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         else:
             b_type = quant_config.quant_dtype
 
-        b_a1 = torch.zeros(
+        b_a1 = torch.empty(
             (num_local_experts, self.max_num_tokens, hidden_dim),
             dtype=b_type,
             device=a1.device)
@@ -790,7 +790,7 @@ def batched_moe_kernel_quantize_input(
         else:
             scale_shape = (E, 1, 1)
 
-        A_q_scale = torch.zeros(scale_shape,
+        A_q_scale = torch.empty(scale_shape,
                                 dtype=torch.float32,
                                 device=A.device)
 
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index 6a9767fc6..53209784a 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -950,7 +950,7 @@ def grouped_topk(
                                    -1).max(dim=-1).values  # [n, n_group]
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,
                            sorted=False)[1]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
+    group_mask = torch.empty_like(group_scores)  # [n, n_group]
     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
     score_mask = group_mask.unsqueeze(-1).expand(
         num_token, num_expert_group,
diff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py
index d0d8c7d6f..e6baf4c58 100644
--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py
+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py
@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):
         """
 
         a1 = hidden_states
-        output = a1 if inplace else torch.zeros_like(a1)
+        output = a1 if inplace else torch.empty_like(a1)
 
         local_num_experts = w1.size(0)
         if global_num_experts == -1:
diff --git a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
index 9a5315b8b..5537f1f0b 100644
--- a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
+++ b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py
@@ -115,11 +115,10 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):
         K = fused_expert_output.size(-1)
 
         if output is None:
-            output = torch.zeros((num_tokens, K),
+            output = torch.empty((num_tokens, K),
                                  device=fused_expert_output.device,
                                  dtype=fused_expert_output.dtype)
-        else:
-            output.fill_(0)
+        output.fill_(0)
 
         assert output.size() == (num_tokens, K), (
             f"Expected output size {(num_tokens, K)}, but got {output.size()}")
