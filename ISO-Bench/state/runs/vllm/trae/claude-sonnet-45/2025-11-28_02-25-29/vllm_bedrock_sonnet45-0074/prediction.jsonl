{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-8bb43b9", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\nindex 61247e930..de71313a2 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n@@ -523,7 +523,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         num_tokens, hidden_dim = a1.size()\n         topk = topk_ids.size(1)\n \n-        tokens_per_expert = torch.zeros(num_experts,\n+        tokens_per_expert = torch.empty(num_experts,\n                                         dtype=torch.int,\n                                         device=a1.device)\n \n@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         else:\n             b_type = quant_config.quant_dtype\n \n-        b_a1 = torch.zeros(\n+        b_a1 = torch.empty(\n             (num_local_experts, self.max_num_tokens, hidden_dim),\n             dtype=b_type,\n             device=a1.device)\n@@ -790,7 +790,7 @@ def batched_moe_kernel_quantize_input(\n         else:\n             scale_shape = (E, 1, 1)\n \n-        A_q_scale = torch.zeros(scale_shape,\n+        A_q_scale = torch.empty(scale_shape,\n                                 dtype=torch.float32,\n                                 device=A.device)\n \ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 6a9767fc6..53209784a 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -950,7 +950,7 @@ def grouped_topk(\n                                    -1).max(dim=-1).values  # [n, n_group]\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,\n                            sorted=False)[1]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n+    group_mask = torch.empty_like(group_scores)  # [n, n_group]\n     group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n     score_mask = group_mask.unsqueeze(-1).expand(\n         num_token, num_expert_group,\ndiff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py\nindex d0d8c7d6f..e6baf4c58 100644\n--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py\n+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py\n@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n         \"\"\"\n \n         a1 = hidden_states\n-        output = a1 if inplace else torch.zeros_like(a1)\n+        output = a1 if inplace else torch.empty_like(a1)\n \n         local_num_experts = w1.size(0)\n         if global_num_experts == -1:\ndiff --git a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\nindex 9a5315b8b..5537f1f0b 100644\n--- a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n+++ b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n@@ -115,11 +115,10 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):\n         K = fused_expert_output.size(-1)\n \n         if output is None:\n-            output = torch.zeros((num_tokens, K),\n+            output = torch.empty((num_tokens, K),\n                                  device=fused_expert_output.device,\n                                  dtype=fused_expert_output.dtype)\n-        else:\n-            output.fill_(0)\n+        output.fill_(0)\n \n         assert output.size() == (num_tokens, K), (\n             f\"Expected output size {(num_tokens, K)}, but got {output.size()}\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
