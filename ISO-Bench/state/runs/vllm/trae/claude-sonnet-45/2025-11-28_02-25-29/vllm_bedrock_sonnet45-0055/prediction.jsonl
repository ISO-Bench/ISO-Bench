{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3014c92", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 9de233896..9a42d494c 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -26,6 +26,7 @@\n import math\n from typing import Any, Optional, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n from transformers import PretrainedConfig\n@@ -1459,14 +1460,30 @@ class MRotaryEmbedding(RotaryEmbedding):\n \n     @staticmethod\n     def get_next_input_positions_tensor(\n+        out: np.ndarray,\n+        out_offset: int,\n         mrope_position_delta: int,\n         context_len: int,\n-        seq_len: int,\n-    ) -> torch.Tensor:\n-        return torch.arange(\n+        num_new_tokens: int,\n+    ) -> None:\n+        \"\"\"Fill output buffer with MRoPE position IDs.\n+        \n+        Args:\n+            out: Output buffer of shape (3, total_tokens)\n+            out_offset: Offset in output buffer to start writing\n+            mrope_position_delta: Delta value for MRoPE positions\n+            context_len: Context length\n+            num_new_tokens: Number of new tokens to generate positions for\n+        \"\"\"\n+        values = np.arange(\n             mrope_position_delta + context_len,\n-            mrope_position_delta + seq_len,\n-        ).expand(3, -1)\n+            mrope_position_delta + context_len + num_new_tokens,\n+            dtype=out.dtype,\n+        )\n+        # Broadcast to all 3 dimensions\n+        out[0, out_offset:out_offset + num_new_tokens] = values\n+        out[1, out_offset:out_offset + num_new_tokens] = values\n+        out[2, out_offset:out_offset + num_new_tokens] = values\n \n     @classmethod\n     def omni_get_updates_use_audio_in_video(\n", "model_name_or_path": "gpt-5-2025-08-07"}
