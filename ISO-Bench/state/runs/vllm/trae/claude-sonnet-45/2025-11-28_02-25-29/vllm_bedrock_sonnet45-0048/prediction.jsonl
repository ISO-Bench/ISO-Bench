{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5fc5ce0", "model_patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 45a3b484e..f43bf8143 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -522,7 +522,7 @@ else()\n   FetchContent_Declare(\n           vllm-flash-attn\n           GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git\n-          GIT_TAG d886f88165702b3c7e7744502772cd98b06be9e1\n+          GIT_TAG fdf6d72b48aea41f4ae6a89139a453dae554abc8\n           GIT_PROGRESS TRUE\n           # Don't share the vllm-flash-attn build between build types\n           BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn\n", "model_name_or_path": "gpt-5-2025-08-07"}
