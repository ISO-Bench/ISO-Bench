{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-51c31bc", "model_patch": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex 6bf5d5130..c7d3d8538 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+      list(REMOVE_ITEM GPU_FLAGS\n+        \"-D__CUDA_NO_HALF_OPERATORS__\"\n+        \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+        \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\"\n+        \"-D__CUDA_NO_HALF2_OPERATORS__\")\n     endif()\n \n   elseif(${GPU_LANG} STREQUAL \"HIP\")\ndiff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex 6d34d014c..f88eb6a2d 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -4,11 +4,18 @@\n \n #include \"dispatch_utils.h\"\n #include \"reduction_utils.cuh\"\n+#ifndef USE_ROCM\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n+#else\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n+#endif\n \n namespace vllm {\n \n-// TODO(woosuk): Further optimize this kernel.\n-template<typename scalar_t>\n+// Optimized RMS norm kernel with vectorized loads\n+template<typename scalar_t, int VEC_SIZE = 4>\n __global__ void rms_norm_kernel(\n   scalar_t* __restrict__ out,             // [..., hidden_size]\n   const scalar_t* __restrict__ input,     // [..., hidden_size]\n@@ -19,24 +26,54 @@ __global__ void rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n-  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float) input[blockIdx.x * hidden_size + idx];\n+  const int token_idx = blockIdx.x;\n+  const scalar_t* token_input = input + token_idx * hidden_size;\n+  scalar_t* token_out = out + token_idx * hidden_size;\n+\n+  // Compute variance with vectorized loads where possible\n+  const int vec_hidden_size = hidden_size / VEC_SIZE;\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    const int offset = idx * VEC_SIZE;\n+    #pragma unroll\n+    for (int i = 0; i < VEC_SIZE; i++) {\n+      const float x = (float) token_input[offset + i];\n+      variance += x * x;\n+    }\n+  }\n+  \n+  // Handle remaining elements\n+  for (int idx = vec_hidden_size * VEC_SIZE + threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n+    const float x = (float) token_input[idx];\n     variance += x * x;\n   }\n+  \n   variance = blockReduceSum<float>(variance);\n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n   }\n   __syncthreads();\n \n-  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    out[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n+  // Apply normalization with vectorized loads\n+  const float norm_factor = s_variance;\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    const int offset = idx * VEC_SIZE;\n+    #pragma unroll\n+    for (int i = 0; i < VEC_SIZE; i++) {\n+      const int pos = offset + i;\n+      float x = (float) token_input[pos];\n+      token_out[pos] = ((scalar_t) (x * norm_factor)) * weight[pos];\n+    }\n+  }\n+  \n+  // Handle remaining elements\n+  for (int idx = vec_hidden_size * VEC_SIZE + threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n+    float x = (float) token_input[idx];\n+    token_out[idx] = ((scalar_t) (x * norm_factor)) * weight[idx];\n   }\n }\n \n-// TODO: Further optimize this kernel.\n-template<typename scalar_t>\n+// Optimized fused add + RMS norm kernel with vectorization\n+template<typename scalar_t, int VEC_SIZE = 4>\n __global__ void fused_add_rms_norm_kernel(\n   scalar_t* __restrict__ input,           // [..., hidden_size]\n   scalar_t* __restrict__ residual,        // [..., hidden_size]\n@@ -47,21 +84,54 @@ __global__ void fused_add_rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n-  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    x += (float) residual[blockIdx.x * hidden_size + idx];\n+  const int token_idx = blockIdx.x;\n+  scalar_t* token_input = input + token_idx * hidden_size;\n+  scalar_t* token_residual = residual + token_idx * hidden_size;\n+\n+  // Fused add and variance computation with vectorization\n+  const int vec_hidden_size = hidden_size / VEC_SIZE;\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    const int offset = idx * VEC_SIZE;\n+    #pragma unroll\n+    for (int i = 0; i < VEC_SIZE; i++) {\n+      const int pos = offset + i;\n+      float x = (float) token_input[pos];\n+      x += (float) token_residual[pos];\n+      variance += x * x;\n+      token_residual[pos] = (scalar_t) x;\n+    }\n+  }\n+  \n+  // Handle remaining elements\n+  for (int idx = vec_hidden_size * VEC_SIZE + threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n+    float x = (float) token_input[idx];\n+    x += (float) token_residual[idx];\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n+    token_residual[idx] = (scalar_t) x;\n   }\n+  \n   variance = blockReduceSum<float>(variance);\n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n   }\n   __syncthreads();\n \n-  for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) residual[blockIdx.x * hidden_size + idx];\n-    input[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];\n+  // Apply normalization with vectorization\n+  const float norm_factor = s_variance;\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    const int offset = idx * VEC_SIZE;\n+    #pragma unroll\n+    for (int i = 0; i < VEC_SIZE; i++) {\n+      const int pos = offset + i;\n+      float x = (float) token_residual[pos];\n+      token_input[pos] = ((scalar_t) (x * norm_factor)) * weight[pos];\n+    }\n+  }\n+  \n+  // Handle remaining elements\n+  for (int idx = vec_hidden_size * VEC_SIZE + threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n+    float x = (float) token_residual[idx];\n+    token_input[idx] = ((scalar_t) (x * norm_factor)) * weight[idx];\n   }\n }\n \n@@ -76,7 +146,12 @@ void rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  // Optimize block size for better occupancy\n+  // Use 256 threads for better SM utilization\n+  const int block_size = (hidden_size < 256) ? ((hidden_size + 31) & ~31) : \n+                         (hidden_size < 512) ? 256 : \n+                         (hidden_size < 1024) ? 512 : 1024;\n+  dim3 block(block_size);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(\n@@ -102,7 +177,11 @@ void fused_add_rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  // Optimize block size for better occupancy\n+  const int block_size = (hidden_size < 256) ? ((hidden_size + 31) & ~31) : \n+                         (hidden_size < 512) ? 256 : \n+                         (hidden_size < 1024) ? 512 : 1024;\n+  dim3 block(block_size);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(\ndiff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nindex c25464e86..838d6f098 100644\n--- a/csrc/reduction_utils.cuh\n+++ b/csrc/reduction_utils.cuh\n@@ -24,7 +24,7 @@ namespace vllm {\n template<typename T>\n __inline__ __device__ T warpReduceSum(T val) {\n #pragma unroll\n-  for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1)\n+  for (int mask = 16; mask > 0; mask >>= 1)\n     val += VLLM_SHFL_XOR_SYNC(val, mask);\n   return val;\n }\n@@ -37,14 +37,12 @@ __inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n   return 5 + (warp_size >> 6);\n }\n \n-/* Calculate the sum of all elements in a block */\n+/* Calculate the sum of all elements in a block - optimized version */\n template<typename T>\n __inline__ __device__ T blockReduceSum(T val) {\n-  static __shared__ T shared[WARP_SIZE];\n-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n-  int lane = threadIdx.x & LANE_MASK;\n-  int wid = threadIdx.x >> WID_SHIFT;\n+  static __shared__ T shared[32];\n+  const int lane = threadIdx.x & 31;\n+  const int wid = threadIdx.x >> 5;\n \n   val = warpReduceSum<T>(val);\n \n@@ -53,10 +51,12 @@ __inline__ __device__ T blockReduceSum(T val) {\n \n   __syncthreads();\n \n-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n-  // blockDim.x is not divided by 32\n-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n-  val = warpReduceSum<T>(val);\n+  // Only threads in the first warp participate in final reduction\n+  const int num_warps = (blockDim.x + 31) >> 5;\n+  val = (threadIdx.x < num_warps) ? shared[lane] : (T)(0.0f);\n+  if (wid == 0)\n+    val = warpReduceSum<T>(val);\n+  \n   return val;\n }\n \ndiff --git a/tests/kernels/test_layernorm.py b/tests/kernels/test_layernorm.py\nindex b1e3c1a7f..20fc3dac9 100644\n--- a/tests/kernels/test_layernorm.py\n+++ b/tests/kernels/test_layernorm.py\n@@ -4,8 +4,8 @@ import torch\n from vllm.model_executor.layers.layernorm import RMSNorm\n \n DTYPES = [torch.half, torch.bfloat16, torch.float]\n-NUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\n-HIDDEN_SIZES = [768, 5120, 8192]  # Arbitrary values for testing\n+NUM_TOKENS = [7, 83, 2048, 4096]  # Arbitrary values for testing\n+HIDDEN_SIZES = [768, 4096, 5120, 8192]  # Arbitrary values for testing\n ADD_RESIDUAL = [False, True]\n SEEDS = [0]\n CUDA_DEVICES = [\n", "model_name_or_path": "gpt-5-2025-08-07"}
