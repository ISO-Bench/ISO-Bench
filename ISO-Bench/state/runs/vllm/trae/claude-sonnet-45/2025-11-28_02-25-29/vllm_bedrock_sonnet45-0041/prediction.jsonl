{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5eda21e", "model_patch": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 398fdc5f0..dc2946614 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -77,8 +77,8 @@ steps:\n   - vllm/\n   - tests/basic_correctness/test_chunked_prefill\n   commands:\n-  - VLLM_ATTENTION_BACKEND=XFORMERS VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n-  - VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py\n \n - label: Core Test # 10min\n   mirror_hardwares: [amd]\n@@ -88,11 +88,7 @@ steps:\n   - vllm/distributed\n   - tests/core\n   commands:\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core/test_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/test_chunked_prefill_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n-  - pytest -v -s core --ignore=core/block/e2e/test_correctness.py --ignore=core/test_scheduler.py --ignore=core/test_chunked_prefill_scheduler.py --ignore=core/block/e2e/test_correctness.py --ignore=core/block/e2e/test_correctness_sliding_window.py\n+  - pytest -v -s core\n \n - label: Entrypoints Test # 40min\n   working_dir: \"/vllm-workspace/tests\"\n@@ -192,8 +188,7 @@ steps:\n   - vllm/\n   - tests/prefix_caching\n   commands:\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s prefix_caching/test_prefix_caching.py\n-    - pytest -v -s prefix_caching --ignore=prefix_caching/test_prefix_caching.py\n+    - pytest -v -s prefix_caching\n \n - label: Samplers Test # 36min\n   source_file_dependencies:\n@@ -217,7 +212,7 @@ steps:\n   - tests/spec_decode\n   commands:\n     - pytest -v -s spec_decode/e2e/test_multistep_correctness.py\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s spec_decode/e2e/test_compatibility.py\n+    - pytest -v -s spec_decode/e2e/test_compatibility.py\n     - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py --ignore=spec_decode/e2e/test_compatibility.py\n \n - label: LoRA Test %N # 15min each\n@@ -405,7 +400,7 @@ steps:\n   - pytest -v -s ./compile/test_basic_correctness.py\n   - pytest -v -s ./compile/test_wrapper.py\n   - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py | grep -q 'Same node test passed'\n-  - TARGET_TEST_SUITE=L4 VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest basic_correctness/ -v -s -m distributed_2_gpus\n+  - TARGET_TEST_SUITE=L4 pytest basic_correctness/ -v -s -m distributed_2_gpus\n   # Avoid importing model tests that cause CUDA reinitialization error\n   - pytest models/encoder_decoder/language/test_bart.py -v -s -m distributed_2_gpus\n   - pytest models/encoder_decoder/vision_language/test_broadcast.py -v -s -m distributed_2_gpus\ndiff --git a/benchmarks/benchmark_latency.py b/benchmarks/benchmark_latency.py\nindex 79a48b2a1..ea1a7788f 100644\n--- a/benchmarks/benchmark_latency.py\n+++ b/benchmarks/benchmark_latency.py\n@@ -38,7 +38,6 @@ def main(args: argparse.Namespace):\n         quantization_param_path=args.quantization_param_path,\n         device=args.device,\n         ray_workers_use_nsight=args.ray_workers_use_nsight,\n-        use_v2_block_manager=args.use_v2_block_manager,\n         enable_chunked_prefill=args.enable_chunked_prefill,\n         download_dir=args.download_dir,\n         block_size=args.block_size,\n@@ -221,9 +220,6 @@ if __name__ == '__main__':\n     parser.add_argument(\"--enable-prefix-caching\",\n                         action='store_true',\n                         help=\"Enable automatic prefix caching\")\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager)\n     parser.add_argument(\n         \"--ray-workers-use-nsight\",\n         action='store_true',\ndiff --git a/benchmarks/benchmark_prefix_caching.py b/benchmarks/benchmark_prefix_caching.py\nindex f14092d34..871df1182 100644\n--- a/benchmarks/benchmark_prefix_caching.py\n+++ b/benchmarks/benchmark_prefix_caching.py\n@@ -134,7 +134,6 @@ def main(args):\n               tokenizer_mode='auto',\n               trust_remote_code=True,\n               enforce_eager=True,\n-              use_v2_block_manager=args.use_v2_block_manager,\n               tensor_parallel_size=args.tensor_parallel_size,\n               enable_prefix_caching=args.enable_prefix_caching)\n \n@@ -176,10 +175,6 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help='Use BlockSpaceMangerV2')\n     parser.add_argument('--num-prompts',\n                         type=int,\n                         default=1,\ndiff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py\nindex b7bc2a640..759c7baf7 100644\n--- a/benchmarks/benchmark_throughput.py\n+++ b/benchmarks/benchmark_throughput.py\n@@ -86,7 +86,6 @@ def run_vllm(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -113,7 +112,6 @@ def run_vllm(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n     )\n \n@@ -176,7 +174,6 @@ async def run_vllm_async(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -204,7 +201,6 @@ async def run_vllm_async(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n         worker_use_ray=False,\n         disable_log_requests=True,\n@@ -341,7 +337,7 @@ def main(args: argparse.Namespace):\n             args.enable_prefix_caching, args.enable_chunked_prefill,\n             args.max_num_batched_tokens, args.distributed_executor_backend,\n             args.gpu_memory_utilization, args.num_scheduler_steps,\n-            args.use_v2_block_manager, args.download_dir, args.load_format,\n+            args.download_dir, args.load_format,\n             args.disable_async_output_proc\n         ]\n \n@@ -471,10 +467,6 @@ if __name__ == \"__main__\":\n         type=int,\n         default=1,\n         help=\"Maximum number of forward steps per scheduler call.\")\n-    parser.add_argument(\"--use-v2-block-manager\",\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help=\"Enable block manager v2.\")\n     parser.add_argument(\n         \"--enable-prefix-caching\",\n         action='store_true',\ndiff --git a/benchmarks/overheads/benchmark_hashing.py b/benchmarks/overheads/benchmark_hashing.py\nindex 203699e9a..d16d6f9fb 100644\n--- a/benchmarks/overheads/benchmark_hashing.py\n+++ b/benchmarks/overheads/benchmark_hashing.py\n@@ -16,7 +16,6 @@ def main(args):\n         enforce_eager=True,\n         enable_prefix_caching=True,\n         tensor_parallel_size=args.tensor_parallel_size,\n-        use_v2_block_manager=args.use_v2_block_manager,\n     )\n \n     sampling_params = SamplingParams(temperature=0, max_tokens=args.output_len)\n@@ -56,8 +55,5 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        help='Use BlockSpaceMangerV2')\n     args = parser.parse_args()\n     main(args)\ndiff --git a/docs/source/models/spec_decode.rst b/docs/source/models/spec_decode.rst\nindex 0dc9cb383..b02c80aeb 100644\n--- a/docs/source/models/spec_decode.rst\n+++ b/docs/source/models/spec_decode.rst\n@@ -30,7 +30,6 @@ The following code configures vLLM in an offline mode to use speculative decodin\n         tensor_parallel_size=1,\n         speculative_model=\"facebook/opt-125m\",\n         num_speculative_tokens=5,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -104,7 +103,6 @@ matching n-grams in the prompt. For more information read `this thread. <https:/\n         speculative_model=\"[ngram]\",\n         num_speculative_tokens=5,\n         ngram_prompt_lookup_max=4,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -135,7 +133,6 @@ For more information see `this blog <https://pytorch.org/blog/hitchhikers-guide-\n         tensor_parallel_size=4,\n         speculative_model=\"ibm-fms/llama3-70b-accelerator\",\n         speculative_draft_tensor_parallel_size=1,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \ndiff --git a/examples/offline_inference_mlpspeculator.py b/examples/offline_inference_mlpspeculator.py\nindex 5dec4a76a..8f0eb65e4 100644\n--- a/examples/offline_inference_mlpspeculator.py\n+++ b/examples/offline_inference_mlpspeculator.py\n@@ -50,8 +50,6 @@ if __name__ == \"__main__\":\n     llm = LLM(\n         model=\"meta-llama/Llama-2-13b-chat-hf\",\n         speculative_model=\"ibm-fms/llama-13b-accelerator\",\n-        # These are currently required for MLPSpeculator decoding\n-        use_v2_block_manager=True,\n     )\n \n     print(\"With speculation\")\ndiff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex e8819688c..43e41ddd2 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -197,7 +197,6 @@ def test_models_with_fp8_kv_cache(\n @pytest.mark.parametrize(\"max_tokens\", [16])\n @pytest.mark.parametrize(\"enforce_eager\", [False])\n @pytest.mark.parametrize(\"chunk_size\", [30, 32])\n-@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n # NOTE: Increasing this in this suite will fail CI because we currently cannot\n # reset distributed env properly. Use a value > 1 just when you test.\n @pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n@@ -206,7 +205,6 @@ def test_with_prefix_caching(\n     max_tokens: int,\n     enforce_eager: bool,\n     chunk_size: int,\n-    use_v2_block_manager: bool,\n     tensor_parallel_size: int,\n ) -> None:\n     \"\"\"\n@@ -234,7 +232,6 @@ def test_with_prefix_caching(\n                 enable_chunked_prefill=True,\n                 enable_prefix_caching=enable,\n                 tensor_parallel_size=tensor_parallel_size,\n-                use_v2_block_manager=use_v2_block_manager,\n                 enforce_eager=enforce_eager,\n                 max_num_seqs=max_num_seqs,\n         ) as vllm_model:\ndiff --git a/tests/utils.py b/tests/utils.py\nindex 924465057..77ac1060a 100644\n--- a/tests/utils.py\n+++ b/tests/utils.py\n@@ -681,9 +681,4 @@ def get_client_text_logprob_generations(\n \n \n def check_deprecated_block_manager_usage(test_name: str):\n-    assert envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1 is True, (\n-        f\"To allow the use of deprecated BlockSpaceManagerV1, set the \"\n-        f\"environment variable VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. \"\n-        f\"You can run the tests with: \"\n-        f\"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest {test_name}`\"  #noqa\n-    )\n+    pass\n", "model_name_or_path": "gpt-5-2025-08-07"}
