{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f168b85", "model_patch": "diff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..b50d44b08 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -29,7 +29,7 @@ from vllm.distributed.parallel_state import get_pp_group\n from vllm.forward_context import get_forward_context\n from vllm.model_executor.layers.activation import ReLUSquaredActivation\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -63,11 +63,12 @@ class NemotronHMLP(nn.Module):\n         config: NemotronHConfig,\n         quant_config: Optional[QuantizationConfig] = None,\n         bias: bool = False,\n+        prefix: str = \"\",\n     ) -> None:\n         super().__init__()\n-        self.up_proj = MergedColumnParallelLinear(\n+        self.up_proj = ColumnParallelLinear(\n             input_size=config.hidden_size,\n-            output_sizes=[config.intermediate_size],\n+            output_size=config.intermediate_size,\n             bias=bias,\n             quant_config=quant_config,\n         )\n@@ -99,9 +100,12 @@ class NemotronHMLPDecoderLayer(nn.Module):\n         super().__init__()\n         self.config = config\n \n-        self.mixer = NemotronHMLP(config,\n-                                  quant_config=quant_config,\n-                                  bias=config.mlp_bias)\n+        self.mixer = NemotronHMLP(\n+            config,\n+            quant_config=quant_config,\n+            bias=config.mlp_bias,\n+            prefix=f\"{prefix}.mixer\",\n+        )\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -355,15 +359,13 @@ class NemotronHModel(nn.Module):\n             hidden_states = intermediate_tensors[\"hidden_states\"]\n             residual = intermediate_tensors[\"residual\"]\n \n-        residual = None\n         num_non_mamba_layers = 0\n-        for i in range(len(self.layers)):\n-            layer = self.layers[i]\n-            layer_mamba_cache_params = None\n+        for i, layer in enumerate(self.layers):\n             if isinstance(layer, NemotronHMambaDecoderLayer):\n                 layer_mamba_cache_params = mamba_cache_params.at_layer_idx(\n                     i - num_non_mamba_layers)\n             else:\n+                layer_mamba_cache_params = None\n                 num_non_mamba_layers += 1\n \n             hidden_states, residual = layer(\n", "model_name_or_path": "gpt-5-2025-08-07"}
