{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-6d917d0", "model_patch": "diff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex ed7e06cab..f815b60b3 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,6 +1,7 @@\n import enum\n+import heapq\n from abc import ABC, abstractmethod\n-from typing import OrderedDict, Tuple\n+from typing import Dict, List, Tuple\n \n \n class EvictionPolicy(enum.Enum):\n@@ -75,8 +76,14 @@ class LRUEvictor(Evictor):\n     highest num_hashed_tokens value, then one will be chose arbitrarily\n     \"\"\"\n \n+    # CLEANUP_THRESHOLD determines the maximum allowable size of the priority\n+    # queue relative to the free table size. When this threshold is exceeded,\n+    # a cleanup operation is triggered to reduce memory usage.\n+    CLEANUP_THRESHOLD = 50\n+\n     def __init__(self):\n-        self.free_table: OrderedDict[int, BlockMetaData] = OrderedDict()\n+        self.free_table: Dict[int, BlockMetaData] = {}\n+        self.priority_queue: List[Tuple[float, int, int]] = []\n \n     def __contains__(self, block_id: int) -> bool:\n         return block_id in self.free_table\n@@ -85,39 +92,76 @@ class LRUEvictor(Evictor):\n         if len(self.free_table) == 0:\n             raise ValueError(\"No usable cache memory left\")\n \n-        evicted_block, evicted_block_id = None, None\n-        # The blocks with the lowest timestamps should be placed consecutively\n-        # at the start of OrderedDict. Loop through all these blocks to\n-        # find the one with maximum number of hashed tokens.\n-        for _id, block in self.free_table.items():\n-            if evicted_block is None:\n-                evicted_block, evicted_block_id = block, _id\n+        # Use lazy deletion: pop from heap until we find a valid block\n+        while self.priority_queue:\n+            last_accessed, neg_num_hashed_tokens, block_id = heapq.heappop(\n+                self.priority_queue)\n+            \n+            # Check if block is still in free_table (lazy deletion)\n+            if block_id not in self.free_table:\n                 continue\n-            if evicted_block.last_accessed < block.last_accessed:\n-                break\n-            if evicted_block.num_hashed_tokens < block.num_hashed_tokens:\n-                evicted_block, evicted_block_id = block, _id\n-\n-        assert evicted_block is not None\n-        assert evicted_block_id is not None\n-        self.free_table.pop(evicted_block_id)\n-\n-        return evicted_block_id, evicted_block.content_hash\n+            \n+            block = self.free_table[block_id]\n+            # Check if the metadata matches (block wasn't updated)\n+            if block.last_accessed == last_accessed:\n+                # Found valid block to evict\n+                self.free_table.pop(block_id)\n+                return block_id, block.content_hash\n+        \n+        # If we get here, all heap entries were stale, rebuild heap\n+        self._rebuild_priority_queue()\n+        \n+        # Try again with rebuilt heap\n+        if self.priority_queue:\n+            last_accessed, neg_num_hashed_tokens, block_id = heapq.heappop(\n+                self.priority_queue)\n+            block = self.free_table[block_id]\n+            self.free_table.pop(block_id)\n+            return block_id, block.content_hash\n+        \n+        raise ValueError(\"No usable cache memory left\")\n \n     def add(self, block_id: int, content_hash: int, num_hashed_tokens: int,\n             last_accessed: float):\n         self.free_table[block_id] = BlockMetaData(content_hash,\n                                                   num_hashed_tokens,\n                                                   last_accessed)\n+        # Push to priority queue: (last_accessed, -num_hashed_tokens, block_id)\n+        # Negative num_hashed_tokens for max-heap behavior on ties\n+        heapq.heappush(self.priority_queue,\n+                      (last_accessed, -num_hashed_tokens, block_id))\n+        \n+        # Cleanup if priority queue gets too large\n+        if len(self.priority_queue) > len(\n+                self.free_table) * self.CLEANUP_THRESHOLD:\n+            self._rebuild_priority_queue()\n \n     def update(self, block_id: int, last_accessed: float):\n-        self.free_table[block_id].last_accessed = last_accessed\n+        block = self.free_table[block_id]\n+        block.last_accessed = last_accessed\n+        # Push updated entry to priority queue (old entry becomes stale)\n+        heapq.heappush(self.priority_queue,\n+                      (last_accessed, -block.num_hashed_tokens, block_id))\n+        \n+        # Cleanup if priority queue gets too large\n+        if len(self.priority_queue) > len(\n+                self.free_table) * self.CLEANUP_THRESHOLD:\n+            self._rebuild_priority_queue()\n \n     def remove(self, block_id: int):\n         if block_id not in self.free_table:\n             raise ValueError(\n                 \"Attempting to remove block that's not in the evictor\")\n         self.free_table.pop(block_id)\n+        # Note: We don't remove from priority_queue, lazy deletion handles it\n+\n+    def _rebuild_priority_queue(self):\n+        \"\"\"Rebuild the priority queue from current free_table to remove stale entries\"\"\"\n+        self.priority_queue = []\n+        for block_id, block in self.free_table.items():\n+            heapq.heappush(\n+                self.priority_queue,\n+                (block.last_accessed, -block.num_hashed_tokens, block_id))\n \n     @property\n     def num_blocks(self) -> int:\n", "model_name_or_path": "gpt-5-2025-08-07"}
