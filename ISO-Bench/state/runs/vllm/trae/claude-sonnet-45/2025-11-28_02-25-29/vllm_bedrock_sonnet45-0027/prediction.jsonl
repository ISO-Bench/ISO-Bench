{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c9e2d64", "model_patch": "diff --git a/tests/weight_loading/test_weight_loading.py b/tests/weight_loading/test_weight_loading.py\nindex e456bfab8..9d6b25da7 100644\n--- a/tests/weight_loading/test_weight_loading.py\n+++ b/tests/weight_loading/test_weight_loading.py\n@@ -12,7 +12,7 @@ MODEL_NAME = os.environ.get(\"MODEL_NAME\",\n                             \"robertgshaw2/zephyr-7b-beta-channelwise-gptq\")\n REVISION = os.environ.get(\"REVISION\", \"main\")\n QUANTIZATION = os.environ.get(\"QUANTIZATION\", \"gptq_marlin\")\n-MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"89\")\n+MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"80\")\n \n \n @pytest.mark.skipif(\ndiff --git a/vllm/model_executor/layers/quantization/awq_marlin.py b/vllm/model_executor/layers/quantization/awq_marlin.py\nindex a43b2e597..1285c06e6 100644\n--- a/vllm/model_executor/layers/quantization/awq_marlin.py\n+++ b/vllm/model_executor/layers/quantization/awq_marlin.py\n@@ -17,6 +17,7 @@ from vllm.model_executor.layers.quantization.awq import (AWQConfig,\n                                                          is_layer_skipped_awq)\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig, QuantizeMethodBase)\n+from vllm.model_executor.layers.quantization.moe_wna16 import MoeWNA16Config\n from vllm.model_executor.layers.quantization.utils import replace_parameter\n from vllm.model_executor.layers.quantization.utils.marlin_utils import (\n     apply_awq_marlin_linear, awq_to_marlin_zero_points, check_marlin_supported,\n@@ -134,6 +135,11 @@ class AWQMarlinConfig(QuantizationConfig):\n                     self.full_config).get_quant_method(layer, prefix)\n             return AWQMarlinLinearMethod(self)\n         elif isinstance(layer, FusedMoE):\n+            # Use moe_wna16 kernel for MoEs with many experts (>=8)\n+            if layer.num_experts >= 8 and MoeWNA16Config.is_moe_wna16_compatible(\n+                    self.full_config):\n+                return MoeWNA16Config.from_config(\n+                    self.full_config).get_quant_method(layer, prefix)\n             return AWQMoEMethod(self)\n         return None\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
