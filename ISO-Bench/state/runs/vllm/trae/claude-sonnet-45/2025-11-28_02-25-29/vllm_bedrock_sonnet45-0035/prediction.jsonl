{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..bf1a40d48 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -455,7 +455,6 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                       is not None)\n         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n-        self.decode_only = True\n \n         # Attention metadata inputs.\n         if self.attn_backend is not None:\n@@ -477,6 +476,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 finished_requests_ids: Optional[List[str]] = None) -> None:\n         self.finished_requests_ids = finished_requests_ids\n \n+        # if the current batch is decode-only.\n+        # will be set to False if there is any non-decode request.\n+        self.decode_only = True\n+\n         # Intermediate data (data in CPU before going to GPU) for\n         # the current sequence group.\n         self.inter_data_list: List[\n", "model_name_or_path": "gpt-5-2025-08-07"}
