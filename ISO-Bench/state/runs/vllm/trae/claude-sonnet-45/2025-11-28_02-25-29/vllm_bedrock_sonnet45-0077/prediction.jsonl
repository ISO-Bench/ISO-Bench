{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-acaea3b", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 5b19e3f35..60a63adcf 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -467,16 +467,10 @@ class MambaMixer2(CustomOp):\n \n             initial_states = None\n \n-            if has_initial_states is not None and torch.any(\n-                    has_initial_states):\n-\n-                # vectorized ssm_state zero init\n-                batched_zero_init_func = torch.vmap(\n-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n-                batched_zero_init_func(\n-                    mamba_cache_params.\n-                    state_indices_tensor[~has_initial_states].unsqueeze(\n-                        dim=-1), )\n+            if has_initial_states is not None and any(has_initial_states):\n+                for idx in mamba_cache_params.state_indices_tensor[\n+                        ~has_initial_states]:\n+                    mamba_cache_params.ssm_state[idx].zero_()\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -500,17 +494,10 @@ class MambaMixer2(CustomOp):\n                 dt_limit=(0.0, float(\"inf\")),\n             )\n \n-            # vectorized ssm state update using vmap\n-            # the 1d state_indices_tensor needs to be unsqueezed to avoid vmap\n-            # limitation which doesn't allow use of `item()`\n-            # Note: the lambda capture can happen where ssm_state is initialized\n-            #       instead of here\n-            batched_copy = torch.vmap(\n-                lambda idx, source_state: mamba_cache_params.ssm_state[\n-                    idx].copy_(source_state))\n-            batched_copy(\n-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n-                varlen_state)\n+            # ssm state update using simple loop\n+            for idx, source_state in zip(\n+                    mamba_cache_params.state_indices_tensor, varlen_state):\n+                mamba_cache_params.ssm_state[idx].copy_(source_state)\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
