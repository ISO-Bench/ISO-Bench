{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d3ea501", "model_patch": "diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py\nindex 5f0eb0019..95e7acd02 100644\n--- a/vllm/worker/neuron_worker.py\n+++ b/vllm/worker/neuron_worker.py\n@@ -76,7 +76,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         # Set the number of GPU blocks to be the same as the maximum number of\n         # sequences that can be processed in a single batch. This is equivalent\n         # to schedule without PagedAttention.\n-        num_gpu_blocks = self.scheduler_config.max_num_seqs\n+        num_gpu_blocks = self.scheduler_config.max_num_seqs + 1\n \n         # Swap not yet supported with Neuron backend.\n         num_cpu_blocks = 0\n@@ -90,7 +90,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n \n         # Different values are not tested.\n         assert num_cpu_blocks == 0\n-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs\n+        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1\n \n         self.cache_config.num_gpu_blocks = num_gpu_blocks\n         self.cache_config.num_cpu_blocks = num_cpu_blocks\n", "model_name_or_path": "gpt-5-2025-08-07"}
