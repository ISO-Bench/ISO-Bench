{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-526078a", "model_patch": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101a0..3aa45640b 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -11,7 +11,8 @@ from tests.v1.attention.utils import (BatchSpec, _Backend,\n                                       create_vllm_config,\n                                       get_attention_backend)\n from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE, cdiv\n-from vllm.v1.attention.backends.utils import CommonAttentionMetadata\n+from vllm.v1.attention.backends.utils import (CommonAttentionMetadata,\n+                                              set_kv_cache_layout)\n from vllm.v1.kv_cache_interface import FullAttentionSpec\n \n BACKENDS_TO_TEST = [\n@@ -155,7 +156,9 @@ def create_and_prepopulate_kv_cache(\n         perm = torch.arange(\n             1, blocks_end)  # Sequential order starting from block 1\n \n-    inv_perm = torch.zeros(blocks_end, dtype=torch.long, device=device)\n+    # Optimize: Use torch.empty instead of torch.zeros since we fill all values\n+    inv_perm = torch.empty(blocks_end, dtype=torch.long, device=device)\n+    inv_perm[0] = 0  # Set the null block explicitly\n     inv_perm[1:] = torch.argsort(\n         perm) + 1  # Add 1 to account for starting from block 1\n     kv_cache[:, 1:blocks_end, ...] = kv_cache[:, perm, ...]\n@@ -212,7 +215,7 @@ def run_attention_backend(backend: _Backend, kv_cache_spec: FullAttentionSpec,\n \n         from vllm.v1.attention.backends.flashinfer import PerLayerParameters\n \n-        def mock_get_per_layer_parameters(vllm_config):\n+        def mock_get_per_layer_parameters(vllm_config, impl_cls):\n             # Return mock parameters for a single layer\n             head_size = vllm_config.model_config.get_head_size()\n             return {\n@@ -297,7 +300,8 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n     5. Comparing the vLLM backend's output to the ground-truth SDPA output.\n     \"\"\"\n     batch_spec = BATCH_SPECS[batch_spec_name]\n-    vllm_config = create_vllm_config(model_name=model)\n+    vllm_config = create_vllm_config(model_name=model,\n+                                     max_model_len=max(batch_spec.seq_lens))\n     device = torch.device(\"cuda:0\")\n \n     kv_cache_spec = create_standard_kv_cache_spec(vllm_config)\ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdda5..938516533 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -43,12 +43,15 @@ def create_common_attn_metadata(\n         max_block_idx: int = 1000) -> CommonAttentionMetadata:\n     \"\"\"Create CommonAttentionMetadata from a BatchSpec and ModelParams.\"\"\"\n     # Create query start locations\n-    query_start_loc = torch.zeros(batch_spec.batch_size + 1,\n+    # Optimize: Use preallocated tensor instead of torch.zeros + assignment\n+    query_lens_tensor = torch.tensor(batch_spec.query_lens,\n+                                     dtype=torch.int32,\n+                                     device=device)\n+    query_start_loc = torch.empty(batch_spec.batch_size + 1,\n                                   dtype=torch.int32,\n                                   device=device)\n-    query_start_loc[1:] = torch.tensor(batch_spec.query_lens,\n-                                       dtype=torch.int32,\n-                                       device=device).cumsum(0)\n+    query_start_loc[0] = 0\n+    torch.cumsum(query_lens_tensor, dim=0, out=query_start_loc[1:])\n     query_start_loc_cpu = query_start_loc.cpu()\n     num_tokens = batch_spec.compute_num_tokens()\n \ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26c8..8ec700111 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -422,12 +422,15 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                 < block_table_bounds.unsqueeze(1))\n         paged_kv_indices = block_table_tensor[mask]\n \n-        paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n-                        dtype=block_table_bounds.dtype,\n-                        device=block_table_bounds.device),\n-            block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n-        ])\n+        # Optimize: Use preallocated tensor instead of torch.zeros + cat\n+        paged_kv_indptr = torch.empty(len(block_table_bounds) + 1,\n+                                      dtype=torch.int32,\n+                                      device=block_table_bounds.device)\n+        paged_kv_indptr[0] = 0\n+        torch.cumsum(block_table_bounds,\n+                     dim=0,\n+                     dtype=torch.int32,\n+                     out=paged_kv_indptr[1:])\n \n         paged_kv_last_page_len = seq_lens % page_size\n         paged_kv_last_page_len = torch.where(paged_kv_last_page_len == 0,\n", "model_name_or_path": "gpt-5-2025-08-07"}
