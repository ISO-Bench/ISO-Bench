{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-63d635d", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ef3d28c80..733c3b2f7 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -259,6 +259,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n         x: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: torch.Tensor,\n+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+        seqlens: Optional[List[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n         # [s, b, c] --> [s, b, head * 3 * head_dim]\n         x, _ = self.qkv(x)\n@@ -285,7 +287,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -321,7 +324,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -364,11 +368,17 @@ class Qwen2_5_VisionBlock(nn.Module):\n                                      quant_config=quant_config,\n                                      prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+    def forward(self,\n+                x: torch.Tensor,\n+                cu_seqlens: torch.Tensor,\n+                rotary_pos_emb: torch.Tensor,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[List[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex ac3d154dd..64ad6efe6 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -307,6 +307,8 @@ class Qwen2VisionAttention(nn.Module):\n         x: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: torch.Tensor,\n+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+        seqlens: Optional[List[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n \n         # [s, b, c] --> [s, b, 3 * head * head_dim]\n@@ -329,7 +331,8 @@ class Qwen2VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -365,7 +368,8 @@ class Qwen2VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -409,11 +413,17 @@ class Qwen2VisionBlock(nn.Module):\n                                   quant_config=quant_config,\n                                   prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+    def forward(self,\n+                x: torch.Tensor,\n+                cu_seqlens: torch.Tensor,\n+                rotary_pos_emb: torch.Tensor,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[List[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
