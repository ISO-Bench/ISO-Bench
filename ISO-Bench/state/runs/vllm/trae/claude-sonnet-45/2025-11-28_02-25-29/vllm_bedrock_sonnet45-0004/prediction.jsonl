{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-2b04c20", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 00260313e..afe547741 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -534,12 +534,14 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n+    tokens_cnts = torch.empty((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    tokens_cnts.fill_(0)\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n+    cumsum[0] = 0\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -1240,15 +1242,20 @@ def fused_experts_impl(hidden_states: torch.Tensor,\n \n     config = get_config_func(M)\n \n-    intermediate_cache1 = torch.empty((M, top_k_num, N),\n-                                      device=hidden_states.device,\n-                                      dtype=hidden_states.dtype)\n+    # We can reuse the memory between these because by the time we need\n+    # cache3, we're done with cache1\n+    cache13 = torch.empty(M * top_k_num * max(N, w2.shape[1]),\n+                          device=hidden_states.device,\n+                          dtype=hidden_states.dtype)\n+    intermediate_cache1 = cache13[:M * top_k_num * N].view(\n+        (M, top_k_num, N))\n+    intermediate_cache3 = cache13[:M * top_k_num * w2.shape[1]].view(\n+        (M, top_k_num, w2.shape[1]))\n+\n+    # This needs separate memory since it's used concurrently with cache1\n     intermediate_cache2 = torch.empty((M * top_k_num, N // 2),\n                                       device=hidden_states.device,\n                                       dtype=hidden_states.dtype)\n-    intermediate_cache3 = torch.empty((M, top_k_num, w2.shape[1]),\n-                                      device=hidden_states.device,\n-                                      dtype=hidden_states.dtype)\n \n     if hidden_states.dtype == torch.bfloat16:\n         compute_type = tl.bfloat16\n@@ -1279,10 +1286,15 @@ def fused_experts_impl(hidden_states: torch.Tensor,\n             # chunk. Note that in most cases we only have one chunk\n             # so the cache size and config are already set correctly and\n             # do not need to be adjusted.\n-            intermediate_cache1 = intermediate_cache1[:tokens_in_chunk]\n+            cache13 = torch.empty(tokens_in_chunk * top_k_num * max(N, w2.shape[1]),\n+                                  device=hidden_states.device,\n+                                  dtype=hidden_states.dtype)\n+            intermediate_cache1 = cache13[:tokens_in_chunk * top_k_num * N].view(\n+                (tokens_in_chunk, top_k_num, N))\n+            intermediate_cache3 = cache13[:tokens_in_chunk * top_k_num * w2.shape[1]].view(\n+                (tokens_in_chunk, top_k_num, w2.shape[1]))\n             intermediate_cache2 = intermediate_cache2[:tokens_in_chunk *\n                                                       topk_ids.shape[1]]\n-            intermediate_cache3 = intermediate_cache3[:tokens_in_chunk]\n             config = get_config_func(tokens_in_chunk)\n \n         curr_topk_ids = topk_ids[begin_chunk_idx:end_chunk_idx]\n", "model_name_or_path": "gpt-5-2025-08-07"}
