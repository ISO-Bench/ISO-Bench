{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-a4d577b", "model_patch": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 8bc33e841..3e810e525 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -26,17 +26,13 @@ def create_logits_tensor(token_ids: List[int],\n def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n     batch_size = len(spec_tokens)\n     return SamplingMetadata(\n-        temperature=0.0,\n+        temperature=torch.tensor([]),\n         all_greedy=True,\n         all_random=False,\n-        rejection_sampling=True,\n         spec_token_ids=spec_tokens,\n         top_p=None,\n         top_k=None,\n-        no_top_p=False,\n-        no_top_k=False,\n         min_p=torch.empty(batch_size, ),\n-        no_min_p=True,\n         generators={},\n         max_num_logprobs=0,\n         no_penalties=False,\n@@ -45,8 +41,7 @@ def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n         presence_penalties=torch.tensor([]),\n         repetition_penalties=torch.tensor([]),\n         output_token_ids=[],\n-        min_tokens=[],\n-        stop_token_ids=[],\n+        min_tokens={},\n         logit_bias=[None] * batch_size,\n     )\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
