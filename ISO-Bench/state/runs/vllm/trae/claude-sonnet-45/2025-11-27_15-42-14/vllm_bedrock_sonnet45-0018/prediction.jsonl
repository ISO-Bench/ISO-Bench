{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5460070", "model_patch": "diff --git a/vllm/core/block/common.py b/vllm/core/block/common.py\nindex d2787d696..21165591e 100644\n--- a/vllm/core/block/common.py\n+++ b/vllm/core/block/common.py\n@@ -38,24 +38,17 @@ class RefCounter(RefCounterProtocol):\n \n     def incr(self, block_id: BlockId) -> RefCount:\n         assert block_id in self._refcounts\n-        pre_incr_refcount = self._refcounts[block_id]\n-\n-        assert pre_incr_refcount >= 0\n-\n-        post_incr_refcount = pre_incr_refcount + 1\n-        self._refcounts[block_id] = post_incr_refcount\n-        return post_incr_refcount\n+        # Optimize: single dictionary access and update\n+        self._refcounts[block_id] += 1\n+        assert self._refcounts[block_id] > 0\n+        return self._refcounts[block_id]\n \n     def decr(self, block_id: BlockId) -> RefCount:\n         assert block_id in self._refcounts\n-        refcount = self._refcounts[block_id]\n-\n-        assert refcount > 0\n-        refcount -= 1\n-\n-        self._refcounts[block_id] = refcount\n-\n-        return refcount\n+        # Optimize: single dictionary access and update\n+        assert self._refcounts[block_id] > 0\n+        self._refcounts[block_id] -= 1\n+        return self._refcounts[block_id]\n \n     def get(self, block_id: BlockId) -> RefCount:\n         assert block_id in self._refcounts\n@@ -186,12 +179,16 @@ def get_all_blocks_recursively(last_block: Block) -> List[Block]:\n         List[Block]: A list of all the blocks in the sequence, in the order they\n             appear.\n     \"\"\"\n-\n-    def recurse(block: Block, lst: List[Block]) -> None:\n-        if block.prev_block is not None:\n-            recurse(block.prev_block, lst)\n-        lst.append(block)\n-\n+    # Optimize: use iterative approach instead of recursion to avoid function call overhead\n     all_blocks: List[Block] = []\n-    recurse(last_block, all_blocks)\n+    current_block: Optional[Block] = last_block\n+    \n+    # First, traverse backwards to collect all blocks\n+    temp_blocks: List[Block] = []\n+    while current_block is not None:\n+        temp_blocks.append(current_block)\n+        current_block = current_block.prev_block\n+    \n+    # Reverse to get correct order\n+    all_blocks = temp_blocks[::-1]\n     return all_blocks\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 2df7d74e4..c6d0301f8 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -111,6 +111,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert device is None\n         assert_prefix_caching_block_or_none(prev_block)\n \n+        # Optimize: create block once and reuse\n         block = self._create_block(\n             prev_block=prev_block,\n             token_ids=token_ids,\n@@ -119,12 +120,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         )\n         assert block.content_hash is not None\n \n-        cached_block_id = self._cached_blocks.get(block.content_hash, None)\n+        # Optimize: single dictionary lookup\n+        cached_block_id = self._cached_blocks.get(block.content_hash)\n         if cached_block_id is not None:\n             block.block_id = cached_block_id\n-            self._incr_refcount_cached_block(block, block.block_id)\n+            self._incr_refcount_cached_block(block, cached_block_id)\n             return block\n \n+        # Cache miss: allocate new mutable block\n         block = self.allocate_mutable(prev_block)\n         block.append_token_ids(token_ids)\n         assert block.content_hash is not None\n@@ -384,10 +387,15 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         If the block is added into evictor, we need to update corresponding\n         info in evictor's metadata.\n         \"\"\"\n+        # Optimize: early exit for empty list\n+        if not block_ids:\n+            return\n \n         for block_id in block_ids:\n-            if block_id in self._blocks:\n-                self._blocks[block_id].last_accessed = now\n+            # Optimize: use dict.get() to avoid double lookup\n+            block = self._blocks.get(block_id)\n+            if block is not None:\n+                block.last_accessed = now\n             elif block_id in self.evictor:\n                 self.evictor.update(block_id, now)\n             else:\n@@ -396,21 +404,27 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n         \"\"\"Mark blocks as computed, used in prefix caching.\"\"\"\n+        # Optimize: early exit for empty list\n+        if not block_ids:\n+            return\n \n         for block_id in block_ids:\n-            if block_id in self._blocks:\n+            # Optimize: use dict.get() to avoid double lookup\n+            block = self._blocks.get(block_id)\n+            if block is not None:\n                 # only those full block is valid for prefix caching\n-                if self._blocks[block_id].is_full:\n-                    self._blocks[block_id].computed = True\n+                if block.is_full:\n+                    block.computed = True\n             elif block_id not in self.evictor:\n                 raise ValueError(f\"Mark {block_id=} as computed which \"\n                                  \"is not belonged to GPU\")\n \n     def block_is_computed(self, block_id: int) -> bool:\n-        if block_id in self._blocks:\n-            return self._blocks[block_id].computed\n-        else:\n-            return block_id in self.evictor\n+        # Optimize: use dict.get() to avoid double lookup\n+        block = self._blocks.get(block_id)\n+        if block is not None:\n+            return block.computed\n+        return block_id in self.evictor\n \n     def get_common_computed_block_ids(\n             self, seq_block_ids: List[List[int]]) -> List[int]:\n@@ -424,16 +438,22 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # prompt is cached. This would cause erroneous behavior in model\n         # runner.\n \n+        # Optimize: filter empty sequences early\n+        non_empty_seqs = [seq[:-1] for seq in seq_block_ids if len(seq) > 1]\n+        if not non_empty_seqs:\n+            return []\n+\n         ids_list = [\n             list(\n                 takewhile(lambda block_id: self.block_is_computed(block_id),\n-                          seq[:-1])) for seq in seq_block_ids\n+                          seq)) for seq in non_empty_seqs\n         ]\n         # It returns a list of int although type annotation says list of string.\n-        return commonprefix([\n-            ids for ids in ids_list  # type: ignore\n-            if ids != []\n-        ])\n+        # Optimize: filter empty lists before commonprefix\n+        non_empty_ids = [ids for ids in ids_list if ids]\n+        if not non_empty_ids:\n+            return []\n+        return commonprefix(non_empty_ids)  # type: ignore\n \n     def get_num_blocks_touched(self,\n                                blocks: List[Block],\n@@ -607,11 +627,17 @@ class PrefixCachingBlock(Block):\n         if self._cached_num_tokens_total is not None:\n             return self._cached_num_tokens_total\n \n+        # Optimize: compute incrementally from previous block's cached value\n+        if self._prev_block is not None and hasattr(self._prev_block, '_cached_num_tokens_total'):\n+            prev_total = self._prev_block._cached_num_tokens_total\n+            if prev_total is not None:\n+                self._cached_num_tokens_total = prev_total + len(self.token_ids)\n+                return self._cached_num_tokens_total\n+\n+        # Fallback to full traversal if previous block doesn't have cached value\n         _block: Optional[Block] = self\n         self._cached_num_tokens_total = 0\n \n-        # TODO: current implement here take O(N^2), we expect future\n-        # we have O(1) here\n         while _block is not None:\n             self._cached_num_tokens_total += len(_block.token_ids)\n             _block = _block.prev_block\n@@ -685,7 +711,8 @@ class PrefixCachingBlock(Block):\n         - int: The computed hash value for the block.\n         \"\"\"\n         assert (prev_block_hash is None) == is_first_block\n-        return hash((is_first_block, prev_block_hash, *cur_block_token_ids))\n+        # Optimize: use tuple() instead of unpacking with * for better performance\n+        return hash((is_first_block, prev_block_hash, tuple(cur_block_token_ids)))\n \n \n def assert_prefix_caching_block_or_none(block: Optional[Block]):\n", "model_name_or_path": "gpt-5-2025-08-07"}
