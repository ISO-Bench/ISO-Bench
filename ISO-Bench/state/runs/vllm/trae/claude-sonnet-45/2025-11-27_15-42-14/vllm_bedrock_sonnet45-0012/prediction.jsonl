{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-029c71d", "model_patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..992777a97 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -708,6 +708,49 @@ class TestPrefixCachingBlockAllocator:\n                                                token_ids=token_ids)\n         assert allocator.get_prefix_cache_hit_rate() > 0.99\n \n+    # Test case for marking cache hit blocks as computed right after\n+    # a batch of prefill sequences are scheduled.\n+    @staticmethod\n+    def test_mark_blocks_as_computed():\n+        block_size = 16\n+        common_blocks = 4\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+\n+        common_token_ids = list(range(block_size * common_blocks))\n+\n+        # Mimic the behavior of allocating the same block chain\n+        # (i.e., common prefix) for a batch of 3 different prefill sequences.\n+        all_blocks = []\n+        for _ in range(3):\n+            blocks = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+                block_size=block_size,\n+                token_ids=common_token_ids,\n+                allocator=allocator,\n+            )\n+            all_blocks.extend(blocks)\n+            \n+        # The allocated blocks should be marked as touched but not computed\n+        # (except the first sequence which allocated new blocks)\n+        block_ids = [block.block_id for block in all_blocks]\n+        \n+        # First 4 blocks are newly allocated, rest are cache hits\n+        # Cache hit blocks should not be marked as computed initially\n+        for i, block in enumerate(all_blocks):\n+            if i < common_blocks:\n+                # First sequence blocks - not computed yet\n+                assert not block.computed\n+            else:\n+                # Cache hit blocks - also not computed initially\n+                assert not block.computed\n+        \n+        # Now mark all blocks as computed\n+        allocator.mark_blocks_as_computed(block_ids)\n+        \n+        # After marking, all blocks should be computed\n+        for block in all_blocks:\n+            assert allocator.block_is_computed(block.block_id)\n+\n     @staticmethod\n     def create_immutable_chain(\n         block_size: int,\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..292266054 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -206,10 +206,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert block.content_hash is None\n         return block\n \n-    def _incr_refcount_cached_block(self, block: Block) -> None:\n-        # Set this block to be \"computed\" since it is pointing to a\n-        # cached block id (which was already computed)\n-        block.computed = True\n+    def _incr_refcount_cached_block(self, block: Block,\n+                                    computed: bool = False) -> None:\n+        # Mark this block as computed if specified\n+        # (typically False when initially allocated, True when restored from evictor)\n+        block.computed = computed\n \n         block_id = block.block_id\n         assert block_id is not None\n@@ -219,8 +220,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             # In case a cached block was evicted, restore its tracking\n             if block_id in self.evictor:\n                 self.evictor.remove(block_id)\n+                # Block from evictor was already computed\n+                computed = True\n+                block.computed = True\n \n-            self._track_block_id(block_id, computed=True)\n+            self._track_block_id(block_id, computed=computed)\n \n     def _decr_refcount_cached_block(self, block: Block) -> None:\n         # Ensure this is immutable/cached block\n@@ -451,7 +455,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # Increment refcount of the cached block and (possibly) restore\n         # it from the evictor.\n         # Note that in this case, the block is marked as computed\n-        self._incr_refcount_cached_block(block)\n+        self._incr_refcount_cached_block(block, computed=True)\n \n         return block.block_id\n \n@@ -507,7 +511,19 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        \"\"\"Mark blocks as computed.\n+        \n+        This is called after blocks have been processed/computed to update\n+        their computed status. This allows batching the marking operation\n+        for better performance.\n+        \n+        Args:\n+            block_ids: List of block IDs to mark as computed\n+        \"\"\"\n+        for block_id in block_ids:\n+            if block_id in self._block_tracker and self._block_tracker[\n+                    block_id].active:\n+                self._block_tracker[block_id].computed = True\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..2c3106a73 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,25 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        \"\"\"Mark blocks as computed for prefix caching optimization.\n+        \n+        This marks the blocks for all sequences in the group as computed,\n+        which is important for prefix caching to track which blocks have\n+        been processed and can be reused.\n+        \"\"\"\n+        if not self.enable_caching:\n+            return\n+        \n+        # Collect all block IDs from all sequences in the group\n+        block_ids = []\n+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            if seq.seq_id in self.block_tables:\n+                block_ids.extend(\n+                    self.block_tables[seq.seq_id].physical_block_ids)\n+        \n+        # Mark the blocks as computed in the allocator\n+        if block_ids:\n+            self.block_allocator.mark_blocks_as_computed(block_ids)\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
