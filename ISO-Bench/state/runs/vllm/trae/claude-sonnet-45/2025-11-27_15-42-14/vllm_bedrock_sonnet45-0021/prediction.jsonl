{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-64172a9", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex d07527304..1e200b4a2 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -1,6 +1,6 @@\n \"\"\"A layer that samples the next tokens from the model's outputs.\"\"\"\n import itertools\n-from typing import Dict, List, Optional, Tuple\n+from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -89,6 +89,7 @@ def _get_bin_counts_and_mask(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     # Compute the bin counts for the tokens.\n     # vocab_size + 1 for padding.\n+    # Use torch.zeros since scatter_add_ requires initialized values\n     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n@@ -361,13 +362,15 @@ def _sample_with_torch(\n         num_tokens = len(sample_indices)\n         if num_tokens == 0:\n             continue\n+        # Convert to long once to avoid repeated conversions\n+        sample_indices_long = sample_indices.long()\n         seq_group_ids = categorized_seq_group_ids[sampling_type]\n         seq_groups = [sampling_metadata.seq_groups[i] for i in seq_group_ids]\n         is_prompts = [i < sampling_metadata.num_prompts for i in seq_group_ids]\n         sample_metadata[sampling_type] = (seq_group_ids, seq_groups,\n                                           is_prompts, sample_indices)\n         if sampling_type == SamplingType.GREEDY:\n-            greedy_samples = torch.argmax(logprobs[sample_indices.long()],\n+            greedy_samples = torch.argmax(logprobs[sample_indices_long],\n                                           dim=-1)\n         elif sampling_type in (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n             max_best_of_in_batch = 1\n@@ -381,7 +384,7 @@ def _sample_with_torch(\n                 \"generators\": sampling_metadata.generators,\n             }\n             multinomial_samples[sampling_type] = _multinomial(\n-                probs[sample_indices.long()], max_best_of_in_batch,\n+                probs[sample_indices_long], max_best_of_in_batch,\n                 **seeded_args)\n         elif sampling_type == SamplingType.BEAM:\n             beam_search_logprobs = logprobs[sample_indices]\n@@ -506,22 +509,30 @@ def _sample(\n     #                                   sampling_tensors)\n \n \n-def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n+def _get_ranks(x: torch.Tensor,\n+               indices: Union[List[int], torch.Tensor]) -> torch.Tensor:\n     \"\"\"\n     This function calculates the ranks of the chosen tokens in a logprob tensor.\n \n     Args:\n         x (torch.Tensor): 2D logprob tensor of shape (N, M)\n                         where N is the no. of tokens and M is the vocab dim.\n-        indices (List[int]): List of chosen token indices.\n+        indices (Union[List[int], torch.Tensor]): List or tensor of chosen token indices.\n \n     Returns:\n         torch.Tensor: 1D tensor of shape (N,) where N is the no. of tokens.\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[range(len(x)), indices]\n-    return (x > vals[:, None]).long().sum(1) + 1\n+    # Convert indices to tensor if it's a list for better performance\n+    if isinstance(indices, list):\n+        indices = torch.tensor(indices, device=x.device, dtype=torch.long)\n+    \n+    # Use torch.arange with device specification instead of range()\n+    vals = x[torch.arange(0, len(x), device=x.device, dtype=torch.long),\n+             indices]\n+    # Use in-place add_ instead of + 1 for better performance\n+    return (x > vals[:, None]).long().sum(1).add_(1)\n \n \n def _get_logprobs(\n", "model_name_or_path": "gpt-5-2025-08-07"}
