{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f1c8520", "model_patch": "diff --git a/Dockerfile b/Dockerfile\nindex f41753aeb..6ef03b843 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -41,14 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads\n \n RUN python3 setup.py build_ext --inplace\n \n-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\n-RUN apt-get install -y git && \\\n-    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n-    cd megablocks && \\\n-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n-\n # image to run unit testing suite\n FROM dev AS test\n \n@@ -85,12 +77,8 @@ FROM vllm-base AS vllm-openai\n RUN --mount=type=cache,target=/root/.cache/pip \\\n     pip install accelerate\n \n-COPY vllm vllm\n COPY --from=build /workspace/vllm/*.so /workspace/vllm/\n-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/\n-RUN --mount=type=cache,target=/root/.cache/pip \\\n-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n+COPY vllm vllm\n \n ENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n \ndiff --git a/README.md b/README.md\nindex 84cadee48..e4b3b5026 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get\n ```bash\n pip install vllm\n ```\n-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n-```bash\n-pip install megablocks\n-```\n \n ## Getting Started\n \ndiff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst\nindex e21cdd65d..3d768e1be 100644\n--- a/docs/source/models/supported_models.rst\n+++ b/docs/source/models/supported_models.rst\n@@ -74,8 +74,7 @@ Otherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for in\n Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.\n \n .. note::\n-    Currently, the ROCm version of vLLM does not support Mixtral.\n-    Additionally, it only supports Mistral for context lengths up to 4096.\n+    Currently, the ROCm version of vLLM only supports Mistral for context lengths up to 4096.\n \n .. tip::\n     The easiest way to check if your model is supported is to run the program below:\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bafa73c7..d14f7b2c6 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -120,15 +120,6 @@ class ModelConfig:\n             if load_format == \"auto\":\n                 load_format = \"pt\"\n \n-        # FIXME(woosuk): This is a temporary hack. Support safetensor weights.\n-        architectures = getattr(self.hf_config, \"architectures\", [])\n-        if \"MixtralForCausalLM\" in architectures and load_format != \"pt\":\n-            logger.info(\n-                \"Currently, only 'pt' format is supported for Mixtral. \"\n-                \"Changing the format to 'pt'. This may re-download the \"\n-                \"weights if you have downloaded the safetensor weights.\")\n-            load_format = \"pt\"\n-\n         self.load_format = load_format\n \n     def _verify_tokenizer_mode(self) -> None:\n@@ -315,6 +306,7 @@ class ParallelConfig:\n         worker_use_ray: Whether to use Ray for model workers. Will be set to\n             True if either pipeline_parallel_size or tensor_parallel_size is\n             greater than 1.\n+        expert_parallel_size: Number of expert parallel groups for MoE models.\n     \"\"\"\n \n     def __init__(\n@@ -323,9 +315,11 @@ class ParallelConfig:\n         tensor_parallel_size: int,\n         worker_use_ray: bool,\n         max_parallel_loading_workers: Optional[int] = None,\n+        expert_parallel_size: int = 1,\n     ) -> None:\n         self.pipeline_parallel_size = pipeline_parallel_size\n         self.tensor_parallel_size = tensor_parallel_size\n+        self.expert_parallel_size = expert_parallel_size\n         self.worker_use_ray = worker_use_ray\n         self.max_parallel_loading_workers = max_parallel_loading_workers\n \ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 5596884f3..5477b5ab1 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -39,7 +39,7 @@ _MODELS = {\n }\n \n # Models not supported by ROCm.\n-_ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n+_ROCM_UNSUPPORTED_MODELS = []\n \n # Models partially supported by ROCm.\n # Architecture -> Reason.\ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 8e0a094c7..5b126a8f1 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -243,10 +243,9 @@ class BlockSparseMoE(nn.Module):\n         column_indices_t = row_indices.gather(0, gather_indices.long())\n         block_offsets_t = gather_indices.int()\n \n-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)\n         nnz_per_column = ops.histogram(column_indices, block_columns)\n         nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)\n-        offsets_t = torch.cat([zero, nnz_per_column])\n+        offsets_t = F.pad(nnz_per_column, (1, 0), value=0)\n         return column_indices_t, offsets_t, block_offsets_t\n \n     def topology(self, x: torch.Tensor,\n", "model_name_or_path": "gpt-5-2025-08-07"}
