{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-733e7c9", "model_patch": "diff --git a/tests/v1/sample/test_topk_topp_sampler.py b/tests/v1/sample/test_topk_topp_sampler.py\nnew file mode 100644\nindex 000000000..243ec6903\n--- /dev/null\n+++ b/tests/v1/sample/test_topk_topp_sampler.py\n@@ -0,0 +1,39 @@\n+# SPDX-License-Identifier: Apache-2.0\n+import torch\n+from torch import Generator\n+import pytest\n+\n+from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p\n+\n+DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+BATCH_SIZE = 1024\n+VOCAB_SIZE = 128 * 1024\n+\n+\n+@pytest.mark.skipif(not torch.cuda.is_available(), \n+                    reason=\"CUDA not available\")\n+def test_topk_impl_equivalance():\n+\n+    with torch.device(DEVICE):\n+        generator = Generator(device=DEVICE).manual_seed(33)\n+\n+        logits = torch.rand((BATCH_SIZE, VOCAB_SIZE), generator=generator)\n+\n+        # Random top-k values between 1 and 9.\n+        k = torch.randint(1, 10, (BATCH_SIZE, ), generator=generator)\n+\n+        # Set k=vocab_size for ~50% of requests in the batch (top-k disabled).\n+        k.masked_fill_(\n+            torch.rand((BATCH_SIZE, ), generator=generator) < 0.5, VOCAB_SIZE)\n+\n+        # Apply top-k using the optimized implementation.\n+        logits_topk = apply_top_k_top_p(logits.clone(), k, None)\n+\n+        # Apply top-k using the reference implementation (via top-k + top-p).\n+        # When p=1.0, top-p is effectively disabled.\n+        p = torch.ones((BATCH_SIZE, ), device=DEVICE)\n+        logits_ref = apply_top_k_top_p(logits.clone(), k, p)\n+\n+        # Check that the results are equivalent.\n+        assert torch.allclose(logits_topk, logits_ref, atol=1e-5)\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea71187..e47319974 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -140,6 +140,85 @@ def apply_top_k_top_p(\n     \"\"\"\n     if k is None and p is None:\n         return logits\n+    \n+    # Fast path for top-k only (no top-p).\n+    # This avoids sorting the entire vocabulary and is much faster.\n+    if k is not None and p is None:\n+        # Use topk to get the k-th largest values efficiently.\n+        # We need to handle variable k values across the batch.\n+        vocab_size = logits.shape[-1]\n+        \n+        # For requests where k >= vocab_size, no masking is needed.\n+        # We'll process them separately to avoid unnecessary work.\n+        k_long = k.to(torch.long)\n+        needs_masking = k_long < vocab_size\n+        \n+        if not needs_masking.any():\n+            # No masking needed for any request.\n+            return logits\n+        \n+        if needs_masking.all():\n+            # All requests need masking. Use the efficient path.\n+            # Get the maximum k to determine how many values to retrieve.\n+            max_k = k_long.max().item()\n+            \n+            # Use topk to get the top max_k values for all requests.\n+            # This is much faster than sorting the entire vocabulary.\n+            topk_values, topk_indices = torch.topk(logits, \n+                                                     min(max_k, vocab_size), \n+                                                     dim=-1, \n+                                                     largest=True, \n+                                                     sorted=False)\n+            \n+            # Create a mask for values to keep.\n+            # We'll use scatter to efficiently mark which indices to keep.\n+            mask = torch.ones_like(logits, dtype=torch.bool)\n+            \n+            # Create a range tensor for indexing.\n+            # This allows us to vectorize the masking operation.\n+            batch_size = logits.shape[0]\n+            range_tensor = torch.arange(max_k, device=logits.device).unsqueeze(0).expand(batch_size, -1)\n+            k_expanded = k_long.unsqueeze(1)\n+            \n+            # Create a mask for which topk indices to keep (those < k for each row).\n+            keep_mask = range_tensor < k_expanded\n+            \n+            # Scatter the keep_mask into the full mask.\n+            # We need to flatten for scatter, then reshape.\n+            batch_indices = torch.arange(batch_size, device=logits.device).unsqueeze(1).expand(-1, max_k)\n+            mask[batch_indices[keep_mask], topk_indices[keep_mask]] = False\n+            \n+            # Mask out (set to -inf) all values not in top-k.\n+            logits.masked_fill_(mask, -float(\"inf\"))\n+            return logits\n+        else:\n+            # Mixed case: some requests need masking, some don't.\n+            # Only process the ones that need masking.\n+            logits_masked = logits[needs_masking]\n+            k_masked = k_long[needs_masking]\n+            \n+            max_k = k_masked.max().item()\n+            topk_values, topk_indices = torch.topk(logits_masked, \n+                                                     min(max_k, vocab_size), \n+                                                     dim=-1, \n+                                                     largest=True, \n+                                                     sorted=False)\n+            \n+            mask = torch.ones_like(logits_masked, dtype=torch.bool)\n+            \n+            batch_size_masked = logits_masked.shape[0]\n+            range_tensor = torch.arange(max_k, device=logits.device).unsqueeze(0).expand(batch_size_masked, -1)\n+            k_expanded = k_masked.unsqueeze(1)\n+            keep_mask = range_tensor < k_expanded\n+            \n+            batch_indices = torch.arange(batch_size_masked, device=logits.device).unsqueeze(1).expand(-1, max_k)\n+            mask[batch_indices[keep_mask], topk_indices[keep_mask]] = False\n+            \n+            logits_masked.masked_fill_(mask, -float(\"inf\"))\n+            logits[needs_masking] = logits_masked\n+            return logits\n+    \n+    # Original path for top-p or combined top-k + top-p.\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n     if k is not None:\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049dc..d69372374 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -106,6 +106,8 @@ class Sampler(nn.Module):\n             logits = self.apply_min_p(logits, sampling_metadata.min_p)\n \n         # Apply top_k and/or top_p.\n+        # The topk_topp_sampler has an optimized fast path for top-k only\n+        # that avoids sorting the entire vocabulary.\n         random_sampled = self.topk_topp_sampler(\n             logits,\n             sampling_metadata.generators,\n", "model_name_or_path": "gpt-5-2025-08-07"}
