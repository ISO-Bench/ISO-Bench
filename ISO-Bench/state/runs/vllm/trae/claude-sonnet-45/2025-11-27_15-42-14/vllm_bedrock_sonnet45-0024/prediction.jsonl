{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0df4d9b", "model_patch": "diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex d073dd6d2..a1f38903c 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -15,15 +15,16 @@ namespace vllm {\n // TODO(woosuk): Further optimize this kernel.\n template <typename scalar_t>\n __global__ void rms_norm_kernel(\n-    scalar_t* __restrict__ out,           // [..., hidden_size]\n-    const scalar_t* __restrict__ input,   // [..., hidden_size]\n+    scalar_t* __restrict__ out,          // [..., hidden_size]\n+    const scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int64_t input_stride,\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[blockIdx.x * input_stride + idx];\n     variance += x * x;\n   }\n \n@@ -37,7 +38,7 @@ __global__ void rms_norm_kernel(\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n+    float x = (float)input[blockIdx.x * input_stride + idx];\n     out[blockIdx.x * hidden_size + idx] =\n         ((scalar_t)(x * s_variance)) * weight[idx];\n   }\n@@ -141,11 +142,11 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n               torch::Tensor& weight,  // [hidden_size]\n               double epsilon) {\n   TORCH_CHECK(out.is_contiguous());\n-  TORCH_CHECK(input.is_contiguous());\n   TORCH_CHECK(weight.is_contiguous());\n \n   int hidden_size = input.size(-1);\n   int num_tokens = input.numel() / hidden_size;\n+  int64_t input_stride = input.stride(-2);\n \n   dim3 grid(num_tokens);\n   dim3 block(std::min(hidden_size, 1024));\n@@ -153,7 +154,7 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\n     vllm::rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        out.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(),\n+        out.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(), input_stride,\n         weight.data_ptr<scalar_t>(), epsilon, num_tokens, hidden_size);\n   });\n }\ndiff --git a/csrc/layernorm_quant_kernels.cu b/csrc/layernorm_quant_kernels.cu\nindex d595b9e88..70f6eb9c0 100644\n--- a/csrc/layernorm_quant_kernels.cu\n+++ b/csrc/layernorm_quant_kernels.cu\n@@ -23,8 +23,9 @@ namespace vllm {\n // TODO(woosuk): Further optimize this kernel.\n template <typename scalar_t, typename fp8_type>\n __global__ void rms_norm_static_fp8_quant_kernel(\n-    fp8_type* __restrict__ out,           // [..., hidden_size]\n-    const scalar_t* __restrict__ input,   // [..., hidden_size]\n+    fp8_type* __restrict__ out,          // [..., hidden_size]\n+    const scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int64_t input_stride,\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float* __restrict__ scale,      // [1]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n@@ -32,7 +33,7 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[blockIdx.x * input_stride + idx];\n     variance += x * x;\n   }\n \n@@ -49,7 +50,7 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   float const scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n+    float x = (float)input[blockIdx.x * input_stride + idx];\n     float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n     out[blockIdx.x * hidden_size + idx] =\n         scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n@@ -171,6 +172,7 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n                                double epsilon) {\n   int hidden_size = input.size(-1);\n   int num_tokens = input.numel() / hidden_size;\n+  int64_t input_stride = input.stride(-2);\n \n   dim3 grid(num_tokens);\n   dim3 block(std::min(hidden_size, 1024));\n@@ -183,8 +185,9 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n               vllm::rms_norm_static_fp8_quant_kernel<scalar_t, fp8_t>\n                   <<<grid, block, 0, stream>>>(\n                       out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),\n-                      weight.data_ptr<scalar_t>(), scale.data_ptr<float>(),\n-                      epsilon, num_tokens, hidden_size);\n+                      input_stride, weight.data_ptr<scalar_t>(),\n+                      scale.data_ptr<float>(), epsilon, num_tokens,\n+                      hidden_size);\n             });\n       });\n }\n", "model_name_or_path": "gpt-5-2025-08-07"}
