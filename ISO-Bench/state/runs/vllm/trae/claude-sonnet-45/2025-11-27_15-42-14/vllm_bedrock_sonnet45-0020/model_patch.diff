diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py
index 5b5643748..efb2139b5 100644
--- a/vllm/_custom_ops.py
+++ b/vllm/_custom_ops.py
@@ -189,10 +189,37 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,
 def scaled_fp8_quant(
     input: torch.Tensor,
     scale: Optional[torch.Tensor] = None,
+    batch_dim_padding: Optional[int] = None,
 ) -> Tuple[torch.Tensor, torch.Tensor]:
-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)
+    """
+    Quantize input tensor to FP8 and return quantized tensor and scale.
+
+    This function supports both static and dynamic quantization: If you
+    provide the scale, it will use static scaling and if you omit it,
+    the scale will be determined dynamically. The function also allows
+    optional padding of the output tensor for downstream kernels that
+    will benefit from padding.
+
+    Args:
+        input: The input tensor to be quantized to FP8
+        scale: Optional scaling factor for the FP8 quantization
+        batch_dim_padding: If specified, pad the first dimension
+            of the output to at least this value.
+
+    Returns:
+        Tuple[torch.Tensor, torch.Tensor]: The output tensor in FP8 and
+            scaling factor.
+    """
+    # Determine if padding is needed
+    if batch_dim_padding is not None:
+        shape = (max(batch_dim_padding, input.shape[0]), *input.shape[1:])
+        output = torch.empty(shape, device=input.device, 
+                           dtype=torch.float8_e4m3fn)
+    else:
+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)
+    
     if scale is None:
-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)
+        scale = torch.empty(1, device=input.device, dtype=torch.float32)
         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)
     else:
         vllm_ops.static_scaled_fp8_quant(output, input, scale)
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index b57e1dde8..1286fce4a 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -231,7 +231,12 @@ class Fp8LinearMethod(LinearMethodBase):
         # ops.scaled_fp8_quant supports both dynamic and static quant.
         #   If dynamic, layer.act_scale is None and x_scale computed from x.
         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.
-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)
+        
+        # Pad the input to improve CUBLASLt performance
+        # CUBLASLt finds better algorithms when batch size >= 16
+        batch_dim_padding = 16 if x.shape[0] < 16 else None
+        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale, 
+                                               batch_dim_padding)
 
         # Fused GEMM_DQ
         output, _ = torch._scaled_mm(
@@ -242,6 +247,10 @@ class Fp8LinearMethod(LinearMethodBase):
             scale_b=layer.weight_scale,
             bias=bias,
         )
+        
+        # If we padded, slice to original size
+        if batch_dim_padding is not None:
+            output = output[:x.shape[0]]
 
         return output
 
