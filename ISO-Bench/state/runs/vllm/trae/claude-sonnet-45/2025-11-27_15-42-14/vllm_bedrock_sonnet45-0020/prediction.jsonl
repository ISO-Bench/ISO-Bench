{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-ebce310", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 5b5643748..efb2139b5 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -189,10 +189,37 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n def scaled_fp8_quant(\n     input: torch.Tensor,\n     scale: Optional[torch.Tensor] = None,\n+    batch_dim_padding: Optional[int] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+    \"\"\"\n+    Quantize input tensor to FP8 and return quantized tensor and scale.\n+\n+    This function supports both static and dynamic quantization: If you\n+    provide the scale, it will use static scaling and if you omit it,\n+    the scale will be determined dynamically. The function also allows\n+    optional padding of the output tensor for downstream kernels that\n+    will benefit from padding.\n+\n+    Args:\n+        input: The input tensor to be quantized to FP8\n+        scale: Optional scaling factor for the FP8 quantization\n+        batch_dim_padding: If specified, pad the first dimension\n+            of the output to at least this value.\n+\n+    Returns:\n+        Tuple[torch.Tensor, torch.Tensor]: The output tensor in FP8 and\n+            scaling factor.\n+    \"\"\"\n+    # Determine if padding is needed\n+    if batch_dim_padding is not None:\n+        shape = (max(batch_dim_padding, input.shape[0]), *input.shape[1:])\n+        output = torch.empty(shape, device=input.device, \n+                           dtype=torch.float8_e4m3fn)\n+    else:\n+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+    \n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex b57e1dde8..1286fce4a 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -231,7 +231,12 @@ class Fp8LinearMethod(LinearMethodBase):\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n+        \n+        # Pad the input to improve CUBLASLt performance\n+        # CUBLASLt finds better algorithms when batch size >= 16\n+        batch_dim_padding = 16 if x.shape[0] < 16 else None\n+        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale, \n+                                               batch_dim_padding)\n \n         # Fused GEMM_DQ\n         output, _ = torch._scaled_mm(\n@@ -242,6 +247,10 @@ class Fp8LinearMethod(LinearMethodBase):\n             scale_b=layer.weight_scale,\n             bias=bias,\n         )\n+        \n+        # If we padded, slice to original size\n+        if batch_dim_padding is not None:\n+            output = output[:x.shape[0]]\n \n         return output\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
