{
  "changed": [],
  "allowed": [
    "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py"
  ],
  "disallowed": [],
  "ok": true
}