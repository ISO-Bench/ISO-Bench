{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d263bd9", "model_patch": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex abcd4b007..67166fb05 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -118,6 +118,12 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n+        self.arange_np = np.arange(max(self.max_num_reqs, self.max_model_len),\n+                                   dtype=np.int32)\n+        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n+        # a faster version of creating a new tensor every time. Thus, we should\n+        # not make any assumptions about the values in these tensors.\n         self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n@@ -269,11 +275,13 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n+        req_indices = np.repeat(self.arange_np[:num_reqs],\n+                                num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n+        arange = np.concatenate(\n+            [self.arange_np[:n] for n in num_scheduled_tokens])\n \n         # Get positions.\n         positions_np = self.positions_np[:total_num_scheduled_tokens]\n", "model_name_or_path": "gpt-5-2025-08-07"}
