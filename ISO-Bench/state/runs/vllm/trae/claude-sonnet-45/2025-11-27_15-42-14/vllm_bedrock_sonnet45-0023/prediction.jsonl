{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c8d70e2", "model_patch": "diff --git a/requirements-common.txt b/requirements-common.txt\nindex b7c94cbdb..c52980bc7 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -1,6 +1,7 @@\n psutil\n sentencepiece  # Required for LLaMA tokenizer.\n numpy < 2.0.0\n+numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding.\n requests >= 2.26.0\n tqdm\n blake3\ndiff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py\nindex 9b116e00a..9d6689d5e 100644\n--- a/vllm/v1/spec_decode/ngram_proposer.py\n+++ b/vllm/v1/spec_decode/ngram_proposer.py\n@@ -1,14 +1,12 @@\n # SPDX-License-Identifier: Apache-2.0\n-from typing import List, Optional\n+from typing import Optional\n \n import numpy as np\n+from numba import jit\n \n \n class NgramProposer:\n \n-    def __init__(self):\n-        pass\n-\n     def propose(\n         self,\n         context_token_ids: np.ndarray,\n@@ -41,66 +39,70 @@ class NgramProposer:\n               followed that pattern. Here we will return [4,2,3] because \n               we only have three tokens after the match.\n         \"\"\"\n-        # TODO: Use c++ to implement the _find_subarray_kmp to\n-        # improve the efficiency\n-        return self._find_subarray_kmp(context_token_ids, n, k)\n+        return _find_subarray_kmp(context_token_ids, n, k)\n \n-    @staticmethod\n-    def _kmp_lps_array(pattern: List[int]) -> List[int]:\n-        \"\"\"\n-        Build the lps (longest proper prefix which is also suffix) \n-        array for the pattern.\n-        \"\"\"\n-        lps = [0] * len(pattern)\n-        prev_lps = 0  # length of the previous longest prefix suffix\n-        i = 1\n \n-        while i < len(pattern):\n-            if pattern[i] == pattern[prev_lps]:\n-                prev_lps += 1\n-                lps[i] = prev_lps\n-                i += 1\n+\n+@jit(nopython=True, cache=True)\n+def _kmp_lps_array(pattern: np.ndarray) -> np.ndarray:\n+    \"\"\"\n+    Build the lps (longest proper prefix which is also suffix) \n+    array for the pattern.\n+    \"\"\"\n+    pattern_len = len(pattern)\n+    lps = np.zeros(pattern_len, dtype=np.int32)\n+    prev_lps = 0  # length of the previous longest prefix suffix\n+    i = 1\n+\n+    while i < pattern_len:\n+        if pattern[i] == pattern[prev_lps]:\n+            prev_lps += 1\n+            lps[i] = prev_lps\n+            i += 1\n+        else:\n+            if prev_lps != 0:\n+                prev_lps = lps[prev_lps - 1]\n             else:\n-                if prev_lps != 0:\n-                    prev_lps = lps[prev_lps - 1]\n-                else:\n-                    lps[i] = 0\n-                    i += 1\n+                lps[i] = 0\n+                i += 1\n \n-        return lps\n+    return lps\n \n-    @staticmethod\n-    def _find_subarray_kmp(\n-        context_token_ids: np.ndarray,\n-        n: int,\n-        k: int,\n-    ) -> Optional[np.ndarray]:\n-        context_len = context_token_ids.shape[0]\n-        assert n > 0\n \n-        pattern = context_token_ids[-n:]\n-        # Precompute lps array for Y\n-        lps = NgramProposer._kmp_lps_array(pattern)\n+@jit(nopython=True, cache=True)\n+def _find_subarray_kmp(\n+    context_token_ids: np.ndarray,\n+    n: int,\n+    k: int,\n+) -> Optional[np.ndarray]:\n+    context_len = context_token_ids.shape[0]\n+    if n <= 0:\n+        return None\n+\n+    pattern = context_token_ids[-n:]\n+    # Precompute lps array for Y\n+    lps = _kmp_lps_array(pattern)\n \n-        i = 0\n-        j = 0\n-        # -n because the last n tokens are used as pattern\n-        while i < context_len - n:\n-            if context_token_ids[i] == pattern[j]:\n-                i += 1\n-                j += 1\n+    i = 0\n+    j = 0\n+    # -n because the last n tokens are used as pattern\n+    while i < context_len - n:\n+        if context_token_ids[i] == pattern[j]:\n+            i += 1\n+            j += 1\n \n-                # If we have matched the entire Y\n-                if j == n:\n-                    # Found pattern in context, gather the next K elements\n-                    return context_token_ids[i:i + k]\n+            # If we have matched the entire Y\n+            if j == n:\n+                # Found pattern in context, gather the next K elements\n+                end_idx = min(i + k, context_len)\n+                return context_token_ids[i:end_idx]\n+        else:\n+            # Mismatch\n+            if j != 0:\n+                # Use the lps array to avoid re-checking elements\n+                j = lps[j - 1]\n             else:\n-                # Mismatch\n-                if j != 0:\n-                    # Use the lps array to avoid re-checking elements\n-                    j = lps[j - 1]\n-                else:\n-                    i += 1\n+                i += 1\n \n-        # Y not found\n-        return None\n+    # Y not found\n+    return None\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0ecc00acc..246544913 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -1012,8 +1012,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self,\n         sampled_token_ids: List[List[int]],\n     ) -> List[List[int]]:\n-        # TODO(woosuk): Optimize.\n         draft_token_ids: List[List[int]] = []\n+        ngram_min = self.speculative_config.ngram_prompt_lookup_min\n+        num_spec_tokens = self.speculative_config.num_speculative_tokens\n+        \n         for i, sampled_ids in enumerate(sampled_token_ids):\n             num_sampled_ids = len(sampled_ids)\n             if not num_sampled_ids:\n@@ -1025,10 +1027,13 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             start_idx = self.input_batch.num_tokens_no_spec[i]\n             end_idx = start_idx + num_sampled_ids\n             self.input_batch.token_ids_cpu[i, start_idx:end_idx] = sampled_ids\n+            \n+            # Use numpy array directly for better performance\n+            context = self.input_batch.token_ids_cpu[i, :end_idx]\n             drafter_output = self.drafter.propose(\n-                self.input_batch.token_ids_cpu[i, :end_idx],\n-                self.speculative_config.ngram_prompt_lookup_min,\n-                self.speculative_config.num_speculative_tokens,\n+                context,\n+                ngram_min,\n+                num_spec_tokens,\n             )\n             if drafter_output is None or len(drafter_output) == 0:\n                 draft_token_ids.append([])\n", "model_name_or_path": "gpt-5-2025-08-07"}
