diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py
index f588f4016..7a2786bcd 100644
--- a/vllm/reasoning/qwen3_reasoning_parser.py
+++ b/vllm/reasoning/qwen3_reasoning_parser.py
@@ -1,6 +1,5 @@
 # SPDX-License-Identifier: Apache-2.0
 
-import re
 from collections.abc import Sequence
 from typing import Optional, Union
 
@@ -31,9 +30,6 @@ class Qwen3ReasoningParser(ReasoningParser):
         self.think_start_token = "<think>"
         self.think_end_token = "</think>"
 
-        self.reasoning_regex = re.compile(
-            rf"{self.think_start_token}(.*?){self.think_end_token}", re.DOTALL)
-
         if not self.model_tokenizer:
             raise ValueError(
                 "The model tokenizer must be passed to the ReasoningParser "
@@ -121,29 +117,31 @@ class Qwen3ReasoningParser(ReasoningParser):
     def extract_reasoning_content(
             self, model_output: str, request: ChatCompletionRequest
     ) -> tuple[Optional[str], Optional[str]]:
+        """
+        Extract reasoning content from the model output.
 
-        # Check if the model output contains the <think> tokens.
-        if (self.think_start_token not in model_output
-                or self.think_end_token not in model_output):
+        For text <think>abc</think>xyz:
+        - 'abc' goes to reasoning_content
+        - 'xyz' goes to content (model_output)
+        """
+        # Find the start and end positions of the think tokens
+        start_index = model_output.find(self.think_start_token)
+        if start_index == -1:
             return None, model_output
-        else:
-            # Use a regex to find the reasoning content
-            reasoning_content = self.reasoning_regex.findall(model_output)[0]
-
-            # Remove the reasoning content from the model output
-            # Although <think> token is always at the
-            # beginning of the line, we cannot guarantee that the
-            # other models will follow this convention.
-            # Therefore, we need to add :start_index.
-            start_index = model_output.find(self.think_start_token)
-            if start_index != -1:
-                end_index = start_index + len(
-                    f"{self.think_start_token}{reasoning_content}{self.think_end_token}"
-                )
-                model_output = model_output[:start_index] + \
-                                model_output[end_index:]
-
-                if len(model_output) == 0:
-                    return reasoning_content, None
-
-            return reasoning_content, model_output
+        
+        end_index = model_output.find(self.think_end_token, start_index)
+        if end_index == -1:
+            return None, model_output
+        
+        # Extract reasoning content between the tokens
+        reasoning_start = start_index + len(self.think_start_token)
+        reasoning_content = model_output[reasoning_start:end_index]
+        
+        # Build the output without the reasoning section
+        content_end = end_index + len(self.think_end_token)
+        content = model_output[:start_index] + model_output[content_end:]
+        
+        if len(content) == 0:
+            return reasoning_content, None
+        
+        return reasoning_content, content
