{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-fbefc8a", "model_patch": "diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nindex f588f4016..7a2786bcd 100644\n--- a/vllm/reasoning/qwen3_reasoning_parser.py\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -1,6 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-import re\n from collections.abc import Sequence\n from typing import Optional, Union\n \n@@ -31,9 +30,6 @@ class Qwen3ReasoningParser(ReasoningParser):\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n \n-        self.reasoning_regex = re.compile(\n-            rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n-\n         if not self.model_tokenizer:\n             raise ValueError(\n                 \"The model tokenizer must be passed to the ReasoningParser \"\n@@ -121,29 +117,31 @@ class Qwen3ReasoningParser(ReasoningParser):\n     def extract_reasoning_content(\n             self, model_output: str, request: ChatCompletionRequest\n     ) -> tuple[Optional[str], Optional[str]]:\n+        \"\"\"\n+        Extract reasoning content from the model output.\n \n-        # Check if the model output contains the <think> tokens.\n-        if (self.think_start_token not in model_output\n-                or self.think_end_token not in model_output):\n+        For text <think>abc</think>xyz:\n+        - 'abc' goes to reasoning_content\n+        - 'xyz' goes to content (model_output)\n+        \"\"\"\n+        # Find the start and end positions of the think tokens\n+        start_index = model_output.find(self.think_start_token)\n+        if start_index == -1:\n             return None, model_output\n-        else:\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            # Remove the reasoning content from the model output\n-            # Although <think> token is always at the\n-            # beginning of the line, we cannot guarantee that the\n-            # other models will follow this convention.\n-            # Therefore, we need to add :start_index.\n-            start_index = model_output.find(self.think_start_token)\n-            if start_index != -1:\n-                end_index = start_index + len(\n-                    f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-                )\n-                model_output = model_output[:start_index] + \\\n-                                model_output[end_index:]\n-\n-                if len(model_output) == 0:\n-                    return reasoning_content, None\n-\n-            return reasoning_content, model_output\n+        \n+        end_index = model_output.find(self.think_end_token, start_index)\n+        if end_index == -1:\n+            return None, model_output\n+        \n+        # Extract reasoning content between the tokens\n+        reasoning_start = start_index + len(self.think_start_token)\n+        reasoning_content = model_output[reasoning_start:end_index]\n+        \n+        # Build the output without the reasoning section\n+        content_end = end_index + len(self.think_end_token)\n+        content = model_output[:start_index] + model_output[content_end:]\n+        \n+        if len(content) == 0:\n+            return reasoning_content, None\n+        \n+        return reasoning_content, content\n", "model_name_or_path": "gpt-5-2025-08-07"}
