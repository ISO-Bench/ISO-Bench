diff --git a/tests/v1/test_serial_utils.py b/tests/v1/test_serial_utils.py
index bc0e0cbd8..9222f3a9c 100644
--- a/tests/v1/test_serial_utils.py
+++ b/tests/v1/test_serial_utils.py
@@ -50,13 +50,13 @@ def test_encode_decode():
         large_non_contig_tensor=torch.rand(1024, 512)[:, 10:20],
     )
 
-    encoder = MsgpackEncoder()
+    encoder = MsgpackEncoder(size_threshold=256)
     decoder = MsgpackDecoder(MyType)
 
     encoded = encoder.encode(obj)
 
     # There should be the main buffer + 4 large tensor buffers
-    # + 1 large numpy array. "large" is <= 512 bytes.
+    # + 1 large numpy array. "large" is <= 256 bytes (with size_threshold=256).
     # The two small tensors are encoded inline.
     assert len(encoded) == 6
 
diff --git a/vllm/envs.py b/vllm/envs.py
index f80bf878f..385756859 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -90,6 +90,7 @@ if TYPE_CHECKING:
     V_SCALE_CONSTANT: int = 100
     VLLM_SERVER_DEV_MODE: bool = False
     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128
+    VLLM_V1_SERIALIZATION_SIZE_THRESHOLD: int = 512
     VLLM_MLA_DISABLE: bool = False
     VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False
     VLLM_RAY_PER_WORKER_GPUS: float = 1.0
@@ -605,6 +606,12 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_V1_OUTPUT_PROC_CHUNK_SIZE":
     lambda: int(os.getenv("VLLM_V1_OUTPUT_PROC_CHUNK_SIZE", "128")),
 
+    # Size threshold in bytes for zero-copy serialization in V1.
+    # Tensors/arrays smaller than this will be encoded inline,
+    # larger ones will use zero-copy buffer references.
+    "VLLM_V1_SERIALIZATION_SIZE_THRESHOLD":
+    lambda: int(os.getenv("VLLM_V1_SERIALIZATION_SIZE_THRESHOLD", "512")),
+
     # If set, vLLM will disable the MLA attention optimizations.
     "VLLM_MLA_DISABLE":
     lambda: bool(int(os.getenv("VLLM_MLA_DISABLE", "0"))),
diff --git a/vllm/v1/serial_utils.py b/vllm/v1/serial_utils.py
index 3af6793fd..89784137f 100644
--- a/vllm/v1/serial_utils.py
+++ b/vllm/v1/serial_utils.py
@@ -29,12 +29,15 @@ class MsgpackEncoder:
     not thread-safe when encoding tensors / numpy arrays.
     """
 
-    def __init__(self):
+    def __init__(self, size_threshold: Optional[int] = None):
         self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)
         # This is used as a local stash of buffers that we can then access from
         # our custom `msgspec` hook, `enc_hook`. We don't have a way to
         # pass custom data to the hook otherwise.
         self.aux_buffers: Optional[list[bytestr]] = None
+        # Size threshold for zero-copy serialization
+        self.size_threshold = (size_threshold if size_threshold is not None 
+                              else MIN_NOCOPY_BUF_SIZE)
 
     def encode(self, obj: Any) -> Sequence[bytestr]:
         try:
@@ -78,7 +81,7 @@ class MsgpackEncoder:
     ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:
         assert self.aux_buffers is not None
         arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()
-        if not obj.shape or obj.nbytes < MIN_NOCOPY_BUF_SIZE:
+        if not obj.shape or obj.nbytes < self.size_threshold:
             # Encode small arrays and scalars inline. Using this extension type
             # ensures we can avoid copying when decoding.
             data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)
