{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3cd91dc", "model_patch": "diff --git a/tests/v1/test_serial_utils.py b/tests/v1/test_serial_utils.py\nindex bc0e0cbd8..9222f3a9c 100644\n--- a/tests/v1/test_serial_utils.py\n+++ b/tests/v1/test_serial_utils.py\n@@ -50,13 +50,13 @@ def test_encode_decode():\n         large_non_contig_tensor=torch.rand(1024, 512)[:, 10:20],\n     )\n \n-    encoder = MsgpackEncoder()\n+    encoder = MsgpackEncoder(size_threshold=256)\n     decoder = MsgpackDecoder(MyType)\n \n     encoded = encoder.encode(obj)\n \n     # There should be the main buffer + 4 large tensor buffers\n-    # + 1 large numpy array. \"large\" is <= 512 bytes.\n+    # + 1 large numpy array. \"large\" is <= 256 bytes (with size_threshold=256).\n     # The two small tensors are encoded inline.\n     assert len(encoded) == 6\n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex f80bf878f..385756859 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -90,6 +90,7 @@ if TYPE_CHECKING:\n     V_SCALE_CONSTANT: int = 100\n     VLLM_SERVER_DEV_MODE: bool = False\n     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n+    VLLM_V1_SERIALIZATION_SIZE_THRESHOLD: int = 512\n     VLLM_MLA_DISABLE: bool = False\n     VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False\n     VLLM_RAY_PER_WORKER_GPUS: float = 1.0\n@@ -605,6 +606,12 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n     lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n \n+    # Size threshold in bytes for zero-copy serialization in V1.\n+    # Tensors/arrays smaller than this will be encoded inline,\n+    # larger ones will use zero-copy buffer references.\n+    \"VLLM_V1_SERIALIZATION_SIZE_THRESHOLD\":\n+    lambda: int(os.getenv(\"VLLM_V1_SERIALIZATION_SIZE_THRESHOLD\", \"512\")),\n+\n     # If set, vLLM will disable the MLA attention optimizations.\n     \"VLLM_MLA_DISABLE\":\n     lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\ndiff --git a/vllm/v1/serial_utils.py b/vllm/v1/serial_utils.py\nindex 3af6793fd..89784137f 100644\n--- a/vllm/v1/serial_utils.py\n+++ b/vllm/v1/serial_utils.py\n@@ -29,12 +29,15 @@ class MsgpackEncoder:\n     not thread-safe when encoding tensors / numpy arrays.\n     \"\"\"\n \n-    def __init__(self):\n+    def __init__(self, size_threshold: Optional[int] = None):\n         self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)\n         # This is used as a local stash of buffers that we can then access from\n         # our custom `msgspec` hook, `enc_hook`. We don't have a way to\n         # pass custom data to the hook otherwise.\n         self.aux_buffers: Optional[list[bytestr]] = None\n+        # Size threshold for zero-copy serialization\n+        self.size_threshold = (size_threshold if size_threshold is not None \n+                              else MIN_NOCOPY_BUF_SIZE)\n \n     def encode(self, obj: Any) -> Sequence[bytestr]:\n         try:\n@@ -78,7 +81,7 @@ class MsgpackEncoder:\n     ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n         assert self.aux_buffers is not None\n         arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()\n-        if not obj.shape or obj.nbytes < MIN_NOCOPY_BUF_SIZE:\n+        if not obj.shape or obj.nbytes < self.size_threshold:\n             # Encode small arrays and scalars inline. Using this extension type\n             # ensures we can avoid copying when decoding.\n             data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)\n", "model_name_or_path": "gpt-5-2025-08-07"}
