{"instance_id": "vllm-project__vllm-3a1e648", "model_patch": "diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62e9..81fe2ec4e 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -133,7 +133,7 @@ class RejectionSampler(nn.Module):\n             # Generate mask with bonus token.\n             generate_mask = torch.cat([\n                 accept_mask,\n-                torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)\n+                torch.empty(accept_mask.size(0), 1, device=accept_mask.device)\n             ],\n                                       dim=1).to(torch.bool) & valid_mask\n             zeros_mask = (generate_mask == 0)\n@@ -204,7 +204,7 @@ class RejectionSampler(nn.Module):\n             # Generate mask with bonus tokens.\n             generate_mask = torch.cat([\n                 accept_mask,\n-                torch.zeros(batch_size, 1, device=accept_mask.device)\n+                torch.empty(batch_size, 1, device=accept_mask.device)\n             ],\n                                       dim=1).to(torch.bool)\n             zeros_mask = (generate_mask == 0)\n@@ -266,7 +266,7 @@ def _create_greedy_token_probs(\n ) -> torch.Tensor:\n     batch_size, num_tokens = token_ids.shape\n \n-    token_probs = torch.zeros(batch_size,\n+    token_probs = torch.empty(batch_size,\n                               num_tokens,\n                               vocab_size,\n                               dtype=torch.float,\n@@ -277,6 +277,7 @@ def _create_greedy_token_probs(\n     valid_indices = token_ids.clone()\n     valid_indices[~valid_mask] = 0\n \n+    token_probs.zero_()\n     token_probs.scatter_(dim=2,\n                          index=valid_indices.unsqueeze(-1),\n                          src=valid_mask.unsqueeze(-1).float())\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 66015382b..f87a84fda 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -234,27 +234,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
