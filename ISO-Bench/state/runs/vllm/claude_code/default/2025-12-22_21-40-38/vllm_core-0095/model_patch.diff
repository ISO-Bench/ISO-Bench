diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
index b53a540ed..c5ec132c4 100644
--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
@@ -467,9 +467,11 @@ class MambaMixer2(CustomOp):
 
             initial_states = None
             if has_initial_states is not None and any(has_initial_states):
-                for idx in mamba_cache_params.state_indices_tensor[
-                        ~has_initial_states]:
-                    mamba_cache_params.ssm_state[idx].zero_()
+                # Batch zero operation for better performance
+                no_initial_state_indices = mamba_cache_params.state_indices_tensor[
+                    ~has_initial_states]
+                if len(no_initial_state_indices) > 0:
+                    mamba_cache_params.ssm_state[no_initial_state_indices] = 0
                 initial_states = mamba_cache_params.ssm_state[
                     mamba_cache_params.state_indices_tensor]
 
@@ -495,23 +497,27 @@ class MambaMixer2(CustomOp):
 
             # update ssm states
             # - varlen state is a (batch, nheads, headdim, dstate) tensor
-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):
-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])
+            # Batch copy for better performance
+            mamba_cache_params.ssm_state[mamba_cache_params.state_indices_tensor] = varlen_state
 
             # - reshape
             hidden_states = scan_output.view(seq_len, -1)
         else:
 
             n_groups = self.n_groups // self.tp_size
-            A = self.A[:, None, ...][:, :, None].expand(
+            num_heads_per_rank = self.num_heads // self.tp_size
+            # Optimize expand operations: use unsqueeze for clarity and efficiency
+            A = self.A.unsqueeze(1).unsqueeze(2).expand(
                 -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)
-            dt = dt[:, :, None].expand(-1, -1, self.head_dim)
-            dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)
-            D = self.D[:, None, ...].expand(-1, self.head_dim)
-            B = B.view(-1, n_groups, B.shape[1] // n_groups)
-            C = C.view(-1, n_groups, C.shape[1] // n_groups)
+            dt = dt.unsqueeze(2).expand(-1, -1, self.head_dim)
+            dt_bias = self.dt_bias.unsqueeze(1).expand(-1, self.head_dim)
+            D = self.D.unsqueeze(1).expand(-1, self.head_dim)
+            # Compute B and C shapes once
+            b_c_inner_dim = B.shape[1] // n_groups
+            B = B.view(-1, n_groups, b_c_inner_dim)
+            C = C.view(-1, n_groups, b_c_inner_dim)
             hidden_states_reshaped = hidden_states.view(
-                -1, self.num_heads // self.tp_size, self.head_dim)
+                -1, num_heads_per_rank, self.head_dim)
 
             # - the hidden is reshaped into number of current batches
             # - in this case there is no more prefill, so the batches gen
@@ -535,7 +541,7 @@ class MambaMixer2(CustomOp):
                 state_batch_indices=mamba_cache_params.state_indices_tensor,
             )
             hidden_states = hidden_states.view(
-                -1, (self.num_heads // self.tp_size) * self.head_dim)
+                -1, num_heads_per_rank * self.head_dim)
 
         # # 4. gated MLP
         hidden_states = self.norm(hidden_states, gate)
