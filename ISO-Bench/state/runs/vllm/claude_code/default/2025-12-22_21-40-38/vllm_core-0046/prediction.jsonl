{"instance_id": "vllm-project__vllm-388596c", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a75..c931b6933 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -258,7 +258,7 @@ def scaled_fp8_quant(\n     else:\n         output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e3d..7245fb3d7 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -302,13 +302,10 @@ def all_close_1d(x: torch.Tensor) -> bool:\n def per_tensor_quantize(tensor: torch.Tensor,\n                         inv_scale: Union[float, torch.Tensor]) -> torch.Tensor:\n     finfo = torch.finfo(torch.float8_e4m3fn)\n-    qweight = (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max)\n-    return qweight.to(torch.float8_e4m3fn)\n+    return (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max).to(torch.float8_e4m3fn)\n \n \n def per_tensor_dequantize(\n         tensor: torch.Tensor, inv_scale: Union[float,\n                                                torch.Tensor]) -> torch.Tensor:\n-    fake_qweight = tensor.to(torch.float16)\n-    dq_weight = fake_qweight * inv_scale\n-    return dq_weight\n+    return tensor.to(torch.float16) * inv_scale\n", "model_name_or_path": "gpt-5-2025-08-07"}
