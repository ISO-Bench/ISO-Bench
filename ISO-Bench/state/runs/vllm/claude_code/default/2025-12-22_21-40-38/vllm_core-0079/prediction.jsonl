{"instance_id": "vllm-project__vllm-c34eeec", "model_patch": "diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b55854de..49a7fc1b7 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,14 +532,14 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+\n+                # Prepend zeros using pad instead of cat to avoid extra allocation\n+                cu_seq_lens = torch.nn.functional.pad(\n+                    _chunk_cu_seq_lens, (1, 0), value=0)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=cu_seq_lens,\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -975,9 +975,8 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n             )\n \n         # slice by `:v.shape[-1]` in order to remove v headdim padding\n-        output = output\\\n-            .view(-1, self.num_heads, q.shape[-1])[..., :v.shape[-1]]\\\n-                .reshape(-1, self.num_heads * v.shape[-1])\n+        # Optimized: direct slice and reshape without intermediate view\n+        output = output[:, :, :v.shape[-1]].reshape(-1, self.num_heads * v.shape[-1])\n \n         return self.o_proj(output)[0]\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
