{"instance_id": "vllm-project__vllm-88faa46", "model_patch": "diff --git a/benchmarks/kernels/benchmark_reshape_and_cache_flash.py b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\nnew file mode 100644\nindex 000000000..724ddd367\n--- /dev/null\n+++ b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\n@@ -0,0 +1,246 @@\n+\"\"\"Benchmark script for reshape_and_cache_flash kernel performance.\"\"\"\n+\n+import argparse\n+import random\n+import time\n+from typing import Optional\n+\n+import torch\n+\n+from vllm import _custom_ops as ops\n+\n+\n+def benchmark_reshape_and_cache_flash(\n+    num_tokens: int,\n+    num_heads: int,\n+    head_size: int,\n+    block_size: int,\n+    num_blocks: int,\n+    dtype: torch.dtype,\n+    kv_cache_dtype: str = \"auto\",\n+    device: str = \"cuda\",\n+    num_iters: int = 100,\n+) -> float:\n+    \"\"\"Benchmark the reshape_and_cache_flash kernel.\n+\n+    Args:\n+        num_tokens: Number of tokens to cache\n+        num_heads: Number of attention heads\n+        head_size: Size of each attention head\n+        block_size: Size of cache blocks\n+        num_blocks: Number of cache blocks\n+        dtype: Data type for tensors\n+        kv_cache_dtype: KV cache dtype (\"auto\" or \"fp8\")\n+        device: Device to run on\n+        num_iters: Number of iterations for benchmarking\n+\n+    Returns:\n+        Average time in milliseconds\n+    \"\"\"\n+    torch.manual_seed(0)\n+    random.seed(0)\n+\n+    # Create slot mapping\n+    num_slots = block_size * num_blocks\n+    slot_mapping_lst = random.sample(range(num_slots), num_tokens)\n+    slot_mapping = torch.tensor(slot_mapping_lst,\n+                                dtype=torch.long,\n+                                device=device)\n+\n+    # Create input data - use empty+fill instead of randn for faster setup\n+    qkv = torch.empty(num_tokens, 3, num_heads, head_size,\n+                     dtype=dtype, device=device)\n+    qkv.normal_()\n+    _, key, value = qkv.unbind(dim=1)\n+\n+    # Create KV caches - use empty instead of zeros\n+    key_cache = torch.empty(num_blocks,\n+                           block_size,\n+                           num_heads,\n+                           head_size,\n+                           dtype=dtype,\n+                           device=device)\n+    value_cache = torch.empty(num_blocks,\n+                             block_size,\n+                             num_heads,\n+                             head_size,\n+                             dtype=dtype,\n+                             device=device)\n+\n+    # Get scales\n+    k_scale = (key.amax() / 64.0).to(torch.float32)\n+    v_scale = (value.amax() / 64.0).to(torch.float32)\n+\n+    # Warmup\n+    for _ in range(10):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                   slot_mapping, kv_cache_dtype, k_scale,\n+                                   v_scale)\n+\n+    if device.startswith(\"cuda\"):\n+        torch.cuda.synchronize()\n+\n+    # Benchmark\n+    start = time.perf_counter()\n+    for _ in range(num_iters):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                   slot_mapping, kv_cache_dtype, k_scale,\n+                                   v_scale)\n+\n+    if device.startswith(\"cuda\"):\n+        torch.cuda.synchronize()\n+\n+    end = time.perf_counter()\n+    avg_time_ms = (end - start) * 1000 / num_iters\n+\n+    return avg_time_ms\n+\n+\n+def benchmark_cache_allocation(num_blocks: int,\n+                               block_size: int,\n+                               entry_size: int,\n+                               dtype: torch.dtype,\n+                               device: str = \"cuda\",\n+                               num_iters: int = 100) -> tuple[float, float]:\n+    \"\"\"Benchmark cache allocation with zeros vs empty.\n+\n+    Returns:\n+        Tuple of (zeros_time_ms, empty_time_ms)\n+    \"\"\"\n+    # Benchmark torch.zeros\n+    if device.startswith(\"cuda\"):\n+        torch.cuda.synchronize()\n+\n+    start = time.perf_counter()\n+    for _ in range(num_iters):\n+        cache = torch.zeros(num_blocks,\n+                          block_size,\n+                          entry_size,\n+                          dtype=dtype,\n+                          device=device)\n+        if device.startswith(\"cuda\"):\n+            torch.cuda.synchronize()\n+\n+    end = time.perf_counter()\n+    zeros_time_ms = (end - start) * 1000 / num_iters\n+\n+    # Benchmark torch.empty\n+    if device.startswith(\"cuda\"):\n+        torch.cuda.synchronize()\n+\n+    start = time.perf_counter()\n+    for _ in range(num_iters):\n+        cache = torch.empty(num_blocks,\n+                          block_size,\n+                          entry_size,\n+                          dtype=dtype,\n+                          device=device)\n+        if device.startswith(\"cuda\"):\n+            torch.cuda.synchronize()\n+\n+    end = time.perf_counter()\n+    empty_time_ms = (end - start) * 1000 / num_iters\n+\n+    speedup = zeros_time_ms / empty_time_ms if empty_time_ms > 0 else 0\n+    print(f\"  torch.zeros: {zeros_time_ms:.4f} ms\")\n+    print(f\"  torch.empty: {empty_time_ms:.4f} ms\")\n+    print(f\"  Speedup: {speedup:.2f}x\")\n+\n+    return zeros_time_ms, empty_time_ms\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(\n+        description=\"Benchmark reshape_and_cache_flash kernel\")\n+    parser.add_argument(\"--num-tokens\",\n+                       type=int,\n+                       default=42,\n+                       help=\"Number of tokens\")\n+    parser.add_argument(\"--num-heads\",\n+                       type=int,\n+                       default=8,\n+                       help=\"Number of heads\")\n+    parser.add_argument(\"--head-size\",\n+                       type=int,\n+                       default=128,\n+                       help=\"Head size\")\n+    parser.add_argument(\"--block-size\",\n+                       type=int,\n+                       default=16,\n+                       help=\"Block size\")\n+    parser.add_argument(\"--num-blocks\",\n+                       type=int,\n+                       default=1024,\n+                       help=\"Number of blocks\")\n+    parser.add_argument(\"--dtype\",\n+                       type=str,\n+                       default=\"float16\",\n+                       choices=[\"float16\", \"bfloat16\", \"float32\"],\n+                       help=\"Data type\")\n+    parser.add_argument(\"--kv-cache-dtype\",\n+                       type=str,\n+                       default=\"auto\",\n+                       choices=[\"auto\", \"fp8\"],\n+                       help=\"KV cache dtype\")\n+    parser.add_argument(\"--device\",\n+                       type=str,\n+                       default=\"cuda\",\n+                       help=\"Device\")\n+    parser.add_argument(\"--num-iters\",\n+                       type=int,\n+                       default=100,\n+                       help=\"Number of iterations\")\n+\n+    args = parser.parse_args()\n+\n+    dtype_map = {\n+        \"float16\": torch.float16,\n+        \"bfloat16\": torch.bfloat16,\n+        \"float32\": torch.float32,\n+    }\n+    dtype = dtype_map[args.dtype]\n+\n+    print(\"=\" * 80)\n+    print(\"Reshape and Cache Flash Kernel Benchmark\")\n+    print(\"=\" * 80)\n+    print(f\"Configuration:\")\n+    print(f\"  num_tokens: {args.num_tokens}\")\n+    print(f\"  num_heads: {args.num_heads}\")\n+    print(f\"  head_size: {args.head_size}\")\n+    print(f\"  block_size: {args.block_size}\")\n+    print(f\"  num_blocks: {args.num_blocks}\")\n+    print(f\"  dtype: {args.dtype}\")\n+    print(f\"  kv_cache_dtype: {args.kv_cache_dtype}\")\n+    print(f\"  device: {args.device}\")\n+    print()\n+\n+    # Test reshape_and_cache_flash kernel\n+    print(\"1. Benchmarking reshape_and_cache_flash kernel:\")\n+    avg_time = benchmark_reshape_and_cache_flash(\n+        args.num_tokens,\n+        args.num_heads,\n+        args.head_size,\n+        args.block_size,\n+        args.num_blocks,\n+        dtype,\n+        args.kv_cache_dtype,\n+        args.device,\n+        args.num_iters,\n+    )\n+    print(f\"  Average time: {avg_time:.4f} ms\")\n+    print()\n+\n+    # Test cache allocation\n+    print(\"2. Benchmarking cache allocation (zeros vs empty):\")\n+    entry_size = args.num_heads * args.head_size\n+    benchmark_cache_allocation(args.num_blocks, args.block_size, entry_size,\n+                              dtype, args.device, args.num_iters)\n+    print()\n+\n+    print(\"=\" * 80)\n+    print(\"Benchmark Complete\")\n+    print(\"=\" * 80)\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 88559c8fe..dbc9ab342 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -133,8 +133,8 @@ void copy_blocks(std::vector<torch::Tensor> const& key_caches,\n \n   // Create data structures for the kernel.\n   // Create an array of pointers to the key and value caches.\n-  int64_t key_cache_ptrs[num_layers];\n-  int64_t value_cache_ptrs[num_layers];\n+  std::vector<int64_t> key_cache_ptrs(num_layers);\n+  std::vector<int64_t> value_cache_ptrs(num_layers);\n   for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {\n     key_cache_ptrs[layer_idx] =\n         reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());\n@@ -148,10 +148,10 @@ void copy_blocks(std::vector<torch::Tensor> const& key_caches,\n   // Move the data structures to the GPU.\n   // NOTE: This synchronizes the CPU and GPU.\n   torch::Tensor key_cache_ptrs_tensor =\n-      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)\n+      torch::from_blob(key_cache_ptrs.data(), {num_layers}, torch::kInt64)\n           .to(cache_device);\n   torch::Tensor value_cache_ptrs_tensor =\n-      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)\n+      torch::from_blob(value_cache_ptrs.data(), {num_layers}, torch::kInt64)\n           .to(cache_device);\n \n   // Launch the kernel.\ndiff --git a/tests/kernels/attention/test_cache.py b/tests/kernels/attention/test_cache.py\nindex 789507615..b6b1cd819 100644\n--- a/tests/kernels/attention/test_cache.py\n+++ b/tests/kernels/attention/test_cache.py\n@@ -474,7 +474,7 @@ def _create_mla_cache(\n     device: str,\n ) -> torch.Tensor:\n     cache_dtype = torch.uint8 if kv_cache_dtype == \"fp8\" else dtype\n-    return torch.zeros(num_blocks,\n+    return torch.empty(num_blocks,\n                        block_size,\n                        entry_size,\n                        dtype=cache_dtype,\n@@ -486,7 +486,7 @@ def _fill_mla_cache(cache: torch.Tensor, kv_cache_dtype: str):\n \n     vals = torch.randn(*cache.shape, device=cache.device, dtype=rand_dtype)\n     if kv_cache_dtype == \"fp8\":\n-        temp = torch.zeros_like(cache)\n+        temp = torch.empty_like(cache)\n         ops.convert_fp8(temp, vals, 1.0, kv_dtype=kv_cache_dtype)\n         vals = temp\n     cache.copy_(vals)\n@@ -532,7 +532,8 @@ def test_concat_and_cache_mla(\n     scale = torch.tensor(0.1, dtype=torch.float32, device=device)\n     kv_cache = _create_mla_cache(num_blocks, block_size, entry_size, dtype,\n                                  kv_cache_dtype, device)\n-    ref_temp = torch.zeros(*kv_cache.shape, dtype=dtype, device=device)\n+    ref_temp = torch.empty(*kv_cache.shape, dtype=dtype, device=device)\n+    ref_temp.zero_()\n \n     for i in range(num_tokens):\n         slot = slot_mapping[i].item()\n@@ -742,7 +743,7 @@ def test_gather_cache_mla(kv_lora_rank, qk_rope_head_dim, block_size,\n         perm = torch.randperm(num_blocks, device=device)\n         block_table[b, :] = perm\n \n-    dst = torch.zeros((total_tokens, entry_size),\n+    dst = torch.empty((total_tokens, entry_size),\n                       dtype=src_cache.dtype,\n                       device=device)\n \n@@ -814,7 +815,8 @@ def test_concat_and_cache_mla_cpu(\n     scale = torch.tensor(0.1, dtype=torch.float32, device=device)\n     kv_cache = _create_mla_cache(num_blocks, block_size, entry_size, dtype,\n                                  kv_cache_dtype, device)\n-    ref_temp = torch.zeros(*kv_cache.shape, dtype=dtype, device=device)\n+    ref_temp = torch.empty(*kv_cache.shape, dtype=dtype, device=device)\n+    ref_temp.zero_()\n \n     for i in range(num_tokens):\n         slot = slot_mapping[i].item()\n", "model_name_or_path": "gpt-5-2025-08-07"}
