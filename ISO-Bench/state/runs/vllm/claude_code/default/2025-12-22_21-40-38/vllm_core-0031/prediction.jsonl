{"instance_id": "vllm-project__vllm-7d94577", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 3aae183df..ed6a45a05 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -202,10 +202,10 @@ def moe_align_block_size(\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     if pad_sorted_ids:\n         max_num_tokens_padded = round_up(max_num_tokens_padded, block_size)\n-    sorted_ids = torch.empty((max_num_tokens_padded, ),\n-                             dtype=torch.int32,\n-                             device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n+    sorted_ids = torch.full((max_num_tokens_padded, ),\n+                            topk_ids.numel(),\n+                            dtype=torch.int32,\n+                            device=topk_ids.device)\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     # Expert ids must be zeroed out to prevent index out of bounds error while\n     # mapping global expert ids to local expert ids in expert parallelism.\n", "model_name_or_path": "gpt-5-2025-08-07"}
