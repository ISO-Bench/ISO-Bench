{"instance_id": "vllm-project__vllm-0df4d9b", "model_patch": "diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex d073dd6d2..b73b33601 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -22,8 +22,13 @@ __global__ void rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  // Compute base offset once to reduce redundant address calculations\n+  const int base_offset = blockIdx.x * hidden_size;\n+  const scalar_t* __restrict__ input_base = input + base_offset;\n+  scalar_t* __restrict__ out_base = out + base_offset;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input_base[idx];\n     variance += x * x;\n   }\n \n@@ -36,10 +41,12 @@ __global__ void rms_norm_kernel(\n   }\n   __syncthreads();\n \n+  // Cache s_variance in register to avoid shared memory access in loop\n+  const float norm_factor = s_variance;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        ((scalar_t)(x * s_variance)) * weight[idx];\n+    float x = (float)input_base[idx];\n+    out_base[idx] = ((scalar_t)(x * norm_factor)) * weight[idx];\n   }\n }\n \n@@ -110,12 +117,17 @@ fused_add_rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  // Compute base offset once to reduce redundant address calculations\n+  const int base_offset = blockIdx.x * hidden_size;\n+  scalar_t* __restrict__ input_base = input + base_offset;\n+  scalar_t* __restrict__ residual_base = residual + base_offset;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n-    z += residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input_base[idx];\n+    z += residual_base[idx];\n     float x = (float)z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = z;\n+    residual_base[idx] = z;\n   }\n \n   using BlockReduce = cub::BlockReduce<float, 1024>;\n@@ -127,10 +139,12 @@ fused_add_rms_norm_kernel(\n   }\n   __syncthreads();\n \n+  // Cache s_variance in register to avoid shared memory access in loop\n+  const float norm_factor = s_variance;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)residual[blockIdx.x * hidden_size + idx];\n-    input[blockIdx.x * hidden_size + idx] =\n-        ((scalar_t)(x * s_variance)) * weight[idx];\n+    float x = (float)residual_base[idx];\n+    input_base[idx] = ((scalar_t)(x * norm_factor)) * weight[idx];\n   }\n }\n \ndiff --git a/csrc/layernorm_quant_kernels.cu b/csrc/layernorm_quant_kernels.cu\nindex d595b9e88..8ff3c1d7d 100644\n--- a/csrc/layernorm_quant_kernels.cu\n+++ b/csrc/layernorm_quant_kernels.cu\n@@ -31,8 +31,13 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  // Compute base offset once to reduce redundant address calculations\n+  const int base_offset = blockIdx.x * hidden_size;\n+  const scalar_t* __restrict__ input_base = input + base_offset;\n+  fp8_type* __restrict__ out_base = out + base_offset;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input_base[idx];\n     variance += x * x;\n   }\n \n@@ -45,14 +50,14 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   }\n   __syncthreads();\n \n-  // invert scale to avoid division\n-  float const scale_inv = 1.0f / *scale;\n+  // Cache shared memory values in registers and invert scale to avoid division\n+  const float norm_factor = s_variance;\n+  const float scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n-    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n+    float x = (float)input_base[idx];\n+    float const out_norm = ((scalar_t)(x * norm_factor)) * weight[idx];\n+    out_base[idx] = scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n   }\n }\n \n@@ -134,12 +139,18 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  // Compute base offset once to reduce redundant address calculations\n+  const int base_offset = blockIdx.x * hidden_size;\n+  scalar_t* __restrict__ input_base = input + base_offset;\n+  scalar_t* __restrict__ residual_base = residual + base_offset;\n+  fp8_type* __restrict__ out_base = out + base_offset;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n-    z += residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input_base[idx];\n+    z += residual_base[idx];\n     float x = (float)z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = z;\n+    residual_base[idx] = z;\n   }\n \n   using BlockReduce = cub::BlockReduce<float, 1024>;\n@@ -151,14 +162,14 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   }\n   __syncthreads();\n \n-  // invert scale to avoid division\n-  float const scale_inv = 1.0f / *scale;\n+  // Cache shared memory values in registers and invert scale to avoid division\n+  const float norm_factor = s_variance;\n+  const float scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)residual[blockIdx.x * hidden_size + idx];\n-    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n+    float x = (float)residual_base[idx];\n+    float const out_norm = ((scalar_t)(x * norm_factor)) * weight[idx];\n+    out_base[idx] = scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n   }\n }\n \ndiff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu\nindex f3f9f669e..ddb7355cd 100644\n--- a/csrc/quantization/fp8/common.cu\n+++ b/csrc/quantization/fp8/common.cu\n@@ -16,13 +16,15 @@ __global__ void scaled_fp8_quant_kernel(fp8_type* __restrict__ out,\n                                         const scalar_t* __restrict__ input,\n                                         const float* __restrict__ scale,\n                                         int64_t num_elems) {\n-  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n+  // Compute thread ID once to reduce instruction overhead\n+  const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n+  const int stride = blockDim.x * gridDim.x;\n \n   // Invert the scale so that we can use multiplications to avoid expensive\n   // division.\n   const float inverted_scale = 1.0f / (*scale);\n   scaled_fp8_conversion_vec<scalar_t, true>(\n-      out, input, inverted_scale, num_elems, tid, blockDim.x * gridDim.x);\n+      out, input, inverted_scale, num_elems, tid, stride);\n }\n \n template <typename scalar_t, typename fp8_type>\n@@ -30,23 +32,24 @@ __global__ void dynamic_per_token_scaled_fp8_quant_kernel(\n     fp8_type* __restrict__ out, float* __restrict__ scale,\n     scalar_t const* __restrict__ input, float const* __restrict__ scale_ub,\n     const int hidden_size) {\n-  int const tid = threadIdx.x;\n-  int const token_idx = blockIdx.x;\n+  const int tid = threadIdx.x;\n+  const int token_idx = blockIdx.x;\n+  const int stride = blockDim.x;\n \n   // Use int64 to avoid overflowing an int32 when calculating this offset\n-  int64_t offset = static_cast<int64_t>(token_idx) * hidden_size;\n+  const int64_t offset = static_cast<int64_t>(token_idx) * hidden_size;\n   scalar_t const* __restrict__ token_input = &input[offset];\n   fp8_type* __restrict__ token_output = &out[offset];\n \n   // For vectorization, token_input and token_output pointers need to be\n   // aligned at 32-byte and 16-byte addresses respectively.\n-  bool const can_vectorize = hidden_size % 16 == 0;\n+  const bool can_vectorize = hidden_size % 16 == 0;\n \n   float absmax_val = 0.0f;\n   if (can_vectorize) {\n-    absmax_val = thread_max_vec(token_input, hidden_size, tid, blockDim.x);\n+    absmax_val = thread_max_vec(token_input, hidden_size, tid, stride);\n   } else {\n-    for (int i = tid; i < hidden_size; i += blockDim.x) {\n+    for (int i = tid; i < hidden_size; i += stride) {\n       float const x = static_cast<float>(token_input[i]);\n       absmax_val = fmaxf(absmax_val, fabsf(x));\n     }\n@@ -58,26 +61,26 @@ __global__ void dynamic_per_token_scaled_fp8_quant_kernel(\n       BlockReduce(reduceStorage).Reduce(absmax_val, cub::Max{}, blockDim.x);\n   __shared__ float token_scale;\n   if (tid == 0) {\n-    if (scale_ub) {\n-      token_scale = fminf(block_absmax_val_maybe, *scale_ub);\n-    } else {\n-      token_scale = block_absmax_val_maybe;\n-    }\n+    float max_val = scale_ub ? fminf(block_absmax_val_maybe, *scale_ub)\n+                             : block_absmax_val_maybe;\n     // token scale computation\n-    token_scale = fmaxf(token_scale / quant_type_max_v<fp8_type>,\n+    token_scale = fmaxf(max_val / quant_type_max_v<fp8_type>,\n                         min_scaling_factor<fp8_type>::val());\n     scale[token_idx] = token_scale;\n   }\n   __syncthreads();\n \n+  // Cache shared memory value in register\n+  const float scale_factor = token_scale;\n+\n   // Note that we don't use inverted scales so we can match FBGemm impl.\n   if (can_vectorize) {\n     scaled_fp8_conversion_vec<scalar_t, false>(\n-        token_output, token_input, token_scale, hidden_size, tid, blockDim.x);\n+        token_output, token_input, scale_factor, hidden_size, tid, stride);\n   } else {\n-    for (int i = tid; i < hidden_size; i += blockDim.x) {\n+    for (int i = tid; i < hidden_size; i += stride) {\n       token_output[i] = scaled_fp8_conversion<false, fp8_type>(\n-          static_cast<float>(token_input[i]), token_scale);\n+          static_cast<float>(token_input[i]), scale_factor);\n     }\n   }\n }\n", "model_name_or_path": "gpt-5-2025-08-07"}
