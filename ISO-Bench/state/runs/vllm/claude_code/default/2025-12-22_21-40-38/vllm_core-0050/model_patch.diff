diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 5f8535eaa..b572d70fb 100644
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -185,6 +185,7 @@ def unified_v1_flash_attention(
     num_actual_tokens = attn_metadata.num_actual_tokens
 
     # Reshape the query, key, and value tensors.
+    # Optimization: Use view for zero-copy reshape of contiguous tensors
     query = query.view(-1, num_heads, head_size)
     key = key.view(-1, num_kv_heads, head_size)
     value = value.view(-1, num_kv_heads, head_size)
@@ -218,9 +219,9 @@ def unified_v1_flash_attention(
         block_table=attn_metadata.block_table,
         softcap=logits_soft_cap,
     )
-    attn_output = attn_output.view(num_actual_tokens, -1)
-    # TODO(woosuk): Optimize this.
-    output[:num_actual_tokens].copy_(attn_output)
+    # Optimization: Directly assign reshaped output, avoiding intermediate view+copy
+    # Reshape the output tensor and write it directly to the output buffer
+    output[:num_actual_tokens] = attn_output.view(num_actual_tokens, -1)
 
 
 def unified_v1_flash_attention_fake(
