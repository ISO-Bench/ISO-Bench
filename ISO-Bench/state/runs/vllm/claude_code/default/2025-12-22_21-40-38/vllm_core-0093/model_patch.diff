diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index 2d8594cb8..a8d89afc6 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -69,10 +69,12 @@ class TokenizerDataCache:
             # Vendored from xgrammar logic since we cannot pickle the tokenizer
             # https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98 # noqa: E501
             try:
-                encoded_vocab = [
-                    token for token, _ in sorted(tokenizer.get_vocab().items(),
-                                                 key=lambda x: x[1])
-                ]
+                vocab_dict = tokenizer.get_vocab()
+                # Pre-allocate list with correct size for better performance
+                vocab_size = len(vocab_dict)
+                encoded_vocab = [None] * vocab_size
+                for token, idx in vocab_dict.items():
+                    encoded_vocab[idx] = token
             except AttributeError as e:
                 raise ValueError(
                     f"Cannot get the vocabulary of the tokenizer "
@@ -240,6 +242,8 @@ class XGrammarLogitsProcessor:
     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)
     batch_size: int = field(default=1)
     prefilled: bool = field(default=False)
+    _token_bitmask_cache: torch.Tensor = None  # type: ignore[assignment]
+    _cached_device: torch.device | None = None
 
     def __getstate__(self) -> dict[str, Any]:
         return {'config': self.config}
@@ -252,6 +256,8 @@ class XGrammarLogitsProcessor:
         self.batch_size = 1
         self.token_bitmask = None  # type: ignore[assignment]
         self.prefilled = False
+        self._token_bitmask_cache = None  # type: ignore[assignment]
+        self._cached_device = None
 
     def _ensure_ctx(self):
         """Lazily initialize the processor in the worker process"""
@@ -283,9 +289,9 @@ class XGrammarLogitsProcessor:
             # Have not sampled a token yet
             self.prefilled = True
         else:
+            sampled_token = input_ids[-1]
             for i, matcher in enumerate(self.matchers):
                 if not matcher.is_terminated():
-                    sampled_token = input_ids[-1]
                     assert self.matchers[i].accept_token(sampled_token)
 
         for i, matcher in enumerate(self.matchers):
@@ -297,20 +303,27 @@ class XGrammarLogitsProcessor:
 
         # token_bitmask is a CPU tensor for use with accept_token and
         # fill_next_token_bitmask so we move it to the device of scores
-        device_type = scores.device.type
+        scores_device = scores.device
+        device_type = scores_device.type
         dtype = scores.dtype
+
+        # Cache token_bitmask on target device to avoid repeated transfers
+        if self._cached_device != scores_device:
+            self._token_bitmask_cache = self.token_bitmask.to(scores_device)
+            self._cached_device = scores_device
+
         if device_type != "cuda":
             # xgrammar on cpu only supports float32 scores
             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22
-            scores = scores.to("cpu").float().unsqueeze(0)
+            scores = scores.to(device="cpu", dtype=torch.float32).unsqueeze(0)
 
         # Note: In this method, if the tensors have different dimensions
         # on CPU device fails, but on GPU it runs without error. Hence the
         # unsqueeze above for scores, to match the token bitmask shape
-        xgr.apply_token_bitmask_inplace(scores,
-                                        self.token_bitmask.to(scores.device))
+        xgr.apply_token_bitmask_inplace(scores, self._token_bitmask_cache)
+
         if device_type != "cuda":
-            scores = scores.to(dtype).to(device_type).squeeze()
+            scores = scores.to(device=device_type, dtype=dtype).squeeze()
 
         return scores
 
