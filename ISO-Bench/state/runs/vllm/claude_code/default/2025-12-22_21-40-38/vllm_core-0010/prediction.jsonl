{"instance_id": "vllm-project__vllm-36fb68f", "model_patch": "diff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py\nindex 046f11d95..212ec01b7 100644\n--- a/tests/kernels/test_moe.py\n+++ b/tests/kernels/test_moe.py\n@@ -15,7 +15,7 @@ from vllm.model_executor.models.mixtral import MixtralMoE\n def torch_moe(a, w1, w2, score, topk):\n     B, D = a.shape\n     a = a.view(B, -1, D).repeat(1, topk, 1).reshape(-1, D)\n-    out = torch.zeros(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n+    out = torch.empty(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n     score = torch.softmax(score, dim=-1, dtype=torch.float32)\n     topk_weight, topk_ids = torch.topk(score, topk)\n     topk_weight = topk_weight.view(-1)\ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 9ff9ba298..6743bc18d 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -112,20 +112,20 @@ class MixtralMoE(nn.Module):\n \n         # Scaling factors for FP8 weights\n         self.ws_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n+            torch.empty(self.num_total_experts, dtype=torch.float32),\n             requires_grad=False) if self.use_fp8 else None\n         self.w2s_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n+            torch.empty(self.num_total_experts, dtype=torch.float32),\n             requires_grad=False) if self.use_fp8 else None\n \n         # Scaling factors for FP8 activations\n         need_act_scales = (self.use_fp8\n                            and quant_config.activation_scheme == \"static\")\n         self.as_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n+            torch.empty(1, dtype=torch.float32),\n             requires_grad=False) if need_act_scales else None\n         self.a2s_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n+            torch.empty(1, dtype=torch.float32),\n             requires_grad=False) if need_act_scales else None\n \n         if need_act_scales:\n", "model_name_or_path": "gpt-5-2025-08-07"}
