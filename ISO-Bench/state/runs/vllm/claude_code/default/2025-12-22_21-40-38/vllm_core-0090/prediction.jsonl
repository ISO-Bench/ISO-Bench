{"instance_id": "vllm-project__vllm-8936316", "model_patch": "diff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex 4bf5cbbd1..83a42bfab 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -165,9 +165,9 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n             for worker in self.workers\n         ]\n-        ip_counts: Dict[str, int] = {}\n+        ip_counts: Dict[str, int] = defaultdict(int)\n         for ip in worker_ips:\n-            ip_counts[ip] = ip_counts.get(ip, 0) + 1\n+            ip_counts[ip] += 1\n \n         def sort_by_driver_then_worker_ip(worker):\n             \"\"\"\n@@ -226,14 +226,18 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 \" each node.\")\n \n         # Set environment variables for the driver and workers.\n+        # Pre-compute attention backend dict for performance\n+        attention_backend_dict = ({\n+            \"VLLM_ATTENTION_BACKEND\": envs.VLLM_ATTENTION_BACKEND\n+        } if envs.VLLM_ATTENTION_BACKEND is not None else {})\n+        trace_function_str = str(envs.VLLM_TRACE_FUNCTION)\n+\n         all_args_to_update_environment_variables = [({\n             \"CUDA_VISIBLE_DEVICES\":\n             \",\".join(map(str, node_gpus[node_id])),\n             \"VLLM_TRACE_FUNCTION\":\n-            str(envs.VLLM_TRACE_FUNCTION),\n-            **({\n-                \"VLLM_ATTENTION_BACKEND\": envs.VLLM_ATTENTION_BACKEND\n-            } if envs.VLLM_ATTENTION_BACKEND is not None else {})\n+            trace_function_str,\n+            **attention_backend_dict\n         }, ) for (node_id, _) in worker_node_and_gpu_ids]\n \n         self._env_vars_for_all_workers = (\n@@ -271,16 +275,15 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                           max_parallel_loading_workers)\n \n         if self.use_ray_spmd_worker:\n-            for pp_rank in range(self.parallel_config.pipeline_parallel_size):\n-                self.pp_tp_workers.append([])\n-                for tp_rank in range(\n-                        self.parallel_config.tensor_parallel_size):\n+            # Pre-allocate pp_tp_workers structure for better performance\n+            pp_size = self.parallel_config.pipeline_parallel_size\n+            tp_size = self.parallel_config.tensor_parallel_size\n+            self.pp_tp_workers = [[] for _ in range(pp_size)]\n+            for pp_rank in range(pp_size):\n+                for tp_rank in range(tp_size):\n                     # PP=2, TP=4\n                     # pp_tp_workers = [[0, 1, 2, 3], [4, 5, 6, 7]]\n-                    rank = (pp_rank * self.parallel_config.tensor_parallel_size\n-                            ) + tp_rank\n-                    assert len(self.pp_tp_workers[pp_rank]) == tp_rank\n-                    assert pp_rank < len(self.pp_tp_workers)\n+                    rank = (pp_rank * tp_size) + tp_rank\n                     self.pp_tp_workers[pp_rank].append(self.workers[rank])\n \n         # This is the list of workers that are rank 0 of each TP group EXCEPT\n@@ -293,10 +296,12 @@ class RayGPUExecutor(DistributedGPUExecutor):\n         self.non_driver_workers: List[RayWorkerWrapper] = []\n \n         # Enforce rank order for correct rank to return final output.\n+        # Pre-allocate lists for better performance\n+        tp_size = self.parallel_config.tensor_parallel_size\n         for index, worker in enumerate(self.workers):\n             # The driver worker is rank 0 and not in self.workers.\n             rank = index + 1\n-            if rank % self.parallel_config.tensor_parallel_size == 0:\n+            if rank % tp_size == 0:\n                 self.tp_driver_workers.append(worker)\n             else:\n                 self.non_driver_workers.append(worker)\n@@ -359,9 +364,13 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             raise NotImplementedError(\n                 \"max_concurrent_workers is not supported yet.\")\n \n-        count = len(self.workers) if not \\\n-            async_run_tensor_parallel_workers_only \\\n-            else len(self.non_driver_workers)\n+        # Optimize by computing worker list selection once\n+        if async_run_tensor_parallel_workers_only:\n+            target_workers = self.non_driver_workers\n+        else:\n+            target_workers = self.workers\n+        count = len(target_workers)\n+\n         # If using SPMD worker, all workers are the same, so we should execute\n         # the args on all workers. Otherwise, we skip the first worker's args\n         # because those args will go to the driver worker.\n@@ -371,14 +380,11 @@ class RayGPUExecutor(DistributedGPUExecutor):\n         all_worker_kwargs = repeat(kwargs, count) if all_kwargs is None \\\n             else islice(all_kwargs, first_worker_args_index, None)\n \n-        # Start the ray workers first.\n-        ray_workers = self.workers\n-        if async_run_tensor_parallel_workers_only:\n-            ray_workers = self.non_driver_workers\n+        # Start the ray workers first - use pre-computed target_workers\n         ray_worker_outputs = [\n             worker.execute_method.remote(method, *worker_args, **worker_kwargs)\n             for (worker, worker_args, worker_kwargs\n-                 ) in zip(ray_workers, all_worker_args, all_worker_kwargs)\n+                 ) in zip(target_workers, all_worker_args, all_worker_kwargs)\n         ]\n \n         if async_run_tensor_parallel_workers_only:\n@@ -527,10 +533,9 @@ class RayGPUExecutorAsync(RayGPUExecutor, DistributedGPUExecutorAsync):\n             # engines can't execute on the same stage at the same time\n             # We create the locks here to avoid creating them in the constructor\n             # which uses a different asyncio loop.\n-            self.pp_locks = [\n-                asyncio.Lock()\n-                for _ in range(self.parallel_config.pipeline_parallel_size)\n-            ]\n+            # Pre-allocate lock list for better performance\n+            pp_size = self.parallel_config.pipeline_parallel_size\n+            self.pp_locks = [asyncio.Lock() for _ in range(pp_size)]\n \n         tasks = [\n             asyncio.create_task(\n", "model_name_or_path": "gpt-5-2025-08-07"}
