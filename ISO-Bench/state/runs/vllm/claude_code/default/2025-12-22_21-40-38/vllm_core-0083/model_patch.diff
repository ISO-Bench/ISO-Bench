diff --git a/vllm/model_executor/guided_decoding/outlines_logits_processors.py b/vllm/model_executor/guided_decoding/outlines_logits_processors.py
index 8ae7c7b6b..ce4ec3084 100644
--- a/vllm/model_executor/guided_decoding/outlines_logits_processors.py
+++ b/vllm/model_executor/guided_decoding/outlines_logits_processors.py
@@ -114,7 +114,7 @@ class BaseLogitsProcessor:
         # but scores.shape == torch.Size([128256])
         # Using NumPy is faster for filtering token ids
         allowed_tokens = np.array(allowed_tokens, dtype=np.int64)
-        allowed_tokens = torch.tensor(allowed_tokens, device=scores.device)
+        allowed_tokens = torch.from_numpy(allowed_tokens).to(device=scores.device)
         allowed_tokens = allowed_tokens.masked_select(
             allowed_tokens < scores.shape[-1])
         mask.index_fill_(0, allowed_tokens, 0)
diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index 8e40da4b3..3aa222658 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -393,7 +393,7 @@ class XGrammarLogitsProcessor:
         xgr.apply_token_bitmask_inplace(
             scores, self.token_bitmask.to(scores.device, non_blocking=True))
         if device_type != "cuda":
-            scores = scores.to(dtype).to(device_type).squeeze()
+            scores = scores.to(dtype=dtype, device=device_type).squeeze()
 
         return scores
 
diff --git a/vllm/sequence.py b/vllm/sequence.py
index f5f9c56a7..3a4a5f1e5 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -1316,7 +1316,7 @@ class HiddenStates(msgspec.Struct, array_like=True,
             # Adding dummy hidden_states to this to maintain same shape
             self.second_last_token_hidden_states = torch.cat([
                 self.second_last_token_hidden_states,
-                torch.zeros_like(hidden_states)
+                torch.empty_like(hidden_states)
                 if second_last_token_hidden_states is None else
                 second_last_token_hidden_states
             ])
