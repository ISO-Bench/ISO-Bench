{"instance_id": "vllm-project__vllm-4ce64e2", "model_patch": "diff --git a/vllm/model_executor/guided_decoding/outlines_logits_processors.py b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\nindex 8ae7c7b6b..ce4ec3084 100644\n--- a/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n+++ b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n@@ -114,7 +114,7 @@ class BaseLogitsProcessor:\n         # but scores.shape == torch.Size([128256])\n         # Using NumPy is faster for filtering token ids\n         allowed_tokens = np.array(allowed_tokens, dtype=np.int64)\n-        allowed_tokens = torch.tensor(allowed_tokens, device=scores.device)\n+        allowed_tokens = torch.from_numpy(allowed_tokens).to(device=scores.device)\n         allowed_tokens = allowed_tokens.masked_select(\n             allowed_tokens < scores.shape[-1])\n         mask.index_fill_(0, allowed_tokens, 0)\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 8e40da4b3..3aa222658 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -393,7 +393,7 @@ class XGrammarLogitsProcessor:\n         xgr.apply_token_bitmask_inplace(\n             scores, self.token_bitmask.to(scores.device, non_blocking=True))\n         if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n+            scores = scores.to(dtype=dtype, device=device_type).squeeze()\n \n         return scores\n \ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex f5f9c56a7..3a4a5f1e5 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1316,7 +1316,7 @@ class HiddenStates(msgspec.Struct, array_like=True,\n             # Adding dummy hidden_states to this to maintain same shape\n             self.second_last_token_hidden_states = torch.cat([\n                 self.second_last_token_hidden_states,\n-                torch.zeros_like(hidden_states)\n+                torch.empty_like(hidden_states)\n                 if second_last_token_hidden_states is None else\n                 second_last_token_hidden_states\n             ])\n", "model_name_or_path": "gpt-5-2025-08-07"}
