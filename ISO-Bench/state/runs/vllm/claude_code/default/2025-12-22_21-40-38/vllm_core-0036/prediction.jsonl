{"instance_id": "vllm-project__vllm-f168b85", "model_patch": "diff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..72a521b51 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -354,17 +354,17 @@ class NemotronHModel(nn.Module):\n             assert intermediate_tensors is not None\n             hidden_states = intermediate_tensors[\"hidden_states\"]\n             residual = intermediate_tensors[\"residual\"]\n+            residual = None\n \n-        residual = None\n-        num_non_mamba_layers = 0\n+        # Pre-compute mamba layer indices to avoid isinstance checks in loop\n+        mamba_layer_idx = 0\n         for i in range(len(self.layers)):\n             layer = self.layers[i]\n             layer_mamba_cache_params = None\n             if isinstance(layer, NemotronHMambaDecoderLayer):\n                 layer_mamba_cache_params = mamba_cache_params.at_layer_idx(\n-                    i - num_non_mamba_layers)\n-            else:\n-                num_non_mamba_layers += 1\n+                    mamba_layer_idx)\n+                mamba_layer_idx += 1\n \n             hidden_states, residual = layer(\n                 positions=positions,\n@@ -399,18 +399,18 @@ class NemotronHModel(nn.Module):\n             if \"A_log\" in name:\n                 name = name.replace(\"A_log\", \"A\")\n                 loaded_weight = loaded_weight.to(torch.float32)\n-\n-            if \"D\" in name:\n+            # Combine dtype conversions for efficiency\n+            elif any(key in name for key in [\"D\", \"dt_bias\"]):\n                 loaded_weight = loaded_weight.to(torch.float32)\n \n-            if \"dt_bias\" in name:\n-                loaded_weight = loaded_weight.to(torch.float32)\n+            # load attn params - optimize to avoid double iteration\n+            weight_name = None\n+            for proj in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n+                if proj in name:\n+                    weight_name = proj\n+                    break\n \n-            # load attn params\n-            if any(proj in name for proj in [\"q_proj\", \"k_proj\", \"v_proj\"]):\n-                weight_name = next(proj\n-                                   for proj in [\"q_proj\", \"k_proj\", \"v_proj\"]\n-                                   if proj in name)\n+            if weight_name:\n                 name = name.replace(weight_name, \"qkv_proj\")\n                 param = params_dict[name]\n                 weight_loader = param.weight_loader\n@@ -515,33 +515,34 @@ class NemotronHForCausalLM(nn.Module, HasInnerState, SupportsLoRA, SupportsPP,\n     def _get_mamba_cache_shape(\n             self) -> tuple[tuple[int, int], tuple[int, int]]:\n         world_size = get_tensor_model_parallel_world_size()\n-        hidden_size = self.config.hidden_size\n-\n-        conv_state_shape, temporal_state_shape = None, None\n+        config = self.config\n+        hidden_size = config.hidden_size\n+        ssm_state_size = config.ssm_state_size\n+        n_groups_base = config.n_groups\n \n-        intermediate_size = self.config.expand * hidden_size\n+        intermediate_size = config.expand * hidden_size\n \n         # if n_groups is not divisible by world_size, need to extend the shards\n         # to ensure all groups needed by a head is sharded along with it\n         n_groups = (\n-            self.config.n_groups +\n-            extra_groups_for_head_shards(self.config.n_groups, world_size))\n+            n_groups_base +\n+            extra_groups_for_head_shards(n_groups_base, world_size))\n \n         # - heads and n_groups are TP-ed\n         conv_dim = (intermediate_size +\n-                    2 * n_groups * self.config.ssm_state_size)\n+                    2 * n_groups * ssm_state_size)\n         conv_state_shape = (\n             divide(conv_dim, world_size),\n-            self.config.conv_kernel - 1,\n+            config.conv_kernel - 1,\n         )\n \n         # These are not TP-ed as they depend on A, dt_bias, D\n         # - they are typically small\n         #   e.g., (h_heads, d_head, d_state) = (128, 64, 128)\n         temporal_state_shape = (\n-            divide(self.config.mamba_num_heads, world_size),\n-            self.config.mamba_head_dim,\n-            self.config.ssm_state_size,\n+            divide(config.mamba_num_heads, world_size),\n+            config.mamba_head_dim,\n+            ssm_state_size,\n         )\n         return conv_state_shape, temporal_state_shape\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
