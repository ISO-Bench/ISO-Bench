{"instance_id": "vllm-project__vllm-acaea3b", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 5b19e3f35..688c1a519 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -47,7 +47,7 @@ class Mixer2RMSNormGated(CustomOp):\n         self.n_groups = full_hidden_size // self.group_size\n \n         self.variance_epsilon = eps\n-        self.weight = nn.Parameter(torch.ones(self.per_rank_hidden_size))\n+        self.weight = nn.Parameter(torch.empty(self.per_rank_hidden_size))\n         set_weight_attrs(self.weight,\n                          {\"weight_loader\": sharded_weight_loader(0)})\n         assert self.full_hidden_size % self.tp_size== 0,\\\n@@ -356,8 +356,8 @@ class MambaMixer2(CustomOp):\n                 divide(num_heads, self.tp_size),\n                 dtype=torch.float32,\n             ))\n-        self.D = nn.Parameter(torch.ones(num_heads // self.tp_size))\n-        self.dt_bias = nn.Parameter(torch.ones(num_heads // self.tp_size))\n+        self.D = nn.Parameter(torch.empty(num_heads // self.tp_size))\n+        self.dt_bias = nn.Parameter(torch.empty(num_heads // self.tp_size))\n \n         set_weight_attrs(self.D, {\"weight_loader\": sharded_weight_loader(0)})\n         a_weight_loader = composed_weight_loader(\n@@ -470,13 +470,10 @@ class MambaMixer2(CustomOp):\n             if has_initial_states is not None and torch.any(\n                     has_initial_states):\n \n-                # vectorized ssm_state zero init\n-                batched_zero_init_func = torch.vmap(\n-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n-                batched_zero_init_func(\n-                    mamba_cache_params.\n-                    state_indices_tensor[~has_initial_states].unsqueeze(\n-                        dim=-1), )\n+                # vectorized ssm_state zero init - optimized with direct indexing\n+                zero_indices = mamba_cache_params.state_indices_tensor[~has_initial_states]\n+                if zero_indices.numel() > 0:\n+                    mamba_cache_params.ssm_state[zero_indices] = 0\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -500,17 +497,9 @@ class MambaMixer2(CustomOp):\n                 dt_limit=(0.0, float(\"inf\")),\n             )\n \n-            # vectorized ssm state update using vmap\n-            # the 1d state_indices_tensor needs to be unsqueezed to avoid vmap\n-            # limitation which doesn't allow use of `item()`\n-            # Note: the lambda capture can happen where ssm_state is initialized\n-            #       instead of here\n-            batched_copy = torch.vmap(\n-                lambda idx, source_state: mamba_cache_params.ssm_state[\n-                    idx].copy_(source_state))\n-            batched_copy(\n-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n-                varlen_state)\n+            # vectorized ssm state update - optimized with direct indexing\n+            mamba_cache_params.ssm_state[\n+                mamba_cache_params.state_indices_tensor] = varlen_state\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n@@ -518,7 +507,7 @@ class MambaMixer2(CustomOp):\n \n             n_groups = self.n_groups // self.tp_size\n             A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n+                -1, self.head_dim, self.ssm_state_size)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n             D = self.D[:, None, ...].expand(-1, self.head_dim)\n", "model_name_or_path": "gpt-5-2025-08-07"}
