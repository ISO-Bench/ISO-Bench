diff --git a/vllm/utils.py b/vllm/utils.py
index 79787303a..0dd10183a 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -251,12 +251,12 @@ class LRUCache(Generic[_K, _V]):
 
     def get(self, key: _K, default: Optional[_V] = None) -> Optional[_V]:
         value: Optional[_V]
-        if key in self.cache:
+        # Use try-except to avoid double dictionary lookup
+        try:
             value = self.cache[key]
             self.cache.move_to_end(key)
-
             self._hits += 1
-        else:
+        except KeyError:
             value = default
 
         self._total += 1
@@ -327,15 +327,14 @@ class PyObjectCache:
         self._obj_builder = obj_builder
         self._index = 0
 
-        self._obj_cache = []
-        for _ in range(128):
-            self._obj_cache.append(self._obj_builder())
+        # Use list comprehension for better performance
+        self._obj_cache = [self._obj_builder() for _ in range(128)]
 
     def _grow_cache(self):
         # Double the size of the cache
         num_objs = len(self._obj_cache)
-        for _ in range(num_objs):
-            self._obj_cache.append(self._obj_builder())
+        # Use list extension with comprehension for better performance
+        self._obj_cache.extend([self._obj_builder() for _ in range(num_objs)])
 
     def get_object(self):
         """Returns a pre-allocated cached object. If there is not enough
@@ -818,7 +817,15 @@ def async_tensor_h2d(
     pin_memory: bool,
 ) -> torch.Tensor:
     """Asynchronously create a tensor and copy it from host to device."""
-    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device="cpu")
+    # Convert to numpy first for better performance with large lists
+    np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE.get(dtype)
+    if np_dtype is not None:
+        np_array = np.array(data, dtype=np_dtype)
+        t = torch.from_numpy(np_array)
+    else:
+        t = torch.tensor(data, dtype=dtype, device="cpu")
+    if pin_memory:
+        t = t.pin_memory()
     return t.to(device=target_device, non_blocking=True)
 
 
