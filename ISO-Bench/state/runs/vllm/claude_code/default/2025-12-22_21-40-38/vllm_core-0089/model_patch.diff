diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index 25d95ac6e..ad4de45c1 100644
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -58,10 +58,10 @@ class InputBatch:
         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)
 
         # Attention-related.
-        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),
+        self.block_table = torch.empty((max_num_reqs, max_num_blocks_per_req),
                                        device=self.device,
                                        dtype=torch.int32)
-        self.block_table_cpu_tensor = torch.zeros(
+        self.block_table_cpu_tensor = torch.empty(
             (max_num_reqs, max_num_blocks_per_req),
             device="cpu",
             dtype=torch.int32,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index e75be21ef..d8d2ac05d 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -106,13 +106,13 @@ class GPUModelRunner:
             reversed(self.vllm_config.compilation_config.capture_sizes))
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -627,7 +627,7 @@ class GPUModelRunner:
             num_blocks, self.block_size, self.num_kv_heads, self.head_size)
         for _ in range(self.num_attn_layers):
             self.kv_caches.append(
-                torch.zeros(kv_cache_shape,
+                torch.empty(kv_cache_shape,
                             dtype=self.kv_cache_dtype,
                             device=self.device))
 
