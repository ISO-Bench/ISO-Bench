{"instance_id": "vllm-project__vllm-7a7929a", "model_patch": "diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nindex 8b132e442..15949d97d 100644\n--- a/cacheflow/models/attention.py\n+++ b/cacheflow/models/attention.py\n@@ -37,7 +37,8 @@ class GPTCacheFlowAttention(nn.Module):\n         prefix_sum = [0]\n         for prompt_len in prompt_lens:\n             prefix_sum.append(prefix_sum[-1] + prompt_len)\n-        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n+        # Use from_numpy for potential performance improvement if available\n+        prefix_sum = torch.as_tensor(prefix_sum, dtype=torch.int, device=device)\n         max_prompt_len = max(prompt_lens)\n \n         # FIXME(woosuk): Unnecessary copy. Optimize this.\ndiff --git a/cacheflow/models/sample.py b/cacheflow/models/sample.py\nindex 87eeccb61..db624f704 100644\n--- a/cacheflow/models/sample.py\n+++ b/cacheflow/models/sample.py\n@@ -31,7 +31,7 @@ class Sampler(nn.Module):\n         temperatures = _get_temperatures(input_metadata)\n         assert len(temperatures) == logits.shape[0]\n         if any(t != 1.0 for t in temperatures):\n-            t = torch.tensor(\n+            t = torch.as_tensor(\n                 temperatures, dtype=logits.dtype, device=logits.device)\n             # Use in-place division to avoid creating a new tensor.\n             logits.div_(t.unsqueeze(dim=1))\n@@ -45,7 +45,7 @@ class Sampler(nn.Module):\n         top_ps = _get_top_ps(input_metadata)\n         assert len(top_ps) == probs.shape[0]\n         if any(p < 1.0 for p in top_ps):\n-            p = torch.tensor(top_ps, dtype=probs.dtype, device=probs.device)\n+            p = torch.as_tensor(top_ps, dtype=probs.dtype, device=probs.device)\n             probs = _apply_top_p(probs, p)\n \n         # Sample the next tokens.\n@@ -177,7 +177,7 @@ def _sample_from_generation_tokens(\n     if sampling_params.use_beam_search:\n         # Beam search.\n         # Add cumulative logprobs for the sequences in the group.\n-        seq_logprobs = torch.tensor(\n+        seq_logprobs = torch.as_tensor(\n             seq_logprobs, dtype=torch.float, device=logprobs.device)\n         logprobs = logprobs + seq_logprobs.unsqueeze(dim=1)\n \ndiff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py\nindex 978ca04e6..d21b44e3a 100644\n--- a/cacheflow/parallel_utils/tensor_parallel/layers.py\n+++ b/cacheflow/parallel_utils/tensor_parallel/layers.py\n@@ -198,8 +198,8 @@ class VocabParallelEmbedding(torch.nn.Module):\n             # Build the mask.\n             input_mask = (input_ < self.vocab_start_index) | \\\n                          (input_ >= self.vocab_end_index)\n-            # Mask the input.\n-            masked_input = input_.clone() - self.vocab_start_index\n+            # Mask the input - subtraction creates new tensor, no need for clone.\n+            masked_input = input_ - self.vocab_start_index\n             masked_input[input_mask] = 0\n         else:\n             masked_input = input_\ndiff --git a/cacheflow/worker/worker.py b/cacheflow/worker/worker.py\nindex f309cf12f..dc0fa99ea 100644\n--- a/cacheflow/worker/worker.py\n+++ b/cacheflow/worker/worker.py\n@@ -81,7 +81,7 @@ class Worker:\n             rank=rank,\n         )\n         # A small all_reduce for warmup.\n-        torch.distributed.all_reduce(torch.zeros(1).cuda())\n+        torch.distributed.all_reduce(torch.empty(1, device='cuda'))\n         initialize_model_parallel(tensor_parallel_size,\n                                   pipeline_parallel_size)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
