{"instance_id": "vllm-project__vllm-5b8a1fd", "model_patch": "diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 36e5e1774..ccc25079d 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -44,6 +44,9 @@ class NGramWorker(NonLLMProposerWorkerBase):\n             vocab_size=self.vocab_size,\n         )\n \n+        # Cache for commonly used tensor sizes to avoid repeated allocations\n+        self._arange_cache = {}\n+\n     def sampler_output(\n         self,\n         execute_model_req: ExecuteModelRequest,\n@@ -96,17 +99,28 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 # a better way to do this.\n                 first_match = matches.max(dim=-1)\n                 if first_match.values.item():\n-                    proposal_start_idx = first_match.indices.add_(ngram_size)\n-                    spec_indices = (\n-                        proposal_start_idx).repeat(sample_len) + torch.arange(\n-                            sample_len, device=self.device)\n-                    spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n+                    proposal_start_idx = first_match.indices.item() + ngram_size\n+                    max_idx = input_ids.shape[-1] - 1\n+                    spec_indices = torch.arange(\n+                        proposal_start_idx,\n+                        min(proposal_start_idx + sample_len, max_idx + 1),\n+                        device=self.device)\n+                    # Pad if needed\n+                    if spec_indices.shape[0] < sample_len:\n+                        spec_indices = torch.cat([\n+                            spec_indices,\n+                            torch.full((sample_len - spec_indices.shape[0],),\n+                                      max_idx, device=self.device, dtype=torch.long)\n+                        ])\n                     res = input_ids.gather(dim=-1, index=spec_indices)\n                     token_id_list.append(res)\n-                    token_prob_list.append(\n-                        torch.nn.functional.one_hot(\n-                            res,\n-                            num_classes=self.vocab_size).to(torch.float32))\n+                    # Optimized one_hot: create empty tensor and scatter 1.0\n+                    token_probs = torch.zeros(\n+                        (res.shape[0], self.vocab_size),\n+                        dtype=torch.float32,\n+                        device=self.device)\n+                    token_probs.scatter_(1, res.unsqueeze(1), 1.0)\n+                    token_prob_list.append(token_probs)\n                     has_spec_out = True\n                     break\n             else:\n@@ -125,7 +139,7 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                     SamplerOutput(\n                         outputs=None,\n                         sampled_token_probs=token_prob_list[idx],\n-                        logprobs=torch.zeros((sample_len, self.vocab_size),\n+                        logprobs=torch.empty((sample_len, self.vocab_size),\n                                              dtype=torch.float32,\n                                              device=self.device),\n                         sampled_token_ids=token_id_list[idx],\n", "model_name_or_path": "gpt-5-2025-08-07"}
