diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py
index 36e5e1774..ccc25079d 100644
--- a/vllm/spec_decode/ngram_worker.py
+++ b/vllm/spec_decode/ngram_worker.py
@@ -44,6 +44,9 @@ class NGramWorker(NonLLMProposerWorkerBase):
             vocab_size=self.vocab_size,
         )
 
+        # Cache for commonly used tensor sizes to avoid repeated allocations
+        self._arange_cache = {}
+
     def sampler_output(
         self,
         execute_model_req: ExecuteModelRequest,
@@ -96,17 +99,28 @@ class NGramWorker(NonLLMProposerWorkerBase):
                 # a better way to do this.
                 first_match = matches.max(dim=-1)
                 if first_match.values.item():
-                    proposal_start_idx = first_match.indices.add_(ngram_size)
-                    spec_indices = (
-                        proposal_start_idx).repeat(sample_len) + torch.arange(
-                            sample_len, device=self.device)
-                    spec_indices.clamp_(max=input_ids.shape[-1] - 1)
+                    proposal_start_idx = first_match.indices.item() + ngram_size
+                    max_idx = input_ids.shape[-1] - 1
+                    spec_indices = torch.arange(
+                        proposal_start_idx,
+                        min(proposal_start_idx + sample_len, max_idx + 1),
+                        device=self.device)
+                    # Pad if needed
+                    if spec_indices.shape[0] < sample_len:
+                        spec_indices = torch.cat([
+                            spec_indices,
+                            torch.full((sample_len - spec_indices.shape[0],),
+                                      max_idx, device=self.device, dtype=torch.long)
+                        ])
                     res = input_ids.gather(dim=-1, index=spec_indices)
                     token_id_list.append(res)
-                    token_prob_list.append(
-                        torch.nn.functional.one_hot(
-                            res,
-                            num_classes=self.vocab_size).to(torch.float32))
+                    # Optimized one_hot: create empty tensor and scatter 1.0
+                    token_probs = torch.zeros(
+                        (res.shape[0], self.vocab_size),
+                        dtype=torch.float32,
+                        device=self.device)
+                    token_probs.scatter_(1, res.unsqueeze(1), 1.0)
+                    token_prob_list.append(token_probs)
                     has_spec_out = True
                     break
             else:
@@ -125,7 +139,7 @@ class NGramWorker(NonLLMProposerWorkerBase):
                     SamplerOutput(
                         outputs=None,
                         sampled_token_probs=token_prob_list[idx],
-                        logprobs=torch.zeros((sample_len, self.vocab_size),
+                        logprobs=torch.empty((sample_len, self.vocab_size),
                                              dtype=torch.float32,
                                              device=self.device),
                         sampled_token_ids=token_id_list[idx],
