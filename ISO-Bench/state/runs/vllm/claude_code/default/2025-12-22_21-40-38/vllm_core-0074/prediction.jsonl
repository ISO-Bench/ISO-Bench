{"instance_id": "vllm-project__vllm-3cdfe1f", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 2b287a5d2..8f7207b0f 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1222,8 +1222,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n+        input_tokens = torch.empty(max_batch_size, dtype=torch.long).cuda()\n+        input_positions = torch.empty(max_batch_size, dtype=torch.long).cuda()\n \n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\ndiff --git a/vllm/worker/tpu_model_runner.py b/vllm/worker/tpu_model_runner.py\nindex 01daa64b5..f14c925f9 100644\n--- a/vllm/worker/tpu_model_runner.py\n+++ b/vllm/worker/tpu_model_runner.py\n@@ -102,7 +102,7 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n         self.block_size = self.cache_config.block_size\n         self.max_num_blocks_per_seq = (self.model_config.max_model_len //\n                                        self.block_size)\n-        self.block_tables = np.zeros(\n+        self.block_tables = np.empty(\n             (self.scheduler_config.max_num_seqs, self.max_num_blocks_per_seq),\n             dtype=np.int32)\n         self.attn_backend = get_attn_backend(\n@@ -159,13 +159,13 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n     ) -> None:\n         if is_prompt:\n             seq_len = (seq_len + 15) // 16 * 16\n-            token_ids = torch.zeros((batch_size, seq_len),\n+            token_ids = torch.empty((batch_size, seq_len),\n                                     dtype=torch.int32,\n                                     device=self.device)\n-            position_ids = torch.zeros((batch_size, seq_len),\n+            position_ids = torch.empty((batch_size, seq_len),\n                                        dtype=torch.int32,\n                                        device=self.device)\n-            slot_mapping = torch.zeros((batch_size, seq_len),\n+            slot_mapping = torch.empty((batch_size, seq_len),\n                                        dtype=torch.int64,\n                                        device=self.device)\n             attn_metadata = self.attn_backend.make_metadata(\n@@ -181,16 +181,16 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n                                     device=self.device)\n         else:\n             assert seq_len == 1\n-            token_ids = torch.zeros((batch_size, seq_len),\n+            token_ids = torch.empty((batch_size, seq_len),\n                                     dtype=torch.int32,\n                                     device=self.device)\n-            position_ids = torch.zeros((batch_size, seq_len),\n+            position_ids = torch.empty((batch_size, seq_len),\n                                        dtype=torch.int32,\n                                        device=self.device)\n-            slot_mapping = torch.zeros((batch_size, seq_len),\n+            slot_mapping = torch.empty((batch_size, seq_len),\n                                        dtype=torch.int64,\n                                        device=self.device)\n-            block_tables = torch.zeros(\n+            block_tables = torch.empty(\n                 (batch_size, self.max_num_blocks_per_seq),\n                 dtype=torch.int32,\n                 device=self.device)\ndiff --git a/vllm/worker/tpu_worker.py b/vllm/worker/tpu_worker.py\nindex 320b15d36..4b563da09 100644\n--- a/vllm/worker/tpu_worker.py\n+++ b/vllm/worker/tpu_worker.py\n@@ -170,15 +170,19 @@ class TPUWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         cpu_cache_shape = self.model_runner.attn_backend.get_kv_cache_shape(\n             num_cpu_blocks, self.block_size, num_kv_heads, head_size)\n         for _ in range(num_layers):\n-            tpu_k_cache = torch.zeros(tpu_cache_shape,\n+            tpu_k_cache = torch.empty(tpu_cache_shape,\n+                                      dtype=dtype,\n+                                      device=self.device)\n+            tpu_v_cache = torch.empty(tpu_cache_shape,\n                                       dtype=dtype,\n                                       device=self.device)\n-            tpu_v_cache = torch.zeros_like(tpu_k_cache)\n             self.tpu_cache.append((tpu_k_cache, tpu_v_cache))\n-            cpu_k_cache = torch.zeros(cpu_cache_shape,\n+            cpu_k_cache = torch.empty(cpu_cache_shape,\n+                                      dtype=dtype,\n+                                      device=\"cpu\")\n+            cpu_v_cache = torch.empty(cpu_cache_shape,\n                                       dtype=dtype,\n                                       device=\"cpu\")\n-            cpu_v_cache = torch.zeros_like(cpu_k_cache)\n             self.cpu_cache.append((cpu_k_cache, cpu_v_cache))\n         self._warmup_model()\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
