{"instance_id": "vllm-project__vllm-3336814", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 48cdebee9..68683aef2 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -107,14 +107,18 @@ class RotaryEmbedding(CustomOp):\n         # use CPU to compute the cache and then move it to GPU. However, we\n         # create the cache on GPU for faster initialization. This may cause\n         # a slight numerical difference between the HF implementation and ours.\n+        # Using current_platform.device_type to create tensors directly on target device\n+        from vllm.platforms import current_platform\n         inv_freq = 1.0 / (base**(torch.arange(\n-            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim))\n+            0, self.rotary_dim, 2, dtype=torch.float,\n+            device=current_platform.device_type) / self.rotary_dim))\n         return inv_freq\n \n     def _compute_cos_sin_cache(self) -> torch.Tensor:\n         \"\"\"Compute the cos and sin cache.\"\"\"\n         inv_freq = self._compute_inv_freq(self.base)\n-        t = torch.arange(self.max_position_embeddings, dtype=torch.float)\n+        t = torch.arange(self.max_position_embeddings, dtype=torch.float,\n+                        device=inv_freq.device)\n \n         freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n         cos = freqs.cos()\n@@ -401,7 +405,7 @@ class LinearScalingRotaryEmbedding(RotaryEmbedding):\n             # Thus, the maximum length after applying the rope scaling is\n             # self.max_position_embeddings * self.scaling_factor.\n             max_len = self.max_position_embeddings * scaling_factor\n-            t = torch.arange(max_len, dtype=torch.float)\n+            t = torch.arange(max_len, dtype=torch.float, device=inv_freq.device)\n             t = t / scaling_factor\n \n             freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n@@ -459,7 +463,7 @@ class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):\n             (self.scaling_factor - 1))**(self.rotary_dim /\n                                          (self.rotary_dim - 2))\n         inv_freq = self._compute_inv_freq(base)\n-        t = torch.arange(max_len, dtype=torch.float)\n+        t = torch.arange(max_len, dtype=torch.float, device=inv_freq.device)\n \n         freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n         cos = freqs.cos()\n@@ -498,7 +502,9 @@ def _yarn_linear_ramp_mask(low: float, high: float, dim: int,\n     if low == high:\n         high += 0.001  # Prevent singularity\n \n-    linear_func = (torch.arange(dim, dtype=dtype) - low) / (high - low)\n+    from vllm.platforms import current_platform\n+    linear_func = (torch.arange(dim, dtype=dtype,\n+                               device=current_platform.device_type) - low) / (high - low)\n     ramp_func = torch.clamp(linear_func, 0, 1)\n     return ramp_func\n \n@@ -542,8 +548,10 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):\n                          is_neox_style, dtype)\n \n     def _compute_inv_freq(self, scaling_factor: float) -> torch.Tensor:\n+        from vllm.platforms import current_platform\n         pos_freqs = self.base**(\n-            torch.arange(0, self.rotary_dim, 2, dtype=torch.float) /\n+            torch.arange(0, self.rotary_dim, 2, dtype=torch.float,\n+                        device=current_platform.device_type) /\n             self.rotary_dim)\n         inv_freq_extrapolation = 1.0 / pos_freqs\n         inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)\n@@ -635,9 +643,12 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n                              persistent=False)\n \n     def _compute_inv_freq(self, rescale_factors: List[float]) -> torch.Tensor:\n-        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32)\n+        from vllm.platforms import current_platform\n+        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32,\n+                                      device=current_platform.device_type)\n         inv_freq = 1.0 / (rescale_factors * (self.base**(torch.arange(\n-            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim)))\n+            0, self.rotary_dim, 2, dtype=torch.float,\n+            device=current_platform.device_type) / self.rotary_dim)))\n         return inv_freq\n \n     def _compute_cos_sin_cache(\n@@ -647,7 +658,8 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n         mscale: float,\n     ) -> torch.Tensor:\n         inv_freq = self._compute_inv_freq(rescale_factors)\n-        t = torch.arange(max_position_embeddings, dtype=torch.float)\n+        t = torch.arange(max_position_embeddings, dtype=torch.float,\n+                        device=inv_freq.device)\n         freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n         cos = freqs.cos() * mscale\n         sin = freqs.sin() * mscale\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex f3fff585b..8ab88e78f 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,9 +532,9 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n+                zero = torch.zeros((num_chunks, 1),\n                                    dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+                                   device=device)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n", "model_name_or_path": "gpt-5-2025-08-07"}
