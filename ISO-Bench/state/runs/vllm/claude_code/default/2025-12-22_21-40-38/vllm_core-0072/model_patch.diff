diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 48cdebee9..68683aef2 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -107,14 +107,18 @@ class RotaryEmbedding(CustomOp):
         # use CPU to compute the cache and then move it to GPU. However, we
         # create the cache on GPU for faster initialization. This may cause
         # a slight numerical difference between the HF implementation and ours.
+        # Using current_platform.device_type to create tensors directly on target device
+        from vllm.platforms import current_platform
         inv_freq = 1.0 / (base**(torch.arange(
-            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim))
+            0, self.rotary_dim, 2, dtype=torch.float,
+            device=current_platform.device_type) / self.rotary_dim))
         return inv_freq
 
     def _compute_cos_sin_cache(self) -> torch.Tensor:
         """Compute the cos and sin cache."""
         inv_freq = self._compute_inv_freq(self.base)
-        t = torch.arange(self.max_position_embeddings, dtype=torch.float)
+        t = torch.arange(self.max_position_embeddings, dtype=torch.float,
+                        device=inv_freq.device)
 
         freqs = torch.einsum("i,j -> ij", t, inv_freq)
         cos = freqs.cos()
@@ -401,7 +405,7 @@ class LinearScalingRotaryEmbedding(RotaryEmbedding):
             # Thus, the maximum length after applying the rope scaling is
             # self.max_position_embeddings * self.scaling_factor.
             max_len = self.max_position_embeddings * scaling_factor
-            t = torch.arange(max_len, dtype=torch.float)
+            t = torch.arange(max_len, dtype=torch.float, device=inv_freq.device)
             t = t / scaling_factor
 
             freqs = torch.einsum("i,j -> ij", t, inv_freq)
@@ -459,7 +463,7 @@ class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):
             (self.scaling_factor - 1))**(self.rotary_dim /
                                          (self.rotary_dim - 2))
         inv_freq = self._compute_inv_freq(base)
-        t = torch.arange(max_len, dtype=torch.float)
+        t = torch.arange(max_len, dtype=torch.float, device=inv_freq.device)
 
         freqs = torch.einsum("i,j -> ij", t, inv_freq)
         cos = freqs.cos()
@@ -498,7 +502,9 @@ def _yarn_linear_ramp_mask(low: float, high: float, dim: int,
     if low == high:
         high += 0.001  # Prevent singularity
 
-    linear_func = (torch.arange(dim, dtype=dtype) - low) / (high - low)
+    from vllm.platforms import current_platform
+    linear_func = (torch.arange(dim, dtype=dtype,
+                               device=current_platform.device_type) - low) / (high - low)
     ramp_func = torch.clamp(linear_func, 0, 1)
     return ramp_func
 
@@ -542,8 +548,10 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):
                          is_neox_style, dtype)
 
     def _compute_inv_freq(self, scaling_factor: float) -> torch.Tensor:
+        from vllm.platforms import current_platform
         pos_freqs = self.base**(
-            torch.arange(0, self.rotary_dim, 2, dtype=torch.float) /
+            torch.arange(0, self.rotary_dim, 2, dtype=torch.float,
+                        device=current_platform.device_type) /
             self.rotary_dim)
         inv_freq_extrapolation = 1.0 / pos_freqs
         inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)
@@ -635,9 +643,12 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
                              persistent=False)
 
     def _compute_inv_freq(self, rescale_factors: List[float]) -> torch.Tensor:
-        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32)
+        from vllm.platforms import current_platform
+        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32,
+                                      device=current_platform.device_type)
         inv_freq = 1.0 / (rescale_factors * (self.base**(torch.arange(
-            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim)))
+            0, self.rotary_dim, 2, dtype=torch.float,
+            device=current_platform.device_type) / self.rotary_dim)))
         return inv_freq
 
     def _compute_cos_sin_cache(
@@ -647,7 +658,8 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
         mscale: float,
     ) -> torch.Tensor:
         inv_freq = self._compute_inv_freq(rescale_factors)
-        t = torch.arange(max_position_embeddings, dtype=torch.float)
+        t = torch.arange(max_position_embeddings, dtype=torch.float,
+                        device=inv_freq.device)
         freqs = torch.einsum("i,j -> ij", t, inv_freq)
         cos = freqs.cos() * mscale
         sin = freqs.sin() * mscale
diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py
index f3fff585b..8ab88e78f 100644
--- a/vllm/v1/attention/backends/mla/common.py
+++ b/vllm/v1/attention/backends/mla/common.py
@@ -532,9 +532,9 @@ class MLACommonMetadataBuilder(Generic[M]):
                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)
                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(
                     torch.int32)
-                zero = torch.zeros(num_chunks,
+                zero = torch.zeros((num_chunks, 1),
                                    dtype=torch.int32,
-                                   device=device).unsqueeze(-1)
+                                   device=device)
 
                 chunked_context_metadata = \
                     MLACommonPrefillMetadata.ChunkedContextMetadata(
