{"instance_id": "vllm-project__vllm-a869bac", "model_patch": "diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 5d8b3f423..22b7c8d1e 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -137,26 +137,23 @@ def apply_top_k_top_p_tpu(\n     probs_sort, _ = probs.sort(dim=-1, descending=False)\n \n     if k is not None:\n-        top_k_count = probs_sort.size(1) - k.to(torch.long)  # shape: (batch, )\n-        top_k_count = top_k_count.unsqueeze(dim=1)\n+        top_k_count = (probs_sort.size(1) - k.to(torch.long)).unsqueeze(dim=1)\n         top_k_cutoff = probs_sort.gather(-1, top_k_count)\n \n         # Make sure the no top-k rows are no-op.\n         no_top_k_mask = (k == logits.shape[1]).unsqueeze(dim=1)\n         top_k_cutoff.masked_fill_(no_top_k_mask, -float(\"inf\"))\n \n-        elements_to_discard = probs < top_k_cutoff\n-        logits.masked_fill_(elements_to_discard, -float(\"inf\"))\n+        logits.masked_fill_(probs < top_k_cutoff, -float(\"inf\"))\n \n     if p is not None:\n         cumprob = torch.cumsum(probs_sort, dim=-1)\n         top_p_mask = cumprob <= 1 - p.unsqueeze(dim=1)\n         top_p_mask[:, -1] = False  # at least one\n \n-        top_p_count = top_p_mask.sum(dim=-1).unsqueeze(1)\n+        top_p_count = top_p_mask.sum(dim=-1, keepdim=True)\n         top_p_cutoff = probs_sort.gather(-1, top_p_count)\n-        elements_to_discard = probs < top_p_cutoff\n-        logits.masked_fill_(elements_to_discard, -float(\"inf\"))\n+        logits.masked_fill_(probs < top_p_cutoff, -float(\"inf\"))\n \n     return logits\n \n@@ -184,24 +181,22 @@ def apply_top_k_top_p(\n \n     if k is not None:\n         # Apply top-k.\n-        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B\n+        top_k_indices = (logits_sort.size(1) - k.to(torch.long)).unsqueeze(dim=1)\n         # Get all the top_k values.\n-        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n-        top_k_mask = logits_sort < top_k_mask\n-        logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n+        top_k_cutoff = logits_sort.gather(1, top_k_indices)\n+        logits_sort.masked_fill_(logits_sort < top_k_cutoff, -float(\"inf\"))\n \n     if p is not None:\n         # Apply top-p.\n         probs_sort = logits_sort.softmax(dim=-1)\n-        probs_sum = torch.cumsum(probs_sort, dim=-1, out=probs_sort)\n-        top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n+        torch.cumsum(probs_sort, dim=-1, out=probs_sort)\n+        top_p_mask = probs_sort <= 1 - p.unsqueeze(dim=1)\n         # at least one\n         top_p_mask[:, -1] = False\n         logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n-    return logits\n+    return logits_sort.scatter_(dim=-1, index=logits_idx, src=logits_sort)\n \n \n def apply_top_k_only(\n@@ -217,11 +212,11 @@ def apply_top_k_only(\n     \"\"\"\n     no_top_k_mask = k == logits.shape[1]\n     # Set non-top-k rows to 1 so that we can gather.\n-    k = k.masked_fill(no_top_k_mask, 1)\n-    max_top_k = k.max()\n+    k_masked = k.masked_fill(no_top_k_mask, 1)\n+    max_top_k = k_masked.max()\n     # topk.values tensor has shape [batch_size, max_top_k].\n     # Convert top k to 0-based index in range [0, max_top_k).\n-    k_index = k.sub_(1).unsqueeze(1)\n+    k_index = (k_masked - 1).unsqueeze(1)\n     top_k_mask = logits.topk(max_top_k, dim=1).values.gather(1, k_index.long())\n     # Handle non-topk rows.\n     top_k_mask.masked_fill_(no_top_k_mask.unsqueeze(1), -float(\"inf\"))\n@@ -250,7 +245,8 @@ def random_sample(\n         # one by one. Optimize this.\n         for i, generator in generators.items():\n             q[i].exponential_(generator=generator)\n-    return probs.div_(q).argmax(dim=-1).view(-1)\n+    probs.div_(q)\n+    return probs.argmax(dim=-1, keepdim=False)\n \n \n def flashinfer_sample(\n", "model_name_or_path": "gpt-5-2025-08-07"}
