{"instance_id": "vllm-project__vllm-1d35662", "model_patch": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 329b03a57..304fe0623 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -74,10 +74,13 @@ class TokenizerDataCache:\n             # Vendored from xgrammar logic since we cannot pickle the tokenizer\n             # https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98 # noqa: E501\n             try:\n-                encoded_vocab = [\n-                    token for token, _ in sorted(tokenizer.get_vocab().items(),\n-                                                 key=lambda x: x[1])\n-                ]\n+                vocab_dict = tokenizer.get_vocab()\n+                # Optimize: Use list comprehension with direct indexing\n+                # Pre-allocate list and fill by index instead of sorting\n+                vocab_size = len(vocab_dict)\n+                encoded_vocab = [None] * vocab_size\n+                for token, idx in vocab_dict.items():\n+                    encoded_vocab[idx] = token\n             except AttributeError as e:\n                 raise ValueError(\n                     f\"Cannot get the vocabulary of the tokenizer \"\n@@ -273,6 +276,8 @@ class XGrammarLogitsProcessor:\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n+    # Cache for device-specific token bitmask to avoid repeated transfers\n+    _device_bitmask_cache: dict[str, torch.Tensor] = field(default_factory=dict)\n \n     def __getstate__(self) -> dict[str, Any]:\n         return {'config': self.config}\n@@ -285,6 +290,7 @@ class XGrammarLogitsProcessor:\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n         self.prefilled = False\n+        self._device_bitmask_cache = {}\n \n     def _ensure_ctx(self):\n         \"\"\"Lazily initialize the processor in the worker process\"\"\"\n@@ -315,35 +321,55 @@ class XGrammarLogitsProcessor:\n         if not self.prefilled:\n             # Have not sampled a token yet\n             self.prefilled = True\n+            # Fill bitmasks for initial state\n+            for i, matcher in enumerate(self.matchers):\n+                if not matcher.is_terminated():\n+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)\n         else:\n+            # Accept token and fill next bitmask in one pass\n+            sampled_token = input_ids[-1]\n             for i, matcher in enumerate(self.matchers):\n                 if not matcher.is_terminated():\n-                    sampled_token = input_ids[-1]\n                     assert self.matchers[i].accept_token(sampled_token)\n-\n-        for i, matcher in enumerate(self.matchers):\n-            if not matcher.is_terminated():\n-                # @ubospica: ideally, fill_next_token_bitmask should be\n-                # parallelized with model decoding\n-                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n-                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+                    # @ubospica: ideally, fill_next_token_bitmask should be\n+                    # parallelized with model decoding\n+                    # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n+                    matcher.fill_next_token_bitmask(self.token_bitmask, i)\n \n         # token_bitmask is a CPU tensor for use with accept_token and\n         # fill_next_token_bitmask so we move it to the device of scores\n         device_type = scores.device.type\n+        device_str = str(scores.device)\n         dtype = scores.dtype\n+\n+        # Cache device-specific bitmask to avoid repeated transfers\n+        if device_str not in self._device_bitmask_cache:\n+            self._device_bitmask_cache[device_str] = self.token_bitmask.to(\n+                scores.device, non_blocking=True)\n+        else:\n+            # Update cached tensor in-place\n+            self._device_bitmask_cache[device_str].copy_(\n+                self.token_bitmask, non_blocking=True)\n+\n+        device_bitmask = self._device_bitmask_cache[device_str]\n+\n         if device_type != \"cuda\":\n             # xgrammar on cpu only supports float32 scores\n             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            scores = scores.to(\"cpu\").float().unsqueeze(0)\n+            # Optimize: Use inplace operations and combine ops\n+            if dtype != torch.float32:\n+                scores = scores.to(dtype=torch.float32)\n+            scores = scores.unsqueeze(0)\n \n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n         # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(\n-            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n+        xgr.apply_token_bitmask_inplace(scores, device_bitmask)\n+\n         if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n+            scores = scores.squeeze_(0)\n+            if dtype != torch.float32:\n+                scores = scores.to(dtype=dtype)\n \n         return scores\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
