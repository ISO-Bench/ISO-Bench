{"instance_id": "vllm-project__vllm-6368e77", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..57bbb9da2 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -79,12 +79,16 @@ def _convert_tokens_to_string_with_added_encoders(\n     # NOTE(woosuk): The following code is slow because it runs a for loop over\n     # the output_tokens. In Python, running a for loop over a list can be slow\n     # even when the loop body is very simple.\n+    # OPTIMIZATION: Cache attribute lookups outside the loop to reduce overhead\n+    special_tokens = tokenizer.all_special_tokens if skip_special_tokens else None\n+    added_tokens = tokenizer.added_tokens_encoder\n+\n     sub_texts = []\n     current_sub_text = []\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if special_tokens is not None and token in special_tokens:\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in added_tokens:\n             if current_sub_text:\n                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                 sub_texts.append(sub_text)\n@@ -118,8 +122,10 @@ def detokenize_incrementally(\n         # 5 is an arbitrary value that should work for all\n         # tokenizers (bigger = more conservative).\n         # Subtract 1 extra to account for the generated token.\n-        prefix_offset = max(len(output_tokens) - 6, 0)\n-        read_offset = max(len(output_tokens) - 1, 0)\n+        # OPTIMIZATION: Cache len(output_tokens) to avoid redundant calls\n+        output_len = len(output_tokens)\n+        prefix_offset = max(output_len - 6, 0)\n+        read_offset = max(output_len - 1, 0)\n     else:\n         # Put new_token_id in a list so skip_special_tokens is respected\n         new_tokens = tokenizer.convert_ids_to_tokens(\n@@ -129,7 +135,9 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n+    # OPTIMIZATION: Cache the added_tokens_encoder check to avoid repeated getattr calls\n+    has_added_tokens = bool(getattr(tokenizer, \"added_tokens_encoder\", {}))\n+    if not has_added_tokens:\n         prefix_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:read_offset])\n         new_text = tokenizer.convert_tokens_to_string(\n@@ -144,12 +152,16 @@ def detokenize_incrementally(\n             output_tokens[prefix_offset:],\n             skip_special_tokens=skip_special_tokens)\n \n-    if len(new_text) > len(prefix_text) and not new_text.endswith(\"\ufffd\"):\n+    # OPTIMIZATION: Cache length calculations to avoid redundant calls\n+    new_text_len = len(new_text)\n+    prefix_text_len = len(prefix_text)\n+\n+    if new_text_len > prefix_text_len and not new_text.endswith(\"\ufffd\"):\n         # utf-8 char at the end means it's a potential unfinished byte sequence\n         # from byte fallback tokenization.\n         # If it's in the middle, it's probably a real invalid id generated\n         # by the model\n-        new_text = new_text[len(prefix_text):]\n+        new_text = new_text[prefix_text_len:]\n         return new_tokens, new_text, read_offset, len(output_tokens)\n     else:\n         return new_tokens, \"\", prefix_offset, read_offset\n", "model_name_or_path": "gpt-5-2025-08-07"}
