diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py
index 6dafdac96..57bbb9da2 100644
--- a/vllm/transformers_utils/tokenizer.py
+++ b/vllm/transformers_utils/tokenizer.py
@@ -79,12 +79,16 @@ def _convert_tokens_to_string_with_added_encoders(
     # NOTE(woosuk): The following code is slow because it runs a for loop over
     # the output_tokens. In Python, running a for loop over a list can be slow
     # even when the loop body is very simple.
+    # OPTIMIZATION: Cache attribute lookups outside the loop to reduce overhead
+    special_tokens = tokenizer.all_special_tokens if skip_special_tokens else None
+    added_tokens = tokenizer.added_tokens_encoder
+
     sub_texts = []
     current_sub_text = []
     for token in output_tokens:
-        if skip_special_tokens and token in tokenizer.all_special_tokens:
+        if special_tokens is not None and token in special_tokens:
             continue
-        if token in tokenizer.added_tokens_encoder:
+        if token in added_tokens:
             if current_sub_text:
                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)
                 sub_texts.append(sub_text)
@@ -118,8 +122,10 @@ def detokenize_incrementally(
         # 5 is an arbitrary value that should work for all
         # tokenizers (bigger = more conservative).
         # Subtract 1 extra to account for the generated token.
-        prefix_offset = max(len(output_tokens) - 6, 0)
-        read_offset = max(len(output_tokens) - 1, 0)
+        # OPTIMIZATION: Cache len(output_tokens) to avoid redundant calls
+        output_len = len(output_tokens)
+        prefix_offset = max(output_len - 6, 0)
+        read_offset = max(output_len - 1, 0)
     else:
         # Put new_token_id in a list so skip_special_tokens is respected
         new_tokens = tokenizer.convert_ids_to_tokens(
@@ -129,7 +135,9 @@ def detokenize_incrementally(
     # The prefix text is necessary only to defeat cleanup algorithms in
     # the decode which decide to add a space or not depending on the
     # surrounding ids.
-    if not getattr(tokenizer, "added_tokens_encoder", {}):
+    # OPTIMIZATION: Cache the added_tokens_encoder check to avoid repeated getattr calls
+    has_added_tokens = bool(getattr(tokenizer, "added_tokens_encoder", {}))
+    if not has_added_tokens:
         prefix_text = tokenizer.convert_tokens_to_string(
             output_tokens[prefix_offset:read_offset])
         new_text = tokenizer.convert_tokens_to_string(
@@ -144,12 +152,16 @@ def detokenize_incrementally(
             output_tokens[prefix_offset:],
             skip_special_tokens=skip_special_tokens)
 
-    if len(new_text) > len(prefix_text) and not new_text.endswith("�"):
+    # OPTIMIZATION: Cache length calculations to avoid redundant calls
+    new_text_len = len(new_text)
+    prefix_text_len = len(prefix_text)
+
+    if new_text_len > prefix_text_len and not new_text.endswith("�"):
         # utf-8 char at the end means it's a potential unfinished byte sequence
         # from byte fallback tokenization.
         # If it's in the middle, it's probably a real invalid id generated
         # by the model
-        new_text = new_text[len(prefix_text):]
+        new_text = new_text[prefix_text_len:]
         return new_tokens, new_text, read_offset, len(output_tokens)
     else:
         return new_tokens, "", prefix_offset, read_offset
