diff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
index 61247e930..843a241ec 100644
--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):
         else:
             b_type = quant_config.quant_dtype
 
-        b_a1 = torch.zeros(
+        b_a1 = torch.empty(
             (num_local_experts, self.max_num_tokens, hidden_dim),
             dtype=b_type,
             device=a1.device)
@@ -790,7 +790,7 @@ def batched_moe_kernel_quantize_input(
         else:
             scale_shape = (E, 1, 1)
 
-        A_q_scale = torch.zeros(scale_shape,
+        A_q_scale = torch.empty(scale_shape,
                                 dtype=torch.float32,
                                 device=A.device)
 
@@ -968,9 +968,6 @@ class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):
         intermediate_cache2 = _resize_cache(workspace2,
                                             (E, max_num_tokens, N // 2))
 
-        if self.use_fp8_w8a8:
-            intermediate_cache1.fill_(0)
-
         a1q_scale = normalize_batched_scales_shape(a1q_scale, E)
 
         # MM1
@@ -990,8 +987,6 @@ class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):
             per_act_token_quant=self.per_act_token_quant,
             block_shape=self.block_shape)
 
-        intermediate_cache2.fill_(0)
-
         # TODO (bnell): use triton utility from batched deep gemm.
         self.activation(activation, intermediate_cache2.view(-1, N // 2),
                         intermediate_cache1.view(-1, N))
diff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py
index d0d8c7d6f..e6baf4c58 100644
--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py
+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py
@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):
         """
 
         a1 = hidden_states
-        output = a1 if inplace else torch.zeros_like(a1)
+        output = a1 if inplace else torch.empty_like(a1)
 
         local_num_experts = w1.size(0)
         if global_num_experts == -1:
diff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py
index 3aae183df..495685326 100644
--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py
+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py
@@ -104,10 +104,10 @@ def moe_align_block_size_triton(
 ) -> None:
     numel = topk_ids.numel()
     grid = (num_experts, )
-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),
+    tokens_cnts = torch.empty((num_experts + 1, num_experts),
                               dtype=torch.int32,
                               device=topk_ids.device)
-    cumsum = torch.zeros((num_experts + 1, ),
+    cumsum = torch.empty((num_experts + 1, ),
                          dtype=torch.int32,
                          device=topk_ids.device)
     tokens_per_thread = cdiv(numel, num_experts)
