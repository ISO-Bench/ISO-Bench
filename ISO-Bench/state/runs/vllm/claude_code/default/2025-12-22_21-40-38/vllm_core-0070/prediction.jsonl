{"instance_id": "vllm-project__vllm-8bb43b9", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\nindex 61247e930..843a241ec 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         else:\n             b_type = quant_config.quant_dtype\n \n-        b_a1 = torch.zeros(\n+        b_a1 = torch.empty(\n             (num_local_experts, self.max_num_tokens, hidden_dim),\n             dtype=b_type,\n             device=a1.device)\n@@ -790,7 +790,7 @@ def batched_moe_kernel_quantize_input(\n         else:\n             scale_shape = (E, 1, 1)\n \n-        A_q_scale = torch.zeros(scale_shape,\n+        A_q_scale = torch.empty(scale_shape,\n                                 dtype=torch.float32,\n                                 device=A.device)\n \n@@ -968,9 +968,6 @@ class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         intermediate_cache2 = _resize_cache(workspace2,\n                                             (E, max_num_tokens, N // 2))\n \n-        if self.use_fp8_w8a8:\n-            intermediate_cache1.fill_(0)\n-\n         a1q_scale = normalize_batched_scales_shape(a1q_scale, E)\n \n         # MM1\n@@ -990,8 +987,6 @@ class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             per_act_token_quant=self.per_act_token_quant,\n             block_shape=self.block_shape)\n \n-        intermediate_cache2.fill_(0)\n-\n         # TODO (bnell): use triton utility from batched deep gemm.\n         self.activation(activation, intermediate_cache2.view(-1, N // 2),\n                         intermediate_cache1.view(-1, N))\ndiff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py\nindex d0d8c7d6f..e6baf4c58 100644\n--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py\n+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py\n@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n         \"\"\"\n \n         a1 = hidden_states\n-        output = a1 if inplace else torch.zeros_like(a1)\n+        output = a1 if inplace else torch.empty_like(a1)\n \n         local_num_experts = w1.size(0)\n         if global_num_experts == -1:\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 3aae183df..495685326 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -104,10 +104,10 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n+    tokens_cnts = torch.empty((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n     tokens_per_thread = cdiv(numel, num_experts)\n", "model_name_or_path": "gpt-5-2025-08-07"}
