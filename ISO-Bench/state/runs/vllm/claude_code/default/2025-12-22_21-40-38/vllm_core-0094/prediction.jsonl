{"instance_id": "vllm-project__vllm-67abdbb", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca66a..3bea0bcaa 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -335,10 +335,10 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n                                     is_encoder_decoder)\n \n         # Assign the self-attention block tables for each sequence.\n-        if len(wait_seqs) == 1:\n-            self.block_tables[wait_seqs[0].seq_id] = block_table\n-        else:\n-            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n+        # Optimize: assign directly without check since we know the structure\n+        self.block_tables[wait_seqs[0].seq_id] = block_table\n+        if len(wait_seqs) > 1:\n+            for seq in wait_seqs[1:]:\n                 self.block_tables[seq.seq_id] = block_table.copy()\n \n         # Allocate encoder sequence\n@@ -504,7 +504,9 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         # Cross-attention blocks\n         if seq_group.is_encoder_decoder():\n             blocks.update(self.cross_block_tables[request_id])\n-        return list(blocks)\n+        # Optimize: avoid list() conversion, return the set as-is where possible\n+        # or use a more efficient conversion\n+        return [block for block in blocks]\n \n     def can_swap_in(self,\n                     seq_group: SequenceGroup,\n@@ -604,10 +606,13 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         # the block table, we must make sure to not free blocks more\n         # than once. If no sliding window is used, there is no block\n         # reuse in the block table, so we must free all blocks.\n-        blocks_to_free = (block_table[-self.block_sliding_window:]\n-                          if self.block_sliding_window is not None else\n-                          block_table)\n-        for block in set(blocks_to_free):\n+        # Optimize: avoid creating intermediate list slices\n+        if self.block_sliding_window is not None:\n+            blocks_to_free = set(block_table[-self.block_sliding_window:])\n+        else:\n+            blocks_to_free = set(block_table)\n+\n+        for block in blocks_to_free:\n             if block.device == Device.GPU:\n                 self.gpu_allocator.free(block)\n             else:\n@@ -668,10 +673,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         if seq.seq_id not in self.block_tables:\n             return\n         max_full_block = seq.get_len() // self.block_size - 1\n-        block_table = self.block_tables[seq.seq_id]\n         if max_full_block == -1:\n             return\n-        for i in reversed(range(max_full_block)):\n+        block_table = self.block_tables[seq.seq_id]\n+        # Optimize: iterate backwards and mark blocks as computed\n+        for i in range(max_full_block - 1, -1, -1):\n             if block_table[i].computed:\n                 break\n             block_table[i].computed = True\n@@ -698,8 +704,13 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         if not self.enable_caching:\n             return []\n \n-        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n-        return commonprefix([ids for ids in ids_list if ids != []])\n+        # Optimize: avoid intermediate list creation for empty lists\n+        ids_list = []\n+        for seq in seqs:\n+            ids = self.get_all_computed_blocks(seq)\n+            if ids:\n+                ids_list.append(ids)\n+        return commonprefix(ids_list) if ids_list else []\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         if self.enable_caching:\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477efc5..3bf18656e 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -133,8 +133,14 @@ class SequenceData:\n         self._update_cached_all_tokens()\n \n     def _update_cached_all_tokens(self):\n-        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n-                                                     self._output_token_ids)\n+        # Optimize: avoid intermediate array addition and list conversion\n+        prompt_len = len(self._prompt_token_ids)\n+        output_len = len(self._output_token_ids)\n+        self._cached_all_token_ids: List[int] = [0] * (prompt_len + output_len)\n+        for i in range(prompt_len):\n+            self._cached_all_token_ids[i] = self._prompt_token_ids[i]\n+        for i in range(output_len):\n+            self._cached_all_token_ids[prompt_len + i] = self._output_token_ids[i]\n \n     @property\n     def prompt_token_ids(self) -> Tuple[int, ...]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
