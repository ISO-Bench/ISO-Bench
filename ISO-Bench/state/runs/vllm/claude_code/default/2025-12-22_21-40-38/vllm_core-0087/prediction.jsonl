{"instance_id": "vllm-project__vllm-10904e6", "model_patch": "diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex cbb6bb268..81d7444f1 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -72,7 +72,7 @@ class BlockPool:\n     def get_cached_block(\n             self, block_hash: BlockHash,\n             kv_cache_group_ids: list[int]) -> Optional[list[KVCacheBlock]]:\n-        \"\"\"Get the cached block by the block hash for each group in \n+        \"\"\"Get the cached block by the block hash for each group in\n         `kv_cache_group_ids`, or None if cache miss for any group.\n         If there are duplicated blocks, we return the first block in the cache.\n \n@@ -83,14 +83,15 @@ class BlockPool:\n         Returns:\n             The cached blocks if exists, or None.\n         \"\"\"\n-        cached_blocks = []\n-        for group_id in kv_cache_group_ids:\n+        # Pre-allocate list with exact size to avoid dynamic resizing\n+        num_groups = len(kv_cache_group_ids)\n+        cached_blocks = [None] * num_groups\n+        for i, group_id in enumerate(kv_cache_group_ids):\n             cached_blocks_one_group = self.cached_block_hash_to_block.get(\n                 BlockHashWithGroupId(block_hash, group_id))\n             if not cached_blocks_one_group:\n                 return None\n-            first_block = next(iter(cached_blocks_one_group.values()))\n-            cached_blocks.append(first_block)\n+            cached_blocks[i] = next(iter(cached_blocks_one_group.values()))\n         return cached_blocks\n \n     def cache_full_blocks(\ndiff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py\nindex 457d95cc7..2305953a3 100644\n--- a/vllm/v1/core/kv_cache_utils.py\n+++ b/vllm/v1/core/kv_cache_utils.py\n@@ -214,11 +214,18 @@ class FreeKVCacheBlockQueue:\n         self.num_free_blocks = len(blocks)\n \n         # Initialize doubly links of consecutive blocks\n-        for i in range(self.num_free_blocks):\n-            if i > 0:\n+        # Optimized to reduce loop overhead by handling edge cases separately\n+        if self.num_free_blocks > 0:\n+            # First block: no prev, has next\n+            if self.num_free_blocks > 1:\n+                blocks[0].next_free_block = blocks[1]\n+            # Middle blocks: have both prev and next\n+            for i in range(1, self.num_free_blocks - 1):\n                 blocks[i].prev_free_block = blocks[i - 1]\n-            if i < self.num_free_blocks - 1:\n                 blocks[i].next_free_block = blocks[i + 1]\n+            # Last block: has prev, no next (only if there are 2+ blocks)\n+            if self.num_free_blocks > 1:\n+                blocks[-1].prev_free_block = blocks[-2]\n \n         # Create a fake head and a tail block for the doubly linked list to\n         # reduce branching in the code\n@@ -454,11 +461,25 @@ def generate_block_hash_extra_keys(\n     mm_extra_keys: list[Any]\n     mm_extra_keys, new_start_mm_idx = _gen_mm_extra_hash_keys(\n         request, start_token_idx, end_token_idx, start_mm_idx)\n-    lora_extra_keys: list[int] = _gen_lora_extra_hash_keys(request)\n-    cache_salt_keys: list[str] = [request.cache_salt] if (\n-        start_token_idx == 0 and request.cache_salt) else []\n \n-    extra_keys: list[Any] = lora_extra_keys + mm_extra_keys + cache_salt_keys\n+    # Build extra_keys list incrementally to avoid multiple list allocations\n+    has_lora = request.lora_request is not None\n+    has_cache_salt = start_token_idx == 0 and request.cache_salt\n+\n+    if not has_lora and not has_cache_salt:\n+        # Fast path: only mm_extra_keys\n+        if not mm_extra_keys:\n+            return None, new_start_mm_idx\n+        return tuple(mm_extra_keys), new_start_mm_idx\n+\n+    # Slower path: need to combine multiple key sources\n+    # Avoid list concatenation by building result directly\n+    extra_keys: list[Any] = []\n+    if has_lora:\n+        extra_keys.append(request.lora_request.lora_int_id)\n+    extra_keys.extend(mm_extra_keys)\n+    if has_cache_salt:\n+        extra_keys.append(request.cache_salt)\n \n     if not extra_keys:\n         return None, new_start_mm_idx\n@@ -510,20 +531,27 @@ def hash_request_tokens(hash_function: Any, block_size: int,\n         The list of computed hash values.\n     \"\"\"\n     token_ids = request.all_token_ids\n+    num_tokens = len(token_ids)\n+\n+    # Calculate number of full blocks upfront\n+    num_full_blocks = num_tokens // block_size\n \n     req_need_extra_keys = need_extra_keys(request)\n     req_extra_keys = None\n     curr_mm_idx = 0\n \n-    ret = []\n+    # Pre-allocate list with exact size to avoid dynamic resizing\n+    ret = [None] * num_full_blocks\n     parent_block_hash_value = None\n-    for start in range(0, len(token_ids), block_size):\n+    block_idx = 0\n+    for start in range(0, num_tokens, block_size):\n         end = start + block_size\n-        block_token_ids = token_ids[start:end]\n-        # Do not hash the block if it is not full.\n-        if len(block_token_ids) < block_size:\n+        # Early exit when we reach incomplete block\n+        if end > num_tokens:\n             break\n \n+        block_token_ids = token_ids[start:end]\n+\n         if req_need_extra_keys:\n             # MM and LoRA requests need extra keys for block-hash computation.\n             req_extra_keys, curr_mm_idx = generate_block_hash_extra_keys(\n@@ -531,8 +559,9 @@ def hash_request_tokens(hash_function: Any, block_size: int,\n \n         block_hash = hash_block_tokens(hash_function, parent_block_hash_value,\n                                        block_token_ids, req_extra_keys)\n-        ret.append(block_hash)\n+        ret[block_idx] = block_hash\n         parent_block_hash_value = block_hash.hash_value\n+        block_idx += 1\n     return ret\n \n \n@@ -873,6 +902,7 @@ def _get_kv_cache_config_uniform_page_size(\n     # Group all layers by type_id.\n     # E.g., 2 full attention layers and 3 sliding window attention layers,\n     # -> (full.0, full.1), (sw.0, sw.1, sw.2).\n+    # Pre-compute to avoid repeated attribute access in tight loop\n     same_type_layers: dict[str, list[str]] = defaultdict(list)\n     for layer_name, layer_spec in kv_cache_spec.items():\n         same_type_layers[layer_spec.type_id].append(layer_name)\n", "model_name_or_path": "gpt-5-2025-08-07"}
