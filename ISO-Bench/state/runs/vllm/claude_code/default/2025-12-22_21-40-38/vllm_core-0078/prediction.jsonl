{"instance_id": "vllm-project__vllm-cc466a3", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 1f19d2053..201f52480 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -142,7 +142,8 @@ def _get_bin_counts_and_mask(\n     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n+    # Use scalar 1 instead of creating a ones tensor for better performance\n+    bin_counts.scatter_add_(1, tokens, 1)\n     bin_counts = bin_counts[:, :vocab_size]\n     mask = bin_counts > 0\n \n@@ -207,10 +208,17 @@ def _apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n     output_bin_counts, output_mask = _get_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n \n-    repetition_penalties = repetition_penalties[:, None].repeat(1, vocab_size)\n-    repetition_penalties[~(prompt_mask | output_mask)] = 1.0\n-    logits = torch.where(logits > 0, logits / repetition_penalties,\n-                         logits * repetition_penalties)\n+    # Optimize by using broadcasting instead of repeat - avoid unnecessary memory allocation\n+    repetition_penalties_expanded = repetition_penalties[:, None]  # Shape: (num_seqs, 1)\n+    # Create mask for tokens that should have penalties applied\n+    penalty_mask = prompt_mask | output_mask\n+    # Apply penalties only where needed using broadcasting\n+    logits = torch.where(\n+        penalty_mask,\n+        torch.where(logits > 0, logits / repetition_penalties_expanded,\n+                    logits * repetition_penalties_expanded),\n+        logits\n+    )\n \n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n@@ -242,7 +250,9 @@ def _apply_top_k_top_p(\n     logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n+    # Optimize by specifying dtype for arange to match logits_idx dtype\n     src = torch.arange(logits_idx.shape[-1],\n+                       dtype=logits_idx.dtype,\n                        device=logits_idx.device).expand_as(logits_idx)\n     logits_idx_inv = torch.empty_like(logits_idx).scatter_(dim=-1,\n                                                            index=logits_idx,\n@@ -678,8 +688,8 @@ def _get_ranks(x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[torch.arange(0, len(x), device=x.device, dtype=indices.dtype),\n-             indices]\n+    # Optimize by using gather instead of advanced indexing with arange\n+    vals = x.gather(1, indices.unsqueeze(1)).squeeze(1)\n     return (x > vals[:, None]).long().sum(1).add_(1)\n \n \n@@ -761,8 +771,9 @@ def _get_logprobs(\n         empty_prompt_logprob: Optional[PromptLogprobs] = None\n         return [empty_prompt_logprob], [empty_sampled_logprob]\n \n-    query_indices_gpu = torch.tensor(query_indices, device=logprobs.device)\n-    next_token_ids_gpu = torch.tensor(next_token_ids, device=logprobs.device)\n+    # Optimize tensor creation by specifying dtype to avoid inference overhead\n+    query_indices_gpu = torch.tensor(query_indices, dtype=torch.long, device=logprobs.device)\n+    next_token_ids_gpu = torch.tensor(next_token_ids, dtype=torch.long, device=logprobs.device)\n \n     # (num_selected_query_tokens, num_logprobs). Note that query_indices can\n     # contain duplicates if beam search is enabled.\n", "model_name_or_path": "gpt-5-2025-08-07"}
