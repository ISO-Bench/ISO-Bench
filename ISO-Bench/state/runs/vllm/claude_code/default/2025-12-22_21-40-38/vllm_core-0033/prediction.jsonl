{"instance_id": "vllm-project__vllm-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..2c549e4a1 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1043,7 +1043,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         # in numpy and only copy the actual input content at every iteration.\n         # The shape of the cached block table will be\n         # (max batch size to capture, max seq len to capture / block size).\n-        self.graph_block_tables = np.zeros(\n+        self.graph_block_tables = np.empty(\n             (self.max_batchsize_to_capture, self.get_max_block_per_batch()),\n             dtype=np.int32)\n \n@@ -1322,7 +1322,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n             # multiplying the list, to avoid Dynamo from treating them as\n             # tensor aliasing.\n             kv_caches = [\n-                torch.tensor([], dtype=torch.float32, device=self.device)\n+                torch.empty(0, dtype=torch.float32, device=self.device)\n                 for _ in range(num_layers)\n             ]\n             finished_requests_ids = [seq.request_id for seq in seqs]\n@@ -1437,10 +1437,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = self.max_batchsize_to_capture\n-        input_tokens = torch.zeros(max_batch_size,\n+        input_tokens = torch.empty(max_batch_size,\n                                    dtype=torch.long,\n                                    device=self.device)\n-        input_positions = torch.zeros(max_batch_size,\n+        input_positions = torch.empty(max_batch_size,\n                                       dtype=torch.long,\n                                       device=self.device)\n         if self.model_config.uses_mrope:\n@@ -1568,10 +1568,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         \"\"\"\n         # During the decode phase encoder_input_ids and encoder_positions are\n         # unset. Do the same thing for graph capture.\n-        capture_inputs[\"encoder_input_ids\"] = torch.tensor([],\n+        capture_inputs[\"encoder_input_ids\"] = torch.empty(0,\n                                                            dtype=torch.long,\n                                                            device=self.device)\n-        capture_inputs[\"encoder_positions\"] = torch.tensor([],\n+        capture_inputs[\"encoder_positions\"] = torch.empty(0,\n                                                            dtype=torch.long,\n                                                            device=self.device)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
