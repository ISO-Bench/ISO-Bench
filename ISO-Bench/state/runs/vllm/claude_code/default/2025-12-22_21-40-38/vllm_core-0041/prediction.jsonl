{"instance_id": "vllm-project__vllm-084a01f", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 5c376797a..180b04df9 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -183,9 +183,10 @@ def _get_bin_counts_and_mask(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     # Compute the bin counts for the tokens.\n     # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n+    bin_counts = torch.empty((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n+    bin_counts.fill_(0)\n     bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n     bin_counts = bin_counts[:, :vocab_size]\n     mask = bin_counts > 0\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 390b5d173..17c3075e1 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -389,24 +389,25 @@ class SamplingTensors:\n                 query_len = seq_group.query_len\n                 assert query_len is not None\n                 prefill_len = len(seq_group.prompt_logprob_indices)\n-                temperatures += [temperature] * prefill_len\n-                top_ps += [top_p] * prefill_len\n-                top_ks += [top_k] * prefill_len\n-                min_ps += [min_p] * prefill_len\n-                presence_penalties += [0] * prefill_len\n-                frequency_penalties += [0] * prefill_len\n-                repetition_penalties += [1] * prefill_len\n+                temperatures.extend([temperature] * prefill_len)\n+                top_ps.extend([top_p] * prefill_len)\n+                top_ks.extend([top_k] * prefill_len)\n+                min_ps.extend([min_p] * prefill_len)\n+                presence_penalties.extend([0] * prefill_len)\n+                frequency_penalties.extend([0] * prefill_len)\n+                repetition_penalties.extend([1] * prefill_len)\n \n             if seq_group.do_sample:\n                 sample_lens = len(seq_group.sample_indices)\n                 assert sample_lens == len(seq_ids)\n-                temperatures += [temperature] * len(seq_ids)\n-                top_ps += [top_p] * len(seq_ids)\n-                top_ks += [top_k] * len(seq_ids)\n-                min_ps += [min_p] * len(seq_ids)\n-                presence_penalties += [p] * len(seq_ids)\n-                frequency_penalties += [f] * len(seq_ids)\n-                repetition_penalties += [r] * len(seq_ids)\n+                seq_len = len(seq_ids)\n+                temperatures.extend([temperature] * seq_len)\n+                top_ps.extend([top_p] * seq_len)\n+                top_ks.extend([top_k] * seq_len)\n+                min_ps.extend([min_p] * seq_len)\n+                presence_penalties.extend([p] * seq_len)\n+                frequency_penalties.extend([f] * seq_len)\n+                repetition_penalties.extend([r] * seq_len)\n \n             if is_prompt:\n                 prompt_best_of.append(sampling_params.best_of)\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 0cd4c7e71..ee5ccade6 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -132,8 +132,9 @@ class SequenceData:\n         self._update_cached_all_tokens()\n \n     def _update_cached_all_tokens(self):\n-        self._cached_all_token_ids: List[int] = (self._prompt_token_ids +\n-                                                 self._output_token_ids)\n+        # Optimize by extending instead of creating new list\n+        self._cached_all_token_ids = self._prompt_token_ids[:]\n+        self._cached_all_token_ids.extend(self._output_token_ids)\n \n     @property\n     def prompt_token_ids(self) -> Tuple[int, ...]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
