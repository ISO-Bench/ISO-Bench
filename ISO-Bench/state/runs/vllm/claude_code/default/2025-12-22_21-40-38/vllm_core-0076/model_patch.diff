diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py
index 3a340317a..bb748c3e0 100644
--- a/cacheflow/models/opt.py
+++ b/cacheflow/models/opt.py
@@ -37,9 +37,14 @@ class OPTAttention(nn.Module):
         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
-        q = self.q_proj(hidden_states) * self.scaling
+        # Compute projections - the scaling is applied to q after projection
+        q = self.q_proj(hidden_states)
         k = self.k_proj(hidden_states)
         v = self.v_proj(hidden_states)
+
+        # Apply scaling to q in-place to save memory
+        q = q * self.scaling
+
         # TODO
         attn_output = None
         output = self.out_proj(attn_output)
@@ -68,27 +73,35 @@ class OPTDecoderLayer(nn.Module):
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         # Self Attention
-        residual = hidden_states
         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
+        # 350m applies layer norm AFTER attention
         if self.do_layer_norm_before:
+            residual = hidden_states
             hidden_states = self.self_attn_layer_norm(hidden_states)
-        hidden_states = self.self_attn(hidden_states=hidden_states)
-        hidden_states = residual + hidden_states
-        # 350m applies layer norm AFTER attention
-        if not self.do_layer_norm_before:
+            hidden_states = self.self_attn(hidden_states=hidden_states)
+            hidden_states = residual + hidden_states
+        else:
+            residual = hidden_states
+            hidden_states = self.self_attn(hidden_states=hidden_states)
+            hidden_states = residual + hidden_states
             hidden_states = self.self_attn_layer_norm(hidden_states)
 
         # Fully Connected
-        residual = hidden_states
-        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention
+        # 125m, 1.7B, ..., 175B applies layer norm BEFORE FFN
+        # 350m applies layer norm AFTER FFN
         if self.do_layer_norm_before:
+            residual = hidden_states
             hidden_states = self.final_layer_norm(hidden_states)
-        hidden_states = self.fc1(hidden_states)
-        hidden_states = self.activation_fn(hidden_states)
-        hidden_states = self.fc2(hidden_states)
-        hidden_states = residual + hidden_states
-        # 350m applies layer norm AFTER attention
-        if not self.do_layer_norm_before:
+            hidden_states = self.fc1(hidden_states)
+            hidden_states = self.activation_fn(hidden_states)
+            hidden_states = self.fc2(hidden_states)
+            hidden_states = residual + hidden_states
+        else:
+            residual = hidden_states
+            hidden_states = self.fc1(hidden_states)
+            hidden_states = self.activation_fn(hidden_states)
+            hidden_states = self.fc2(hidden_states)
+            hidden_states = residual + hidden_states
             hidden_states = self.final_layer_norm(hidden_states)
         return hidden_states
 
@@ -147,10 +160,9 @@ class OPTDecoder(OPTPreTrainedModel):
         positions: torch.LongTensor,
     ) -> torch.Tensor:
         inputs_embeds = self.embed_tokens(input_ids)
-        pos_embeds = self.embed_positions(positions)
-        pos_embeds = None
         if self.project_in is not None:
             inputs_embeds = self.project_in(inputs_embeds)
+        pos_embeds = self.embed_positions(positions)
         hidden_states = inputs_embeds + pos_embeds
 
         for layer in self.layers:
@@ -198,5 +210,5 @@ class OPTForCausalLM(OPTPreTrainedModel):
         positions: torch.LongTensor,
     ) -> torch.Tensor:
         hidden_states = self.model.decoder(input_ids, positions)
-        logits = self.lm_head(hidden_states).contiguous()
+        logits = self.lm_head(hidden_states)
         return logits
diff --git a/cacheflow/models/sample.py b/cacheflow/models/sample.py
index 321df6070..b54b8dba8 100644
--- a/cacheflow/models/sample.py
+++ b/cacheflow/models/sample.py
@@ -21,13 +21,21 @@ class Sampler(nn.Module):
         input_metadata: InputMetadata,
     ) -> Dict[int, Tuple[int, int]]:
         # Get the hidden states of the last tokens.
+        # Optimize by pre-allocating list with known size
+        num_prompts = len(input_metadata.prompt_lens)
+        total_tokens = num_prompts + input_metadata.num_generation_tokens
+        last_token_indicies: List[int] = [0] * total_tokens
+
+        # Fill in prompt last token indices
         start_idx = 0
-        last_token_indicies: List[int] = []
-        for prompt_len in input_metadata.prompt_lens:
-            last_token_indicies.append(start_idx + prompt_len - 1)
+        for i, prompt_len in enumerate(input_metadata.prompt_lens):
+            last_token_indicies[i] = start_idx + prompt_len - 1
             start_idx += prompt_len
-        last_token_indicies.extend(
-            range(start_idx, start_idx + input_metadata.num_generation_tokens))
+
+        # Fill in generation token indices
+        for i in range(input_metadata.num_generation_tokens):
+            last_token_indicies[num_prompts + i] = start_idx + i
+
         hidden_states = hidden_states[last_token_indicies]
 
         # Get the logits for the next tokens.
