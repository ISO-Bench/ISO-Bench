{"instance_id": "vllm-project__vllm-b56b6ca", "model_patch": "diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py\nindex 3a340317a..bb748c3e0 100644\n--- a/cacheflow/models/opt.py\n+++ b/cacheflow/models/opt.py\n@@ -37,9 +37,14 @@ class OPTAttention(nn.Module):\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        q = self.q_proj(hidden_states) * self.scaling\n+        # Compute projections - the scaling is applied to q after projection\n+        q = self.q_proj(hidden_states)\n         k = self.k_proj(hidden_states)\n         v = self.v_proj(hidden_states)\n+\n+        # Apply scaling to q in-place to save memory\n+        q = q * self.scaling\n+\n         # TODO\n         attn_output = None\n         output = self.out_proj(attn_output)\n@@ -68,27 +73,35 @@ class OPTDecoderLayer(nn.Module):\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # Self Attention\n-        residual = hidden_states\n         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n+        # 350m applies layer norm AFTER attention\n         if self.do_layer_norm_before:\n+            residual = hidden_states\n             hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states = self.self_attn(hidden_states=hidden_states)\n-        hidden_states = residual + hidden_states\n-        # 350m applies layer norm AFTER attention\n-        if not self.do_layer_norm_before:\n+            hidden_states = self.self_attn(hidden_states=hidden_states)\n+            hidden_states = residual + hidden_states\n+        else:\n+            residual = hidden_states\n+            hidden_states = self.self_attn(hidden_states=hidden_states)\n+            hidden_states = residual + hidden_states\n             hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Fully Connected\n-        residual = hidden_states\n-        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n+        # 125m, 1.7B, ..., 175B applies layer norm BEFORE FFN\n+        # 350m applies layer norm AFTER FFN\n         if self.do_layer_norm_before:\n+            residual = hidden_states\n             hidden_states = self.final_layer_norm(hidden_states)\n-        hidden_states = self.fc1(hidden_states)\n-        hidden_states = self.activation_fn(hidden_states)\n-        hidden_states = self.fc2(hidden_states)\n-        hidden_states = residual + hidden_states\n-        # 350m applies layer norm AFTER attention\n-        if not self.do_layer_norm_before:\n+            hidden_states = self.fc1(hidden_states)\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = self.fc2(hidden_states)\n+            hidden_states = residual + hidden_states\n+        else:\n+            residual = hidden_states\n+            hidden_states = self.fc1(hidden_states)\n+            hidden_states = self.activation_fn(hidden_states)\n+            hidden_states = self.fc2(hidden_states)\n+            hidden_states = residual + hidden_states\n             hidden_states = self.final_layer_norm(hidden_states)\n         return hidden_states\n \n@@ -147,10 +160,9 @@ class OPTDecoder(OPTPreTrainedModel):\n         positions: torch.LongTensor,\n     ) -> torch.Tensor:\n         inputs_embeds = self.embed_tokens(input_ids)\n-        pos_embeds = self.embed_positions(positions)\n-        pos_embeds = None\n         if self.project_in is not None:\n             inputs_embeds = self.project_in(inputs_embeds)\n+        pos_embeds = self.embed_positions(positions)\n         hidden_states = inputs_embeds + pos_embeds\n \n         for layer in self.layers:\n@@ -198,5 +210,5 @@ class OPTForCausalLM(OPTPreTrainedModel):\n         positions: torch.LongTensor,\n     ) -> torch.Tensor:\n         hidden_states = self.model.decoder(input_ids, positions)\n-        logits = self.lm_head(hidden_states).contiguous()\n+        logits = self.lm_head(hidden_states)\n         return logits\ndiff --git a/cacheflow/models/sample.py b/cacheflow/models/sample.py\nindex 321df6070..b54b8dba8 100644\n--- a/cacheflow/models/sample.py\n+++ b/cacheflow/models/sample.py\n@@ -21,13 +21,21 @@ class Sampler(nn.Module):\n         input_metadata: InputMetadata,\n     ) -> Dict[int, Tuple[int, int]]:\n         # Get the hidden states of the last tokens.\n+        # Optimize by pre-allocating list with known size\n+        num_prompts = len(input_metadata.prompt_lens)\n+        total_tokens = num_prompts + input_metadata.num_generation_tokens\n+        last_token_indicies: List[int] = [0] * total_tokens\n+\n+        # Fill in prompt last token indices\n         start_idx = 0\n-        last_token_indicies: List[int] = []\n-        for prompt_len in input_metadata.prompt_lens:\n-            last_token_indicies.append(start_idx + prompt_len - 1)\n+        for i, prompt_len in enumerate(input_metadata.prompt_lens):\n+            last_token_indicies[i] = start_idx + prompt_len - 1\n             start_idx += prompt_len\n-        last_token_indicies.extend(\n-            range(start_idx, start_idx + input_metadata.num_generation_tokens))\n+\n+        # Fill in generation token indices\n+        for i in range(input_metadata.num_generation_tokens):\n+            last_token_indicies[num_prompts + i] = start_idx + i\n+\n         hidden_states = hidden_states[last_token_indicies]\n \n         # Get the logits for the next tokens.\n", "model_name_or_path": "gpt-5-2025-08-07"}
