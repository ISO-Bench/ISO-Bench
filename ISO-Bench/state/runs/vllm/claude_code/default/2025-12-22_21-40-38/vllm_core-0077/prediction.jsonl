{"instance_id": "vllm-project__vllm-ec82c3e", "model_patch": "diff --git a/tests/v1/spec_decode/test_eagle.py b/tests/v1/spec_decode/test_eagle.py\nindex e000d955c..9b1b6ffa1 100644\n--- a/tests/v1/spec_decode/test_eagle.py\n+++ b/tests/v1/spec_decode/test_eagle.py\n@@ -212,12 +212,12 @@ def test_propose(num_speculative_tokens):\n     for i in range(num_speculative_tokens):\n         if i == 0:\n             # First call uses all tokens\n-            h_logits = torch.zeros(total_tokens, hidden_size, device=device)\n-            h_states = torch.zeros(total_tokens, hidden_size, device=device)\n+            h_logits = torch.empty(total_tokens, hidden_size, device=device)\n+            h_states = torch.empty(total_tokens, hidden_size, device=device)\n         else:\n             # Subsequent calls use batch_size tokens\n-            h_logits = torch.zeros(batch_size, hidden_size, device=device)\n-            h_states = torch.zeros(batch_size, hidden_size, device=device)\n+            h_logits = torch.empty(batch_size, hidden_size, device=device)\n+            h_states = torch.empty(batch_size, hidden_size, device=device)\n         forward_returns.append((h_logits, h_states))\n \n     # For single token case, we only need the first item;\n@@ -289,7 +289,7 @@ def test_propose(num_speculative_tokens):\n     else:\n         # Example for num_speculative_tokens=3:\n         # [[42, 43, 44], [60, 61, 62]]\n-        expected_tokens = torch.zeros((batch_size, num_speculative_tokens),\n+        expected_tokens = torch.empty((batch_size, num_speculative_tokens),\n                                       dtype=torch.int64,\n                                       device=device)\n         for i in range(batch_size):\ndiff --git a/vllm/v1/spec_decode/eagle.py b/vllm/v1/spec_decode/eagle.py\nindex 3926a86ee..790937d8c 100644\n--- a/vllm/v1/spec_decode/eagle.py\n+++ b/vllm/v1/spec_decode/eagle.py\n@@ -55,13 +55,13 @@ class EagleProposer:\n                 self.vllm_config.compilation_config.cudagraph_capture_sizes))\n \n         # persistent buffers for cuda graph\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=device)\n-        self.hidden_states = torch.zeros(\n+        self.hidden_states = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=device)\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 42847e2f8..a1efa46f9 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -201,19 +201,19 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n-        self.query_start_loc = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc = torch.empty(self.max_num_reqs + 1,\n                                            dtype=torch.int32,\n                                            device=self.device)\n-        self.seq_lens = torch.zeros(self.max_num_reqs,\n+        self.seq_lens = torch.empty(self.max_num_reqs,\n                                     dtype=torch.int32,\n                                     device=self.device)\n-        self.slot_mapping = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping = torch.empty(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=self.device)\n \n@@ -232,10 +232,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n@@ -244,7 +244,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # Only relevant for models using ALiBi (e.g, MPT)\n         self.use_alibi = check_use_alibi(model_config)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -258,21 +258,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1805,7 +1805,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # NOTE(woosuk): Here, we should use int32 because the sampler uses\n             # int32 for bonus_token_ids. If the dtype mismatches, re-compilation\n             # will occur at runtime.\n-            bonus_token_ids = torch.zeros(num_reqs,\n+            bonus_token_ids = torch.empty(num_reqs,\n                                           device=self.device,\n                                           dtype=torch.int32)\n             self.rejection_sampler(\n", "model_name_or_path": "gpt-5-2025-08-07"}
