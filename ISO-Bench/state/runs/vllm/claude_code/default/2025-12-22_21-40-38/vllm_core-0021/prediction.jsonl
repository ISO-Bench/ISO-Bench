{"instance_id": "vllm-project__vllm-edc4fa3", "model_patch": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 1be806bbf..b7263b682 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -81,14 +81,11 @@ __global__ void copy_blocks_kernel(int64_t* key_cache_ptrs,\n \n   const int64_t src_block_offset = src_block_number * numel_per_block;\n   const int64_t dst_block_offset = dst_block_number * numel_per_block;\n+  // Process both key and value caches in a single loop for better cache locality\n   for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n     int64_t src_offset = src_block_offset + i;\n     int64_t dst_offset = dst_block_offset + i;\n     key_cache[dst_offset] = key_cache[src_offset];\n-  }\n-  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n-    int64_t src_offset = src_block_offset + i;\n-    int64_t dst_offset = dst_block_offset + i;\n     value_cache[dst_offset] = value_cache[src_offset];\n   }\n }\n@@ -110,34 +107,50 @@ void copy_blocks(std::vector<torch::Tensor> const& key_caches,\n   TORCH_CHECK(cache_device.is_cuda());\n \n   // Create data structures for the kernel.\n+  // block_mapping is a 2D tensor with shape (num_pairs, 2).\n+  int num_pairs = block_mapping.size(0);\n+\n+  // Set device guard before any GPU operations\n+  const at::cuda::OptionalCUDAGuard device_guard(cache_device);\n+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+\n+  // Directly allocate tensors on GPU to avoid CPU-GPU synchronization.\n+  // Use empty instead of zeros since we'll overwrite all values immediately.\n+  torch::Tensor key_cache_ptrs_tensor =\n+      torch::empty({num_layers}, torch::TensorOptions()\n+                                      .dtype(torch::kInt64)\n+                                      .device(cache_device));\n+  torch::Tensor value_cache_ptrs_tensor =\n+      torch::empty({num_layers}, torch::TensorOptions()\n+                                      .dtype(torch::kInt64)\n+                                      .device(cache_device));\n+\n   // Create an array of pointers to the key and value caches.\n-  int64_t key_cache_ptrs[num_layers];\n-  int64_t value_cache_ptrs[num_layers];\n+  int64_t* key_cache_ptrs = key_cache_ptrs_tensor.data_ptr<int64_t>();\n+  int64_t* value_cache_ptrs = value_cache_ptrs_tensor.data_ptr<int64_t>();\n+\n+  // Fill the GPU tensors directly from GPU pointers\n+  std::vector<int64_t> key_ptrs_host(num_layers);\n+  std::vector<int64_t> value_ptrs_host(num_layers);\n   for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {\n-    key_cache_ptrs[layer_idx] =\n+    key_ptrs_host[layer_idx] =\n         reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());\n-    value_cache_ptrs[layer_idx] =\n+    value_ptrs_host[layer_idx] =\n         reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());\n   }\n \n-  // block_mapping is a 2D tensor with shape (num_pairs, 2).\n-  int num_pairs = block_mapping.size(0);\n-\n-  // Move the data structures to the GPU.\n-  // NOTE: This synchronizes the CPU and GPU.\n-  torch::Tensor key_cache_ptrs_tensor =\n-      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)\n-          .to(cache_device);\n-  torch::Tensor value_cache_ptrs_tensor =\n-      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)\n-          .to(cache_device);\n+  // Use async memcpy to transfer pointer arrays to GPU\n+  cudaMemcpyAsync(key_cache_ptrs, key_ptrs_host.data(),\n+                  num_layers * sizeof(int64_t),\n+                  cudaMemcpyHostToDevice, stream);\n+  cudaMemcpyAsync(value_cache_ptrs, value_ptrs_host.data(),\n+                  num_layers * sizeof(int64_t),\n+                  cudaMemcpyHostToDevice, stream);\n \n   // Launch the kernel.\n   const int numel_per_block = key_caches[0][0].numel();\n   dim3 grid(num_layers, num_pairs);\n   dim3 block(std::min(1024, numel_per_block));\n-  const at::cuda::OptionalCUDAGuard device_guard(cache_device);\n-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_AND_BYTE_TYPES(\n       key_caches[0].scalar_type(), \"copy_blocks_kernel\", ([&] {\n         vllm::copy_blocks_kernel<scalar_t><<<grid, block, 0, stream>>>(\n@@ -280,7 +293,13 @@ void reshape_and_cache(\n   int value_stride = value.stride(0);\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  // Optimize block size to be a multiple of warp size (32) for better occupancy\n+  int threads = num_heads * head_size;\n+  threads = std::min(threads, 1024);\n+  // Round down to nearest multiple of 32 for optimal warp utilization\n+  threads = (threads / 32) * 32;\n+  threads = std::max(threads, 32);\n+  dim3 block(threads);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \n@@ -321,7 +340,13 @@ void reshape_and_cache_flash(\n   TORCH_CHECK(key_cache.stride(0) == value_cache.stride(0));\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  // Optimize block size to be a multiple of warp size (32) for better occupancy\n+  int threads = num_heads * head_size;\n+  threads = std::min(threads, 1024);\n+  // Round down to nearest multiple of 32 for optimal warp utilization\n+  threads = (threads / 32) * 32;\n+  threads = std::max(threads, 32);\n+  dim3 block(threads);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \ndiff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex d37989055..227b712bc 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -80,7 +80,9 @@ class FlashAttentionImpl(AttentionImpl):\n         self.scale = float(scale)\n         self.num_kv_heads = num_kv_heads\n         if alibi_slopes is not None:\n-            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n+            # Create tensor directly on CUDA to avoid CPU allocation + transfer\n+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32,\n+                                       device='cuda')\n         self.alibi_slopes = alibi_slopes\n         if sliding_window is None:\n             self.sliding_window = (-1, -1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
