{"instance_id": "vllm-project__vllm-2f38518", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e79145300..675c64285 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -59,7 +59,7 @@ class InputBatch:\n         # Find a way to reduce the CPU memory usage.\n         # This buffer is not directly transferred to the GPU, so it does not\n         # need to be pinned.\n-        self.token_ids_cpu_tensor = torch.zeros(\n+        self.token_ids_cpu_tensor = torch.empty(\n             (max_num_reqs, max_model_len),\n             device=\"cpu\",\n             dtype=torch.int32,\n@@ -67,15 +67,15 @@ class InputBatch:\n         )\n         self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n-        self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n+        self.num_prompt_tokens = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros(\n+        self.block_table = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=self.device,\n             dtype=torch.int32,\n         )\n-        self.block_table_cpu_tensor = torch.zeros(\n+        self.block_table_cpu_tensor = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54e8..ced71df60 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -125,13 +125,13 @@ class GPUModelRunner:\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -143,32 +143,40 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                              dtype=torch.int32,\n                                              device=\"cpu\",\n                                              pin_memory=self.pin_memory)\n         self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n \n+        # Pre-allocated buffers for cascade attention to avoid repeated allocations\n+        self.cu_prefix_query_lens_buf = torch.empty(2,\n+                                                     dtype=torch.int32,\n+                                                     device=self.device)\n+        self.cu_prefix_kv_lens_buf = torch.empty(2,\n+                                                  dtype=torch.int32,\n+                                                  device=self.device)\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n         # Keep the states of the pre-empted requests.\n@@ -426,14 +434,14 @@ class GPUModelRunner:\n             )\n \n         if use_cascade:\n-            # TODO: Optimize.\n-            cu_prefix_query_lens = torch.tensor(\n-                [0, total_num_scheduled_tokens],\n-                dtype=torch.int32,\n-                device=self.device)\n-            cu_prefix_kv_lens = torch.tensor([0, common_prefix_len],\n-                                             dtype=torch.int32,\n-                                             device=self.device)\n+            # Use pre-allocated buffers to avoid repeated tensor allocations\n+            self.cu_prefix_query_lens_buf[0] = 0\n+            self.cu_prefix_query_lens_buf[1] = total_num_scheduled_tokens\n+            cu_prefix_query_lens = self.cu_prefix_query_lens_buf\n+\n+            self.cu_prefix_kv_lens_buf[0] = 0\n+            self.cu_prefix_kv_lens_buf[1] = common_prefix_len\n+            cu_prefix_kv_lens = self.cu_prefix_kv_lens_buf\n             cu_suffix_kv_lens = (\n                 self.seq_start_loc_np[:num_reqs + 1] -\n                 self.arange_np[:num_reqs + 1] * common_prefix_len)\n", "model_name_or_path": "gpt-5-2025-08-07"}
