{"instance_id": "vllm-project__vllm-92b0ce2", "model_patch": "diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b0f52167..aa1b77424 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -531,14 +531,13 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+                # Optimize: Use F.pad instead of creating zero tensor and cat\n+                _chunk_cu_seq_lens_padded = torch.nn.functional.pad(\n+                    _chunk_cu_seq_lens, (1, 0), value=0)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=_chunk_cu_seq_lens_padded,\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -861,6 +860,13 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         assert prefill_metadata.chunked_context is not None\n \n         output = None\n+        output_lse = None\n+        # Preallocate buffers for merge operation to avoid repeated allocations\n+        output_tmp = None\n+        output_lse_tmp = None\n+        # Preallocate buffers for concatenated k tensor\n+        k_buffer = None\n+        v_padded_buffer = None\n         iters = len(prefill_metadata.chunked_context.seq_tot)\n         workspace = prefill_metadata.chunked_context.workspace\n \n@@ -886,14 +892,33 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n             k_nope, v = kv_nope\\\n                 .split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n \n-            k = torch.cat((k_nope, k_pe.expand((*k_nope.shape[:-1], -1))),\n-                          dim=-1)\n+            # Optimize: Preallocate k buffer on first iteration\n+            k_pe_expanded = k_pe.expand((*k_nope.shape[:-1], -1))\n+            qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n+            if k_buffer is None or k_buffer.shape[0] != toks:\n+                k_buffer = torch.empty(\n+                    (toks, self.num_heads, qk_head_dim),\n+                    dtype=k_nope.dtype,\n+                    device=k_nope.device\n+                )\n+            k_buffer[:toks, :, :self.qk_nope_head_dim] = k_nope\n+            k_buffer[:toks, :, self.qk_nope_head_dim:] = k_pe_expanded\n+            k = k_buffer[:toks]\n \n             # For MLA the v head dim is smaller than qk head dim so we pad\n             # out v with 0s to match the qk head dim\n-            v_padded = torch.nn.functional.pad(v,\n-                                               [0, q.shape[-1] - v.shape[-1]],\n-                                               value=0)\n+            # Optimize: Preallocate v_padded buffer on first iteration\n+            if v_padded_buffer is None or v_padded_buffer.shape[0] != toks:\n+                v_padded_buffer = torch.empty(\n+                    (toks, self.num_heads, q.shape[-1]),\n+                    dtype=v.dtype,\n+                    device=v.device\n+                )\n+            # Zero the padding region and copy v\n+            v_padded_buffer[:toks, :, :v.shape[-1]] = v\n+            if v.shape[-1] < q.shape[-1]:\n+                v_padded_buffer[:toks, :, v.shape[-1]:].zero_()\n+            v_padded = v_padded_buffer[:toks]\n \n             attn_output, attn_softmax_lse = self.flash_attn_varlen_func(\n                 q=q,\n@@ -912,8 +937,10 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                # Reuse preallocated buffers or create them on first merge\n+                if output_tmp is None:\n+                    output_tmp = torch.empty_like(output)\n+                    output_lse_tmp = torch.empty_like(output_lse)\n                 merge_attn_states(\n                     output=output_tmp,\n                     output_lse=output_lse_tmp,\n@@ -922,8 +949,9 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                # Swap buffers instead of copying\n+                output, output_tmp = output_tmp, output\n+                output_lse, output_lse_tmp = output_lse_tmp, output_lse\n \n         return output, output_lse\n \n@@ -943,12 +971,28 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         k_nope, v = kv_nope\\\n             .split([self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n \n-        k = torch.cat((k_nope, k_pe.expand((*k_nope.shape[:-1], -1))), dim=-1)\n+        # Optimize: Use tensor slicing instead of cat to reduce allocations\n+        k_pe_expanded = k_pe.expand((*k_nope.shape[:-1], -1))\n+        qk_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n+        k = torch.empty(\n+            (*k_nope.shape[:-1], qk_head_dim),\n+            dtype=k_nope.dtype,\n+            device=k_nope.device\n+        )\n+        k[..., :self.qk_nope_head_dim] = k_nope\n+        k[..., self.qk_nope_head_dim:] = k_pe_expanded\n \n         # For MLA the v head dim is smaller than qk head dim so we pad out\n         # v with 0s to match the qk head dim\n-        v_padded = torch.nn.functional.pad(v, [0, q.shape[-1] - v.shape[-1]],\n-                                           value=0)\n+        # Optimize: Use torch.empty and slicing instead of pad\n+        v_padded = torch.empty(\n+            (*v.shape[:-1], q.shape[-1]),\n+            dtype=v.dtype,\n+            device=v.device\n+        )\n+        v_padded[..., :v.shape[-1]] = v\n+        if v.shape[-1] < q.shape[-1]:\n+            v_padded[..., v.shape[-1]:].zero_()\n \n         output = self.flash_attn_varlen_func(\n             q=q,\n", "model_name_or_path": "gpt-5-2025-08-07"}
