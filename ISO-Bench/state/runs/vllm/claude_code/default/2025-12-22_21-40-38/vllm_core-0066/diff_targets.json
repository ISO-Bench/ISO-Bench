{
  "changed": [
    "vllm/attention/backends/triton_mla.py",
    "vllm/model_executor/layers/quantization/utils/quant_utils.py",
    "vllm/worker/cache_engine.py"
  ],
  "allowed": [
    "vllm/attention/layer.py",
    "vllm/model_executor/model_loader/loader.py",
    "vllm/attention/backends/mla/utils.py",
    "vllm/model_executor/layers/quantization/utils/quant_utils.py",
    "vllm/attention/backends/triton_mla.py",
    "vllm/model_executor/models/deepseek_v3.py",
    "vllm/worker/cache_engine.py",
    "vllm/envs.py",
    "vllm/config.py",
    "vllm/model_executor/layers/quantization/utils/fp8_utils.py"
  ],
  "disallowed": [],
  "ok": true
}