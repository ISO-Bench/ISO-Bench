{"instance_id": "vllm-project__vllm-dd2a6a8", "model_patch": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex fc6f829c3..f4bc57c55 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -189,7 +189,8 @@ def test_with_prefix_caching(\n     full_prompts = [f\"{common_prompt}\\n{p}\" for p in unique_prompts]\n \n     max_num_batched_tokens = max_num_seqs = chunk_size\n-    outputs = {}  # type: ignore\n+    # Optimize: Pre-allocate dict to avoid resize\n+    outputs = {True: [], False: []}  # type: ignore\n     check_result = True\n     for enable in (True, False):\n         with vllm_runner(\n@@ -207,12 +208,12 @@ def test_with_prefix_caching(\n             # size is not a multiple of block size (16).\n             should_fail = chunk_size % 16 != 0 and enable\n             check_result &= not should_fail\n-            outputs[enable] = []\n             # Send the request one-by-one to ensure the cache is populated.\n             with pytest.raises(ValueError) if should_fail else nullcontext():\n                 for prompt in full_prompts:\n-                    outputs[enable] += vllm_model.generate_greedy([prompt],\n-                                                                  max_tokens)\n+                    # Optimize: Use extend instead of += to avoid intermediate list\n+                    outputs[enable].extend(vllm_model.generate_greedy([prompt],\n+                                                                       max_tokens))\n \n     # Check results only if we did not expect a failure.\n     if check_result:\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 4c2f71582..869b0953c 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -935,14 +935,12 @@ class Scheduler:\n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n         # Update new running requests.\n-        if len(prefills.seq_groups) > 0:\n-            self.running.extend([s.seq_group for s in prefills.seq_groups])\n+        # Optimize: Use generator expressions to avoid intermediate list allocation\n+        self.running.extend(s.seq_group for s in prefills.seq_groups)\n \n         self.running.extend(running_scheduled.decode_seq_groups_list)\n \n-        if len(swapped_in.decode_seq_groups) > 0:\n-            self.running.extend(\n-                [s.seq_group for s in swapped_in.decode_seq_groups])\n+        self.running.extend(s.seq_group for s in swapped_in.decode_seq_groups)\n \n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n@@ -954,18 +952,18 @@ class Scheduler:\n         assert len(running_scheduled.prefill_seq_groups) == 0\n         assert len(swapped_in.prefill_seq_groups) == 0\n \n-        # Merge lists\n+        # Optimize: Build scheduled_seq_groups using extend to avoid extra allocation\n         num_prefill_groups = len(prefills.seq_groups)\n-        if num_prefill_groups > 0:\n-            scheduled_seq_groups = prefills.seq_groups\n-            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n-        else:\n-            scheduled_seq_groups = running_scheduled.decode_seq_groups\n+        scheduled_seq_groups = []\n+        scheduled_seq_groups.extend(prefills.seq_groups)\n+        scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n         scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n \n+        # Optimize: Build blocks_to_copy using extend instead of concatenation\n         blocks_to_copy = running_scheduled.blocks_to_copy\n         blocks_to_copy.extend(swapped_in.blocks_to_copy)\n \n+        # Optimize: Build ignored_seq_groups using extend instead of concatenation\n         ignored_seq_groups = prefills.ignored_seq_groups\n         ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n \n@@ -1028,33 +1026,41 @@ class Scheduler:\n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n         # Update new running requests.\n-        self.running.extend([s.seq_group for s in prefills.seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in running_scheduled.decode_seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in swapped_in.decode_seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in swapped_in.prefill_seq_groups])\n+        # Optimize: Use generator expressions and cached lists to reduce allocations\n+        self.running.extend(s.seq_group for s in prefills.seq_groups)\n+        self.running.extend(running_scheduled.decode_seq_groups_list)\n+        self.running.extend(running_scheduled.prefill_seq_groups_list)\n+        self.running.extend(s.seq_group for s in swapped_in.decode_seq_groups)\n+        self.running.extend(s.seq_group for s in swapped_in.prefill_seq_groups)\n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n+\n+        # Optimize: Build scheduled_seq_groups using extend instead of concatenation\n+        scheduled_seq_groups = []\n+        scheduled_seq_groups.extend(prefills.seq_groups)\n+        scheduled_seq_groups.extend(running_scheduled.prefill_seq_groups)\n+        scheduled_seq_groups.extend(swapped_in.prefill_seq_groups)\n+        scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n+        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n+\n+        # Optimize: Build blocks_to_copy using extend instead of concatenation\n+        blocks_to_copy = running_scheduled.blocks_to_copy\n+        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n+\n+        # Optimize: Build ignored_seq_groups using extend instead of concatenation\n+        ignored_seq_groups = prefills.ignored_seq_groups\n+        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n+\n         return SchedulerOutputs(\n-            scheduled_seq_groups=(prefills.seq_groups +\n-                                  running_scheduled.prefill_seq_groups +\n-                                  swapped_in.prefill_seq_groups +\n-                                  running_scheduled.decode_seq_groups +\n-                                  swapped_in.decode_seq_groups),\n+            scheduled_seq_groups=scheduled_seq_groups,\n             num_prefill_groups=(len(prefills.seq_groups) +\n                                 len(swapped_in.prefill_seq_groups) +\n                                 len(running_scheduled.prefill_seq_groups)),\n             num_batched_tokens=budget.num_batched_tokens,\n             blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n             blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n-            blocks_to_copy=running_scheduled.blocks_to_copy +\n-            swapped_in.blocks_to_copy,\n-            ignored_seq_groups=prefills.ignored_seq_groups +\n-            swapped_in.infeasible_seq_groups,\n+            blocks_to_copy=blocks_to_copy,\n+            ignored_seq_groups=ignored_seq_groups,\n             num_lookahead_slots=running_scheduled.num_lookahead_slots,\n             running_queue_size=len(self.running),\n             preempted=(len(running_scheduled.preempted) +\n@@ -1119,13 +1125,11 @@ class Scheduler:\n \n             seq_group_metadata = self._seq_group_metadata_cache[\n                 self.cache_id].get_object()\n-            seq_group_metadata.seq_data.clear()\n-            seq_group_metadata.block_tables.clear()\n-\n-            # seq_id -> SequenceData\n-            seq_data: Dict[int, SequenceData] = {}\n-            # seq_id -> physical block numbers\n-            block_tables: Dict[int, List[int]] = {}\n+            # Optimize: Reuse existing dicts instead of clearing and recreating\n+            seq_data = seq_group_metadata.seq_data\n+            seq_data.clear()\n+            block_tables = seq_group_metadata.block_tables\n+            block_tables.clear()\n \n             if seq_group.is_encoder_decoder():\n                 # Encoder associated with SequenceGroup\n", "model_name_or_path": "gpt-5-2025-08-07"}
