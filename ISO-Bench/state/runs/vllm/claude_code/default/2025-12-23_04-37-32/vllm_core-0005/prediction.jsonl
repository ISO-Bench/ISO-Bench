{"instance_id": "vllm-project__vllm-b0e96aa", "model_patch": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex 79787303a..0dd10183a 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -251,12 +251,12 @@ class LRUCache(Generic[_K, _V]):\n \n     def get(self, key: _K, default: Optional[_V] = None) -> Optional[_V]:\n         value: Optional[_V]\n-        if key in self.cache:\n+        # Use try-except to avoid double dictionary lookup\n+        try:\n             value = self.cache[key]\n             self.cache.move_to_end(key)\n-\n             self._hits += 1\n-        else:\n+        except KeyError:\n             value = default\n \n         self._total += 1\n@@ -327,15 +327,14 @@ class PyObjectCache:\n         self._obj_builder = obj_builder\n         self._index = 0\n \n-        self._obj_cache = []\n-        for _ in range(128):\n-            self._obj_cache.append(self._obj_builder())\n+        # Use list comprehension for better performance\n+        self._obj_cache = [self._obj_builder() for _ in range(128)]\n \n     def _grow_cache(self):\n         # Double the size of the cache\n         num_objs = len(self._obj_cache)\n-        for _ in range(num_objs):\n-            self._obj_cache.append(self._obj_builder())\n+        # Use list extension with comprehension for better performance\n+        self._obj_cache.extend([self._obj_builder() for _ in range(num_objs)])\n \n     def get_object(self):\n         \"\"\"Returns a pre-allocated cached object. If there is not enough\n@@ -818,7 +817,15 @@ def async_tensor_h2d(\n     pin_memory: bool,\n ) -> torch.Tensor:\n     \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n-    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n+    # Convert to numpy first for better performance with large lists\n+    np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE.get(dtype)\n+    if np_dtype is not None:\n+        np_array = np.array(data, dtype=np_dtype)\n+        t = torch.from_numpy(np_array)\n+    else:\n+        t = torch.tensor(data, dtype=dtype, device=\"cpu\")\n+    if pin_memory:\n+        t = t.pin_memory()\n     return t.to(device=target_device, non_blocking=True)\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
