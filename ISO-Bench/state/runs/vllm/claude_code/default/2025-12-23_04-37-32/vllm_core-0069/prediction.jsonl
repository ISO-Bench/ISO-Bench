{"instance_id": "vllm-project__vllm-cf2f084", "model_patch": "diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 442173939..62440d001 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -41,7 +41,7 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n         if prev_tokens is None:\n             prev_tokens = new_tokens\n         else:\n-            prev_tokens += new_tokens\n+            prev_tokens.extend(new_tokens)\n     return decoded_text\n \n \ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 724782841..29c45d672 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -737,8 +737,10 @@ class LLMEngine:\n     def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:\n         \"\"\"Decodes the new token for a sequence.\"\"\"\n         all_input_ids = seq.get_token_ids()\n-        self._decode_logprobs(seq, prms, seq.output_logprobs[-1],\n-                              all_input_ids)\n+        output_logprobs = seq.output_logprobs\n+        if output_logprobs:\n+            self._decode_logprobs(seq, prms, output_logprobs[-1],\n+                                  all_input_ids)\n \n         (new_tokens, new_output_text, prefix_offset,\n          read_offset) = detokenize_incrementally(\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19a8..9d99e6f97 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -22,10 +22,13 @@ def get_cached_tokenizer(\n     each time they are called, leading to a significant slowdown. This\n     function caches these properties for faster access.\"\"\"\n \n+    # Cache as sets for O(1) membership testing\n     tokenizer_all_special_ids = set(tokenizer.all_special_ids)\n     tokenizer_all_special_tokens_extended = (\n         tokenizer.all_special_tokens_extended)\n     tokenizer_all_special_tokens = set(tokenizer.all_special_tokens)\n+    # Pre-cache added_vocab for performance\n+    tokenizer_added_vocab = tokenizer.get_added_vocab()\n \n     class CachedTokenizer(tokenizer.__class__):\n \n@@ -41,6 +44,9 @@ def get_cached_tokenizer(\n         def all_special_tokens_extended(self):\n             return tokenizer_all_special_tokens_extended\n \n+        def get_added_vocab(self):\n+            return tokenizer_added_vocab\n+\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     tokenizer.__class__ = CachedTokenizer\n@@ -132,30 +138,35 @@ def _convert_tokens_to_string_with_added_encoders(\n ) -> str:\n     # Adapted from\n     # https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/tokenization_utils.py#L921\n-    # NOTE(woosuk): The following code is slow because it runs a for loop over\n-    # the output_tokens. In Python, running a for loop over a list can be slow\n-    # even when the loop body is very simple.\n+    # Optimized to reduce overhead from repeated function calls and membership tests\n+    if not output_tokens:\n+        return \"\"\n+\n+    # Use cached properties to avoid repeated lookups\n+    all_special_tokens = tokenizer.all_special_tokens\n+    added_vocab = tokenizer.get_added_vocab()\n+\n     sub_texts = []\n     current_sub_text = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+\n     for token in output_tokens:\n         if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.get_added_vocab():\n+        if token in added_vocab:\n             if current_sub_text:\n-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-                sub_texts.append(sub_text)\n+                sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n                 current_sub_text = []\n             sub_texts.append(token)\n         else:\n             current_sub_text.append(token)\n+\n     if current_sub_text:\n-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-        sub_texts.append(sub_text)\n-    if spaces_between_special_tokens:\n-        return \" \".join(sub_texts)\n-    else:\n-        return \"\".join(sub_texts)\n+        sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n+\n+    if not sub_texts:\n+        return \"\"\n+\n+    return \" \".join(sub_texts) if spaces_between_special_tokens else \"\".join(sub_texts)\n \n \n # Based on\n@@ -179,12 +190,13 @@ def detokenize_incrementally(\n         # 5 is an arbitrary value that should work for all\n         # tokenizers (bigger = more conservative).\n         # Subtract 1 extra to account for the generated token.\n-        prefix_offset = max(len(output_tokens) - 6, 0)\n+        output_len = len(output_tokens)\n+        prefix_offset = max(output_len - 6, 0)\n         # If the first new token is a special token, we can't skip 1 extra token\n         if skip_special_tokens and new_token_id in tokenizer.all_special_ids:\n-            read_offset = max(len(output_tokens), 0)\n+            read_offset = output_len\n         else:\n-            read_offset = max(len(output_tokens) - 1, 0)\n+            read_offset = max(output_len - 1, 0)\n     else:\n         # Put new_token_id in a list so skip_special_tokens is respected\n         new_tokens = tokenizer.convert_ids_to_tokens(\n", "model_name_or_path": "gpt-5-2025-08-07"}
