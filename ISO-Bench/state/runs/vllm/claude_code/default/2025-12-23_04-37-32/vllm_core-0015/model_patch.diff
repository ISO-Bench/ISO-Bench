diff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py
index fda4d007c..ef98d805b 100644
--- a/vllm/distributed/device_communicators/pynccl.py
+++ b/vllm/distributed/device_communicators/pynccl.py
@@ -98,7 +98,7 @@ class PyNcclCommunicator:
 
             stream = torch.cuda.current_stream()
             # A small all_reduce for warmup.
-            data = torch.zeros(1, device=device)
+            data = torch.empty(1, device=device)
             self.all_reduce(data)
             stream.synchronize()
             del data
diff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py
index a837c1dc5..928acbe74 100644
--- a/vllm/distributed/parallel_state.py
+++ b/vllm/distributed/parallel_state.py
@@ -1209,7 +1209,7 @@ def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],
         ranks = list(range(world_size))
 
     # local tensor in each process to store the result
-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)
+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)
 
     magic_message = b"magic_message"
     shm = None
diff --git a/vllm/utils.py b/vllm/utils.py
index a92b77efd..2b0edea68 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -663,7 +663,7 @@ def create_kv_caches_with_random(
     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)
 
     scale = head_size**-0.5
-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()
+    x = 16 // get_dtype_size(torch_dtype)
     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)
     key_caches: List[torch.Tensor] = []
     for _ in range(num_layers):
@@ -792,9 +792,10 @@ def async_tensor_h2d(
     return t.to(device=target_device, non_blocking=True)
 
 
+@lru_cache(maxsize=None)
 def get_dtype_size(dtype: torch.dtype) -> int:
     """Get the size of the data type in bytes."""
-    return torch.tensor([], dtype=dtype).element_size()
+    return torch.empty(0, dtype=dtype).element_size()
 
 
 # `collections` helpers
diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py
index a2c2cebf8..2def51aa3 100644
--- a/vllm/worker/multi_step_model_runner.py
+++ b/vllm/worker/multi_step_model_runner.py
@@ -480,7 +480,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):
         # if CPU is ahead.
         if self.is_driver_worker and get_pp_group().is_last_rank:
             if self.pinned_sampled_token_ids is None:
-                self.pinned_sampled_token_ids = torch.zeros(
+                self.pinned_sampled_token_ids = torch.empty(
                     (self.scheduler_config.max_num_seqs, 1),
                     dtype=torch.long,
                     device="cpu",
