{"instance_id": "vllm-project__vllm-a732900", "model_patch": "diff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py\nindex fda4d007c..ef98d805b 100644\n--- a/vllm/distributed/device_communicators/pynccl.py\n+++ b/vllm/distributed/device_communicators/pynccl.py\n@@ -98,7 +98,7 @@ class PyNcclCommunicator:\n \n             stream = torch.cuda.current_stream()\n             # A small all_reduce for warmup.\n-            data = torch.zeros(1, device=device)\n+            data = torch.empty(1, device=device)\n             self.all_reduce(data)\n             stream.synchronize()\n             del data\ndiff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py\nindex a837c1dc5..928acbe74 100644\n--- a/vllm/distributed/parallel_state.py\n+++ b/vllm/distributed/parallel_state.py\n@@ -1209,7 +1209,7 @@ def in_the_same_node_as(pg: Union[ProcessGroup, StatelessProcessGroup],\n         ranks = list(range(world_size))\n \n     # local tensor in each process to store the result\n-    is_in_the_same_node = torch.tensor([0] * world_size, dtype=torch.int32)\n+    is_in_the_same_node = torch.zeros(world_size, dtype=torch.int32)\n \n     magic_message = b\"magic_message\"\n     shm = None\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex a92b77efd..2b0edea68 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -663,7 +663,7 @@ def create_kv_caches_with_random(\n     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n \n     scale = head_size**-0.5\n-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n+    x = 16 // get_dtype_size(torch_dtype)\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n     key_caches: List[torch.Tensor] = []\n     for _ in range(num_layers):\n@@ -792,9 +792,10 @@ def async_tensor_h2d(\n     return t.to(device=target_device, non_blocking=True)\n \n \n+@lru_cache(maxsize=None)\n def get_dtype_size(dtype: torch.dtype) -> int:\n     \"\"\"Get the size of the data type in bytes.\"\"\"\n-    return torch.tensor([], dtype=dtype).element_size()\n+    return torch.empty(0, dtype=dtype).element_size()\n \n \n # `collections` helpers\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex a2c2cebf8..2def51aa3 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -480,7 +480,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         # if CPU is ahead.\n         if self.is_driver_worker and get_pp_group().is_last_rank:\n             if self.pinned_sampled_token_ids is None:\n-                self.pinned_sampled_token_ids = torch.zeros(\n+                self.pinned_sampled_token_ids = torch.empty(\n                     (self.scheduler_config.max_num_seqs, 1),\n                     dtype=torch.long,\n                     device=\"cpu\",\n", "model_name_or_path": "gpt-5-2025-08-07"}
