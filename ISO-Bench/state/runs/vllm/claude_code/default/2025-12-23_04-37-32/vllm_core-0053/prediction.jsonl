{"instance_id": "vllm-project__vllm-beebf47", "model_patch": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed86..78f70fb4d 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -553,8 +553,10 @@ class LLM:\n             instances.append(BeamSearchInstance(prompt_tokens))\n \n         for _ in range(max_tokens):\n-            all_beams: list[BeamSearchSequence] = list(\n-                sum((instance.beams for instance in instances), []))\n+            # Optimize list concatenation using extend instead of sum\n+            all_beams: list[BeamSearchSequence] = []\n+            for instance in instances:\n+                all_beams.extend(instance.beams)\n             pos = [0] + list(\n                 itertools.accumulate(\n                     len(instance.beams) for instance in instances))\n@@ -1383,11 +1385,12 @@ class LLM:\n                             # Calculate tokens only for RequestOutput\n                             assert output.prompt_token_ids is not None\n                             total_in_toks += len(output.prompt_token_ids)\n-                            in_spd = total_in_toks / pbar.format_dict[\"elapsed\"]\n+                            # Cache format_dict access\n+                            elapsed = pbar.format_dict[\"elapsed\"]\n+                            in_spd = total_in_toks / elapsed\n                             total_out_toks += sum(\n                                 len(stp.token_ids) for stp in output.outputs)\n-                            out_spd = (total_out_toks /\n-                                       pbar.format_dict[\"elapsed\"])\n+                            out_spd = total_out_toks / elapsed\n                             pbar.postfix = (\n                                 f\"est. speed input: {in_spd:.2f} toks/s, \"\n                                 f\"output: {out_spd:.2f} toks/s\")\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0fc..fe88bdc42 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -296,8 +296,10 @@ class OpenAIServingChat(OpenAIServing):\n         num_prompt_tokens = 0\n         num_cached_tokens = None\n \n-        if isinstance(request.tool_choice, ChatCompletionNamedToolChoiceParam):\n-            tool_choice_function_name = request.tool_choice.function.name\n+        # Cache request attributes for faster access\n+        request_tool_choice = request.tool_choice\n+        if isinstance(request_tool_choice, ChatCompletionNamedToolChoiceParam):\n+            tool_choice_function_name = request_tool_choice.function.name\n         else:\n             tool_choice_function_name = None\n \n@@ -316,7 +318,8 @@ class OpenAIServingChat(OpenAIServing):\n         if tool_choice_auto or should_stream_with_reasoning_parsing:\n             # These are only required in \"auto\" tool choice case\n             previous_texts = [\"\"] * num_choices\n-            all_previous_token_ids = [[]] * num_choices\n+            # Fixed: Use list comprehension to avoid shared references\n+            all_previous_token_ids = [[] for _ in range(num_choices)]\n         else:\n             previous_texts, all_previous_token_ids = None, None\n \n@@ -338,9 +341,10 @@ class OpenAIServingChat(OpenAIServing):\n         # Prepare the tool parser if it's needed\n         try:\n             if tool_choice_auto and self.tool_parser:\n+                # Create separate parser instances for each choice\n                 tool_parsers: list[Optional[ToolParser]] = [\n-                    self.tool_parser(tokenizer)\n-                ] * num_choices\n+                    self.tool_parser(tokenizer) for _ in range(num_choices)\n+                ]\n             else:\n                 tool_parsers = [None] * num_choices\n         except Exception as e:\n@@ -358,6 +362,11 @@ class OpenAIServingChat(OpenAIServing):\n         else:\n             include_usage, include_continuous_usage = False, False\n \n+        # Cache frequently accessed request attributes\n+        request_echo = request.echo\n+        request_logprobs = request.logprobs\n+        request_top_logprobs = request.top_logprobs\n+\n         try:\n             async for res in result_generator:\n                 if res.prompt_token_ids is not None:\n@@ -404,7 +413,7 @@ class OpenAIServingChat(OpenAIServing):\n \n                     # Send response to echo the input portion of the\n                     # last message\n-                    if request.echo:\n+                    if request_echo:\n                         last_msg_content: Union[str, list[dict[str, str]]] = \"\"\n                         if conversation and \"content\" in conversation[\n                                 -1] and conversation[-1].get(\"role\") == role:\n@@ -443,14 +452,14 @@ class OpenAIServingChat(OpenAIServing):\n                     if finish_reason_sent[i]:\n                         continue\n \n-                    if request.logprobs and request.top_logprobs is not None:\n+                    if request_logprobs and request_top_logprobs is not None:\n                         assert output.logprobs is not None, (\n                             \"Did not output logprobs\")\n                         logprobs = self._create_chat_logprobs(\n                             token_ids=output.token_ids,\n                             top_logprobs=output.logprobs,\n                             tokenizer=tokenizer,\n-                            num_output_top_logprobs=request.top_logprobs,\n+                            num_output_top_logprobs=request_top_logprobs,\n                         )\n                     else:\n                         logprobs = None\n@@ -695,16 +704,24 @@ class OpenAIServingChat(OpenAIServing):\n         choices: list[ChatCompletionResponseChoice] = []\n \n         role = self.get_chat_request_role(request)\n+\n+        # Cache request attributes for performance\n+        request_logprobs = request.logprobs\n+        request_top_logprobs = request.top_logprobs\n+        request_tool_choice = request.tool_choice\n+        request_tools = request.tools\n+        request_echo = request.echo\n+\n         for output in final_res.outputs:\n             token_ids = output.token_ids\n             out_logprobs = output.logprobs\n \n-            if request.logprobs and request.top_logprobs is not None:\n+            if request_logprobs and request_top_logprobs is not None:\n                 assert out_logprobs is not None, \"Did not output logprobs\"\n                 logprobs = self._create_chat_logprobs(\n                     token_ids=token_ids,\n                     top_logprobs=out_logprobs,\n-                    num_output_top_logprobs=request.top_logprobs,\n+                    num_output_top_logprobs=request_top_logprobs,\n                     tokenizer=tokenizer,\n                 )\n             else:\n@@ -741,12 +758,12 @@ class OpenAIServingChat(OpenAIServing):\n             #   outlines is not being used\n             elif (not self.enable_auto_tools\n                   or not self.tool_parser) and not isinstance(\n-                      request.tool_choice, ChatCompletionNamedToolChoiceParam):\n+                      request_tool_choice, ChatCompletionNamedToolChoiceParam):\n                 message = ChatMessage(role=role, content=output.text)\n \n             # if the request uses tools and specified a tool choice\n-            elif request.tool_choice and type(\n-                    request.tool_choice) is ChatCompletionNamedToolChoiceParam:\n+            elif request_tool_choice and type(\n+                    request_tool_choice) is ChatCompletionNamedToolChoiceParam:\n \n                 tool_call_class = MistralToolCall if isinstance(\n                     tokenizer, MistralTokenizer) else ToolCall\n@@ -755,20 +772,20 @@ class OpenAIServingChat(OpenAIServing):\n                     content=\"\",\n                     tool_calls=[\n                         tool_call_class(function=FunctionCall(\n-                            name=request.tool_choice.function.name,\n+                            name=request_tool_choice.function.name,\n                             arguments=output.text))\n                     ])\n \n             # if the request doesn't use tool choice\n             # OR specifies to not use a tool\n-            elif not request.tool_choice or request.tool_choice == \"none\":\n+            elif not request_tool_choice or request_tool_choice == \"none\":\n \n                 message = ChatMessage(role=role, content=output.text)\n \n             # handle when there are tools and tool choice is auto\n-            elif request.tools and (\n-                    request.tool_choice == \"auto\"\n-                    or request.tool_choice is None) and self.enable_auto_tools \\\n+            elif request_tools and (\n+                    request_tool_choice == \"auto\"\n+                    or request_tool_choice is None) and self.enable_auto_tools \\\n                     and self.tool_parser:\n \n                 try:\n@@ -810,7 +827,7 @@ class OpenAIServingChat(OpenAIServing):\n                 stop_reason=output.stop_reason)\n             choices.append(choice_data)\n \n-        if request.echo:\n+        if request_echo:\n             last_msg_content: Union[str, list[dict[str, str]]] = \"\"\n             if conversation and \"content\" in conversation[-1] and conversation[\n                     -1].get(\"role\") == role:\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af84f..2e46cc57e 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -246,9 +246,10 @@ class OpenAIServingCompletion(OpenAIServing):\n         request_metadata: RequestResponseMetadata,\n     ) -> AsyncGenerator[str, None]:\n         num_choices = 1 if request.n is None else request.n\n-        previous_text_lens = [0] * num_choices * num_prompts\n-        previous_num_tokens = [0] * num_choices * num_prompts\n-        has_echoed = [False] * num_choices * num_prompts\n+        total_size = num_choices * num_prompts\n+        previous_text_lens = [0] * total_size\n+        previous_num_tokens = [0] * total_size\n+        has_echoed = [False] * total_size\n         num_prompt_tokens = [0] * num_prompts\n \n         stream_options = request.stream_options\n@@ -259,6 +260,11 @@ class OpenAIServingCompletion(OpenAIServing):\n         else:\n             include_usage, include_continuous_usage = False, False\n \n+        # Cache request attributes for performance\n+        request_max_tokens = request.max_tokens\n+        request_echo = request.echo\n+        request_logprobs = request.logprobs\n+\n         try:\n             async for prompt_idx, res in result_generator:\n                 prompt_token_ids = res.prompt_token_ids\n@@ -276,11 +282,11 @@ class OpenAIServingCompletion(OpenAIServing):\n                 for output in res.outputs:\n                     i = output.index + prompt_idx * num_choices\n \n-                    assert request.max_tokens is not None\n-                    if request.echo and not has_echoed[i]:\n+                    assert request_max_tokens is not None\n+                    if request_echo and not has_echoed[i]:\n                         assert prompt_token_ids is not None\n                         assert prompt_text is not None\n-                        if request.max_tokens == 0:\n+                        if request_max_tokens == 0:\n                             # only return the prompt\n                             delta_text = prompt_text\n                             delta_token_ids = prompt_token_ids\n@@ -308,13 +314,13 @@ class OpenAIServingCompletion(OpenAIServing):\n                             # Chunked prefill case, don't return empty chunks\n                             continue\n \n-                    if request.logprobs is not None:\n+                    if request_logprobs is not None:\n                         assert out_logprobs is not None, (\n                             \"Did not output logprobs\")\n                         logprobs = self._create_completion_logprobs(\n                             token_ids=delta_token_ids,\n                             top_logprobs=out_logprobs,\n-                            num_output_top_logprobs=request.logprobs,\n+                            num_output_top_logprobs=request_logprobs,\n                             tokenizer=tokenizer,\n                             initial_text_offset=previous_text_lens[i],\n                         )\n@@ -393,6 +399,11 @@ class OpenAIServingCompletion(OpenAIServing):\n         num_prompt_tokens = 0\n         num_generated_tokens = 0\n \n+        # Cache request attributes for performance\n+        request_max_tokens = request.max_tokens\n+        request_echo = request.echo\n+        request_logprobs = request.logprobs\n+\n         for final_res in final_res_batch:\n             prompt_token_ids = final_res.prompt_token_ids\n             assert prompt_token_ids is not None\n@@ -410,17 +421,17 @@ class OpenAIServingCompletion(OpenAIServing):\n                                                                  Logprob]]]]\n \n             for output in final_res.outputs:\n-                assert request.max_tokens is not None\n-                if request.echo:\n+                assert request_max_tokens is not None\n+                if request_echo:\n                     assert prompt_text is not None\n-                    if request.max_tokens == 0:\n+                    if request_max_tokens == 0:\n                         token_ids = prompt_token_ids\n                         out_logprobs = prompt_logprobs\n                         output_text = prompt_text\n                     else:\n                         token_ids = [*prompt_token_ids, *output.token_ids]\n \n-                        if request.logprobs is None:\n+                        if request_logprobs is None:\n                             out_logprobs = None\n                         else:\n                             assert prompt_logprobs is not None\n@@ -436,13 +447,13 @@ class OpenAIServingCompletion(OpenAIServing):\n                     out_logprobs = output.logprobs\n                     output_text = output.text\n \n-                if request.logprobs is not None:\n+                if request_logprobs is not None:\n                     assert out_logprobs is not None, \"Did not output logprobs\"\n                     logprobs = self._create_completion_logprobs(\n                         token_ids=token_ids,\n                         top_logprobs=out_logprobs,\n                         tokenizer=tokenizer,\n-                        num_output_top_logprobs=request.logprobs,\n+                        num_output_top_logprobs=request_logprobs,\n                     )\n                 else:\n                     logprobs = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
