diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py
index 5601c62e9..81fe2ec4e 100644
--- a/vllm/v1/sample/rejection_sampler.py
+++ b/vllm/v1/sample/rejection_sampler.py
@@ -133,7 +133,7 @@ class RejectionSampler(nn.Module):
             # Generate mask with bonus token.
             generate_mask = torch.cat([
                 accept_mask,
-                torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)
+                torch.empty(accept_mask.size(0), 1, device=accept_mask.device)
             ],
                                       dim=1).to(torch.bool) & valid_mask
             zeros_mask = (generate_mask == 0)
@@ -204,7 +204,7 @@ class RejectionSampler(nn.Module):
             # Generate mask with bonus tokens.
             generate_mask = torch.cat([
                 accept_mask,
-                torch.zeros(batch_size, 1, device=accept_mask.device)
+                torch.empty(batch_size, 1, device=accept_mask.device)
             ],
                                       dim=1).to(torch.bool)
             zeros_mask = (generate_mask == 0)
@@ -266,7 +266,7 @@ def _create_greedy_token_probs(
 ) -> torch.Tensor:
     batch_size, num_tokens = token_ids.shape
 
-    token_probs = torch.zeros(batch_size,
+    token_probs = torch.empty(batch_size,
                               num_tokens,
                               vocab_size,
                               dtype=torch.float,
@@ -277,6 +277,7 @@ def _create_greedy_token_probs(
     valid_indices = token_ids.clone()
     valid_indices[~valid_mask] = 0
 
+    token_probs.zero_()
     token_probs.scatter_(dim=2,
                          index=valid_indices.unsqueeze(-1),
                          src=valid_mask.unsqueeze(-1).float())
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 66015382b..f87a84fda 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -234,27 +234,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
