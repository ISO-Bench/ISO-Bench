{"instance_id": "vllm-project__vllm-7206ce4", "model_patch": "diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..a36695e36 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -324,7 +324,7 @@ async def ping(raw_request: Request) -> Response:\n @router.post(\"/tokenize\")\n @with_cancellation\n async def tokenize(request: TokenizeRequest, raw_request: Request):\n-    handler = tokenization(raw_request)\n+    handler = raw_request.app.state.openai_serving_tokenization\n \n     generator = await handler.create_tokenize(request, raw_request)\n     if isinstance(generator, ErrorResponse):\n@@ -339,7 +339,7 @@ async def tokenize(request: TokenizeRequest, raw_request: Request):\n @router.post(\"/detokenize\")\n @with_cancellation\n async def detokenize(request: DetokenizeRequest, raw_request: Request):\n-    handler = tokenization(raw_request)\n+    handler = raw_request.app.state.openai_serving_tokenization\n \n     generator = await handler.create_detokenize(request, raw_request)\n     if isinstance(generator, ErrorResponse):\n@@ -369,9 +369,9 @@ async def show_version():\n @with_cancellation\n async def create_chat_completion(request: ChatCompletionRequest,\n                                  raw_request: Request):\n-    handler = chat(raw_request)\n+    handler = raw_request.app.state.openai_serving_chat\n     if handler is None:\n-        return base(raw_request).create_error_response(\n+        return raw_request.app.state.openai_serving_tokenization.create_error_response(\n             message=\"The model does not support Chat Completions API\")\n \n     generator = await handler.create_chat_completion(request, raw_request)\n@@ -389,9 +389,9 @@ async def create_chat_completion(request: ChatCompletionRequest,\n @router.post(\"/v1/completions\")\n @with_cancellation\n async def create_completion(request: CompletionRequest, raw_request: Request):\n-    handler = completion(raw_request)\n+    handler = raw_request.app.state.openai_serving_completion\n     if handler is None:\n-        return base(raw_request).create_error_response(\n+        return raw_request.app.state.openai_serving_tokenization.create_error_response(\n             message=\"The model does not support Completions API\")\n \n     generator = await handler.create_completion(request, raw_request)\ndiff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..15ea60ec2 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -41,17 +41,23 @@ assert _LONG_INFO.max == _MOCK_LONG_INFO.max\n class OpenAIBaseModel(BaseModel):\n     # OpenAI API does allow extra fields\n     model_config = ConfigDict(extra=\"allow\")\n+    # Cache field names per class to avoid recomputation\n+    _field_names_cache: dict = {}\n \n     @model_validator(mode=\"before\")\n     @classmethod\n     def __log_extra_fields__(cls, data):\n         if isinstance(data, dict):\n-            # Get all class field names and their potential aliases\n-            field_names = set()\n-            for field_name, field in cls.model_fields.items():\n-                field_names.add(field_name)\n-                if hasattr(field, 'alias') and field.alias:\n-                    field_names.add(field.alias)\n+            # Use cached field names if available\n+            if cls not in cls._field_names_cache:\n+                field_names = set()\n+                for field_name, field in cls.model_fields.items():\n+                    field_names.add(field_name)\n+                    if hasattr(field, 'alias') and field.alias:\n+                        field_names.add(field.alias)\n+                cls._field_names_cache[cls] = field_names\n+            else:\n+                field_names = cls._field_names_cache[cls]\n \n             # Compare against both field names and aliases\n             extra_fields = data.keys() - field_names\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..eea2b30fd 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -134,9 +134,10 @@ class OutputProcessor:\n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n         iteration_stats = IterationStats(self.log_stats)\n+        request_states = self.request_states  # Cache dict lookup\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n-            req_state = self.request_states.get(req_id)\n+            req_state = request_states.get(req_id)\n             if req_state is None:\n                 # Ignore output for already-aborted request.\n                 continue\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..ef29ed4af 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -48,7 +48,7 @@ class Request:\n         self.prompt_token_ids = prompt_token_ids\n         self.num_prompt_tokens = len(self.prompt_token_ids)\n         self._output_token_ids: List[int] = []\n-        self._all_token_ids: List[int] = self.prompt_token_ids.copy()\n+        self._all_token_ids: List[int] = list(self.prompt_token_ids)\n         self.num_computed_tokens = 0\n \n         # Multi-modal related\n", "model_name_or_path": "gpt-5-2025-08-07"}
