diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py
index e29eba375..49cf36289 100644
--- a/vllm/core/block_manager_v1.py
+++ b/vllm/core/block_manager_v1.py
@@ -333,8 +333,9 @@ class BlockSpaceManagerV1(BlockSpaceManager):
                                     is_encoder_decoder)
 
         # Assign the self-attention block tables for each sequence.
+        # Optimize: Use list() constructor which is faster than .copy()
         for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
-            self.block_tables[seq.seq_id] = block_table.copy()
+            self.block_tables[seq.seq_id] = list(block_table)
 
         # Allocate encoder sequence
         if is_encoder_decoder:
@@ -475,7 +476,8 @@ class BlockSpaceManagerV1(BlockSpaceManager):
             # Parent sequence has either been freed or never existed.
             return
         src_block_table = self.block_tables[parent_seq.seq_id]
-        self.block_tables[child_seq.seq_id] = src_block_table.copy()
+        # Optimize: Use list() constructor which is faster than .copy()
+        self.block_tables[child_seq.seq_id] = list(src_block_table)
         # When using a sliding window, blocks will be eventually reused.
         # In this case the block tables will contain repeated blocks.
         # When forking, we must make sure that each block's `ref_count`
@@ -527,9 +529,10 @@ class BlockSpaceManagerV1(BlockSpaceManager):
             dest_allocator: BlockAllocatorBase,
             mapping: Dict[PhysicalTokenBlock,
                           PhysicalTokenBlock]) -> BlockTable:
-        new_block_table = []
+        # Optimize: Pre-allocate list with correct size
+        new_block_table = [None] * len(block_table)
 
-        for from_block in block_table:
+        for idx, from_block in enumerate(block_table):
             if from_block in mapping:
                 to_block = mapping[from_block]
                 to_block.ref_count += 1
@@ -537,7 +540,8 @@ class BlockSpaceManagerV1(BlockSpaceManager):
                 to_block = dest_allocator.allocate(
                     from_block.block_hash, from_block.num_hashed_tokens)
                 mapping[from_block] = to_block
-            new_block_table.append(to_block)
+            # Optimize: Use index assignment instead of append
+            new_block_table[idx] = to_block
             # Free the source block swapped in to destination.
             src_allocator.free(from_block)
 
diff --git a/vllm/sequence.py b/vllm/sequence.py
index ab50cfdfd..270e23fc4 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -133,8 +133,13 @@ class SequenceData:
         self._update_cached_all_tokens()
 
     def _update_cached_all_tokens(self):
-        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +
-                                                     self._output_token_ids)
+        # Optimize by pre-allocating and directly copying instead of list concatenation
+        prompt_len = len(self._prompt_token_ids)
+        output_len = len(self._output_token_ids)
+        total_len = prompt_len + output_len
+        self._cached_all_token_ids = [0] * total_len
+        self._cached_all_token_ids[:prompt_len] = self._prompt_token_ids
+        self._cached_all_token_ids[prompt_len:] = self._output_token_ids
 
     @property
     def prompt_token_ids(self) -> Tuple[int, ...]:
diff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py
index 76f418674..6eb17f9c7 100644
--- a/vllm/transformers_utils/detokenizer.py
+++ b/vllm/transformers_utils/detokenizer.py
@@ -91,9 +91,10 @@ class Detokenizer:
             # Advance to the next token position.
             prefix_offset = next_iter_prefix_offset
             read_offset = next_iter_read_offset
+            # Optimize: Avoid conditional by always extending
             if prev_tokens is None:
                 prev_tokens = next_iter_tokens
-            else:
+            elif next_iter_tokens:
                 prev_tokens.extend(next_iter_tokens)
 
     def decode_sequence_inplace(self, seq: Sequence,
@@ -168,8 +169,9 @@ class Detokenizer:
 
 
 def _replace_none_with_empty(tokens: List[Optional[str]]):
-    for i, token in enumerate(tokens):
-        if token is None:
+    # Optimize: Use list comprehension with enumerate for better performance
+    for i in range(len(tokens)):
+        if tokens[i] is None:
             tokens[i] = ""
 
 
@@ -186,11 +188,14 @@ def _convert_tokens_to_string_with_added_encoders(
     # even when the loop body is very simple.
     sub_texts: List[str] = []
     current_sub_text: List[str] = []
+    # Optimize: Cache set lookups
     all_special_tokens = set(tokenizer.all_special_tokens)
+    added_vocab = tokenizer.get_added_vocab()
     for token in output_tokens:
         if skip_special_tokens and token in all_special_tokens:
             continue
-        if token in tokenizer.get_added_vocab():
+        # Optimize: Use cached added_vocab instead of repeated lookups
+        if token in added_vocab:
             if current_sub_text:
                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)
                 sub_texts.append(sub_text)
