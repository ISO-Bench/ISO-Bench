{"instance_id": "vllm-project__vllm-ebce310", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 5b5643748..42e986e19 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -192,7 +192,7 @@ def scaled_fp8_quant(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex b57e1dde8..bc4e90fd2 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -199,7 +199,7 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                   layer.weight_scale[idx])\n \n                 layer.weight[start:end, :] = per_tensor_quantize(\n-                    weight_dq, layer.weight_scale.max())\n+                    weight_dq, max_w_scale)\n                 start = end\n             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n \n@@ -248,7 +248,9 @@ class Fp8LinearMethod(LinearMethodBase):\n \n def all_close_1d(x: torch.Tensor) -> bool:\n     assert len(x.shape) == 1\n-    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))\n+    if x.shape[0] <= 1:\n+        return True\n+    return torch.allclose(x, x[0].expand_as(x))\n \n \n def per_tensor_quantize(tensor: torch.Tensor,\n", "model_name_or_path": "gpt-5-2025-08-07"}
