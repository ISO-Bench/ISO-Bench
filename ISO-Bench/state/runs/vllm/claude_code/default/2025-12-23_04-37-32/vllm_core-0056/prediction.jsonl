{"instance_id": "vllm-project__vllm-e642ec9", "model_patch": "diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py\nindex 8184b0732..2fa278b7d 100644\n--- a/vllm/attention/backends/mla/common.py\n+++ b/vllm/attention/backends/mla/common.py\n@@ -342,13 +342,17 @@ class MLACommonState(AttentionState, Generic[T]):\n                                               PAD_SLOT_ID,\n                                               dtype=torch.long,\n                                               device=self.runner.device)\n-        self._graph_seq_lens = torch.ones(max_batch_size,\n-                                          dtype=torch.int32,\n-                                          device=self.runner.device)\n+        # Optimization: use empty instead of ones for better performance\n+        # This tensor will be overwritten before use in graph execution\n+        self._graph_seq_lens = torch.empty(max_batch_size,\n+                                           dtype=torch.int32,\n+                                           device=self.runner.device)\n         self._graph_block_tables = torch.from_numpy(\n             self.runner.graph_block_tables).to(device=self.runner.device)\n \n-        self._positions = torch.zeros((max_batch_size, ),\n+        # Optimization: use empty instead of zeros for better performance\n+        # This tensor will be overwritten before use in graph execution\n+        self._positions = torch.empty((max_batch_size, ),\n                                       dtype=torch.long,\n                                       device=self.runner.device)\n \n@@ -961,10 +965,9 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):\n             chunk_seq_lens = (chunk_ends - context_chunk_starts).clamp(min=0)\n             _context_chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                 torch.int32)\n-            zero = torch.zeros(num_chunks, dtype=torch.int32, device=device)\\\n-                .unsqueeze(-1)\n+            # Optimization: use pad instead of zeros + cat for better performance\n             context_chunk_cu_seq_lens = \\\n-                torch.cat([zero, _context_chunk_cu_seq_lens], dim=1)\n+                torch.nn.functional.pad(_context_chunk_cu_seq_lens, (1, 0), value=0)\n             context_chunk_max_seq_lens = \\\n                 chunk_seq_lens.max(dim=1).values.tolist()\n             context_chunk_seq_tot = chunk_seq_lens.sum(dim=1).tolist()\n@@ -1287,6 +1290,11 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         assert prefill_metadata.context_lens_tensor is not None\n \n         output = None\n+        output_lse = None\n+        # Optimization: pre-allocate temporary buffers outside loop to avoid\n+        # repeated allocations in the merge path\n+        output_tmp = None\n+        output_lse_tmp = None\n         iters = len(prefill_metadata.context_chunk_seq_tot)\n \n         # Fetch from attn_metadata directly, since it late bound by\n@@ -1373,8 +1381,10 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                # Optimization: reuse pre-allocated buffers for merge operations\n+                if output_tmp is None:\n+                    output_tmp = torch.empty_like(output)\n+                    output_lse_tmp = torch.empty_like(output_lse)\n                 merge_attn_states(\n                     output=output_tmp,\n                     output_lse=output_lse_tmp,\n@@ -1383,8 +1393,9 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                # Swap buffers to avoid extra copy\n+                output, output_tmp = output_tmp, output\n+                output_lse, output_lse_tmp = output_lse_tmp, output_lse\n \n         return output, output_lse\n \ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex c98262eea..0a0f75c21 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,14 +532,12 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+                # Optimization: use pad instead of zeros + cat for better performance\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=torch.nn.functional.pad(\n+                        _chunk_cu_seq_lens, (1, 0), value=0),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -858,6 +856,11 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         assert prefill_metadata.chunked_context is not None\n \n         output = None\n+        output_lse = None\n+        # Optimization: pre-allocate temporary buffers outside loop to avoid\n+        # repeated allocations in the merge path\n+        output_tmp = None\n+        output_lse_tmp = None\n         iters = len(prefill_metadata.chunked_context.seq_tot)\n         workspace = prefill_metadata.chunked_context.workspace\n \n@@ -909,8 +912,10 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                # Optimization: reuse pre-allocated buffers for merge operations\n+                if output_tmp is None:\n+                    output_tmp = torch.empty_like(output)\n+                    output_lse_tmp = torch.empty_like(output_lse)\n                 merge_attn_states(\n                     output=output_tmp,\n                     output_lse=output_lse_tmp,\n@@ -919,8 +924,9 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                # Swap buffers to avoid extra copy\n+                output, output_tmp = output_tmp, output\n+                output_lse, output_lse_tmp = output_lse_tmp, output_lse\n \n         return output, output_lse\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
