{"instance_id": "vllm-project__vllm-25373b6", "model_patch": "diff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py\nindex 00c1a2911..a7fffe34e 100644\n--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py\n+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py\n@@ -163,7 +163,7 @@ def generate_continuous_batched_examples(example_lens_by_batch,\n \n         # get the metadata\n         cu_seqlens = torch.tensor((0, ) + spec, device=device).cumsum(dim=0)\n-        seq_idx = torch.zeros(cu_seqlens[-1],\n+        seq_idx = torch.empty(cu_seqlens[-1],\n                               dtype=torch.int32,\n                               device=cu_seqlens.device)\n         for i, (srt, end) in enumerate(zip(\ndiff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py\nindex a4ded2b7a..27dcb4c11 100644\n--- a/vllm/model_executor/models/phi4flash.py\n+++ b/vllm/model_executor/models/phi4flash.py\n@@ -129,16 +129,16 @@ class SambaYAttention(nn.Module):\n \n         self.lambda_init = self.lambda_init_fn(layer_idx)\n         self.lambda_q1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_q2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.subln = nn.RMSNorm(2 * self.head_dim,\n                                 eps=1e-5,\n", "model_name_or_path": "gpt-5-2025-08-07"}
