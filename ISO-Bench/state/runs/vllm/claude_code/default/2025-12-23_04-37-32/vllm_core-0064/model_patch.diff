diff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py
index 00c1a2911..a7fffe34e 100644
--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py
+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py
@@ -163,7 +163,7 @@ def generate_continuous_batched_examples(example_lens_by_batch,
 
         # get the metadata
         cu_seqlens = torch.tensor((0, ) + spec, device=device).cumsum(dim=0)
-        seq_idx = torch.zeros(cu_seqlens[-1],
+        seq_idx = torch.empty(cu_seqlens[-1],
                               dtype=torch.int32,
                               device=cu_seqlens.device)
         for i, (srt, end) in enumerate(zip(
diff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py
index a4ded2b7a..27dcb4c11 100644
--- a/vllm/model_executor/models/phi4flash.py
+++ b/vllm/model_executor/models/phi4flash.py
@@ -129,16 +129,16 @@ class SambaYAttention(nn.Module):
 
         self.lambda_init = self.lambda_init_fn(layer_idx)
         self.lambda_q1 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_k1 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_q2 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_k2 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.subln = nn.RMSNorm(2 * self.head_dim,
                                 eps=1e-5,
