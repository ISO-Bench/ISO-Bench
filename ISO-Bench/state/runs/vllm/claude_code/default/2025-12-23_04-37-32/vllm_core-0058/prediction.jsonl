{"instance_id": "vllm-project__vllm-d666466", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\nindex 7016ff34c..c6bce577b 100644\n--- a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n+++ b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n@@ -33,6 +33,9 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # From https://github.com/deepseek-ai/DeepEP/blob/9fe9021f29c9083cd1808ab36b740208524d9f63/deep_ep/buffer.py#L164\n         self.available_rank_configs = [2, 4, 8, 16, 24, 32, 64, 128, 144, 160]\n \n+        # Optimize: Cache the weight_and_reduce implementation to avoid repeated instantiation\n+        self._cached_weight_and_reduce = TopKWeightAndReduceContiguous()\n+\n     def num_dispatchers(self) -> int:\n         return self.num_dispatchers_\n \n@@ -112,10 +115,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # DeepEP's topk_ids output refers to the local experts directly. Offset\n         # the topk_ids to move it back to the global experts space so it aligns\n         # with existing vLLM interfaces.\n-        expert_topk_ids = torch.where(\n-            expert_topk_ids == -1,\n-            num_experts - 1 if self.rank_expert_offset == 0 else 0,\n-            expert_topk_ids + self.rank_expert_offset)\n+        # Optimize: Use in-place operations instead of torch.where to reduce memory allocation\n+        mask = expert_topk_ids == -1\n+        expert_topk_ids.add_(self.rank_expert_offset)\n+        expert_topk_ids[mask] = num_experts - 1 if self.rank_expert_offset == 0 else 0\n \n         # Makes a GPU-CPU copy.\n         # TODO (varun): Maybe it is better to re-compute the expert_num_tokens\n@@ -142,7 +145,9 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n             # TODO: this only works for topK=1, will need to update for topK>1\n             assert topk == 1, (\n                 \"apply_router_weight_on_input is only implemented for topk=1\")\n-            a1 = a1 * topk_weights.to(a1.dtype)\n+            # Optimize: Avoid unnecessary dtype conversion when dtypes match\n+            topk_weights_typed = topk_weights if topk_weights.dtype == a1.dtype else topk_weights.to(a1.dtype)\n+            a1 = a1 * topk_weights_typed\n \n         if quant_config.per_act_token_quant:\n             a1q, a1q_scale = moe_kernel_quantize_input(\n@@ -195,8 +200,9 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # fused_expert_output can have 0 tokens - This happens when none of the\n         # tokens from the all2all reach this EP rank.\n         if fused_expert_output.numel() != 0:\n+            # Optimize: Use cached instance instead of creating new one each time\n             if isinstance(weight_and_reduce_impl, TopKWeightAndReduceDelegate):\n-                weight_and_reduce_impl = TopKWeightAndReduceContiguous()\n+                weight_and_reduce_impl = self._cached_weight_and_reduce\n             fused_expert_output = weight_and_reduce_impl.apply(\n                 output=None,\n                 fused_expert_output=fused_expert_output,\n", "model_name_or_path": "gpt-5-2025-08-07"}
