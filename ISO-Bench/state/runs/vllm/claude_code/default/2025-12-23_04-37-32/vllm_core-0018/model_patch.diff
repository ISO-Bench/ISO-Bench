diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py
index 1dea71187..d904d3f24 100644
--- a/vllm/v1/sample/ops/topk_topp_sampler.py
+++ b/vllm/v1/sample/ops/topk_topp_sampler.py
@@ -117,7 +117,7 @@ class TopKTopPSampler(nn.Module):
         if k is not None and p is None:
             topk_values, topk_indices = torch.topk(logits, k, dim=-1)
 
-            mask = torch.ones_like(logits, dtype=torch.bool)
+            mask = torch.full_like(logits, True, dtype=torch.bool)
             mask.scatter_(-1, topk_indices, False)
             logits.masked_fill_(mask, float('-inf'))
         else:
@@ -144,10 +144,10 @@ def apply_top_k_top_p(
 
     if k is not None:
         # Apply top-k.
-        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B
+        k_indices = logits_sort.size(1) - k.to(torch.long)  # shape: B
         # Get all the top_k values.
-        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))
-        top_k_mask = logits_sort < top_k_mask
+        top_k_threshold = logits_sort.gather(1, k_indices.unsqueeze(dim=1))
+        top_k_mask = logits_sort < top_k_threshold
         logits_sort.masked_fill_(top_k_mask, -float("inf"))
 
     if p is not None:
@@ -173,7 +173,7 @@ def random_sample(
     We use this function instead of torch.multinomial because torch.multinomial
     causes CPU-GPU synchronization.
     """
-    q = torch.empty_like(probs)
+    q = torch.empty_like(probs, dtype=torch.float32)
     # NOTE(woosuk): To batch-process the requests without their own seeds,
     # which is the common case, we first assume that every request does
     # not have its own seed. Then, we overwrite the values for the requests
@@ -212,7 +212,8 @@ def flashinfer_sample(
     max_top_k_round = 32
     batch_size = probs.shape[0]
     uniform_samples = torch.empty((max_top_k_round, batch_size),
-                                  device=probs.device)
+                                  device=probs.device,
+                                  dtype=torch.float32)
     if len(generators) != batch_size:
         uniform_samples.uniform_()
     if generators:
diff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py
index 397a049dc..f5cc6398b 100644
--- a/vllm/v1/sample/sampler.py
+++ b/vllm/v1/sample/sampler.py
@@ -157,15 +157,15 @@ class Sampler(nn.Module):
                                                  num_logprobs,
                                                  dim=-1)
 
-        # Get with the logprob of the prompt or sampled token.
-        token_ids = token_ids.unsqueeze(-1)
-        token_logprobs = logprobs.gather(-1, token_ids)
+        # Get the logprob of the prompt or sampled token.
+        token_ids_unsqueezed = token_ids.unsqueeze(-1)
+        token_logprobs = logprobs.gather(-1, token_ids_unsqueezed)
 
         # Compute the ranks of the actual token.
-        token_ranks = (logprobs >= token_logprobs).sum(-1)
+        token_ranks = (logprobs >= token_logprobs).sum(-1, dtype=torch.int32)
 
         # Concatenate together with the topk.
-        indices = torch.cat((token_ids, topk_indices), dim=1)
+        indices = torch.cat((token_ids_unsqueezed, topk_indices), dim=1)
         logprobs = torch.cat((token_logprobs, topk_logprobs), dim=1)
 
         # Use int32 to reduce the tensor size.
@@ -205,15 +205,11 @@ class Sampler(nn.Module):
         # Convert logits to probability distribution
         probability_values = torch.nn.functional.softmax(logits, dim=-1)
         # Calculate maximum probabilities per sequence
-        max_probabilities = torch.amax(probability_values,
-                                       dim=-1,
-                                       keepdim=True)
-        # Reshape min_p for broadcasting
+        max_probabilities = probability_values.amax(dim=-1, keepdim=True)
+        # Reshape min_p for broadcasting and compute threshold
         adjusted_min_p = min_p.unsqueeze(1) * max_probabilities
-        # Identify valid tokens using threshold comparison
-        valid_token_mask = probability_values >= adjusted_min_p
-        # Apply mask using boolean indexing
-        logits[~valid_token_mask] = -float('inf')
+        # Apply mask in-place for better performance
+        logits.masked_fill_(probability_values < adjusted_min_p, -float('inf'))
         return logits
 
     def apply_logits_bias(
