{"instance_id": "vllm-project__vllm-733e7c9", "model_patch": "diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea71187..d904d3f24 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -117,7 +117,7 @@ class TopKTopPSampler(nn.Module):\n         if k is not None and p is None:\n             topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n \n-            mask = torch.ones_like(logits, dtype=torch.bool)\n+            mask = torch.full_like(logits, True, dtype=torch.bool)\n             mask.scatter_(-1, topk_indices, False)\n             logits.masked_fill_(mask, float('-inf'))\n         else:\n@@ -144,10 +144,10 @@ def apply_top_k_top_p(\n \n     if k is not None:\n         # Apply top-k.\n-        top_k_mask = logits_sort.size(1) - k.to(torch.long)  # shape: B\n+        k_indices = logits_sort.size(1) - k.to(torch.long)  # shape: B\n         # Get all the top_k values.\n-        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n-        top_k_mask = logits_sort < top_k_mask\n+        top_k_threshold = logits_sort.gather(1, k_indices.unsqueeze(dim=1))\n+        top_k_mask = logits_sort < top_k_threshold\n         logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n \n     if p is not None:\n@@ -173,7 +173,7 @@ def random_sample(\n     We use this function instead of torch.multinomial because torch.multinomial\n     causes CPU-GPU synchronization.\n     \"\"\"\n-    q = torch.empty_like(probs)\n+    q = torch.empty_like(probs, dtype=torch.float32)\n     # NOTE(woosuk): To batch-process the requests without their own seeds,\n     # which is the common case, we first assume that every request does\n     # not have its own seed. Then, we overwrite the values for the requests\n@@ -212,7 +212,8 @@ def flashinfer_sample(\n     max_top_k_round = 32\n     batch_size = probs.shape[0]\n     uniform_samples = torch.empty((max_top_k_round, batch_size),\n-                                  device=probs.device)\n+                                  device=probs.device,\n+                                  dtype=torch.float32)\n     if len(generators) != batch_size:\n         uniform_samples.uniform_()\n     if generators:\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049dc..f5cc6398b 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -157,15 +157,15 @@ class Sampler(nn.Module):\n                                                  num_logprobs,\n                                                  dim=-1)\n \n-        # Get with the logprob of the prompt or sampled token.\n-        token_ids = token_ids.unsqueeze(-1)\n-        token_logprobs = logprobs.gather(-1, token_ids)\n+        # Get the logprob of the prompt or sampled token.\n+        token_ids_unsqueezed = token_ids.unsqueeze(-1)\n+        token_logprobs = logprobs.gather(-1, token_ids_unsqueezed)\n \n         # Compute the ranks of the actual token.\n-        token_ranks = (logprobs >= token_logprobs).sum(-1)\n+        token_ranks = (logprobs >= token_logprobs).sum(-1, dtype=torch.int32)\n \n         # Concatenate together with the topk.\n-        indices = torch.cat((token_ids, topk_indices), dim=1)\n+        indices = torch.cat((token_ids_unsqueezed, topk_indices), dim=1)\n         logprobs = torch.cat((token_logprobs, topk_logprobs), dim=1)\n \n         # Use int32 to reduce the tensor size.\n@@ -205,15 +205,11 @@ class Sampler(nn.Module):\n         # Convert logits to probability distribution\n         probability_values = torch.nn.functional.softmax(logits, dim=-1)\n         # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Reshape min_p for broadcasting\n+        max_probabilities = probability_values.amax(dim=-1, keepdim=True)\n+        # Reshape min_p for broadcasting and compute threshold\n         adjusted_min_p = min_p.unsqueeze(1) * max_probabilities\n-        # Identify valid tokens using threshold comparison\n-        valid_token_mask = probability_values >= adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[~valid_token_mask] = -float('inf')\n+        # Apply mask in-place for better performance\n+        logits.masked_fill_(probability_values < adjusted_min_p, -float('inf'))\n         return logits\n \n     def apply_logits_bias(\n", "model_name_or_path": "gpt-5-2025-08-07"}
