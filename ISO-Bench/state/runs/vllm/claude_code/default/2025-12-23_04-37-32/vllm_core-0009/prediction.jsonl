{"instance_id": "vllm-project__vllm-f728ab8", "model_patch": "diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex a9ef97391..3d614444b 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -15,7 +15,9 @@ def get_token_bin_counts_and_mask(\n     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n+    # Use constant tensor instead of ones_like to reduce memory allocation\n+    ones = torch.ones((), dtype=torch.long, device=tokens.device)\n+    bin_counts.scatter_add_(1, tokens, ones.expand_as(tokens))\n     bin_counts = bin_counts[:, :vocab_size]\n     mask = bin_counts > 0\n \n@@ -45,12 +47,19 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                                    vocab_size, num_seqs)\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n-        1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n+\n+    # Compute combined mask once to avoid redundant operations\n+    combined_mask = prompt_mask | output_mask\n+\n+    # Expand penalties using unsqueeze instead of repeat to avoid memory copy\n+    repetition_penalties_expanded = repetition_penalties.unsqueeze(dim=1)\n+\n+    # Apply repetition penalties using broadcasting instead of indexing\n+    positive_mask = logits > 0\n+\n+    penalty_values = torch.where(combined_mask, repetition_penalties_expanded, 1.0)\n+    logits = torch.where(positive_mask, logits / penalty_values, logits * penalty_values)\n+\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n     logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts\n", "model_name_or_path": "gpt-5-2025-08-07"}
