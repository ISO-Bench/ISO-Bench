{"instance_id": "vllm-project__vllm-5c04bb8", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex 5904ad1f1..1edaeae06 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -198,7 +198,7 @@ class Qwen2_5_VisionMLP(nn.Module):\n def all_gather_interleave(local_tensor, hidden_size: int, tp_size: int):\n     \"\"\"All-gather the input tensor interleavely across model parallel group.\"\"\"\n     import torch.distributed as dist\n-    gathered_tensors = [torch.zeros_like(local_tensor) for _ in range(tp_size)]\n+    gathered_tensors = [torch.empty_like(local_tensor) for _ in range(tp_size)]\n     dist.all_gather(gathered_tensors,\n                     local_tensor,\n                     group=parallel_state.get_tp_group().device_group)\n@@ -569,23 +569,29 @@ class Qwen2_5_VisionTransformer(nn.Module):\n \n     def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n         pos_ids = []\n+        device = grid_thw.device\n         for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+            h_val, w_val = int(h), int(w)\n+            # Use arange with device specified to avoid device transfers\n+            hpos_ids = torch.arange(h_val, device=device).unsqueeze(1).expand(-1, w_val)\n+            wpos_ids = torch.arange(w_val, device=device).unsqueeze(0).expand(h_val, -1)\n             hpos_ids = hpos_ids.reshape(\n-                h // self.spatial_merge_size,\n+                h_val // self.spatial_merge_size,\n                 self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n+                w_val // self.spatial_merge_size,\n                 self.spatial_merge_size,\n             ).permute(0, 2, 1, 3).flatten()\n             wpos_ids = wpos_ids.reshape(\n-                h // self.spatial_merge_size,\n+                h_val // self.spatial_merge_size,\n                 self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n+                w_val // self.spatial_merge_size,\n                 self.spatial_merge_size,\n             ).permute(0, 2, 1, 3).flatten()\n-            pos_ids.append(\n-                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n+            # Stack and repeat in one operation\n+            stacked_ids = torch.stack([hpos_ids, wpos_ids], dim=-1)\n+            if t > 1:\n+                stacked_ids = stacked_ids.repeat(int(t), 1)\n+            pos_ids.append(stacked_ids)\n         pos_ids = torch.cat(pos_ids, dim=0)\n         max_grid_size = grid_thw[:, 1:].max()\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n@@ -593,39 +599,47 @@ class Qwen2_5_VisionTransformer(nn.Module):\n         return rotary_pos_emb\n \n     def get_window_index(self, grid_thw):\n-        window_index: list = []\n-        cu_window_seqlens: list = [0]\n+        window_index_list: list = []\n+        cu_window_seqlens_list: list = []\n         window_index_id = 0\n         vit_merger_window_size = (self.window_size //\n                                   self.spatial_merge_size // self.patch_size)\n+        device = grid_thw.device\n+        cu_offset = 0\n \n         for grid_t, grid_h, grid_w in grid_thw:\n-            llm_grid_h = grid_h // self.spatial_merge_size\n-            llm_grid_w = grid_w // self.spatial_merge_size\n-            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(\n-                grid_t, llm_grid_h, llm_grid_w)\n+            llm_grid_h = int(grid_h) // self.spatial_merge_size\n+            llm_grid_w = int(grid_w) // self.spatial_merge_size\n+            grid_t_val = int(grid_t)\n+            total_elements = grid_t_val * llm_grid_h * llm_grid_w\n+            index = torch.arange(total_elements, device=device).reshape(\n+                grid_t_val, llm_grid_h, llm_grid_w)\n             pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size\n             pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size\n             num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size\n             num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size\n             index_padded = F.pad(index, (0, pad_w, 0, pad_h), 'constant', -100)\n-            index_padded = index_padded.reshape(grid_t, num_windows_h,\n+            index_padded = index_padded.reshape(grid_t_val, num_windows_h,\n                                                 vit_merger_window_size,\n                                                 num_windows_w,\n                                                 vit_merger_window_size)\n             index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(\n-                grid_t, num_windows_h * num_windows_w, vit_merger_window_size,\n+                grid_t_val, num_windows_h * num_windows_w, vit_merger_window_size,\n                 vit_merger_window_size)\n             seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)\n             index_padded = index_padded.reshape(-1)\n             index_new = index_padded[index_padded != -100]\n-            window_index.append(index_new + window_index_id)\n-            cu_seqlens_tmp = seqlens.cumsum(\n-                0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n-            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n-            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n-        window_index = torch.cat(window_index, dim=0)\n-        return window_index, cu_window_seqlens\n+            window_index_list.append(index_new + window_index_id)\n+            cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit + cu_offset\n+            cu_window_seqlens_list.append(cu_seqlens_tmp)\n+            cu_offset = cu_seqlens_tmp[-1].item()\n+            window_index_id += total_elements\n+\n+        window_index = torch.cat(window_index_list, dim=0)\n+        cu_window_seqlens_tensor = torch.cat(cu_window_seqlens_list, dim=0)\n+        # Prepend 0 to match expected format\n+        cu_window_seqlens_list = [0] + cu_window_seqlens_tensor.tolist()\n+        return window_index, cu_window_seqlens_list\n \n     def compute_attn_mask_seqlen(\n         self,\n@@ -848,9 +862,9 @@ class Qwen2_5_VLForConditionalGeneration(nn.Module, SupportsMultiModal,\n                 raise ValueError(f\"{name} should be 2D or batched 3D tensor. \"\n                                  f\"Got ndim: {mm_input.ndim} \"\n                                  f\"(shape={mm_input.shape})\")\n-            return torch.concat(list(mm_input))\n+            return torch.cat([mm_input[i] for i in range(mm_input.shape[0])], dim=0)\n         else:\n-            return torch.concat(mm_input)\n+            return torch.cat(mm_input, dim=0)\n \n     def _parse_and_validate_image_input(\n             self, **kwargs: object) -> Optional[Qwen2_5_VLImageInputs]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
