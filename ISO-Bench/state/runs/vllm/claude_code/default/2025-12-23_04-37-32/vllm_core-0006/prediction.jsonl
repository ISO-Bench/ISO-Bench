{"instance_id": "vllm-project__vllm-a6d795d", "model_patch": "diff --git a/vllm/attention/ops/triton_unified_attention.py b/vllm/attention/ops/triton_unified_attention.py\nindex c65f09523..71525b2a9 100644\n--- a/vllm/attention/ops/triton_unified_attention.py\n+++ b/vllm/attention/ops/triton_unified_attention.py\n@@ -145,6 +145,10 @@ def kernel_unified_attention_2d(\n                               mask=query_mask_1,\n                               other=0.0)\n \n+    # Load scale factors once before the loop\n+    k_scale_val = tl.load(k_scale)\n+    v_scale_val = tl.load(v_scale)\n+\n     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n \n     # iterate through tiles\n@@ -173,7 +177,7 @@ def kernel_unified_attention_2d(\n             if Q.dtype.is_fp8():\n                 K = K_load\n             else:\n-                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n+                K = (K_load.to(tl.float32) * k_scale_val).to(Q.dtype)\n         else:\n             K = K_load\n \n@@ -186,7 +190,7 @@ def kernel_unified_attention_2d(\n             if Q.dtype.is_fp8():\n                 V = V_load\n             else:\n-                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n+                V = (V_load.to(tl.float32) * v_scale_val).to(Q.dtype)\n         else:\n             V = V_load\n \n@@ -195,9 +199,7 @@ def kernel_unified_attention_2d(\n         seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1\n \n         # S : (BLOCK_M, BLOCK_SIZE)\n-        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)\n-\n-        S += scale * tl.dot(Q, K)\n+        S = scale * tl.dot(Q, K)\n \n         if USE_SOFTCAP:\n             S = apply_softcap(S, softcap)\n@@ -362,6 +364,10 @@ def kernel_unified_attention_3d(\n                               mask=query_mask_1,\n                               other=0.0)\n \n+    # Load scale factors once before the loop\n+    k_scale_val = tl.load(k_scale)\n+    v_scale_val = tl.load(v_scale)\n+\n     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n \n     # iterate through tiles within current segment\n@@ -392,7 +398,7 @@ def kernel_unified_attention_3d(\n             if Q.dtype.is_fp8():\n                 K = K_load\n             else:\n-                K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)\n+                K = (K_load.to(tl.float32) * k_scale_val).to(Q.dtype)\n         else:\n             K = K_load\n \n@@ -405,7 +411,7 @@ def kernel_unified_attention_3d(\n             if Q.dtype.is_fp8():\n                 V = V_load\n             else:\n-                V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)\n+                V = (V_load.to(tl.float32) * v_scale_val).to(Q.dtype)\n         else:\n             V = V_load\n \n@@ -414,9 +420,7 @@ def kernel_unified_attention_3d(\n         seq_mask = seq_offset[None, :] < context_len + query_pos[:, None] + 1\n \n         # S : (BLOCK_M, BLOCK_SIZE)\n-        S = tl.zeros(shape=(BLOCK_M, BLOCK_SIZE), dtype=tl.float32)\n-\n-        S += scale * tl.dot(Q, K)\n+        S = scale * tl.dot(Q, K)\n \n         if USE_SOFTCAP:\n             S = apply_softcap(S, softcap)\n@@ -511,8 +515,7 @@ def reduce_segments(\n \n     # create masks for subsequent loads\n     act_num_segments = cdiv_fn(seq_len, blocks_per_segment * BLOCK_SIZE)\n-    segm_mask = tl.arange(0, NUM_SEGMENTS_PER_SEQ) < tl.full(\n-        [NUM_SEGMENTS_PER_SEQ], act_num_segments, dtype=tl.int32)\n+    segm_mask = tl.arange(0, NUM_SEGMENTS_PER_SEQ) < act_num_segments\n     dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1,\n                         0).to(tl.int1)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
