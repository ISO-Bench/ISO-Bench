{"instance_id": "vllm-project__vllm-8c1e77f", "model_patch": "diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 5f8535eaa..b572d70fb 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -185,6 +185,7 @@ def unified_v1_flash_attention(\n     num_actual_tokens = attn_metadata.num_actual_tokens\n \n     # Reshape the query, key, and value tensors.\n+    # Optimization: Use view for zero-copy reshape of contiguous tensors\n     query = query.view(-1, num_heads, head_size)\n     key = key.view(-1, num_kv_heads, head_size)\n     value = value.view(-1, num_kv_heads, head_size)\n@@ -218,9 +219,9 @@ def unified_v1_flash_attention(\n         block_table=attn_metadata.block_table,\n         softcap=logits_soft_cap,\n     )\n-    attn_output = attn_output.view(num_actual_tokens, -1)\n-    # TODO(woosuk): Optimize this.\n-    output[:num_actual_tokens].copy_(attn_output)\n+    # Optimization: Directly assign reshaped output, avoiding intermediate view+copy\n+    # Reshape the output tensor and write it directly to the output buffer\n+    output[:num_actual_tokens] = attn_output.view(num_actual_tokens, -1)\n \n \n def unified_v1_flash_attention_fake(\n", "model_name_or_path": "gpt-5-2025-08-07"}
