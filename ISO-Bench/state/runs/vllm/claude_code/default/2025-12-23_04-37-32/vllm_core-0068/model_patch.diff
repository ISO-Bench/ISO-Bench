diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py
index 8785e9dcf..ac87424cd 100644
--- a/vllm/model_executor/models/llama4.py
+++ b/vllm/model_executor/models/llama4.py
@@ -236,7 +236,7 @@ class Llama4Attention(nn.Module):
         # and (before) attention.
         if self.attn_temperature_tuning and self.nope:
             attn_scale = self._get_attn_scale(positions)
-            q = (q * attn_scale).to(q.dtype)
+            q = q * attn_scale.to(q.dtype)
         attn_output = self.attn(q, k, v)
         output, _ = self.o_proj(attn_output)
         return output
diff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py
index f197434f3..6ec84c226 100644
--- a/vllm/model_executor/models/utils.py
+++ b/vllm/model_executor/models/utils.py
@@ -471,6 +471,7 @@ def merge_multimodal_embeddings(
     """
     if isinstance(placeholder_token_id, list):
         placeholder_token_id = torch.tensor(placeholder_token_id,
+                                            dtype=input_ids.dtype,
                                             device=input_ids.device)
         return _merge_multimodal_embeddings(
             inputs_embeds,
@@ -654,7 +655,7 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):
     ) -> IntermediateTensors:
         return IntermediateTensors({
             key:
-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)
+            torch.empty((batch_size, hidden_size), dtype=dtype, device=device)
             for key in keys
         })
 
