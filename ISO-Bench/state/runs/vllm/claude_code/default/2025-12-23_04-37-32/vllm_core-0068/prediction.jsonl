{"instance_id": "vllm-project__vllm-802329d", "model_patch": "diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9dcf..ac87424cd 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -236,7 +236,7 @@ class Llama4Attention(nn.Module):\n         # and (before) attention.\n         if self.attn_temperature_tuning and self.nope:\n             attn_scale = self._get_attn_scale(positions)\n-            q = (q * attn_scale).to(q.dtype)\n+            q = q * attn_scale.to(q.dtype)\n         attn_output = self.attn(q, k, v)\n         output, _ = self.o_proj(attn_output)\n         return output\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434f3..6ec84c226 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -471,6 +471,7 @@ def merge_multimodal_embeddings(\n     \"\"\"\n     if isinstance(placeholder_token_id, list):\n         placeholder_token_id = torch.tensor(placeholder_token_id,\n+                                            dtype=input_ids.dtype,\n                                             device=input_ids.device)\n         return _merge_multimodal_embeddings(\n             inputs_embeds,\n@@ -654,7 +655,7 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):\n     ) -> IntermediateTensors:\n         return IntermediateTensors({\n             key:\n-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)\n+            torch.empty((batch_size, hidden_size), dtype=dtype, device=device)\n             for key in keys\n         })\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
