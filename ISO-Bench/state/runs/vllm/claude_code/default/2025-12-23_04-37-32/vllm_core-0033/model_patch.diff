diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index cf2f1c6b3..2c549e4a1 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -1043,7 +1043,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         # in numpy and only copy the actual input content at every iteration.
         # The shape of the cached block table will be
         # (max batch size to capture, max seq len to capture / block size).
-        self.graph_block_tables = np.zeros(
+        self.graph_block_tables = np.empty(
             (self.max_batchsize_to_capture, self.get_max_block_per_batch()),
             dtype=np.int32)
 
@@ -1322,7 +1322,7 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
             # multiplying the list, to avoid Dynamo from treating them as
             # tensor aliasing.
             kv_caches = [
-                torch.tensor([], dtype=torch.float32, device=self.device)
+                torch.empty(0, dtype=torch.float32, device=self.device)
                 for _ in range(num_layers)
             ]
             finished_requests_ids = [seq.request_id for seq in seqs]
@@ -1437,10 +1437,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
 
         # Prepare dummy inputs. These will be reused for all batch sizes.
         max_batch_size = self.max_batchsize_to_capture
-        input_tokens = torch.zeros(max_batch_size,
+        input_tokens = torch.empty(max_batch_size,
                                    dtype=torch.long,
                                    device=self.device)
-        input_positions = torch.zeros(max_batch_size,
+        input_positions = torch.empty(max_batch_size,
                                       dtype=torch.long,
                                       device=self.device)
         if self.model_config.uses_mrope:
@@ -1568,10 +1568,10 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
         """
         # During the decode phase encoder_input_ids and encoder_positions are
         # unset. Do the same thing for graph capture.
-        capture_inputs["encoder_input_ids"] = torch.tensor([],
+        capture_inputs["encoder_input_ids"] = torch.empty(0,
                                                            dtype=torch.long,
                                                            device=self.device)
-        capture_inputs["encoder_positions"] = torch.tensor([],
+        capture_inputs["encoder_positions"] = torch.empty(0,
                                                            dtype=torch.long,
                                                            device=self.device)
 
