{"instance_id": "vllm-project__vllm-526078a", "model_patch": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101a0..0913d7a0f 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -155,7 +155,8 @@ def create_and_prepopulate_kv_cache(\n         perm = torch.arange(\n             1, blocks_end)  # Sequential order starting from block 1\n \n-    inv_perm = torch.zeros(blocks_end, dtype=torch.long, device=device)\n+    inv_perm = torch.empty(blocks_end, dtype=torch.long, device=device)\n+    inv_perm[0] = 0\n     inv_perm[1:] = torch.argsort(\n         perm) + 1  # Add 1 to account for starting from block 1\n     kv_cache[:, 1:blocks_end, ...] = kv_cache[:, perm, ...]\ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdda5..b3779ec24 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -43,12 +43,13 @@ def create_common_attn_metadata(\n         max_block_idx: int = 1000) -> CommonAttentionMetadata:\n     \"\"\"Create CommonAttentionMetadata from a BatchSpec and ModelParams.\"\"\"\n     # Create query start locations\n+    query_lens_tensor = torch.tensor(batch_spec.query_lens,\n+                                     dtype=torch.int32,\n+                                     device=device)\n     query_start_loc = torch.zeros(batch_spec.batch_size + 1,\n                                   dtype=torch.int32,\n                                   device=device)\n-    query_start_loc[1:] = torch.tensor(batch_spec.query_lens,\n-                                       dtype=torch.int32,\n-                                       device=device).cumsum(0)\n+    query_start_loc[1:] = query_lens_tensor.cumsum(0)\n     query_start_loc_cpu = query_start_loc.cpu()\n     num_tokens = batch_spec.compute_num_tokens()\n \n@@ -59,11 +60,7 @@ def create_common_attn_metadata(\n     seq_lens_cpu = seq_lens.cpu()\n \n     # Create computed tokens (context length for each sequence)\n-    context_lens = [\n-        batch_spec.seq_lens[i] - batch_spec.query_lens[i]\n-        for i in range(batch_spec.batch_size)\n-    ]\n-    num_computed_tokens_cpu = torch.tensor(context_lens, dtype=torch.int32)\n+    num_computed_tokens_cpu = seq_lens_cpu - query_lens_tensor.cpu()\n \n     # Create block table (random for testing)\n     max_blocks = max(batch_spec.seq_lens) // block_size + 1\ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26c8..002f34f6f 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -396,17 +396,16 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             # Grab the blocks of the shared prefix from the first request.\n             assert common_prefix_len % page_size == 0\n             num_common_kv_blocks = common_prefix_len // page_size\n-            shared_qo_indptr = torch.tensor([0, num_actual_tokens],\n-                                            dtype=torch.int32,\n-                                            device=device)\n-            shared_kv_page_indptr = torch.tensor([0, num_common_kv_blocks],\n-                                                 dtype=torch.int32,\n-                                                 device=device)\n+            shared_qo_indptr = torch.empty(2, dtype=torch.int32, device=device)\n+            shared_qo_indptr[0] = 0\n+            shared_qo_indptr[1] = num_actual_tokens\n+            shared_kv_page_indptr = torch.empty(2, dtype=torch.int32, device=device)\n+            shared_kv_page_indptr[0] = 0\n+            shared_kv_page_indptr[1] = num_common_kv_blocks\n             shared_kv_page_indices = block_table_tensor[\n                 0, :num_common_kv_blocks]\n-            shared_kv_last_page_len = torch.tensor([page_size],\n-                                                   dtype=torch.int32,\n-                                                   device=device)\n+            shared_kv_last_page_len = torch.empty(1, dtype=torch.int32, device=device)\n+            shared_kv_last_page_len[0] = page_size\n             # Remove the blocks of the shared prefix from all requests.\n             block_table_tensor = block_table_tensor[:, num_common_kv_blocks:]\n             block_table_bounds -= num_common_kv_blocks\n@@ -423,7 +422,7 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n         paged_kv_indices = block_table_tensor[mask]\n \n         paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n+            torch.tensor([0],\n                         dtype=block_table_bounds.dtype,\n                         device=block_table_bounds.device),\n             block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n@@ -652,7 +651,8 @@ class FlashInferImpl(AttentionImpl):\n                 # This path needs to be enabled with VLLM_KV_CACHE_LAYOUT = HND\n                 if num_decode_tokens > 0:\n                     # decode_query may be non-contiguous\n-                    decode_query = decode_query.contiguous()\n+                    if not decode_query.is_contiguous():\n+                        decode_query = decode_query.contiguous()\n                     block_tables_decode = attn_metadata.block_table_tensor[:\n                                                                            num_decode_tokens]\n                     seq_lens_decode = attn_metadata.seq_lens[:\n", "model_name_or_path": "gpt-5-2025-08-07"}
