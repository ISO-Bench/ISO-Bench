diff --git a/CMakeLists.txt b/CMakeLists.txt
index 412b9c0cd..04d9a6971 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -10,7 +10,7 @@ include(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)
 # Supported python versions.  These versions will be searched in order, the
 # first match will be selected.  These should be kept in sync with setup.py.
 #
-set(PYTHON_SUPPORTED_VERSIONS "3.8" "3.9" "3.10" "3.11")
+set(PYTHON_SUPPORTED_VERSIONS "3.8" "3.9" "3.10" "3.11" "3.12")
 
 # Supported NVIDIA architectures.
 set(CUDA_SUPPORTED_ARCHS "7.0;7.5;8.0;8.6;8.9;9.0")
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index 6bf5d5130..9291a3c8e 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -102,6 +102,9 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)
       list(APPEND GPU_FLAGS "-DENABLE_FP8_E5M2")
     endif()
 
+    # Add optimization flags for better performance
+    list(APPEND GPU_FLAGS "--use_fast_math" "-Xptxas=-v")
+
   elseif(${GPU_LANG} STREQUAL "HIP")
     #
     # Get common HIP/HIPCC flags from torch.
diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu
index 6d34d014c..aea82ea5b 100644
--- a/csrc/layernorm_kernels.cu
+++ b/csrc/layernorm_kernels.cu
@@ -19,8 +19,11 @@ __global__ void rms_norm_kernel(
   __shared__ float s_variance;
   float variance = 0.0f;
 
+  const int base_offset = blockIdx.x * hidden_size;
+
+  #pragma unroll 4
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    const float x = (float) input[blockIdx.x * hidden_size + idx];
+    const float x = (float) input[base_offset + idx];
     variance += x * x;
   }
   variance = blockReduceSum<float>(variance);
@@ -29,9 +32,12 @@ __global__ void rms_norm_kernel(
   }
   __syncthreads();
 
+  const float norm_factor = s_variance;
+  #pragma unroll 4
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) input[blockIdx.x * hidden_size + idx];
-    out[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];
+    const int offset = base_offset + idx;
+    const float x = (float) input[offset];
+    out[offset] = ((scalar_t) (x * norm_factor)) * weight[idx];
   }
 }
 
@@ -47,11 +53,15 @@ __global__ void fused_add_rms_norm_kernel(
   __shared__ float s_variance;
   float variance = 0.0f;
 
+  const int base_offset = blockIdx.x * hidden_size;
+
+  #pragma unroll 4
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) input[blockIdx.x * hidden_size + idx];
-    x += (float) residual[blockIdx.x * hidden_size + idx];
+    const int offset = base_offset + idx;
+    float x = (float) input[offset];
+    x += (float) residual[offset];
     variance += x * x;
-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;
+    residual[offset] = (scalar_t) x;
   }
   variance = blockReduceSum<float>(variance);
   if (threadIdx.x == 0) {
@@ -59,9 +69,12 @@ __global__ void fused_add_rms_norm_kernel(
   }
   __syncthreads();
 
+  const float norm_factor = s_variance;
+  #pragma unroll 4
   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {
-    float x = (float) residual[blockIdx.x * hidden_size + idx];
-    input[blockIdx.x * hidden_size + idx] = ((scalar_t) (x * s_variance)) * weight[idx];
+    const int offset = base_offset + idx;
+    const float x = (float) residual[offset];
+    input[offset] = ((scalar_t) (x * norm_factor)) * weight[idx];
   }
 }
 
diff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh
index c25464e86..b58b9b4db 100644
--- a/csrc/reduction_utils.cuh
+++ b/csrc/reduction_utils.cuh
@@ -55,7 +55,8 @@ __inline__ __device__ T blockReduceSum(T val) {
 
   // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
   // blockDim.x is not divided by 32
-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);
+  // Optimized: Use integer arithmetic instead of float division
+  val = (threadIdx.x < ((blockDim.x + WARP_SIZE - 1) >> WID_SHIFT)) ? shared[lane] : (T)(0.0f);
   val = warpReduceSum<T>(val);
   return val;
 }
