{"instance_id": "vllm-project__vllm-51e971d", "model_patch": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 13746cef2..5dc15d00e 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -129,10 +129,15 @@ class SequenceData:\n         # The number of tokens that are computed (that run against the model).\n         self._num_computed_tokens = 0\n         self._stage: SequenceStage = SequenceStage.PREFILL\n+        # Cache for concatenated token ids\n+        self._cached_token_ids: Optional[List[int]] = None\n+        self._cached_token_ids_len = 0\n \n     def append_token_id(self, token_id: int, logprob: float) -> None:\n         self.output_token_ids.append(token_id)\n         self.cumulative_logprob += logprob\n+        # Invalidate cache\n+        self._cached_token_ids = None\n \n     def get_len(self) -> int:\n         return len(self.output_token_ids) + len(self.prompt_token_ids)\n@@ -144,7 +149,12 @@ class SequenceData:\n         return len(self.output_token_ids)\n \n     def get_token_ids(self) -> List[int]:\n-        return self.prompt_token_ids + self.output_token_ids\n+        # Cache the concatenated list to avoid repeated allocations\n+        current_len = len(self.prompt_token_ids) + len(self.output_token_ids)\n+        if self._cached_token_ids is None or self._cached_token_ids_len != current_len:\n+            self._cached_token_ids = self.prompt_token_ids + self.output_token_ids\n+            self._cached_token_ids_len = current_len\n+        return self._cached_token_ids\n \n     def get_prefix_token_ids(\n             self, num_tokens: int\n@@ -152,9 +162,14 @@ class SequenceData:\n         \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n         prompt_length = len(self.prompt_token_ids)\n         if num_tokens > prompt_length:\n+            # Only create tuple for output tokens that are needed\n+            output_end = num_tokens - prompt_length\n             return (self._prompt_token_ids_tuple,\n-                    tuple(self.output_token_ids[:num_tokens - prompt_length]))\n+                    tuple(self.output_token_ids[:output_end]))\n         else:\n+            # Reuse cached tuple when possible\n+            if num_tokens == prompt_length:\n+                return (self._prompt_token_ids_tuple, None)\n             return (self._prompt_token_ids_tuple[:num_tokens], None)\n \n     def get_num_computed_tokens(self) -> int:\n@@ -177,6 +192,8 @@ class SequenceData:\n         \"\"\"\n         self._num_computed_tokens = 0\n         self._stage = SequenceStage.PREFILL\n+        # Invalidate cache\n+        self._cached_token_ids = None\n \n     def get_num_uncomputed_tokens(self) -> int:\n         \"\"\"Return the number of prefill tokens that are not computed.\"\"\"\n@@ -500,9 +517,14 @@ class SequenceGroup:\n         self,\n         status: Optional[SequenceStatus] = None,\n     ) -> List[Sequence]:\n-        return list(self.seqs_dict.values()) if status is None else [\n-            seq for seq in self.seqs_dict.values() if seq.status == status\n-        ]\n+        if status is None:\n+            return list(self.seqs_dict.values())\n+        # Optimized list comprehension - avoid creating intermediate list\n+        result = []\n+        for seq in self.seqs_dict.values():\n+            if seq.status == status:\n+                result.append(seq)\n+        return result\n \n     def is_encoder_decoder(self) -> bool:\n         return self.encoder_seq is not None\n@@ -511,22 +533,33 @@ class SequenceGroup:\n         return self.encoder_seq\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n-        return [\n-            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n-        ]\n+        # Optimized to avoid list comprehension overhead\n+        result = []\n+        for seq in self.seqs_dict.values():\n+            if not seq.is_finished():\n+                result.append(seq)\n+        return result\n \n     def get_finished_seqs(self) -> List[Sequence]:\n-        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n+        # Optimized to avoid list comprehension overhead\n+        result = []\n+        for seq in self.seqs_dict.values():\n+            if seq.is_finished():\n+                result.append(seq)\n+        return result\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n+        # Optimized: avoid repeated attribute lookups\n         for seq in self.seqs_dict.values():\n             if not seq.is_finished():\n-                seq.data.update_num_computed_tokens(num_new_computed_tokens)\n+                seq_data = seq.data\n+                seq_data.update_num_computed_tokens(num_new_computed_tokens)\n \n     def get_num_uncomputed_tokens(self) -> int:\n+        # Optimized: iterate dict values directly, avoid get_seqs() call\n         num_uncomputed_tokens = 0\n-        for seq in self.get_seqs():\n+        for seq in self.seqs_dict.values():\n             if not seq.is_finished():\n                 num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n         return num_uncomputed_tokens\n@@ -540,10 +573,20 @@ class SequenceGroup:\n         return len(self.get_seqs(status))\n \n     def num_unfinished_seqs(self) -> int:\n-        return len(self.get_unfinished_seqs())\n+        # Avoid creating intermediate list - just count\n+        count = 0\n+        for seq in self.seqs_dict.values():\n+            if not seq.is_finished():\n+                count += 1\n+        return count\n \n     def num_finished_seqs(self) -> int:\n-        return len(self.get_finished_seqs())\n+        # Avoid creating intermediate list - just count\n+        count = 0\n+        for seq in self.seqs_dict.values():\n+            if seq.is_finished():\n+                count += 1\n+        return count\n \n     def find(self, seq_id: int) -> Sequence:\n         if seq_id not in self.seqs_dict:\n@@ -561,11 +604,16 @@ class SequenceGroup:\n         del self.seqs_dict[seq_id]\n \n     def is_finished(self) -> bool:\n-        return all(seq.is_finished() for seq in self.get_seqs())\n+        # Optimized: iterate dict values directly, short-circuit on first unfinished\n+        for seq in self.seqs_dict.values():\n+            if not seq.is_finished():\n+                return False\n+        return True\n \n     def is_prefill(self) -> bool:\n         # Every sequence should be in the same stage.\n-        return self.get_seqs()[0].is_prefill()\n+        # Optimized: get first value directly without creating a list\n+        return next(iter(self.seqs_dict.values())).is_prefill()\n \n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\n@@ -857,7 +905,9 @@ class HiddenStates:\n         seq_ids = get_all_seq_ids(seq_group_metadata_list)\n         if seq_ids != self.seq_ids:\n             # Batch contents changed - prune removed sequences.\n-            index = [self.seq_ids.index(seq_id) for seq_id in seq_ids]\n+            # Optimized: use list comprehension with single pass\n+            seq_ids_index_map = {seq_id: i for i, seq_id in enumerate(self.seq_ids)}\n+            index = [seq_ids_index_map[seq_id] for seq_id in seq_ids]\n             self.hidden_states = self.hidden_states[index]\n             self.seq_ids = seq_ids\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
