diff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py
index 046f11d95..212ec01b7 100644
--- a/tests/kernels/test_moe.py
+++ b/tests/kernels/test_moe.py
@@ -15,7 +15,7 @@ from vllm.model_executor.models.mixtral import MixtralMoE
 def torch_moe(a, w1, w2, score, topk):
     B, D = a.shape
     a = a.view(B, -1, D).repeat(1, topk, 1).reshape(-1, D)
-    out = torch.zeros(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)
+    out = torch.empty(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)
     score = torch.softmax(score, dim=-1, dtype=torch.float32)
     topk_weight, topk_ids = torch.topk(score, topk)
     topk_weight = topk_weight.view(-1)
diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py
index 9ff9ba298..6743bc18d 100644
--- a/vllm/model_executor/models/mixtral.py
+++ b/vllm/model_executor/models/mixtral.py
@@ -112,20 +112,20 @@ class MixtralMoE(nn.Module):
 
         # Scaling factors for FP8 weights
         self.ws_scale = nn.Parameter(
-            torch.ones(self.num_total_experts, dtype=torch.float32),
+            torch.empty(self.num_total_experts, dtype=torch.float32),
             requires_grad=False) if self.use_fp8 else None
         self.w2s_scale = nn.Parameter(
-            torch.ones(self.num_total_experts, dtype=torch.float32),
+            torch.empty(self.num_total_experts, dtype=torch.float32),
             requires_grad=False) if self.use_fp8 else None
 
         # Scaling factors for FP8 activations
         need_act_scales = (self.use_fp8
                            and quant_config.activation_scheme == "static")
         self.as_scale = nn.Parameter(
-            torch.zeros(1, dtype=torch.float32),
+            torch.empty(1, dtype=torch.float32),
             requires_grad=False) if need_act_scales else None
         self.a2s_scale = nn.Parameter(
-            torch.zeros(1, dtype=torch.float32),
+            torch.empty(1, dtype=torch.float32),
             requires_grad=False) if need_act_scales else None
 
         if need_act_scales:
