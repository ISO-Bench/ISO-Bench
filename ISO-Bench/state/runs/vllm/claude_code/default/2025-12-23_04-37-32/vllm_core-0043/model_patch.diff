diff --git a/vllm/config.py b/vllm/config.py
index 6bfe94b76..285c75647 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -494,26 +494,28 @@ class ModelConfig:
         excluding anything before input ids/embeddings and after
         the final hidden states.
         """
-        factors: list[Any] = []
-        factors.append(self.model)
-        factors.append(self.dtype)
-        factors.append(self.quantization)
-        factors.append(self.revision)
-        factors.append(self.code_revision)
-        factors.append(self.max_model_len)
-        factors.append(self.max_logprobs)
-        factors.append(self.disable_sliding_window)
-        factors.append(self.trust_remote_code)
-        factors.append(self.generation_config)
-        factors.append(self.model_impl)
-        factors.append(self.override_generation_config)
-        factors.append(self.rope_scaling)
-        factors.append(self.rope_theta)
-        # hf_config can control how the model looks!
-        factors.append(self.hf_config.to_json_string())
+        # Optimize: Build factors list directly without intermediate appends
+        factors: list[Any] = [
+            self.model,
+            self.dtype,
+            self.quantization,
+            self.revision,
+            self.code_revision,
+            self.max_model_len,
+            self.max_logprobs,
+            self.disable_sliding_window,
+            self.trust_remote_code,
+            self.generation_config,
+            self.model_impl,
+            self.override_generation_config,
+            self.rope_scaling,
+            self.rope_theta,
+            # hf_config can control how the model looks!
+            self.hf_config.to_json_string()
+        ]
         str_factors = str(factors)
         assert_hashable(str_factors)
-        return hashlib.sha256(str(factors).encode()).hexdigest()
+        return hashlib.sha256(str_factors.encode()).hexdigest()
 
     def __post_init__(self) -> None:
         # Set the default seed to 0 in V1.
@@ -1815,8 +1817,8 @@ class CacheConfig:
         excluding anything before input ids/embeddings and after
         the final hidden states.
         """
-        factors: list[Any] = []
-        factors.append(self.cache_dtype)
+        # Optimize: Direct list construction
+        factors: list[Any] = [self.cache_dtype]
         # `cpu_offload_gb` does not use `torch.compile` yet.
         hash_str = hashlib.md5(str(factors).encode(),
                                usedforsecurity=False).hexdigest()
@@ -2177,12 +2179,14 @@ class ParallelConfig:
         excluding anything before input ids/embeddings and after
         the final hidden states.
         """
-        factors: list[Any] = []
-        factors.append(self.pipeline_parallel_size)
-        factors.append(self.tensor_parallel_size)
-        factors.append(self.enable_expert_parallel)
-        factors.append(self.data_parallel_size)
-        factors.append(envs.VLLM_ALL2ALL_BACKEND)
+        # Optimize: Direct list construction
+        factors: list[Any] = [
+            self.pipeline_parallel_size,
+            self.tensor_parallel_size,
+            self.enable_expert_parallel,
+            self.data_parallel_size,
+            envs.VLLM_ALL2ALL_BACKEND
+        ]
         return hashlib.sha256(str(factors).encode()).hexdigest()
 
     def __post_init__(self) -> None:
@@ -4502,82 +4506,53 @@ class VllmConfig:
         excluding anything before input ids/embeddings and after
         the final hidden states.
         """
-        factors: list[Any] = []
-
-        # summarize vllm config
-        vllm_factors: list[Any] = []
+        # Optimize: Build list directly and use inline conditionals
         from vllm import __version__
-        vllm_factors.append(__version__)
-        vllm_factors.append(envs.VLLM_USE_V1)
-        if self.model_config:
-            vllm_factors.append(self.model_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.cache_config:
-            vllm_factors.append(self.cache_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.parallel_config:
-            vllm_factors.append(self.parallel_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.scheduler_config:
-            vllm_factors.append(self.scheduler_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.device_config:
-            vllm_factors.append(self.device_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.load_config:
-            vllm_factors.append(self.load_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.lora_config:
-            vllm_factors.append(self.lora_config.compute_hash())
-            # LoRA creates static buffers based on max_num_batched_tokens.
-            # The tensor sizes and strides get captured in the torch.compile
-            # graph explicitly.
-            vllm_factors.append(
-                str(self.scheduler_config.max_num_batched_tokens))
-        else:
-            vllm_factors.append("None")
-        if self.speculative_config:
-            vllm_factors.append(self.speculative_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.decoding_config:
-            vllm_factors.append(self.decoding_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.observability_config:
-            vllm_factors.append(self.observability_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.quant_config:
-            pass  # should be captured by model_config.quantization
-        if self.compilation_config:
-            vllm_factors.append(self.compilation_config.compute_hash())
-        else:
-            vllm_factors.append("None")
-        if self.kv_transfer_config:
-            vllm_factors.append(self.kv_transfer_config.compute_hash())
-        else:
-            vllm_factors.append("None")
+
+        # Optimize: Compute additional_config_hash once
+        additional_hash = "None"
         if self.additional_config:
-            if isinstance(additional_config := self.additional_config, dict):
-                additional_config_hash = hashlib.md5(
-                    json.dumps(additional_config, sort_keys=True).encode(),
+            if isinstance(self.additional_config, dict):
+                additional_hash = hashlib.md5(
+                    json.dumps(self.additional_config, sort_keys=True).encode(),
                     usedforsecurity=False,
                 ).hexdigest()
             else:
-                additional_config_hash = additional_config.compute_hash()
-            vllm_factors.append(additional_config_hash)
+                additional_hash = self.additional_config.compute_hash()
+
+        # Optimize: Build vllm_factors list directly
+        vllm_factors: list[Any] = [
+            __version__,
+            envs.VLLM_USE_V1,
+            self.model_config.compute_hash() if self.model_config else "None",
+            self.cache_config.compute_hash() if self.cache_config else "None",
+            self.parallel_config.compute_hash() if self.parallel_config else "None",
+            self.scheduler_config.compute_hash() if self.scheduler_config else "None",
+            self.device_config.compute_hash() if self.device_config else "None",
+            self.load_config.compute_hash() if self.load_config else "None",
+        ]
+
+        # LoRA handling
+        if self.lora_config:
+            vllm_factors.extend([
+                self.lora_config.compute_hash(),
+                str(self.scheduler_config.max_num_batched_tokens)
+            ])
         else:
             vllm_factors.append("None")
-        factors.append(vllm_factors)
 
-        hash_str = hashlib.md5(str(factors).encode(),
+        # Optimize: Extend list with remaining configs
+        vllm_factors.extend([
+            self.speculative_config.compute_hash() if self.speculative_config else "None",
+            self.decoding_config.compute_hash() if self.decoding_config else "None",
+            self.observability_config.compute_hash() if self.observability_config else "None",
+            self.compilation_config.compute_hash() if self.compilation_config else "None",
+            self.kv_transfer_config.compute_hash() if self.kv_transfer_config else "None",
+            additional_hash
+        ])
+
+        # Optimize: Direct string conversion
+        hash_str = hashlib.md5(str([vllm_factors]).encode(),
                                usedforsecurity=False).hexdigest()[:10]
         return hash_str
 
diff --git a/vllm/envs.py b/vllm/envs.py
index 0eff74151..cb2f62bf1 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -995,11 +995,17 @@ environment_variables: dict[str, Callable[[], Any]] = {
 
 # --8<-- [end:env-vars-definition]
 
+# Cache for environment variable values to avoid repeated lambda calls
+_env_cache: dict[str, Any] = {}
+
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation of environment variables with caching
     if name in environment_variables:
-        return environment_variables[name]()
+        # Check cache first for better performance
+        if name not in _env_cache:
+            _env_cache[name] = environment_variables[name]()
+        return _env_cache[name]
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
@@ -1021,6 +1027,8 @@ def set_vllm_use_v1(use_v1: bool):
             "explicitly by the user. Please raise this as a Github "
             "Issue and explicitly set VLLM_USE_V1=0 or 1.")
     os.environ["VLLM_USE_V1"] = "1" if use_v1 else "0"
+    # Optimize: Clear cache when environment changes
+    _env_cache.pop("VLLM_USE_V1", None)
 
 
 def compute_hash() -> str:
@@ -1033,19 +1041,8 @@ def compute_hash() -> str:
     affect the choice of different kernels or attention backends should
     also be included in the factors list.
     """
-    factors: list[Any] = []
-
-    # summarize environment variables
-    def factorize(name: str):
-        if __getattr__(name):
-            factors.append(__getattr__(name))
-        else:
-            factors.append("None")
-
+    # Optimize: Build factors list directly without inner function
     # The values of envs may affects the computation graph.
-    # TODO(DefTruth): hash all environment variables?
-    # for key in environment_variables:
-    #     factorize(key)
     environment_variables_to_hash = [
         "VLLM_PP_LAYER_PARTITION",
         "VLLM_MLA_DISABLE",
@@ -1056,9 +1053,13 @@ def compute_hash() -> str:
         "VLLM_USE_STANDALONE_COMPILE",
         "VLLM_FUSED_MOE_CHUNK_SIZE",
     ]
-    for key in environment_variables_to_hash:
-        if key in environment_variables:
-            factorize(key)
+
+    # Optimize: Build list comprehension for better performance
+    factors: list[Any] = [
+        __getattr__(key) if __getattr__(key) else "None"
+        for key in environment_variables_to_hash
+        if key in environment_variables
+    ]
 
     hash_str = hashlib.md5(str(factors).encode(),
                            usedforsecurity=False).hexdigest()
