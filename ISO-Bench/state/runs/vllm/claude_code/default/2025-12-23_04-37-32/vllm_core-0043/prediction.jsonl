{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..285c75647 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -494,26 +494,28 @@ class ModelConfig:\n         excluding anything before input ids/embeddings and after\n         the final hidden states.\n         \"\"\"\n-        factors: list[Any] = []\n-        factors.append(self.model)\n-        factors.append(self.dtype)\n-        factors.append(self.quantization)\n-        factors.append(self.revision)\n-        factors.append(self.code_revision)\n-        factors.append(self.max_model_len)\n-        factors.append(self.max_logprobs)\n-        factors.append(self.disable_sliding_window)\n-        factors.append(self.trust_remote_code)\n-        factors.append(self.generation_config)\n-        factors.append(self.model_impl)\n-        factors.append(self.override_generation_config)\n-        factors.append(self.rope_scaling)\n-        factors.append(self.rope_theta)\n-        # hf_config can control how the model looks!\n-        factors.append(self.hf_config.to_json_string())\n+        # Optimize: Build factors list directly without intermediate appends\n+        factors: list[Any] = [\n+            self.model,\n+            self.dtype,\n+            self.quantization,\n+            self.revision,\n+            self.code_revision,\n+            self.max_model_len,\n+            self.max_logprobs,\n+            self.disable_sliding_window,\n+            self.trust_remote_code,\n+            self.generation_config,\n+            self.model_impl,\n+            self.override_generation_config,\n+            self.rope_scaling,\n+            self.rope_theta,\n+            # hf_config can control how the model looks!\n+            self.hf_config.to_json_string()\n+        ]\n         str_factors = str(factors)\n         assert_hashable(str_factors)\n-        return hashlib.sha256(str(factors).encode()).hexdigest()\n+        return hashlib.sha256(str_factors.encode()).hexdigest()\n \n     def __post_init__(self) -> None:\n         # Set the default seed to 0 in V1.\n@@ -1815,8 +1817,8 @@ class CacheConfig:\n         excluding anything before input ids/embeddings and after\n         the final hidden states.\n         \"\"\"\n-        factors: list[Any] = []\n-        factors.append(self.cache_dtype)\n+        # Optimize: Direct list construction\n+        factors: list[Any] = [self.cache_dtype]\n         # `cpu_offload_gb` does not use `torch.compile` yet.\n         hash_str = hashlib.md5(str(factors).encode(),\n                                usedforsecurity=False).hexdigest()\n@@ -2177,12 +2179,14 @@ class ParallelConfig:\n         excluding anything before input ids/embeddings and after\n         the final hidden states.\n         \"\"\"\n-        factors: list[Any] = []\n-        factors.append(self.pipeline_parallel_size)\n-        factors.append(self.tensor_parallel_size)\n-        factors.append(self.enable_expert_parallel)\n-        factors.append(self.data_parallel_size)\n-        factors.append(envs.VLLM_ALL2ALL_BACKEND)\n+        # Optimize: Direct list construction\n+        factors: list[Any] = [\n+            self.pipeline_parallel_size,\n+            self.tensor_parallel_size,\n+            self.enable_expert_parallel,\n+            self.data_parallel_size,\n+            envs.VLLM_ALL2ALL_BACKEND\n+        ]\n         return hashlib.sha256(str(factors).encode()).hexdigest()\n \n     def __post_init__(self) -> None:\n@@ -4502,82 +4506,53 @@ class VllmConfig:\n         excluding anything before input ids/embeddings and after\n         the final hidden states.\n         \"\"\"\n-        factors: list[Any] = []\n-\n-        # summarize vllm config\n-        vllm_factors: list[Any] = []\n+        # Optimize: Build list directly and use inline conditionals\n         from vllm import __version__\n-        vllm_factors.append(__version__)\n-        vllm_factors.append(envs.VLLM_USE_V1)\n-        if self.model_config:\n-            vllm_factors.append(self.model_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.cache_config:\n-            vllm_factors.append(self.cache_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.parallel_config:\n-            vllm_factors.append(self.parallel_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.scheduler_config:\n-            vllm_factors.append(self.scheduler_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.device_config:\n-            vllm_factors.append(self.device_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.load_config:\n-            vllm_factors.append(self.load_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.lora_config:\n-            vllm_factors.append(self.lora_config.compute_hash())\n-            # LoRA creates static buffers based on max_num_batched_tokens.\n-            # The tensor sizes and strides get captured in the torch.compile\n-            # graph explicitly.\n-            vllm_factors.append(\n-                str(self.scheduler_config.max_num_batched_tokens))\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.speculative_config:\n-            vllm_factors.append(self.speculative_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.decoding_config:\n-            vllm_factors.append(self.decoding_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.observability_config:\n-            vllm_factors.append(self.observability_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.quant_config:\n-            pass  # should be captured by model_config.quantization\n-        if self.compilation_config:\n-            vllm_factors.append(self.compilation_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n-        if self.kv_transfer_config:\n-            vllm_factors.append(self.kv_transfer_config.compute_hash())\n-        else:\n-            vllm_factors.append(\"None\")\n+\n+        # Optimize: Compute additional_config_hash once\n+        additional_hash = \"None\"\n         if self.additional_config:\n-            if isinstance(additional_config := self.additional_config, dict):\n-                additional_config_hash = hashlib.md5(\n-                    json.dumps(additional_config, sort_keys=True).encode(),\n+            if isinstance(self.additional_config, dict):\n+                additional_hash = hashlib.md5(\n+                    json.dumps(self.additional_config, sort_keys=True).encode(),\n                     usedforsecurity=False,\n                 ).hexdigest()\n             else:\n-                additional_config_hash = additional_config.compute_hash()\n-            vllm_factors.append(additional_config_hash)\n+                additional_hash = self.additional_config.compute_hash()\n+\n+        # Optimize: Build vllm_factors list directly\n+        vllm_factors: list[Any] = [\n+            __version__,\n+            envs.VLLM_USE_V1,\n+            self.model_config.compute_hash() if self.model_config else \"None\",\n+            self.cache_config.compute_hash() if self.cache_config else \"None\",\n+            self.parallel_config.compute_hash() if self.parallel_config else \"None\",\n+            self.scheduler_config.compute_hash() if self.scheduler_config else \"None\",\n+            self.device_config.compute_hash() if self.device_config else \"None\",\n+            self.load_config.compute_hash() if self.load_config else \"None\",\n+        ]\n+\n+        # LoRA handling\n+        if self.lora_config:\n+            vllm_factors.extend([\n+                self.lora_config.compute_hash(),\n+                str(self.scheduler_config.max_num_batched_tokens)\n+            ])\n         else:\n             vllm_factors.append(\"None\")\n-        factors.append(vllm_factors)\n \n-        hash_str = hashlib.md5(str(factors).encode(),\n+        # Optimize: Extend list with remaining configs\n+        vllm_factors.extend([\n+            self.speculative_config.compute_hash() if self.speculative_config else \"None\",\n+            self.decoding_config.compute_hash() if self.decoding_config else \"None\",\n+            self.observability_config.compute_hash() if self.observability_config else \"None\",\n+            self.compilation_config.compute_hash() if self.compilation_config else \"None\",\n+            self.kv_transfer_config.compute_hash() if self.kv_transfer_config else \"None\",\n+            additional_hash\n+        ])\n+\n+        # Optimize: Direct string conversion\n+        hash_str = hashlib.md5(str([vllm_factors]).encode(),\n                                usedforsecurity=False).hexdigest()[:10]\n         return hash_str\n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..cb2f62bf1 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -995,11 +995,17 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n # --8<-- [end:env-vars-definition]\n \n+# Cache for environment variable values to avoid repeated lambda calls\n+_env_cache: dict[str, Any] = {}\n+\n \n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation of environment variables with caching\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        # Check cache first for better performance\n+        if name not in _env_cache:\n+            _env_cache[name] = environment_variables[name]()\n+        return _env_cache[name]\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n@@ -1021,6 +1027,8 @@ def set_vllm_use_v1(use_v1: bool):\n             \"explicitly by the user. Please raise this as a Github \"\n             \"Issue and explicitly set VLLM_USE_V1=0 or 1.\")\n     os.environ[\"VLLM_USE_V1\"] = \"1\" if use_v1 else \"0\"\n+    # Optimize: Clear cache when environment changes\n+    _env_cache.pop(\"VLLM_USE_V1\", None)\n \n \n def compute_hash() -> str:\n@@ -1033,19 +1041,8 @@ def compute_hash() -> str:\n     affect the choice of different kernels or attention backends should\n     also be included in the factors list.\n     \"\"\"\n-    factors: list[Any] = []\n-\n-    # summarize environment variables\n-    def factorize(name: str):\n-        if __getattr__(name):\n-            factors.append(__getattr__(name))\n-        else:\n-            factors.append(\"None\")\n-\n+    # Optimize: Build factors list directly without inner function\n     # The values of envs may affects the computation graph.\n-    # TODO(DefTruth): hash all environment variables?\n-    # for key in environment_variables:\n-    #     factorize(key)\n     environment_variables_to_hash = [\n         \"VLLM_PP_LAYER_PARTITION\",\n         \"VLLM_MLA_DISABLE\",\n@@ -1056,9 +1053,13 @@ def compute_hash() -> str:\n         \"VLLM_USE_STANDALONE_COMPILE\",\n         \"VLLM_FUSED_MOE_CHUNK_SIZE\",\n     ]\n-    for key in environment_variables_to_hash:\n-        if key in environment_variables:\n-            factorize(key)\n+\n+    # Optimize: Build list comprehension for better performance\n+    factors: list[Any] = [\n+        __getattr__(key) if __getattr__(key) else \"None\"\n+        for key in environment_variables_to_hash\n+        if key in environment_variables\n+    ]\n \n     hash_str = hashlib.md5(str(factors).encode(),\n                            usedforsecurity=False).hexdigest()\n", "model_name_or_path": "gpt-5-2025-08-07"}
