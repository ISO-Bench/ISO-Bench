diff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py
index 9b116e00a..201d3b513 100644
--- a/vllm/v1/spec_decode/ngram_proposer.py
+++ b/vllm/v1/spec_decode/ngram_proposer.py
@@ -1,5 +1,5 @@
 # SPDX-License-Identifier: Apache-2.0
-from typing import List, Optional
+from typing import Optional
 
 import numpy as np
 
@@ -46,16 +46,17 @@ class NgramProposer:
         return self._find_subarray_kmp(context_token_ids, n, k)
 
     @staticmethod
-    def _kmp_lps_array(pattern: List[int]) -> List[int]:
+    def _kmp_lps_array(pattern: np.ndarray) -> np.ndarray:
         """
-        Build the lps (longest proper prefix which is also suffix) 
+        Build the lps (longest proper prefix which is also suffix)
         array for the pattern.
         """
-        lps = [0] * len(pattern)
+        pattern_len = len(pattern)
+        lps = np.zeros(pattern_len, dtype=np.int32)
         prev_lps = 0  # length of the previous longest prefix suffix
         i = 1
 
-        while i < len(pattern):
+        while i < pattern_len:
             if pattern[i] == pattern[prev_lps]:
                 prev_lps += 1
                 lps[i] = prev_lps
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 0ecc00acc..dad24d04b 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -154,10 +154,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         self.num_sms = self.device_properties.multi_processor_count
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
         # None in the first PP rank. The rest are set after load_model.
@@ -175,16 +175,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             # identical position IDs, making M-RoPE functionally equivalent to
             # 1D-RoPE.
             # See page 5 of https://arxiv.org/abs/2409.12191
-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
                 pin_memory=self.pin_memory)
 
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -197,27 +197,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
