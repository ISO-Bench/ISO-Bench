{"instance_id": "vllm-project__vllm-4a18fd1", "model_patch": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..0e9c721b8 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,9 +90,9 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n+        gate_up = self.gate_up_proj(x)[0]\n         x = self.act_fn(gate_up)\n-        x, _ = self.down_proj(x)\n+        x = self.down_proj(x)[0]\n         return x\n \n \n@@ -133,6 +133,9 @@ class LlamaAttention(nn.Module):\n                                 self.hidden_size // self.total_num_heads)\n         self.q_size = self.num_heads * self.head_dim\n         self.kv_size = self.num_kv_heads * self.head_dim\n+        # Pre-compute split indices for QKV slicing\n+        self.q_end = self.q_size\n+        self.k_end = self.q_end + self.kv_size\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = rope_theta\n         self.max_position_embeddings = max_position_embeddings\n@@ -183,11 +186,14 @@ class LlamaAttention(nn.Module):\n         kv_cache: torch.Tensor,\n         attn_metadata: AttentionMetadata,\n     ) -> torch.Tensor:\n-        qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        qkv = self.qkv_proj(hidden_states)[0]\n+        # Use pre-computed indices and slicing for better performance\n+        q = qkv[..., :self.q_end]\n+        k = qkv[..., self.q_end:self.k_end]\n+        v = qkv[..., self.k_end:]\n         q, k = self.rotary_emb(positions, q, k)\n         attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n-        output, _ = self.o_proj(attn_output)\n+        output = self.o_proj(attn_output)[0]\n         return output\n \n \n@@ -256,10 +262,8 @@ class LlamaDecoderLayer(nn.Module):\n         else:\n             hidden_states, residual = self.input_layernorm(\n                 hidden_states, residual)\n-        hidden_states = self.self_attn(positions=positions,\n-                                       hidden_states=hidden_states,\n-                                       kv_cache=kv_cache,\n-                                       attn_metadata=attn_metadata)\n+        hidden_states = self.self_attn(\n+            positions, hidden_states, kv_cache, attn_metadata)\n \n         # Fully Connected\n         hidden_states, residual = self.post_attention_layernorm(\n@@ -335,11 +339,13 @@ class LlamaModel(nn.Module):\n             hidden_states = intermediate_tensors[\"hidden_states\"]\n             residual = intermediate_tensors[\"residual\"]\n \n-        for i in range(self.start_layer, self.end_layer):\n-            layer = self.layers[i]\n-            hidden_states, residual = layer(positions, hidden_states,\n-                                            kv_caches[i - self.start_layer],\n-                                            attn_metadata, residual)\n+        # Cache start_layer to avoid repeated attribute access\n+        start_layer = self.start_layer\n+        for i in range(start_layer, self.end_layer):\n+            hidden_states, residual = self.layers[i](\n+                positions, hidden_states,\n+                kv_caches[i - start_layer],\n+                attn_metadata, residual)\n \n         if not get_pp_group().is_last_rank:\n             return IntermediateTensors({\n", "model_name_or_path": "gpt-5-2025-08-07"}
