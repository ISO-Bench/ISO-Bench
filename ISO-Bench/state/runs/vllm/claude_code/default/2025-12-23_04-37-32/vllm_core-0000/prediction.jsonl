{"instance_id": "vllm-project__vllm-fbefc8a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nindex f588f4016..135e87d1d 100644\n--- a/vllm/reasoning/qwen3_reasoning_parser.py\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -1,6 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-import re\n from collections.abc import Sequence\n from typing import Optional, Union\n \n@@ -31,8 +30,9 @@ class Qwen3ReasoningParser(ReasoningParser):\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n \n-        self.reasoning_regex = re.compile(\n-            rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n+        # Cache token lengths for better performance\n+        self.think_start_token_len = len(self.think_start_token)\n+        self.think_end_token_len = len(self.think_end_token)\n \n         if not self.model_tokenizer:\n             raise ValueError(\n@@ -47,6 +47,9 @@ class Qwen3ReasoningParser(ReasoningParser):\n                 \"Qwen3 reasoning parser could not locate think start/end \"\n                 \"tokens in the tokenizer!\")\n \n+        # Cache special token set for faster membership testing\n+        self._special_tokens = {self.think_start_token_id, self.think_end_token_id}\n+\n     def is_reasoning_end(self, input_ids: list[int]) -> bool:\n         return self.think_end_token_id in input_ids\n \n@@ -54,10 +57,13 @@ class Qwen3ReasoningParser(ReasoningParser):\n         \"\"\"\n         Extract the content after the end tokens\n         \"\"\"\n-        if self.think_end_token_id not in input_ids[:-1]:\n+        # Single-pass search: find the index and extract in one operation\n+        try:\n+            end_idx = input_ids.index(self.think_end_token_id)\n+            # Return slice starting after the end token\n+            return input_ids[end_idx + 1:]\n+        except ValueError:\n             return []\n-        else:\n-            return input_ids[input_ids.index(self.think_end_token_id) + 1:]\n \n     def extract_reasoning_content_streaming(\n         self,\n@@ -76,22 +82,40 @@ class Qwen3ReasoningParser(ReasoningParser):\n         - 'abc' goes to reasoning_content\n         - 'xyz' goes to content\n         \"\"\"\n+        # Cache lengths for performance\n+        delta_len = len(delta_token_ids)\n+\n         # Skip single special tokens\n-        if len(delta_token_ids) == 1 and (delta_token_ids[0] in [\n-                self.think_start_token_id, self.think_end_token_id\n-        ]):\n+        if delta_len == 1 and delta_token_ids[0] in self._special_tokens:\n             return None\n \n-        if self.think_start_token_id in previous_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        # Optimize for common case of small sequences\n+        # Only convert to frozenset if sequence length > threshold (amortizes overhead)\n+        prev_len = len(previous_token_ids)\n+        if prev_len > 10:\n+            prev_token_set = frozenset(previous_token_ids)\n+            start_in_prev = self.think_start_token_id in prev_token_set\n+            end_in_prev = self.think_end_token_id in prev_token_set\n+        else:\n+            start_in_prev = self.think_start_token_id in previous_token_ids\n+            end_in_prev = self.think_end_token_id in previous_token_ids\n+\n+        if delta_len > 10:\n+            delta_token_set = frozenset(delta_token_ids)\n+            start_in_delta = self.think_start_token_id in delta_token_set\n+            end_in_delta = self.think_end_token_id in delta_token_set\n+        else:\n+            start_in_delta = self.think_start_token_id in delta_token_ids\n+            end_in_delta = self.think_end_token_id in delta_token_ids\n+\n+        if start_in_prev:\n+            if end_in_delta:\n                 # <think> in previous, </think> in delta,\n                 # extract reasoning content\n-                end_index = delta_text.find(self.think_end_token)\n-                reasoning_content = delta_text[:end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                reasoning_content, _, content = delta_text.partition(self.think_end_token)\n                 return DeltaMessage(reasoning_content=reasoning_content,\n-                                    content=content if content else None)\n-            elif self.think_end_token_id in previous_token_ids:\n+                                    content=content or None)\n+            elif end_in_prev:\n                 # <think> in previous, </think> in previous,\n                 # reasoning content continues\n                 return DeltaMessage(content=delta_text)\n@@ -99,17 +123,14 @@ class Qwen3ReasoningParser(ReasoningParser):\n                 # <think> in previous, no </think> in previous or delta,\n                 # reasoning content continues\n                 return DeltaMessage(reasoning_content=delta_text)\n-        elif self.think_start_token_id in delta_token_ids:\n-            if self.think_end_token_id in delta_token_ids:\n+        elif start_in_delta:\n+            if end_in_delta:\n                 # <think> in delta, </think> in delta, extract reasoning content\n-                start_index = delta_text.find(self.think_start_token)\n-                end_index = delta_text.find(self.think_end_token)\n-                reasoning_content = delta_text[start_index +\n-                                               len(self.think_start_token\n-                                                   ):end_index]\n-                content = delta_text[end_index + len(self.think_end_token):]\n+                # Use partition for efficient string splitting\n+                _, _, after_start = delta_text.partition(self.think_start_token)\n+                reasoning_content, _, content = after_start.partition(self.think_end_token)\n                 return DeltaMessage(reasoning_content=reasoning_content,\n-                                    content=content if content else None)\n+                                    content=content or None)\n             else:\n                 # <think> in delta, no </think> in delta,\n                 # reasoning content continues\n@@ -121,29 +142,40 @@ class Qwen3ReasoningParser(ReasoningParser):\n     def extract_reasoning_content(\n             self, model_output: str, request: ChatCompletionRequest\n     ) -> tuple[Optional[str], Optional[str]]:\n+        \"\"\"\n+        Extract reasoning content from the model output.\n \n-        # Check if the model output contains the <think> tokens.\n-        if (self.think_start_token not in model_output\n-                or self.think_end_token not in model_output):\n+        For text <think>abc</think>xyz:\n+        - 'abc' goes to reasoning_content\n+        - 'xyz' goes to content (return value)\n+        \"\"\"\n+        # Use find for faster checks, only partition if found\n+        start_idx = model_output.find(self.think_start_token)\n+        if start_idx == -1:\n+            # No start token found\n             return None, model_output\n+\n+        # Find the end token after start token\n+        search_start = start_idx + self.think_start_token_len\n+        end_idx = model_output.find(self.think_end_token, search_start)\n+        if end_idx == -1:\n+            # No end token found\n+            return None, model_output\n+\n+        # Extract reasoning content between tokens\n+        reasoning = model_output[search_start:end_idx]\n+\n+        # Build content by concatenating before and after\n+        content_end = end_idx + self.think_end_token_len\n+        # Optimize common case where think tag is at the start\n+        if start_idx == 0:\n+            after = model_output[content_end:]\n+            content = after if after else None\n+        elif content_end >= len(model_output):\n+            # Think tag at the end, only before part\n+            content = model_output[:start_idx] if start_idx > 0 else None\n         else:\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            # Remove the reasoning content from the model output\n-            # Although <think> token is always at the\n-            # beginning of the line, we cannot guarantee that the\n-            # other models will follow this convention.\n-            # Therefore, we need to add :start_index.\n-            start_index = model_output.find(self.think_start_token)\n-            if start_index != -1:\n-                end_index = start_index + len(\n-                    f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-                )\n-                model_output = model_output[:start_index] + \\\n-                                model_output[end_index:]\n-\n-                if len(model_output) == 0:\n-                    return reasoning_content, None\n-\n-            return reasoning_content, model_output\n+            # Think tag in the middle\n+            content = model_output[:start_idx] + model_output[content_end:]\n+\n+        return reasoning, content\n", "model_name_or_path": "gpt-5-2025-08-07"}
