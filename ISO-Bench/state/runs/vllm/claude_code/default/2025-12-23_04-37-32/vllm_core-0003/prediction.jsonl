{"instance_id": "vllm-project__vllm-2b04c20", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 00260313e..9db253b7e 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -19,6 +19,13 @@ from vllm.utils import direct_register_custom_op\n \n logger = init_logger(__name__)\n \n+# Mapping from PyTorch dtypes to Triton compute types\n+_DTYPE_TO_COMPUTE_TYPE = {\n+    torch.bfloat16: tl.bfloat16,\n+    torch.float16: tl.float16,\n+    torch.float32: tl.float32,\n+}\n+\n \n @triton.jit\n def write_zeros_to_output(c_ptr, stride_cm, stride_cn, pid_n, N, offs_token,\n@@ -537,9 +544,10 @@ def moe_align_block_size_triton(\n     tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n+    cumsum[0] = 0\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -621,10 +629,10 @@ def moe_align_block_size(\n         by block_size for proper block matrix operations.\n     \"\"\"\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n-    sorted_ids = torch.empty((max_num_tokens_padded, ),\n-                             dtype=torch.int32,\n-                             device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n+    sorted_ids = torch.full((max_num_tokens_padded, ),\n+                            topk_ids.numel(),\n+                            dtype=torch.int32,\n+                            device=topk_ids.device)\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     # Expert ids must be zeroed out to prevent index out of bounds error while\n     # mapping global expert ids to local expert ids in expert parallelism.\n@@ -1250,13 +1258,8 @@ def fused_experts_impl(hidden_states: torch.Tensor,\n                                       device=hidden_states.device,\n                                       dtype=hidden_states.dtype)\n \n-    if hidden_states.dtype == torch.bfloat16:\n-        compute_type = tl.bfloat16\n-    elif hidden_states.dtype == torch.float16:\n-        compute_type = tl.float16\n-    elif hidden_states.dtype == torch.float32:\n-        compute_type = tl.float32\n-    else:\n+    compute_type = _DTYPE_TO_COMPUTE_TYPE.get(hidden_states.dtype)\n+    if compute_type is None:\n         raise ValueError(f\"Unsupported compute_type: {hidden_states.dtype}\")\n \n     if inplace:\n", "model_name_or_path": "gpt-5-2025-08-07"}
