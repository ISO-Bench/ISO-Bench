{"instance_id": "vllm-project__vllm-89ac266", "model_patch": "diff --git a/test_opt.py b/test_opt.py\nnew file mode 100644\nindex 000000000..5192898ec\n--- /dev/null\n+++ b/test_opt.py\n@@ -0,0 +1,37 @@\n+import os\n+import time\n+\n+# Ensure V1 path to exercise config logic paths if imported\n+os.environ.setdefault(\"VLLM_USE_V1\", \"1\")\n+\n+\n+def time_env_access(n: int = 200000):\n+    import vllm.envs as envs  # noqa: F401\n+    t0 = time.time()\n+    # access a bunch of env-driven properties to simulate runtime polling\n+    for _ in range(n):\n+        _ = envs.VLLM_USE_V1\n+        _ = envs.VLLM_ENABLE_V1_MULTIPROCESSING\n+        _ = envs.VLLM_ATTENTION_BACKEND\n+    return time.time() - t0\n+\n+\n+def main():\n+    # Baseline: environment access\n+    t_env = time_env_access()\n+    print(f\"envs access time: {t_env:.4f}s\")\n+\n+    # Verify the new env flag is wired correctly\n+    import vllm.envs as envs\n+    os.environ.pop(\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", None)\n+    print(\"ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\",\n+          envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE)\n+    os.environ[\"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\"] = \"1\"\n+    print(\"ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE (set):\",\n+          envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE)\n+\n+    print(\"OK\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..e6d442faf 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4769,12 +4769,21 @@ class VllmConfig:\n                 # Hybrid KV cache manager is not compatible with KV events.\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n-                self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning_once(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\")\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..9e0efc6dd 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -991,6 +991,13 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # The default value is \"VLLM\".\n     \"VLLM_PROCESS_NAME_PREFIX\":\n     lambda: os.getenv(\"VLLM_PROCESS_NAME_PREFIX\", \"VLLM\"),\n+\n+    # Allow enabling hybrid KV cache with chunked local attention despite\n+    # known latency regressions. Default to disabled for better perf.\n+    # Set to 1 to allow the combination.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: os.environ.get(\n+        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\").lower() in (\"1\", \"true\"),\n }\n \n # --8<-- [end:env-vars-definition]\n", "model_name_or_path": "gpt-5-2025-08-07"}
