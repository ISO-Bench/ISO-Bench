diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/tests/v1/core/test_kv_cache_utils.py b/tests/v1/core/test_kv_cache_utils.py
index 68b060156..f4bda3dbb 100644
--- a/tests/v1/core/test_kv_cache_utils.py
+++ b/tests/v1/core/test_kv_cache_utils.py
@@ -184,6 +184,80 @@ def test_free_kv_cache_block_queue_operations():
     assert str(e.value) == "No free blocks available"
 
 
+
+
+def test_free_kv_cache_block_queue_append_n():
+    # Create an empty FreeKVCacheBlockQueue
+    queue = FreeKVCacheBlockQueue([])
+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]
+
+    # Append 0 blocks: still empty
+    queue.append_n([])
+    assert queue.num_free_blocks == 0
+    assert (queue.fake_free_list_head.next_free_block
+            is queue.fake_free_list_tail)
+    assert (queue.fake_free_list_tail.prev_free_block
+            is queue.fake_free_list_head)
+
+    # Append 1 block
+    queue.append_n(blocks[0:1])
+    assert queue.num_free_blocks == 1
+    assert queue.fake_free_list_head.next_free_block is blocks[0]
+    assert blocks[0].prev_free_block is queue.fake_free_list_head
+    assert blocks[0].next_free_block is queue.fake_free_list_tail
+    assert queue.fake_free_list_tail.prev_free_block is blocks[0]
+
+    # Append 2 more blocks
+    queue.append_n(blocks[1:3])
+    assert queue.num_free_blocks == 3
+    assert queue.fake_free_list_head.next_free_block is blocks[0]
+    assert blocks[0].next_free_block is blocks[1]
+    assert blocks[1].prev_free_block is blocks[0]
+    assert blocks[1].next_free_block is blocks[2]
+    assert blocks[2].prev_free_block is blocks[1]
+    assert blocks[2].next_free_block is queue.fake_free_list_tail
+    assert queue.fake_free_list_tail.prev_free_block is blocks[2]
+
+    # Append a single block again
+    queue.append(blocks[3])
+    assert queue.num_free_blocks == 4
+    assert queue.fake_free_list_tail.prev_free_block is blocks[3]
+    assert blocks[3].prev_free_block is blocks[2]
+    assert blocks[3].next_free_block is queue.fake_free_list_tail
+
+
+def test_free_kv_cache_block_queue_popleft_n():
+    # Initialize 6 blocks in queue
+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]
+    queue = FreeKVCacheBlockQueue(blocks)
+
+    # Pop zero returns empty
+    popped0 = queue.popleft_n(0)
+    assert popped0 == []
+    assert queue.num_free_blocks == 6
+
+    # Pop two
+    popped = queue.popleft_n(2)
+    assert [b.block_id for b in popped] == [0, 1]
+    assert queue.num_free_blocks == 4
+    assert queue.fake_free_list_head.next_free_block is blocks[2]
+    # Ensure popped blocks are detached
+    assert all(b.prev_free_block is None and b.next_free_block is None
+               for b in popped)
+
+    # Pop the rest
+    popped_rest = queue.popleft_n(10)
+    assert [b.block_id for b in popped_rest] == [2, 3, 4, 5]
+    assert queue.num_free_blocks == 0
+    assert (queue.fake_free_list_head.next_free_block
+            is queue.fake_free_list_tail)
+    assert (queue.fake_free_list_tail.prev_free_block
+            is queue.fake_free_list_head)
+
+    # Now any popleft should error
+    with pytest.raises(ValueError):
+        queue.popleft()
+
 def test_free_kv_cache_block_queue_get_all_free_blocks():
     # Create a list of KVCacheBlock objects
     blocks = [KVCacheBlock(block_id=i) for i in range(5)]
diff --git a/vllm/model_executor/guided_decoding/guidance_logits_processors.py b/vllm/model_executor/guided_decoding/guidance_logits_processors.py
index 379b5eaa3..816550cb8 100644
--- a/vllm/model_executor/guided_decoding/guidance_logits_processors.py
+++ b/vllm/model_executor/guided_decoding/guidance_logits_processors.py
@@ -41,17 +41,28 @@ class GuidanceLogitsProcessor:
         self.bitmask = None
         self.new_sampling = False
         self.initialized = False
+        # Cache of device-specific bitmask tensors to avoid reallocations
+        self._device_bitmasks: dict[str, torch.Tensor] = {}
 
     def clone(self) -> "GuidanceLogitsProcessor":
         cloned = copy.copy(self)
+        # A cloned processor should not share mutable buffers/state
+        cloned._device_bitmasks = {}
         if self.initialized:
             cloned.ll_matcher = llguidance.LLMatcher(
                 self.ll_tokenizer,  # type: ignore[assignment]
                 self.grammar,
                 log_level=int(os.environ.get("LLGUIDANCE_LOG_LEVEL", "1")),
             )
-            self.bitmask = llguidance.torch.allocate_token_bitmask(
+            # Allocate a fresh CPU bitmask for the clone
+            cloned.bitmask = llguidance.torch.allocate_token_bitmask(
                 1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]
+            # Prefer pinned memory to speed up H2D copies when available
+            try:
+                if torch.cuda.is_available():
+                    cloned.bitmask = cloned.bitmask.pin_memory()  # type: ignore[assignment]
+            except Exception:
+                pass
         return cloned
 
     def _initialize(self):
@@ -71,9 +82,15 @@ class GuidanceLogitsProcessor:
             log_level=int(os.environ.get("LLGUIDANCE_LOG_LEVEL", "1")),
         )
 
-        # create reusable bitmask
+        # create reusable CPU bitmask
         self.bitmask = llguidance.torch.allocate_token_bitmask(
             1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]
+        # Prefer pinned memory for faster non_blocking H2D transfer
+        try:
+            if torch.cuda.is_available():
+                self.bitmask = self.bitmask.pin_memory()  # type: ignore[assignment]
+        except Exception:
+            pass
 
         self.initialized = True
 
@@ -93,11 +110,25 @@ class GuidanceLogitsProcessor:
             if err:
                 logger.warning("Error in LLMatcher: %s", err)
 
+        # Update CPU bitmask with allowed tokens
         llguidance.torch.fill_next_token_bitmask(self.ll_matcher, self.bitmask,
                                                  0)
-        llguidance.torch.apply_token_bitmask_inplace(
-            scores,
-            self.bitmask.to(scores.device))  # type: ignore[attr-defined]
+
+        # Apply mask efficiently, reusing device buffers to avoid reallocations
+        if scores.device.type == "cpu":
+            llguidance.torch.apply_token_bitmask_inplace(
+                scores, self.bitmask)  # type: ignore[attr-defined]
+        else:
+            key = str(scores.device)
+            dev_mask = self._device_bitmasks.get(key)
+            if (dev_mask is None or dev_mask.device != scores.device
+                    or dev_mask.shape != self.bitmask.shape
+                    or dev_mask.dtype != self.bitmask.dtype):
+                dev_mask = torch.empty_like(self.bitmask, device=scores.device)
+                self._device_bitmasks[key] = dev_mask
+            # Non-blocking copy from (pinned) CPU to device
+            dev_mask.copy_(self.bitmask, non_blocking=True)
+            llguidance.torch.apply_token_bitmask_inplace(scores, dev_mask)
 
         self.new_sampling = True
 
diff --git a/vllm/model_executor/guided_decoding/outlines_logits_processors.py b/vllm/model_executor/guided_decoding/outlines_logits_processors.py
index 7f047a1df..15f9321b1 100644
--- a/vllm/model_executor/guided_decoding/outlines_logits_processors.py
+++ b/vllm/model_executor/guided_decoding/outlines_logits_processors.py
@@ -41,11 +41,19 @@ class BaseLogitsProcessor:
         self._eos_token_id: int = eos_token_id
         self._reasoner: Optional[ReasoningParser] = reasoner
         self._mask: Optional[torch.Tensor] = None
+        # Cached device-side mask to avoid repeated allocations/transfers
+        self._mask_device: Optional[torch.Tensor] = None
 
     def __call__(self, input_ids: list[int],
                  scores: torch.Tensor) -> torch.Tensor:
         if self._mask is None:
             self._mask = allocate_token_bitmask(scores.size(-1))
+            # Prefer pinned memory so H2D copies can be non_blocking
+            try:
+                if torch.cuda.is_available():
+                    self._mask = self._mask.pin_memory()
+            except Exception:
+                pass
 
         # Skip the structured logits processing if reasoning is not finished.
         # reasoner is not None only when `--reasoning-parser` is set.
@@ -77,11 +85,22 @@ class BaseLogitsProcessor:
         # be ignored by the kernel, taking care of the issue with
         # models such as Llama 3.2 Vision with an `<|image|>` token
         # with id 128256, but scores.shape == torch.Size([128256])
-        _apply_token_bitmask_inplace_kernel(
-            logits=scores.unsqueeze(dim=0),
-            # mask must be on same device
-            mask=self._mask.to(scores.device, non_blocking=True))
-        self._mask.to("cpu", non_blocking=True)
+        if scores.device.type == "cpu":
+            _apply_token_bitmask_inplace_kernel(
+                logits=scores.unsqueeze(dim=0),
+                mask=self._mask)
+        else:
+            # Reuse a device-side mask and only copy the data
+            if (self._mask_device is None or
+                    self._mask_device.device != scores.device or
+                    self._mask_device.shape != self._mask.shape or
+                    self._mask_device.dtype != self._mask.dtype):
+                self._mask_device = torch.empty_like(self._mask,
+                                                     device=scores.device)
+            self._mask_device.copy_(self._mask, non_blocking=True)
+            _apply_token_bitmask_inplace_kernel(
+                logits=scores.unsqueeze(dim=0),
+                mask=self._mask_device)
 
         return scores
 
diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
index bdd3a1a9c..b9cc9331d 100644
--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py
+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py
@@ -298,6 +298,8 @@ class XGrammarLogitsProcessor:
     ctx: xgr.CompiledGrammar | None = None
     tokenizer_info: xgr.TokenizerInfo = None  # type: ignore[assignment]
     token_bitmask: torch.Tensor = None  # type: ignore[assignment]
+    # Cached device-side copy of token_bitmask to avoid per-step allocations
+    device_bitmask: torch.Tensor | None = None
     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)
     batch_size: int = field(default=1)
     prefilled: bool = field(default=False)
@@ -320,6 +322,7 @@ class XGrammarLogitsProcessor:
         self.matchers = []
         self.batch_size = 1
         self.token_bitmask = None  # type: ignore[assignment]
+        self.device_bitmask = None
         self.prefilled = False
 
     def _ensure_ctx(self):
@@ -392,8 +395,18 @@ class XGrammarLogitsProcessor:
         # Note: In this method, if the tensors have different dimensions
         # on CPU device fails, but on GPU it runs without error. Hence the
         # unsqueeze above for scores, to match the token bitmask shape
-        xgr.apply_token_bitmask_inplace(
-            scores, self.token_bitmask.to(scores.device, non_blocking=True))
+        if device_type == "cuda":
+            # Reuse a device-side mask; copy CPU mask contents each step
+            if (self.device_bitmask is None or
+                    self.device_bitmask.device != scores.device or
+                    self.device_bitmask.shape != self.token_bitmask.shape or
+                    self.device_bitmask.dtype != self.token_bitmask.dtype):
+                self.device_bitmask = torch.empty_like(self.token_bitmask,
+                                                       device=scores.device)
+            self.device_bitmask.copy_(self.token_bitmask, non_blocking=True)
+            xgr.apply_token_bitmask_inplace(scores, self.device_bitmask)
+        else:
+            xgr.apply_token_bitmask_inplace(scores, self.token_bitmask)
         if device_type != "cuda":
             scores = scores.to(dtype).to(device_type).squeeze()
 
@@ -414,9 +427,11 @@ class XGrammarLogitsProcessor:
                 xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)
             ]
 
-        # Create a new token bitmask with the same size
+        # Create a new token bitmask with the same size (do not share)
         if hasattr(self, 'token_bitmask') and self.token_bitmask is not None:
-            new_processor.token_bitmask = self.token_bitmask
+            new_processor.token_bitmask = xgr.allocate_token_bitmask(
+                self.batch_size, self.tokenizer_info.vocab_size)
+            new_processor.device_bitmask = None
 
         # Copy simple attributes
         new_processor.batch_size = self.batch_size
diff --git a/vllm/sequence.py b/vllm/sequence.py
index 99208fbad..c9faabbe6 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -286,7 +286,7 @@ class SequenceData(msgspec.Struct,
 
     @output_embeds.setter
     def output_embeds(self, new_output_token_embeds: torch.Tensor) -> None:
-        self._output_token_embeds = new_output_token_embeds
+        self._output_embeds = new_output_token_embeds
         self._update_cached_all_token_embeds()
 
     @property
diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py
index cbb6bb268..4d339d4c0 100644
--- a/vllm/v1/core/block_pool.py
+++ b/vllm/v1/core/block_pool.py
@@ -215,20 +215,15 @@ class BlockPool:
                 f"Cannot get {num_blocks} free blocks from the pool")
 
         ret: list[KVCacheBlock] = []
-        idx = 0
-        while idx < num_blocks:
-            # First allocate blocks.
-            curr_block = self.free_block_queue.popleft()
+        # Batch-pop from the free list to reduce Python overhead.
+        popped_blocks = self.free_block_queue.popleft_n(num_blocks)
+        for curr_block in popped_blocks:
             assert curr_block.ref_cnt == 0
-
             # If the block is cached, evict it.
             if self.enable_caching:
                 self._maybe_evict_cached_block(curr_block)
-
             curr_block.incr_ref()
             ret.append(curr_block)
-            idx += 1
-
         return ret
 
     def _maybe_evict_cached_block(self, block: KVCacheBlock) -> bool:
@@ -289,11 +284,14 @@ class BlockPool:
             ordered_blocks: A list of blocks to free ordered by their eviction
                 priority.
         """
+        append_list: list[KVCacheBlock] = []
         for block in ordered_blocks:
             block.decr_ref()
             # null_block should not be added to the free list.
             if block.ref_cnt == 0 and not block.is_null:
-                self.free_block_queue.append(block)
+                append_list.append(block)
+        if append_list:
+            self.free_block_queue.append_n(append_list)
 
     def reset_prefix_cache(self) -> bool:
         """Reset prefix cache. This function may be used in RLHF
diff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py
index 457d95cc7..4ad783109 100644
--- a/vllm/v1/core/kv_cache_utils.py
+++ b/vllm/v1/core/kv_cache_utils.py
@@ -293,27 +293,93 @@ class FreeKVCacheBlockQueue:
         block.prev_free_block = block.next_free_block = None
         self.num_free_blocks -= 1
 
-    def append(self, block: KVCacheBlock) -> None:
-        """Put a block back into the free list and increase
-        num_free_blocks by 1.
+    def popleft_n(self, n: int) -> list[KVCacheBlock]:
+        """Pop up to n free blocks from the head in a single splice.
+
+        Args:
+            n: Maximum number of blocks to pop.
+
+        Returns:
+            A list of popped blocks in order from head to tail.
+        """
+        if n <= 0:
+            return []
+        if (self.fake_free_list_head.next_free_block is self.fake_free_list_tail
+                or self.fake_free_list_head.next_free_block is None):
+            assert self.num_free_blocks == 0, (
+                f"num_free_blocks ({self.num_free_blocks}) is out of sync "
+                "with the free list.")
+            raise ValueError("No free blocks available")
+
+        # Determine the actual number of blocks to pop and the tail of the slice.
+        first_block: KVCacheBlock = self.fake_free_list_head.next_free_block  # type: ignore[assignment]
+        count = 1
+        tail_block = first_block
+        while count < n and tail_block.next_free_block is not None \
+                and tail_block.next_free_block is not self.fake_free_list_tail:
+            tail_block = tail_block.next_free_block
+            count += 1
+
+        # Detach [first_block, ..., tail_block] from the free list by splicing.
+        next_after_tail = tail_block.next_free_block  # could be fake tail
+        assert next_after_tail is not None
+        self.fake_free_list_head.next_free_block = next_after_tail
+        next_after_tail.prev_free_block = self.fake_free_list_head
+
+        # Collect popped blocks and clear their free-list links.
+        popped: list[KVCacheBlock] = []
+        curr = first_block
+        while True:
+            next_curr = curr.next_free_block
+            curr.prev_free_block = None
+            curr.next_free_block = None
+            popped.append(curr)
+            if curr is tail_block:
+                break
+            assert next_curr is not None
+            curr = next_curr
+
+        self.num_free_blocks -= count
+        return popped
+
+    def append_n(self, blocks: list[KVCacheBlock]) -> None:
+        """Append a list of blocks to the tail in a single splice.
 
         Args:
-            block: The block to append.
+            blocks: The blocks to append in order.
         """
+        if not blocks:
+            return
         if self.fake_free_list_tail.prev_free_block is None:
             raise RuntimeError(
                 "prev_free_block of fake_free_list_tail should always exist")
         last_block: KVCacheBlock = self.fake_free_list_tail.prev_free_block
 
-        # Connect the new block after the last block.
-        last_block.next_free_block = block
-        block.prev_free_block = last_block
+        # Link the first new block after the current last block.
+        first_new = blocks[0]
+        last_block.next_free_block = first_new
+        first_new.prev_free_block = last_block
 
-        # Connect the fake tail after the new block.
-        block.next_free_block = self.fake_free_list_tail
-        self.fake_free_list_tail.prev_free_block = block
+        # Chain the internal blocks.
+        for i in range(len(blocks) - 1):
+            a = blocks[i]
+            b = blocks[i + 1]
+            a.next_free_block = b
+            b.prev_free_block = a
+
+        # Link the last new block to the fake tail.
+        last_new = blocks[-1]
+        last_new.next_free_block = self.fake_free_list_tail
+        self.fake_free_list_tail.prev_free_block = last_new
+
+        self.num_free_blocks += len(blocks)
+
+    def append(self, block: KVCacheBlock) -> None:
+        """Append a single block to the tail.
+        This is a compatibility wrapper around append_n.
+        """
+        self.append_n([block])
 
-        self.num_free_blocks += 1
 
     def get_all_free_blocks(self) -> list[KVCacheBlock]:
         """Get all free blocks in the free list. Mainly used for testing.
