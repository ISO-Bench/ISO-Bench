{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-4ce64e2", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/core/test_kv_cache_utils.py b/tests/v1/core/test_kv_cache_utils.py\nindex 68b060156..f4bda3dbb 100644\n--- a/tests/v1/core/test_kv_cache_utils.py\n+++ b/tests/v1/core/test_kv_cache_utils.py\n@@ -184,6 +184,80 @@ def test_free_kv_cache_block_queue_operations():\n     assert str(e.value) == \"No free blocks available\"\n \n \n+\n+\n+def test_free_kv_cache_block_queue_append_n():\n+    # Create an empty FreeKVCacheBlockQueue\n+    queue = FreeKVCacheBlockQueue([])\n+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]\n+\n+    # Append 0 blocks: still empty\n+    queue.append_n([])\n+    assert queue.num_free_blocks == 0\n+    assert (queue.fake_free_list_head.next_free_block\n+            is queue.fake_free_list_tail)\n+    assert (queue.fake_free_list_tail.prev_free_block\n+            is queue.fake_free_list_head)\n+\n+    # Append 1 block\n+    queue.append_n(blocks[0:1])\n+    assert queue.num_free_blocks == 1\n+    assert queue.fake_free_list_head.next_free_block is blocks[0]\n+    assert blocks[0].prev_free_block is queue.fake_free_list_head\n+    assert blocks[0].next_free_block is queue.fake_free_list_tail\n+    assert queue.fake_free_list_tail.prev_free_block is blocks[0]\n+\n+    # Append 2 more blocks\n+    queue.append_n(blocks[1:3])\n+    assert queue.num_free_blocks == 3\n+    assert queue.fake_free_list_head.next_free_block is blocks[0]\n+    assert blocks[0].next_free_block is blocks[1]\n+    assert blocks[1].prev_free_block is blocks[0]\n+    assert blocks[1].next_free_block is blocks[2]\n+    assert blocks[2].prev_free_block is blocks[1]\n+    assert blocks[2].next_free_block is queue.fake_free_list_tail\n+    assert queue.fake_free_list_tail.prev_free_block is blocks[2]\n+\n+    # Append a single block again\n+    queue.append(blocks[3])\n+    assert queue.num_free_blocks == 4\n+    assert queue.fake_free_list_tail.prev_free_block is blocks[3]\n+    assert blocks[3].prev_free_block is blocks[2]\n+    assert blocks[3].next_free_block is queue.fake_free_list_tail\n+\n+\n+def test_free_kv_cache_block_queue_popleft_n():\n+    # Initialize 6 blocks in queue\n+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]\n+    queue = FreeKVCacheBlockQueue(blocks)\n+\n+    # Pop zero returns empty\n+    popped0 = queue.popleft_n(0)\n+    assert popped0 == []\n+    assert queue.num_free_blocks == 6\n+\n+    # Pop two\n+    popped = queue.popleft_n(2)\n+    assert [b.block_id for b in popped] == [0, 1]\n+    assert queue.num_free_blocks == 4\n+    assert queue.fake_free_list_head.next_free_block is blocks[2]\n+    # Ensure popped blocks are detached\n+    assert all(b.prev_free_block is None and b.next_free_block is None\n+               for b in popped)\n+\n+    # Pop the rest\n+    popped_rest = queue.popleft_n(10)\n+    assert [b.block_id for b in popped_rest] == [2, 3, 4, 5]\n+    assert queue.num_free_blocks == 0\n+    assert (queue.fake_free_list_head.next_free_block\n+            is queue.fake_free_list_tail)\n+    assert (queue.fake_free_list_tail.prev_free_block\n+            is queue.fake_free_list_head)\n+\n+    # Now any popleft should error\n+    with pytest.raises(ValueError):\n+        queue.popleft()\n+\n def test_free_kv_cache_block_queue_get_all_free_blocks():\n     # Create a list of KVCacheBlock objects\n     blocks = [KVCacheBlock(block_id=i) for i in range(5)]\ndiff --git a/vllm/model_executor/guided_decoding/guidance_logits_processors.py b/vllm/model_executor/guided_decoding/guidance_logits_processors.py\nindex 379b5eaa3..816550cb8 100644\n--- a/vllm/model_executor/guided_decoding/guidance_logits_processors.py\n+++ b/vllm/model_executor/guided_decoding/guidance_logits_processors.py\n@@ -41,17 +41,28 @@ class GuidanceLogitsProcessor:\n         self.bitmask = None\n         self.new_sampling = False\n         self.initialized = False\n+        # Cache of device-specific bitmask tensors to avoid reallocations\n+        self._device_bitmasks: dict[str, torch.Tensor] = {}\n \n     def clone(self) -> \"GuidanceLogitsProcessor\":\n         cloned = copy.copy(self)\n+        # A cloned processor should not share mutable buffers/state\n+        cloned._device_bitmasks = {}\n         if self.initialized:\n             cloned.ll_matcher = llguidance.LLMatcher(\n                 self.ll_tokenizer,  # type: ignore[assignment]\n                 self.grammar,\n                 log_level=int(os.environ.get(\"LLGUIDANCE_LOG_LEVEL\", \"1\")),\n             )\n-            self.bitmask = llguidance.torch.allocate_token_bitmask(\n+            # Allocate a fresh CPU bitmask for the clone\n+            cloned.bitmask = llguidance.torch.allocate_token_bitmask(\n                 1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]\n+            # Prefer pinned memory to speed up H2D copies when available\n+            try:\n+                if torch.cuda.is_available():\n+                    cloned.bitmask = cloned.bitmask.pin_memory()  # type: ignore[assignment]\n+            except Exception:\n+                pass\n         return cloned\n \n     def _initialize(self):\n@@ -71,9 +82,15 @@ class GuidanceLogitsProcessor:\n             log_level=int(os.environ.get(\"LLGUIDANCE_LOG_LEVEL\", \"1\")),\n         )\n \n-        # create reusable bitmask\n+        # create reusable CPU bitmask\n         self.bitmask = llguidance.torch.allocate_token_bitmask(\n             1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]\n+        # Prefer pinned memory for faster non_blocking H2D transfer\n+        try:\n+            if torch.cuda.is_available():\n+                self.bitmask = self.bitmask.pin_memory()  # type: ignore[assignment]\n+        except Exception:\n+            pass\n \n         self.initialized = True\n \n@@ -93,11 +110,25 @@ class GuidanceLogitsProcessor:\n             if err:\n                 logger.warning(\"Error in LLMatcher: %s\", err)\n \n+        # Update CPU bitmask with allowed tokens\n         llguidance.torch.fill_next_token_bitmask(self.ll_matcher, self.bitmask,\n                                                  0)\n-        llguidance.torch.apply_token_bitmask_inplace(\n-            scores,\n-            self.bitmask.to(scores.device))  # type: ignore[attr-defined]\n+\n+        # Apply mask efficiently, reusing device buffers to avoid reallocations\n+        if scores.device.type == \"cpu\":\n+            llguidance.torch.apply_token_bitmask_inplace(\n+                scores, self.bitmask)  # type: ignore[attr-defined]\n+        else:\n+            key = str(scores.device)\n+            dev_mask = self._device_bitmasks.get(key)\n+            if (dev_mask is None or dev_mask.device != scores.device\n+                    or dev_mask.shape != self.bitmask.shape\n+                    or dev_mask.dtype != self.bitmask.dtype):\n+                dev_mask = torch.empty_like(self.bitmask, device=scores.device)\n+                self._device_bitmasks[key] = dev_mask\n+            # Non-blocking copy from (pinned) CPU to device\n+            dev_mask.copy_(self.bitmask, non_blocking=True)\n+            llguidance.torch.apply_token_bitmask_inplace(scores, dev_mask)\n \n         self.new_sampling = True\n \ndiff --git a/vllm/model_executor/guided_decoding/outlines_logits_processors.py b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\nindex 7f047a1df..15f9321b1 100644\n--- a/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n+++ b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n@@ -41,11 +41,19 @@ class BaseLogitsProcessor:\n         self._eos_token_id: int = eos_token_id\n         self._reasoner: Optional[ReasoningParser] = reasoner\n         self._mask: Optional[torch.Tensor] = None\n+        # Cached device-side mask to avoid repeated allocations/transfers\n+        self._mask_device: Optional[torch.Tensor] = None\n \n     def __call__(self, input_ids: list[int],\n                  scores: torch.Tensor) -> torch.Tensor:\n         if self._mask is None:\n             self._mask = allocate_token_bitmask(scores.size(-1))\n+            # Prefer pinned memory so H2D copies can be non_blocking\n+            try:\n+                if torch.cuda.is_available():\n+                    self._mask = self._mask.pin_memory()\n+            except Exception:\n+                pass\n \n         # Skip the structured logits processing if reasoning is not finished.\n         # reasoner is not None only when `--reasoning-parser` is set.\n@@ -77,11 +85,22 @@ class BaseLogitsProcessor:\n         # be ignored by the kernel, taking care of the issue with\n         # models such as Llama 3.2 Vision with an `<|image|>` token\n         # with id 128256, but scores.shape == torch.Size([128256])\n-        _apply_token_bitmask_inplace_kernel(\n-            logits=scores.unsqueeze(dim=0),\n-            # mask must be on same device\n-            mask=self._mask.to(scores.device, non_blocking=True))\n-        self._mask.to(\"cpu\", non_blocking=True)\n+        if scores.device.type == \"cpu\":\n+            _apply_token_bitmask_inplace_kernel(\n+                logits=scores.unsqueeze(dim=0),\n+                mask=self._mask)\n+        else:\n+            # Reuse a device-side mask and only copy the data\n+            if (self._mask_device is None or\n+                    self._mask_device.device != scores.device or\n+                    self._mask_device.shape != self._mask.shape or\n+                    self._mask_device.dtype != self._mask.dtype):\n+                self._mask_device = torch.empty_like(self._mask,\n+                                                     device=scores.device)\n+            self._mask_device.copy_(self._mask, non_blocking=True)\n+            _apply_token_bitmask_inplace_kernel(\n+                logits=scores.unsqueeze(dim=0),\n+                mask=self._mask_device)\n \n         return scores\n \ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex bdd3a1a9c..b9cc9331d 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -298,6 +298,8 @@ class XGrammarLogitsProcessor:\n     ctx: xgr.CompiledGrammar | None = None\n     tokenizer_info: xgr.TokenizerInfo = None  # type: ignore[assignment]\n     token_bitmask: torch.Tensor = None  # type: ignore[assignment]\n+    # Cached device-side copy of token_bitmask to avoid per-step allocations\n+    device_bitmask: torch.Tensor | None = None\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n@@ -320,6 +322,7 @@ class XGrammarLogitsProcessor:\n         self.matchers = []\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n+        self.device_bitmask = None\n         self.prefilled = False\n \n     def _ensure_ctx(self):\n@@ -392,8 +395,18 @@ class XGrammarLogitsProcessor:\n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n         # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(\n-            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n+        if device_type == \"cuda\":\n+            # Reuse a device-side mask; copy CPU mask contents each step\n+            if (self.device_bitmask is None or\n+                    self.device_bitmask.device != scores.device or\n+                    self.device_bitmask.shape != self.token_bitmask.shape or\n+                    self.device_bitmask.dtype != self.token_bitmask.dtype):\n+                self.device_bitmask = torch.empty_like(self.token_bitmask,\n+                                                       device=scores.device)\n+            self.device_bitmask.copy_(self.token_bitmask, non_blocking=True)\n+            xgr.apply_token_bitmask_inplace(scores, self.device_bitmask)\n+        else:\n+            xgr.apply_token_bitmask_inplace(scores, self.token_bitmask)\n         if device_type != \"cuda\":\n             scores = scores.to(dtype).to(device_type).squeeze()\n \n@@ -414,9 +427,11 @@ class XGrammarLogitsProcessor:\n                 xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n             ]\n \n-        # Create a new token bitmask with the same size\n+        # Create a new token bitmask with the same size (do not share)\n         if hasattr(self, 'token_bitmask') and self.token_bitmask is not None:\n-            new_processor.token_bitmask = self.token_bitmask\n+            new_processor.token_bitmask = xgr.allocate_token_bitmask(\n+                self.batch_size, self.tokenizer_info.vocab_size)\n+            new_processor.device_bitmask = None\n \n         # Copy simple attributes\n         new_processor.batch_size = self.batch_size\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 99208fbad..c9faabbe6 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -286,7 +286,7 @@ class SequenceData(msgspec.Struct,\n \n     @output_embeds.setter\n     def output_embeds(self, new_output_token_embeds: torch.Tensor) -> None:\n-        self._output_token_embeds = new_output_token_embeds\n+        self._output_embeds = new_output_token_embeds\n         self._update_cached_all_token_embeds()\n \n     @property\ndiff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex cbb6bb268..4d339d4c0 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -215,20 +215,15 @@ class BlockPool:\n                 f\"Cannot get {num_blocks} free blocks from the pool\")\n \n         ret: list[KVCacheBlock] = []\n-        idx = 0\n-        while idx < num_blocks:\n-            # First allocate blocks.\n-            curr_block = self.free_block_queue.popleft()\n+        # Batch-pop from the free list to reduce Python overhead.\n+        popped_blocks = self.free_block_queue.popleft_n(num_blocks)\n+        for curr_block in popped_blocks:\n             assert curr_block.ref_cnt == 0\n-\n             # If the block is cached, evict it.\n             if self.enable_caching:\n                 self._maybe_evict_cached_block(curr_block)\n-\n             curr_block.incr_ref()\n             ret.append(curr_block)\n-            idx += 1\n-\n         return ret\n \n     def _maybe_evict_cached_block(self, block: KVCacheBlock) -> bool:\n@@ -289,11 +284,14 @@ class BlockPool:\n             ordered_blocks: A list of blocks to free ordered by their eviction\n                 priority.\n         \"\"\"\n+        append_list: list[KVCacheBlock] = []\n         for block in ordered_blocks:\n             block.decr_ref()\n             # null_block should not be added to the free list.\n             if block.ref_cnt == 0 and not block.is_null:\n-                self.free_block_queue.append(block)\n+                append_list.append(block)\n+        if append_list:\n+            self.free_block_queue.append_n(append_list)\n \n     def reset_prefix_cache(self) -> bool:\n         \"\"\"Reset prefix cache. This function may be used in RLHF\ndiff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py\nindex 457d95cc7..4ad783109 100644\n--- a/vllm/v1/core/kv_cache_utils.py\n+++ b/vllm/v1/core/kv_cache_utils.py\n@@ -293,27 +293,93 @@ class FreeKVCacheBlockQueue:\n         block.prev_free_block = block.next_free_block = None\n         self.num_free_blocks -= 1\n \n-    def append(self, block: KVCacheBlock) -> None:\n-        \"\"\"Put a block back into the free list and increase\n-        num_free_blocks by 1.\n+    def popleft_n(self, n: int) -> list[KVCacheBlock]:\n+        \"\"\"Pop up to n free blocks from the head in a single splice.\n+\n+        Args:\n+            n: Maximum number of blocks to pop.\n+\n+        Returns:\n+            A list of popped blocks in order from head to tail.\n+        \"\"\"\n+        if n <= 0:\n+            return []\n+        if (self.fake_free_list_head.next_free_block is self.fake_free_list_tail\n+                or self.fake_free_list_head.next_free_block is None):\n+            assert self.num_free_blocks == 0, (\n+                f\"num_free_blocks ({self.num_free_blocks}) is out of sync \"\n+                \"with the free list.\")\n+            raise ValueError(\"No free blocks available\")\n+\n+        # Determine the actual number of blocks to pop and the tail of the slice.\n+        first_block: KVCacheBlock = self.fake_free_list_head.next_free_block  # type: ignore[assignment]\n+        count = 1\n+        tail_block = first_block\n+        while count < n and tail_block.next_free_block is not None \\\n+                and tail_block.next_free_block is not self.fake_free_list_tail:\n+            tail_block = tail_block.next_free_block\n+            count += 1\n+\n+        # Detach [first_block, ..., tail_block] from the free list by splicing.\n+        next_after_tail = tail_block.next_free_block  # could be fake tail\n+        assert next_after_tail is not None\n+        self.fake_free_list_head.next_free_block = next_after_tail\n+        next_after_tail.prev_free_block = self.fake_free_list_head\n+\n+        # Collect popped blocks and clear their free-list links.\n+        popped: list[KVCacheBlock] = []\n+        curr = first_block\n+        while True:\n+            next_curr = curr.next_free_block\n+            curr.prev_free_block = None\n+            curr.next_free_block = None\n+            popped.append(curr)\n+            if curr is tail_block:\n+                break\n+            assert next_curr is not None\n+            curr = next_curr\n+\n+        self.num_free_blocks -= count\n+        return popped\n+\n+    def append_n(self, blocks: list[KVCacheBlock]) -> None:\n+        \"\"\"Append a list of blocks to the tail in a single splice.\n \n         Args:\n-            block: The block to append.\n+            blocks: The blocks to append in order.\n         \"\"\"\n+        if not blocks:\n+            return\n         if self.fake_free_list_tail.prev_free_block is None:\n             raise RuntimeError(\n                 \"prev_free_block of fake_free_list_tail should always exist\")\n         last_block: KVCacheBlock = self.fake_free_list_tail.prev_free_block\n \n-        # Connect the new block after the last block.\n-        last_block.next_free_block = block\n-        block.prev_free_block = last_block\n+        # Link the first new block after the current last block.\n+        first_new = blocks[0]\n+        last_block.next_free_block = first_new\n+        first_new.prev_free_block = last_block\n \n-        # Connect the fake tail after the new block.\n-        block.next_free_block = self.fake_free_list_tail\n-        self.fake_free_list_tail.prev_free_block = block\n+        # Chain the internal blocks.\n+        for i in range(len(blocks) - 1):\n+            a = blocks[i]\n+            b = blocks[i + 1]\n+            a.next_free_block = b\n+            b.prev_free_block = a\n+\n+        # Link the last new block to the fake tail.\n+        last_new = blocks[-1]\n+        last_new.next_free_block = self.fake_free_list_tail\n+        self.fake_free_list_tail.prev_free_block = last_new\n+\n+        self.num_free_blocks += len(blocks)\n+\n+    def append(self, block: KVCacheBlock) -> None:\n+        \"\"\"Append a single block to the tail.\n+        This is a compatibility wrapper around append_n.\n+        \"\"\"\n+        self.append_n([block])\n \n-        self.num_free_blocks += 1\n \n     def get_all_free_blocks(self) -> list[KVCacheBlock]:\n         \"\"\"Get all free blocks in the free list. Mainly used for testing.\n", "model_name_or_path": "gpt-5-2025-08-07"}
