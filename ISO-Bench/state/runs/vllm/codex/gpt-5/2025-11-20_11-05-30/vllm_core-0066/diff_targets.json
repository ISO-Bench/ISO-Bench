{
  "changed": [
    ".buildkite/check-wheel-size.py",
    ".buildkite/nightly-benchmarks/benchmark-pipeline.yaml",
    ".buildkite/nightly-benchmarks/scripts/nightly-annotate.sh",
    ".buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh",
    ".buildkite/nightly-benchmarks/tests/genai-perf-tests.json",
    ".buildkite/release-pipeline.yaml",
    ".buildkite/run-cpu-test.sh",
    ".buildkite/run-gh200-test.sh",
    ".buildkite/run-hpu-test.sh",
    ".buildkite/run-neuron-test.sh",
    ".buildkite/run-openvino-test.sh",
    ".buildkite/run-tpu-test.sh",
    ".buildkite/run-xpu-test.sh",
    ".buildkite/test-pipeline.yaml",
    ".github/CODEOWNERS",
    ".github/ISSUE_TEMPLATE/600-new-model.yml",
    ".github/workflows/actionlint.yml",
    ".github/workflows/clang-format.yml",
    ".github/workflows/codespell.yml",
    ".github/workflows/lint-and-deploy.yaml",
    ".github/workflows/matchers/ruff.json",
    ".github/workflows/mypy.yaml",
    ".github/workflows/png-lint.yml",
    ".github/workflows/pre-commit.yml",
    ".github/workflows/ruff.yml",
    ".github/workflows/shellcheck.yml",
    ".github/workflows/sphinx-lint.yml",
    ".github/workflows/yapf.yml",
    ".gitignore",
    ".pre-commit-config.yaml",
    "CMakeLists.txt",
    "Dockerfile",
    "Dockerfile.cpu",
    "Dockerfile.hpu",
    "Dockerfile.neuron",
    "Dockerfile.openvino",
    "Dockerfile.ppc64le",
    "Dockerfile.rocm",
    "Dockerfile.rocm_base",
    "Dockerfile.tpu",
    "README.md",
    "SECURITY.md",
    "benchmarks/backend_request_func.py",
    "benchmarks/benchmark_latency.py",
    "benchmarks/benchmark_long_document_qa_throughput.py",
    "benchmarks/benchmark_prefix_caching.py",
    "benchmarks/benchmark_serving.py",
    "benchmarks/cutlass_benchmarks/w8a8_benchmarks.py",
    "benchmarks/kernels/benchmark_lora.py",
    "benchmarks/kernels/benchmark_moe.py",
    "benchmarks/kernels/benchmark_paged_attention.py",
    "benchmarks/kernels/utils.py",
    "cmake/cpu_extension.cmake",
    "cmake/utils.cmake",
    "csrc/activation_kernels.cu",
    "csrc/attention/attention_kernels.cuh",
    "csrc/attention/paged_attention_v1.cu",
    "csrc/attention/paged_attention_v2.cu",
    "csrc/cache.h",
    "csrc/cache_kernels.cu",
    "csrc/core/math.hpp",
    "csrc/core/scalar_type.hpp",
    "csrc/cpu/attention.cpp",
    "csrc/cpu/cache.cpp",
    "csrc/cpu/cpu_types.hpp",
    "csrc/cpu/cpu_types_arm.hpp",
    "csrc/cpu/cpu_types_vsx.hpp",
    "csrc/cpu/cpu_types_x86.hpp",
    "csrc/cpu/quant.cpp",
    "csrc/cpu/torch_bindings.cpp",
    "csrc/cpu/utils.cpp",
    "csrc/cumem_allocator.cpp",
    "csrc/custom_all_reduce.cuh",
    "csrc/cutlass_extensions/common.hpp",
    "csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp",
    "csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp",
    "csrc/cutlass_extensions/gemm/collective/collective_builder.hpp",
    "csrc/cutlass_extensions/gemm/collective/fp8_accumulation.hpp",
    "csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp",
    "csrc/cutlass_extensions/gemm/dispatch_policy.hpp",
    "csrc/cutlass_extensions/torch_utils.hpp",
    "csrc/cutlass_extensions/vllm_collective_builder.cuh",
    "csrc/mamba/causal_conv1d/causal_conv1d.cu",
    "csrc/mamba/mamba_ssm/selective_scan_fwd.cu",
    "csrc/moe/marlin_kernels/marlin_moe_kernel.h",
    "csrc/moe/moe_align_sum_kernels.cu",
    "csrc/ops.h",
    "csrc/prepare_inputs/advance_step.cu",
    "csrc/quantization/compressed_tensors/int8_quant_kernels.cu",
    "csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu",
    "csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu",
    "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
    "csrc/quantization/gptq_marlin/gptq_marlin.cu",
    "csrc/quantization/machete/generate.py",
    "csrc/quantization/machete/machete_mainloop.cuh",
    "csrc/quantization/machete/machete_mm_kernel.cuh",
    "csrc/quantization/machete/machete_mm_launcher.cuh",
    "csrc/quantization/machete/machete_prepack_launcher.cuh",
    "csrc/quantization/machete/machete_pytorch.cu",
    "csrc/quantization/marlin/dense/marlin_cuda_kernel.cu",
    "csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu",
    "csrc/quantization/marlin/sparse/common/mma.h",
    "csrc/rocm/attention.cu",
    "csrc/rocm/ops.h",
    "csrc/rocm/torch_bindings.cpp",
    "csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu",
    "csrc/sparse/cutlass/sparse_scaled_mm_entry.cu",
    "csrc/torch_bindings.cpp",
    "docs/Makefile",
    "docs/README.md",
    "docs/requirements-docs.txt",
    "docs/source/_static/custom.js",
    "docs/source/api/engine/async_llm_engine.md",
    "docs/source/api/engine/index.md",
    "docs/source/api/engine/llm_engine.md",
    "docs/source/api/inference_params.md",
    "docs/source/api/model/adapters.md",
    "docs/source/api/model/index.md",
    "docs/source/api/model/interfaces.md",
    "docs/source/api/model/interfaces_base.md",
    "docs/source/api/multimodal/index.md",
    "docs/source/api/multimodal/inputs.md",
    "docs/source/api/multimodal/parse.md",
    "docs/source/api/multimodal/processing.md",
    "docs/source/api/multimodal/profiling.md",
    "docs/source/api/multimodal/registry.md",
    "docs/source/api/offline_inference/index.md",
    "docs/source/api/offline_inference/llm.md",
    "docs/source/api/offline_inference/llm_inputs.md",
    "docs/source/assets/contributing/dockerfile-stages-dependency.png",
    "docs/source/assets/deployment/architecture_helm_deployment.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-1.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-3.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-4.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-5.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-6.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-7.png",
    "docs/source/assets/design/v1/prefix_caching/free.png",
    "docs/source/assets/design/v1/prefix_caching/overview.png",
    "docs/source/assets/features/disagg_prefill/abstraction.jpg",
    "docs/source/assets/features/disagg_prefill/overview.jpg",
    "docs/source/assets/logos/vllm-logo-only-light.ico",
    "docs/source/community/blog.md",
    "docs/source/community/meetups.md",
    "docs/source/community/sponsors.md",
    "docs/source/conf.py",
    "docs/source/contributing/dockerfile/dockerfile.md",
    "docs/source/contributing/model/basic.md",
    "docs/source/contributing/model/index.md",
    "docs/source/contributing/model/multimodal.md",
    "docs/source/contributing/model/registration.md",
    "docs/source/contributing/model/tests.md",
    "docs/source/contributing/overview.md",
    "docs/source/contributing/profiling/profiling_index.md",
    "docs/source/contributing/vulnerability_management.md",
    "docs/source/deployment/docker.md",
    "docs/source/deployment/frameworks/bentoml.md",
    "docs/source/deployment/frameworks/cerebrium.md",
    "docs/source/deployment/frameworks/dstack.md",
    "docs/source/deployment/frameworks/helm.md",
    "docs/source/deployment/frameworks/index.md",
    "docs/source/deployment/frameworks/lws.md",
    "docs/source/deployment/frameworks/modal.md",
    "docs/source/deployment/frameworks/skypilot.md",
    "docs/source/deployment/frameworks/triton.md",
    "docs/source/deployment/integrations/index.md",
    "docs/source/deployment/integrations/kserve.md",
    "docs/source/deployment/integrations/kubeai.md",
    "docs/source/deployment/integrations/llamastack.md",
    "docs/source/deployment/k8s.md",
    "docs/source/deployment/nginx.md",
    "docs/source/design/arch_overview.md",
    "docs/source/design/automatic_prefix_caching.md",
    "docs/source/design/input_processing/input_processing_pipeline.md",
    "docs/source/design/input_processing/model_inputs_index.md",
    "docs/source/design/kernel/paged_attention.md",
    "docs/source/design/mm_processing.md",
    "docs/source/design/multimodal/adding_multimodal_plugin.md",
    "docs/source/design/multimodal/multimodal_index.md",
    "docs/source/design/multiprocessing.md",
    "docs/source/design/v1/prefix_caching.md",
    "docs/source/dev/pooling_params.md",
    "docs/source/dev/sampling_params.md",
    "docs/source/features/automatic_prefix_caching.md",
    "docs/source/features/compatibility_matrix.md",
    "docs/source/features/disagg_prefill.md",
    "docs/source/features/lora.md",
    "docs/source/features/quantization/auto_awq.md",
    "docs/source/features/quantization/bnb.md",
    "docs/source/features/quantization/fp8.md",
    "docs/source/features/quantization/gguf.md",
    "docs/source/features/quantization/index.md",
    "docs/source/features/quantization/int4.md",
    "docs/source/features/quantization/int8.md",
    "docs/source/features/quantization/quantized_kvcache.md",
    "docs/source/features/quantization/supported_hardware.md",
    "docs/source/features/reasoning_outputs.md",
    "docs/source/features/spec_decode.md",
    "docs/source/features/structured_outputs.md",
    "docs/source/features/tool_calling.md",
    "docs/source/generate_examples.py",
    "docs/source/getting_started/amd-installation.md",
    "docs/source/getting_started/arm-installation.md",
    "docs/source/getting_started/examples/examples_index.template.md",
    "docs/source/getting_started/faq.md",
    "docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/index.md",
    "docs/source/getting_started/installation/ai_accelerator/neuron.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/openvino.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/tpu.inc.md",
    "docs/source/getting_started/installation/cpu/apple.inc.md",
    "docs/source/getting_started/installation/cpu/arm.inc.md",
    "docs/source/getting_started/installation/cpu/build.inc.md",
    "docs/source/getting_started/installation/cpu/index.md",
    "docs/source/getting_started/installation/cpu/x86.inc.md",
    "docs/source/getting_started/installation/device.template.md",
    "docs/source/getting_started/installation/gpu/cuda.inc.md",
    "docs/source/getting_started/installation/gpu/index.md",
    "docs/source/getting_started/installation/gpu/rocm.inc.md",
    "docs/source/getting_started/installation/gpu/xpu.inc.md",
    "docs/source/getting_started/installation/index.md",
    "docs/source/getting_started/installation/python_env_setup.inc.md",
    "docs/source/getting_started/quickstart.md",
    "docs/source/getting_started/troubleshooting.md",
    "docs/source/index.md",
    "docs/source/models/adding_model.md",
    "docs/source/models/enabling_multimodal_inputs.md",
    "docs/source/models/extensions/index.md",
    "docs/source/models/extensions/runai_model_streamer.md",
    "docs/source/models/extensions/tensorizer.md",
    "docs/source/models/generative_models.md",
    "docs/source/models/pooling_models.md",
    "docs/source/models/supported_models.md",
    "docs/source/performance/optimization.md",
    "docs/source/quantization/fp8_e4m3_kvcache.md",
    "docs/source/quantization/fp8_e5m2_kvcache.md",
    "docs/source/quantization/supported_hardware.md",
    "docs/source/serving/deploying_with_helm.md",
    "docs/source/serving/deploying_with_k8s.md",
    "docs/source/serving/distributed_serving.md",
    "docs/source/serving/engine_args.md",
    "docs/source/serving/env_vars.md",
    "docs/source/serving/integrations.md",
    "docs/source/serving/integrations/index.md",
    "docs/source/serving/integrations/langchain.md",
    "docs/source/serving/integrations/llamaindex.md",
    "docs/source/serving/metrics.md",
    "docs/source/serving/multimodal_inputs.md",
    "docs/source/serving/offline_inference.md",
    "docs/source/serving/openai_compatible_server.md",
    "docs/source/serving/usage_stats.md",
    "docs/source/usage/compatibility_matrix.md",
    "examples/fp8/README.md",
    "examples/fp8/extract_scales.py",
    "examples/fp8/quantizer/README.md",
    "examples/fp8/quantizer/quantize.py",
    "examples/gguf_inference.py",
    "examples/offline_inference/aqlm_example.py",
    "examples/offline_inference/arctic.py",
    "examples/offline_inference/audio_language.py",
    "examples/offline_inference/basic.py",
    "examples/offline_inference/basic_with_model_default_sampling.py",
    "examples/offline_inference/chat.py",
    "examples/offline_inference/chat_with_tools.py",
    "examples/offline_inference/classification.py",
    "examples/offline_inference/cli.py",
    "examples/offline_inference/cpu_offload.py",
    "examples/offline_inference/distributed.py",
    "examples/offline_inference/embedding.py",
    "examples/offline_inference/encoder_decoder.py",
    "examples/offline_inference/florence2_inference.py",
    "examples/offline_inference/gguf_inference.py",
    "examples/offline_inference/llm_engine_example.py",
    "examples/offline_inference/lora_with_quantization_inference.py",
    "examples/offline_inference/mlpspeculator.py",
    "examples/offline_inference/multilora_inference.py",
    "examples/offline_inference/neuron.py",
    "examples/offline_inference/neuron_int8_quantization.py",
    "examples/offline_inference/openai/openai_batch.md",
    "examples/offline_inference/openai/openai_example_batch.jsonl",
    "examples/offline_inference/pixtral.py",
    "examples/offline_inference/prefix_caching.py",
    "examples/offline_inference/profiling.py",
    "examples/offline_inference/profiling_tpu/README.md",
    "examples/offline_inference/profiling_tpu/profiling.py",
    "examples/offline_inference/rlhf.py",
    "examples/offline_inference/save_sharded_state.py",
    "examples/offline_inference/scoring.py",
    "examples/offline_inference/simple_profiling.py",
    "examples/offline_inference/structured_outputs.py",
    "examples/offline_inference/torchrun_example.py",
    "examples/offline_inference/tpu.py",
    "examples/offline_inference/vision_language.py",
    "examples/offline_inference/vision_language_embedding.py",
    "examples/offline_inference/vision_language_multi_image.py",
    "examples/offline_inference/whisper.py",
    "examples/online_serving/api_client.py",
    "examples/online_serving/chart-helm/.helmignore",
    "examples/online_serving/chart-helm/Chart.yaml",
    "examples/online_serving/chart-helm/README.md",
    "examples/online_serving/chart-helm/ct.yaml",
    "examples/online_serving/chart-helm/lintconf.yaml",
    "examples/online_serving/chart-helm/templates/_helpers.tpl",
    "examples/online_serving/chart-helm/templates/configmap.yaml",
    "examples/online_serving/chart-helm/templates/custom-objects.yaml",
    "examples/online_serving/chart-helm/templates/deployment.yaml",
    "examples/online_serving/chart-helm/templates/hpa.yaml",
    "examples/online_serving/chart-helm/templates/job.yaml",
    "examples/online_serving/chart-helm/templates/poddisruptionbudget.yaml",
    "examples/online_serving/chart-helm/templates/pvc.yaml",
    "examples/online_serving/chart-helm/templates/secrets.yaml",
    "examples/online_serving/chart-helm/templates/service.yaml",
    "examples/online_serving/chart-helm/values.schema.json",
    "examples/online_serving/chart-helm/values.yaml",
    "examples/online_serving/cohere_rerank_client.py",
    "examples/online_serving/disaggregated_prefill.sh",
    "examples/online_serving/gradio_openai_chatbot_webserver.py",
    "examples/online_serving/gradio_webserver.py",
    "examples/online_serving/jinaai_rerank_client.py",
    "examples/online_serving/openai_chat_completion_client.py",
    "examples/online_serving/openai_chat_completion_client_for_multimodal.py",
    "examples/online_serving/openai_chat_completion_client_with_tools.py",
    "examples/online_serving/openai_chat_completion_structured_outputs.py",
    "examples/online_serving/openai_chat_completion_with_reasoning.py",
    "examples/online_serving/openai_chat_completion_with_reasoning_streaming.py",
    "examples/online_serving/openai_chat_embedding_client_for_multimodal.py",
    "examples/online_serving/openai_completion_client.py",
    "examples/online_serving/openai_cross_encoder_score.py",
    "examples/online_serving/openai_embedding_client.py",
    "examples/online_serving/openai_pooling_client.py",
    "examples/online_serving/opentelemetry/Otel.md",
    "examples/online_serving/opentelemetry/dummy_client.py",
    "examples/online_serving/prometheus_grafana/README.md",
    "examples/online_serving/prometheus_grafana/docker-compose.yaml",
    "examples/online_serving/prometheus_grafana/grafana.json",
    "examples/online_serving/prometheus_grafana/prometheus.yaml",
    "examples/online_serving/run_cluster.sh",
    "examples/online_serving/sagemaker-entrypoint.sh",
    "examples/other/logging_configuration.md",
    "examples/other/tensorize_vllm_model.py",
    "examples/template_deepseek_vl2.jinja",
    "examples/template_pixtral_hf.jinja",
    "format.sh",
    "model_patch.diff",
    "pyproject.toml",
    "python_only_dev.py",
    "requirements-common.txt",
    "requirements-cpu.txt",
    "requirements-cuda.txt",
    "requirements-hpu.txt",
    "requirements-lint.txt",
    "requirements-test.in",
    "requirements-test.txt",
    "requirements-tpu.txt",
    "setup.py",
    "tests/async_engine/test_api_server.py",
    "tests/basic_correctness/test_basic_correctness.py",
    "tests/basic_correctness/test_cumem.py",
    "tests/basic_correctness/test_preemption.py",
    "tests/compile/test_basic_correctness.py",
    "tests/conftest.py",
    "tests/core/block/test_prefix_caching_block.py",
    "tests/distributed/test_custom_all_reduce.py",
    "tests/distributed/test_pynccl.py",
    "tests/distributed/test_torchrun_example.py",
    "tests/engine/test_custom_executor.py",
    "tests/engine/test_multiproc_workers.py",
    "tests/entrypoints/llm/test_collective_rpc.py",
    "tests/entrypoints/llm/test_encode.py",
    "tests/entrypoints/openai/reasoning_parsers/__init__.py",
    "tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py",
    "tests/entrypoints/openai/reasoning_parsers/utils.py",
    "tests/entrypoints/openai/test_cli_args.py",
    "tests/entrypoints/openai/test_lora_adapters.py",
    "tests/entrypoints/openai/test_lora_lineage.py",
    "tests/entrypoints/openai/test_metrics.py",
    "tests/entrypoints/openai/test_rerank.py",
    "tests/entrypoints/openai/test_run_batch.py",
    "tests/entrypoints/openai/test_score.py",
    "tests/entrypoints/openai/test_serving_chat.py",
    "tests/entrypoints/openai/test_serving_models.py",
    "tests/entrypoints/openai/test_shutdown.py",
    "tests/entrypoints/test_chat_utils.py",
    "tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json",
    "tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json",
    "tests/kernels/test_activation.py",
    "tests/kernels/test_attention.py",
    "tests/kernels/test_attention_selector.py",
    "tests/kernels/test_block_fp8.py",
    "tests/kernels/test_blocksparse_attention.py",
    "tests/kernels/test_cache.py",
    "tests/kernels/test_cascade_flash_attn.py",
    "tests/kernels/test_cutlass.py",
    "tests/kernels/test_cutlass_2of4_sparse.py",
    "tests/kernels/test_encoder_decoder_attn.py",
    "tests/kernels/test_flash_attn.py",
    "tests/kernels/test_flashinfer.py",
    "tests/kernels/test_mha_attn.py",
    "tests/kernels/test_moe.py",
    "tests/kernels/test_prefix_prefill.py",
    "tests/kernels/test_semi_structured.py",
    "tests/kernels/test_triton_decode_attention.py",
    "tests/kernels/test_triton_scaled_mm.py",
    "tests/kernels/utils.py",
    "tests/kv_transfer/test_lookup_buffer.py",
    "tests/kv_transfer/test_send_recv.py",
    "tests/lora/conftest.py",
    "tests/lora/test_layers.py",
    "tests/lora/test_lora_checkpoints.py",
    "tests/lora/test_lora_huggingface.py",
    "tests/lora/test_lora_manager.py",
    "tests/lora/test_minicpmv.py",
    "tests/lora/test_minicpmv_tp.py",
    "tests/lora/test_mixtral.py",
    "tests/lora/test_peft_helper.py",
    "tests/lora/test_punica_ops_sizes.py",
    "tests/lora/test_punica_ops_variation.py",
    "tests/lora/test_quant_model.py",
    "tests/lora/test_qwen2vl.py",
    "tests/lora/utils.py",
    "tests/model_executor/test_model_load_with_params.py",
    "tests/models/decoder_only/audio_language/test_ultravox.py",
    "tests/models/decoder_only/language/test_fp8.py",
    "tests/models/decoder_only/language/test_gguf.py",
    "tests/models/decoder_only/language/test_jamba.py",
    "tests/models/decoder_only/language/test_mamba.py",
    "tests/models/decoder_only/language/test_models.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py",
    "tests/models/decoder_only/vision_language/test_models.py",
    "tests/models/decoder_only/vision_language/test_pixtral.py",
    "tests/models/decoder_only/vision_language/test_qwen2_vl.py",
    "tests/models/decoder_only/vision_language/vlm_utils/model_utils.py",
    "tests/models/embedding/language/test_cls_models.py",
    "tests/models/embedding/language/test_embedding.py",
    "tests/models/embedding/language/test_scoring.py",
    "tests/models/encoder_decoder/audio_language/__init__.py",
    "tests/models/encoder_decoder/audio_language/test_whisper.py",
    "tests/models/encoder_decoder/vision_language/test_mllama.py",
    "tests/models/multimodal/__init__.py",
    "tests/models/multimodal/processing/__init__.py",
    "tests/models/multimodal/processing/test_common.py",
    "tests/models/multimodal/processing/test_idefics3.py",
    "tests/models/multimodal/processing/test_internvl.py",
    "tests/models/multimodal/processing/test_llava_next.py",
    "tests/models/multimodal/processing/test_llava_onevision.py",
    "tests/models/multimodal/processing/test_phi3v.py",
    "tests/models/multimodal/processing/test_qwen2_vl.py",
    "tests/models/registry.py",
    "tests/models/test_initialization.py",
    "tests/models/test_registry.py",
    "tests/multi_step/test_correctness_async_llm.py",
    "tests/multi_step/test_correctness_llm.py",
    "tests/multimodal/test_processing.py",
    "tests/multimodal/test_utils.py",
    "tests/multimodal/utils.py",
    "tests/neuron/test_prefix_prefill.py",
    "tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py",
    "tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py",
    "tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py",
    "tests/plugins_tests/test_platform_plugins.py",
    "tests/quantization/test_compressed_tensors.py",
    "tests/quantization/test_fp8.py",
    "tests/quantization/test_lm_head.py",
    "tests/quantization/test_quark.py",
    "tests/quantization/test_register_quantization_config.py",
    "tests/samplers/test_rejection_sampler.py",
    "tests/samplers/test_seeded_generate.py",
    "tests/spec_decode/e2e/conftest.py",
    "tests/spec_decode/e2e/test_integration_dist_tp2.py",
    "tests/spec_decode/e2e/test_integration_dist_tp4.py",
    "tests/spec_decode/e2e/test_logprobs.py",
    "tests/spec_decode/e2e/test_medusa_correctness.py",
    "tests/spec_decode/e2e/test_mlp_correctness.py",
    "tests/spec_decode/e2e/test_multistep_correctness.py",
    "tests/spec_decode/e2e/test_ngram_correctness.py",
    "tests/spec_decode/test_scorer.py",
    "tests/spec_decode/test_spec_decode_worker.py",
    "tests/spec_decode/utils.py",
    "tests/tensorizer_loader/test_tensorizer.py",
    "tests/test_config.py",
    "tests/test_utils.py",
    "tests/tpu/test_quantization_accuracy.py",
    "tests/tracing/test_tracing.py",
    "tests/utils.py",
    "tests/v1/core/test_kv_cache_utils.py",
    "tests/v1/core/test_prefix_caching.py",
    "tests/v1/engine/test_async_llm.py",
    "tests/v1/engine/test_engine_core.py",
    "tests/v1/engine/test_engine_core_client.py",
    "tests/v1/engine/test_output_processor.py",
    "tests/v1/test_stats.py",
    "tests/v1/test_utils.py",
    "tests/vllm_test_utils/vllm_test_utils/__init__.py",
    "tests/vllm_test_utils/vllm_test_utils/monitor.py",
    "tests/weight_loading/models.txt",
    "tests/weight_loading/run_model_weight_loading_test.sh",
    "tests/weight_loading/test_weight_loading.py",
    "tests/worker/test_model_input.py",
    "tools/actionlint.sh",
    "tools/mypy.sh",
    "tools/profiler/print_layerwise_table.py",
    "tools/profiler/visualize_layerwise_profile.py",
    "tools/report_build_time_ninja.py",
    "tools/shellcheck.sh",
    "tools/sphinx-lint.sh",
    "vllm/__init__.py",
    "vllm/_custom_ops.py",
    "vllm/assets/image.py",
    "vllm/attention/backends/abstract.py",
    "vllm/attention/backends/blocksparse_attn.py",
    "vllm/attention/backends/flash_attn.py",
    "vllm/attention/backends/flashinfer.py",
    "vllm/attention/backends/hpu_attn.py",
    "vllm/attention/backends/ipex_attn.py",
    "vllm/attention/backends/mla/__init__.py",
    "vllm/attention/backends/mla/utils.py",
    "vllm/attention/backends/pallas.py",
    "vllm/attention/backends/placeholder_attn.py",
    "vllm/attention/backends/rocm_flash_attn.py",
    "vllm/attention/backends/torch_sdpa.py",
    "vllm/attention/backends/triton_mla.py",
    "vllm/attention/backends/utils.py",
    "vllm/attention/backends/xformers.py",
    "vllm/attention/layer.py",
    "vllm/attention/ops/ipex_attn.py",
    "vllm/attention/ops/nki_flash_attn.py",
    "vllm/attention/ops/paged_attn.py",
    "vllm/attention/ops/prefix_prefill.py",
    "vllm/attention/ops/triton_decode_attention.py",
    "vllm/attention/ops/triton_flash_attention.py",
    "vllm/attention/selector.py",
    "vllm/compilation/backends.py",
    "vllm/compilation/decorators.py",
    "vllm/compilation/wrapper.py",
    "vllm/config.py",
    "vllm/connections.py",
    "vllm/core/block/block_table.py",
    "vllm/core/block/common.py",
    "vllm/core/block/cpu_gpu_block_allocator.py",
    "vllm/core/block/interfaces.py",
    "vllm/core/block/naive_block.py",
    "vllm/core/block/prefix_caching_block.py",
    "vllm/core/block_manager.py",
    "vllm/core/interfaces.py",
    "vllm/core/placeholder_block_space_manager.py",
    "vllm/core/scheduler.py",
    "vllm/device_allocator/__init__.py",
    "vllm/device_allocator/cumem.py",
    "vllm/distributed/device_communicators/pynccl.py",
    "vllm/distributed/device_communicators/shm_broadcast.py",
    "vllm/distributed/kv_transfer/README.md",
    "vllm/distributed/kv_transfer/kv_connector/simple_connector.py",
    "vllm/distributed/parallel_state.py",
    "vllm/engine/arg_utils.py",
    "vllm/engine/async_llm_engine.py",
    "vllm/engine/llm_engine.py",
    "vllm/engine/metrics.py",
    "vllm/engine/multiprocessing/__init__.py",
    "vllm/engine/multiprocessing/client.py",
    "vllm/engine/multiprocessing/engine.py",
    "vllm/engine/output_processor/multi_step.py",
    "vllm/engine/output_processor/single_step.py",
    "vllm/engine/protocol.py",
    "vllm/entrypoints/chat_utils.py",
    "vllm/entrypoints/llm.py",
    "vllm/entrypoints/openai/api_server.py",
    "vllm/entrypoints/openai/cli_args.py",
    "vllm/entrypoints/openai/protocol.py",
    "vllm/entrypoints/openai/reasoning_parsers/__init__.py",
    "vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py",
    "vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py",
    "vllm/entrypoints/openai/run_batch.py",
    "vllm/entrypoints/openai/serving_chat.py",
    "vllm/entrypoints/openai/serving_completion.py",
    "vllm/entrypoints/openai/serving_engine.py",
    "vllm/entrypoints/openai/serving_models.py",
    "vllm/entrypoints/openai/serving_rerank.py",
    "vllm/entrypoints/openai/serving_score.py",
    "vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py",
    "vllm/envs.py",
    "vllm/executor/cpu_executor.py",
    "vllm/executor/distributed_gpu_executor.py",
    "vllm/executor/executor_base.py",
    "vllm/executor/gpu_executor.py",
    "vllm/executor/hpu_executor.py",
    "vllm/executor/mp_distributed_executor.py",
    "vllm/executor/multiproc_worker_utils.py",
    "vllm/executor/multiproc_xpu_executor.py",
    "vllm/executor/neuron_executor.py",
    "vllm/executor/openvino_executor.py",
    "vllm/executor/ray_distributed_executor.py",
    "vllm/executor/ray_hpu_executor.py",
    "vllm/executor/ray_tpu_executor.py",
    "vllm/executor/ray_utils.py",
    "vllm/executor/ray_xpu_executor.py",
    "vllm/executor/tpu_executor.py",
    "vllm/executor/uniproc_executor.py",
    "vllm/executor/xpu_executor.py",
    "vllm/forward_context.py",
    "vllm/inputs/__init__.py",
    "vllm/inputs/data.py",
    "vllm/inputs/preprocess.py",
    "vllm/inputs/registry.py",
    "vllm/logger.py",
    "vllm/lora/layers.py",
    "vllm/lora/models.py",
    "vllm/lora/ops/sgmv_expand.py",
    "vllm/lora/ops/sgmv_expand_slice.py",
    "vllm/lora/ops/torch_ops/__init__.py",
    "vllm/lora/ops/torch_ops/lora_ops.py",
    "vllm/lora/ops/triton_ops/__init__.py",
    "vllm/lora/ops/triton_ops/bgmv_expand.py",
    "vllm/lora/ops/triton_ops/bgmv_expand_slice.py",
    "vllm/lora/ops/triton_ops/bgmv_shrink.py",
    "vllm/lora/ops/triton_ops/sgmv_expand.py",
    "vllm/lora/ops/triton_ops/sgmv_shrink.py",
    "vllm/lora/ops/triton_ops/utils.py",
    "vllm/lora/ops/utils.py",
    "vllm/lora/peft_helper.py",
    "vllm/lora/punica_wrapper/punica_cpu.py",
    "vllm/lora/punica_wrapper/punica_gpu.py",
    "vllm/lora/punica_wrapper/punica_selector.py",
    "vllm/lora/worker_manager.py",
    "vllm/model_executor/custom_op.py",
    "vllm/model_executor/guided_decoding/utils.py",
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py",
    "vllm/model_executor/layers/activation.py",
    "vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/fused_marlin_moe.py",
    "vllm/model_executor/layers/fused_moe/fused_moe.py",
    "vllm/model_executor/layers/fused_moe/layer.py",
    "vllm/model_executor/layers/fused_moe/moe_torch_iterative.py",
    "vllm/model_executor/layers/linear.py",
    "vllm/model_executor/layers/logits_processor.py",
    "vllm/model_executor/layers/quantization/__init__.py",
    "vllm/model_executor/layers/quantization/awq_marlin.py",
    "vllm/model_executor/layers/quantization/base_config.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/utils.py",
    "vllm/model_executor/layers/quantization/experts_int8.py",
    "vllm/model_executor/layers/quantization/fp8.py",
    "vllm/model_executor/layers/quantization/gptq_marlin.py",
    "vllm/model_executor/layers/quantization/kernels/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py",
    "vllm/model_executor/layers/quantization/kv_cache.py",
    "vllm/model_executor/layers/quantization/moe_wna16.py",
    "vllm/model_executor/layers/quantization/quark/__init__.py",
    "vllm/model_executor/layers/quantization/quark/quark.py",
    "vllm/model_executor/layers/quantization/quark/quark_moe.py",
    "vllm/model_executor/layers/quantization/quark/schemes/__init__.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py",
    "vllm/model_executor/layers/quantization/quark/utils.py",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
    "vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py",
    "vllm/model_executor/layers/quantization/utils/quant_utils.py",
    "vllm/model_executor/layers/quantization/utils/w8a8_utils.py",
    "vllm/model_executor/layers/rejection_sampler.py",
    "vllm/model_executor/layers/resampler.py",
    "vllm/model_executor/layers/rotary_embedding.py",
    "vllm/model_executor/layers/sampler.py",
    "vllm/model_executor/layers/vocab_parallel_embedding.py",
    "vllm/model_executor/model_loader/loader.py",
    "vllm/model_executor/model_loader/tensorizer.py",
    "vllm/model_executor/model_loader/utils.py",
    "vllm/model_executor/model_loader/weight_utils.py",
    "vllm/model_executor/models/aria.py",
    "vllm/model_executor/models/baichuan.py",
    "vllm/model_executor/models/bart.py",
    "vllm/model_executor/models/bert.py",
    "vllm/model_executor/models/blip2.py",
    "vllm/model_executor/models/chameleon.py",
    "vllm/model_executor/models/chatglm.py",
    "vllm/model_executor/models/clip.py",
    "vllm/model_executor/models/commandr.py",
    "vllm/model_executor/models/dbrx.py",
    "vllm/model_executor/models/deepseek_v2.py",
    "vllm/model_executor/models/deepseek_v3.py",
    "vllm/model_executor/models/deepseek_vl2.py",
    "vllm/model_executor/models/eagle.py",
    "vllm/model_executor/models/exaone.py",
    "vllm/model_executor/models/fairseq2_llama.py",
    "vllm/model_executor/models/falcon.py",
    "vllm/model_executor/models/fuyu.py",
    "vllm/model_executor/models/gemma.py",
    "vllm/model_executor/models/gemma2.py",
    "vllm/model_executor/models/glm4_vision_encoder.py",
    "vllm/model_executor/models/gpt2.py",
    "vllm/model_executor/models/gpt_j.py",
    "vllm/model_executor/models/granite.py",
    "vllm/model_executor/models/granitemoe.py",
    "vllm/model_executor/models/idefics3.py",
    "vllm/model_executor/models/interfaces.py",
    "vllm/model_executor/models/interfaces_base.py",
    "vllm/model_executor/models/intern_vit.py",
    "vllm/model_executor/models/llama.py",
    "vllm/model_executor/models/llava.py",
    "vllm/model_executor/models/llava_next.py",
    "vllm/model_executor/models/llava_next_video.py",
    "vllm/model_executor/models/llava_onevision.py",
    "vllm/model_executor/models/minicpm.py",
    "vllm/model_executor/models/minicpm3.py",
    "vllm/model_executor/models/minicpmo.py",
    "vllm/model_executor/models/minicpmv.py",
    "vllm/model_executor/models/mixtral.py",
    "vllm/model_executor/models/mllama.py",
    "vllm/model_executor/models/mlp_speculator.py",
    "vllm/model_executor/models/molmo.py",
    "vllm/model_executor/models/nemotron.py",
    "vllm/model_executor/models/olmoe.py",
    "vllm/model_executor/models/opt.py",
    "vllm/model_executor/models/paligemma.py",
    "vllm/model_executor/models/phi.py",
    "vllm/model_executor/models/phi3.py",
    "vllm/model_executor/models/phi3_small.py",
    "vllm/model_executor/models/phi3v.py",
    "vllm/model_executor/models/phimoe.py",
    "vllm/model_executor/models/pixtral.py",
    "vllm/model_executor/models/qwen.py",
    "vllm/model_executor/models/qwen2.py",
    "vllm/model_executor/models/qwen2_audio.py",
    "vllm/model_executor/models/qwen2_moe.py",
    "vllm/model_executor/models/qwen2_rm.py",
    "vllm/model_executor/models/qwen2_vl.py",
    "vllm/model_executor/models/registry.py",
    "vllm/model_executor/models/roberta.py",
    "vllm/model_executor/models/siglip.py",
    "vllm/model_executor/models/solar.py",
    "vllm/model_executor/models/stablelm.py",
    "vllm/model_executor/models/starcoder2.py",
    "vllm/model_executor/models/ultravox.py",
    "vllm/model_executor/models/utils.py",
    "vllm/model_executor/models/vision.py",
    "vllm/model_executor/models/whisper.py",
    "vllm/model_executor/parameter.py",
    "vllm/model_executor/sampling_metadata.py",
    "vllm/multimodal/__init__.py",
    "vllm/multimodal/base.py",
    "vllm/multimodal/hasher.py",
    "vllm/multimodal/inputs.py",
    "vllm/multimodal/parse.py",
    "vllm/multimodal/processing.py",
    "vllm/multimodal/profiling.py",
    "vllm/multimodal/registry.py",
    "vllm/multimodal/utils.py",
    "vllm/outputs.py",
    "vllm/platforms/__init__.py",
    "vllm/platforms/cpu.py",
    "vllm/platforms/cuda.py",
    "vllm/platforms/hpu.py",
    "vllm/platforms/interface.py",
    "vllm/platforms/neuron.py",
    "vllm/platforms/openvino.py",
    "vllm/platforms/rocm.py",
    "vllm/platforms/tpu.py",
    "vllm/platforms/xpu.py",
    "vllm/plugins/__init__.py",
    "vllm/pooling_params.py",
    "vllm/profiler/layerwise_profile.py",
    "vllm/prompt_adapter/utils.py",
    "vllm/scalar_type.py",
    "vllm/scripts.py",
    "vllm/sequence.py",
    "vllm/spec_decode/batch_expansion.py",
    "vllm/spec_decode/interfaces.py",
    "vllm/spec_decode/medusa_worker.py",
    "vllm/spec_decode/mqa_scorer.py",
    "vllm/spec_decode/multi_step_worker.py",
    "vllm/spec_decode/ngram_worker.py",
    "vllm/spec_decode/smaller_tp_proposer_worker.py",
    "vllm/spec_decode/spec_decode_worker.py",
    "vllm/spec_decode/top1_proposer.py",
    "vllm/spec_decode/util.py",
    "vllm/tracing.py",
    "vllm/transformers_utils/config.py",
    "vllm/transformers_utils/configs/__init__.py",
    "vllm/transformers_utils/configs/aria.py",
    "vllm/transformers_utils/configs/deepseek_vl2.py",
    "vllm/transformers_utils/configs/nemotron.py",
    "vllm/transformers_utils/processors/__init__.py",
    "vllm/transformers_utils/processors/deepseek_vl2.py",
    "vllm/transformers_utils/s3_utils.py",
    "vllm/transformers_utils/tokenizer.py",
    "vllm/transformers_utils/tokenizer_group/__init__.py",
    "vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py",
    "vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py",
    "vllm/transformers_utils/tokenizer_group/tokenizer_group.py",
    "vllm/transformers_utils/tokenizers/mistral.py",
    "vllm/usage/usage_lib.py",
    "vllm/utils.py",
    "vllm/v1/attention/backends/flash_attn.py",
    "vllm/v1/core/encoder_cache_manager.py",
    "vllm/v1/core/kv_cache_manager.py",
    "vllm/v1/core/kv_cache_utils.py",
    "vllm/v1/core/scheduler.py",
    "vllm/v1/engine/__init__.py",
    "vllm/v1/engine/async_llm.py",
    "vllm/v1/engine/core.py",
    "vllm/v1/engine/core_client.py",
    "vllm/v1/engine/detokenizer.py",
    "vllm/v1/engine/llm_engine.py",
    "vllm/v1/engine/mm_input_mapper.py",
    "vllm/v1/engine/output_processor.py",
    "vllm/v1/engine/processor.py",
    "vllm/v1/executor/abstract.py",
    "vllm/v1/executor/multiproc_executor.py",
    "vllm/v1/executor/ray_executor.py",
    "vllm/v1/executor/ray_utils.py",
    "vllm/v1/executor/uniproc_executor.py",
    "vllm/v1/kv_cache_interface.py",
    "vllm/v1/metrics/__init__.py",
    "vllm/v1/metrics/loggers.py",
    "vllm/v1/metrics/stats.py",
    "vllm/v1/outputs.py",
    "vllm/v1/request.py",
    "vllm/v1/sample/sampler.py",
    "vllm/v1/stats/__init__.py",
    "vllm/v1/stats/common.py",
    "vllm/v1/utils.py",
    "vllm/v1/worker/block_table.py",
    "vllm/v1/worker/gpu_input_batch.py",
    "vllm/v1/worker/gpu_model_runner.py",
    "vllm/v1/worker/gpu_worker.py",
    "vllm/worker/cache_engine.py",
    "vllm/worker/cpu_enc_dec_model_runner.py",
    "vllm/worker/cpu_model_runner.py",
    "vllm/worker/cpu_pooling_model_runner.py",
    "vllm/worker/cpu_worker.py",
    "vllm/worker/enc_dec_model_runner.py",
    "vllm/worker/hpu_model_runner.py",
    "vllm/worker/hpu_worker.py",
    "vllm/worker/model_runner.py",
    "vllm/worker/model_runner_base.py",
    "vllm/worker/multi_step_model_runner.py",
    "vllm/worker/neuron_model_runner.py",
    "vllm/worker/neuron_worker.py",
    "vllm/worker/openvino_model_runner.py",
    "vllm/worker/openvino_worker.py",
    "vllm/worker/pooling_model_runner.py",
    "vllm/worker/tpu_model_runner.py",
    "vllm/worker/tpu_worker.py",
    "vllm/worker/utils.py",
    "vllm/worker/worker.py",
    "vllm/worker/worker_base.py",
    "vllm/worker/xpu_model_runner.py",
    "vllm/worker/xpu_worker.py"
  ],
  "allowed": [
    "vllm/v1/worker/gpu_input_batch.py",
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "disallowed": [
    ".buildkite/check-wheel-size.py",
    ".buildkite/nightly-benchmarks/benchmark-pipeline.yaml",
    ".buildkite/nightly-benchmarks/scripts/nightly-annotate.sh",
    ".buildkite/nightly-benchmarks/scripts/run-nightly-benchmarks.sh",
    ".buildkite/nightly-benchmarks/tests/genai-perf-tests.json",
    ".buildkite/release-pipeline.yaml",
    ".buildkite/run-cpu-test.sh",
    ".buildkite/run-gh200-test.sh",
    ".buildkite/run-hpu-test.sh",
    ".buildkite/run-neuron-test.sh",
    ".buildkite/run-openvino-test.sh",
    ".buildkite/run-tpu-test.sh",
    ".buildkite/run-xpu-test.sh",
    ".buildkite/test-pipeline.yaml",
    ".github/CODEOWNERS",
    ".github/ISSUE_TEMPLATE/600-new-model.yml",
    ".github/workflows/actionlint.yml",
    ".github/workflows/clang-format.yml",
    ".github/workflows/codespell.yml",
    ".github/workflows/lint-and-deploy.yaml",
    ".github/workflows/matchers/ruff.json",
    ".github/workflows/mypy.yaml",
    ".github/workflows/png-lint.yml",
    ".github/workflows/pre-commit.yml",
    ".github/workflows/ruff.yml",
    ".github/workflows/shellcheck.yml",
    ".github/workflows/sphinx-lint.yml",
    ".github/workflows/yapf.yml",
    ".gitignore",
    ".pre-commit-config.yaml",
    "CMakeLists.txt",
    "Dockerfile",
    "Dockerfile.cpu",
    "Dockerfile.hpu",
    "Dockerfile.neuron",
    "Dockerfile.openvino",
    "Dockerfile.ppc64le",
    "Dockerfile.rocm",
    "Dockerfile.rocm_base",
    "Dockerfile.tpu",
    "README.md",
    "SECURITY.md",
    "benchmarks/backend_request_func.py",
    "benchmarks/benchmark_latency.py",
    "benchmarks/benchmark_long_document_qa_throughput.py",
    "benchmarks/benchmark_prefix_caching.py",
    "benchmarks/benchmark_serving.py",
    "benchmarks/cutlass_benchmarks/w8a8_benchmarks.py",
    "benchmarks/kernels/benchmark_lora.py",
    "benchmarks/kernels/benchmark_moe.py",
    "benchmarks/kernels/benchmark_paged_attention.py",
    "benchmarks/kernels/utils.py",
    "cmake/cpu_extension.cmake",
    "cmake/utils.cmake",
    "csrc/activation_kernels.cu",
    "csrc/attention/attention_kernels.cuh",
    "csrc/attention/paged_attention_v1.cu",
    "csrc/attention/paged_attention_v2.cu",
    "csrc/cache.h",
    "csrc/cache_kernels.cu",
    "csrc/core/math.hpp",
    "csrc/core/scalar_type.hpp",
    "csrc/cpu/attention.cpp",
    "csrc/cpu/cache.cpp",
    "csrc/cpu/cpu_types.hpp",
    "csrc/cpu/cpu_types_arm.hpp",
    "csrc/cpu/cpu_types_vsx.hpp",
    "csrc/cpu/cpu_types_x86.hpp",
    "csrc/cpu/quant.cpp",
    "csrc/cpu/torch_bindings.cpp",
    "csrc/cpu/utils.cpp",
    "csrc/cumem_allocator.cpp",
    "csrc/custom_all_reduce.cuh",
    "csrc/cutlass_extensions/common.hpp",
    "csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c2x.hpp",
    "csrc/cutlass_extensions/epilogue/scaled_mm_epilogues_c3x.hpp",
    "csrc/cutlass_extensions/gemm/collective/collective_builder.hpp",
    "csrc/cutlass_extensions/gemm/collective/fp8_accumulation.hpp",
    "csrc/cutlass_extensions/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8_blockwise_scaling.hpp",
    "csrc/cutlass_extensions/gemm/dispatch_policy.hpp",
    "csrc/cutlass_extensions/torch_utils.hpp",
    "csrc/cutlass_extensions/vllm_collective_builder.cuh",
    "csrc/mamba/causal_conv1d/causal_conv1d.cu",
    "csrc/mamba/mamba_ssm/selective_scan_fwd.cu",
    "csrc/moe/marlin_kernels/marlin_moe_kernel.h",
    "csrc/moe/moe_align_sum_kernels.cu",
    "csrc/ops.h",
    "csrc/prepare_inputs/advance_step.cu",
    "csrc/quantization/compressed_tensors/int8_quant_kernels.cu",
    "csrc/quantization/cutlass_w8a8/c3x/cutlass_gemm_caller.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_azp_sm90_int8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_blockwise_sm90_fp8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_kernels.hpp",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_fp8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8.cu",
    "csrc/quantization/cutlass_w8a8/c3x/scaled_mm_sm90_int8_dispatch.cuh",
    "csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu",
    "csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu",
    "csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu",
    "csrc/quantization/gptq_marlin/gptq_marlin.cu",
    "csrc/quantization/machete/generate.py",
    "csrc/quantization/machete/machete_mainloop.cuh",
    "csrc/quantization/machete/machete_mm_kernel.cuh",
    "csrc/quantization/machete/machete_mm_launcher.cuh",
    "csrc/quantization/machete/machete_prepack_launcher.cuh",
    "csrc/quantization/machete/machete_pytorch.cu",
    "csrc/quantization/marlin/dense/marlin_cuda_kernel.cu",
    "csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu",
    "csrc/quantization/marlin/sparse/common/mma.h",
    "csrc/rocm/attention.cu",
    "csrc/rocm/ops.h",
    "csrc/rocm/torch_bindings.cpp",
    "csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu",
    "csrc/sparse/cutlass/sparse_scaled_mm_entry.cu",
    "csrc/torch_bindings.cpp",
    "docs/Makefile",
    "docs/README.md",
    "docs/requirements-docs.txt",
    "docs/source/_static/custom.js",
    "docs/source/api/engine/async_llm_engine.md",
    "docs/source/api/engine/index.md",
    "docs/source/api/engine/llm_engine.md",
    "docs/source/api/inference_params.md",
    "docs/source/api/model/adapters.md",
    "docs/source/api/model/index.md",
    "docs/source/api/model/interfaces.md",
    "docs/source/api/model/interfaces_base.md",
    "docs/source/api/multimodal/index.md",
    "docs/source/api/multimodal/inputs.md",
    "docs/source/api/multimodal/parse.md",
    "docs/source/api/multimodal/processing.md",
    "docs/source/api/multimodal/profiling.md",
    "docs/source/api/multimodal/registry.md",
    "docs/source/api/offline_inference/index.md",
    "docs/source/api/offline_inference/llm.md",
    "docs/source/api/offline_inference/llm_inputs.md",
    "docs/source/assets/contributing/dockerfile-stages-dependency.png",
    "docs/source/assets/deployment/architecture_helm_deployment.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-1.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-3.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-4.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-5.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-6.png",
    "docs/source/assets/design/v1/prefix_caching/example-time-7.png",
    "docs/source/assets/design/v1/prefix_caching/free.png",
    "docs/source/assets/design/v1/prefix_caching/overview.png",
    "docs/source/assets/features/disagg_prefill/abstraction.jpg",
    "docs/source/assets/features/disagg_prefill/overview.jpg",
    "docs/source/assets/logos/vllm-logo-only-light.ico",
    "docs/source/community/blog.md",
    "docs/source/community/meetups.md",
    "docs/source/community/sponsors.md",
    "docs/source/conf.py",
    "docs/source/contributing/dockerfile/dockerfile.md",
    "docs/source/contributing/model/basic.md",
    "docs/source/contributing/model/index.md",
    "docs/source/contributing/model/multimodal.md",
    "docs/source/contributing/model/registration.md",
    "docs/source/contributing/model/tests.md",
    "docs/source/contributing/overview.md",
    "docs/source/contributing/profiling/profiling_index.md",
    "docs/source/contributing/vulnerability_management.md",
    "docs/source/deployment/docker.md",
    "docs/source/deployment/frameworks/bentoml.md",
    "docs/source/deployment/frameworks/cerebrium.md",
    "docs/source/deployment/frameworks/dstack.md",
    "docs/source/deployment/frameworks/helm.md",
    "docs/source/deployment/frameworks/index.md",
    "docs/source/deployment/frameworks/lws.md",
    "docs/source/deployment/frameworks/modal.md",
    "docs/source/deployment/frameworks/skypilot.md",
    "docs/source/deployment/frameworks/triton.md",
    "docs/source/deployment/integrations/index.md",
    "docs/source/deployment/integrations/kserve.md",
    "docs/source/deployment/integrations/kubeai.md",
    "docs/source/deployment/integrations/llamastack.md",
    "docs/source/deployment/k8s.md",
    "docs/source/deployment/nginx.md",
    "docs/source/design/arch_overview.md",
    "docs/source/design/automatic_prefix_caching.md",
    "docs/source/design/input_processing/input_processing_pipeline.md",
    "docs/source/design/input_processing/model_inputs_index.md",
    "docs/source/design/kernel/paged_attention.md",
    "docs/source/design/mm_processing.md",
    "docs/source/design/multimodal/adding_multimodal_plugin.md",
    "docs/source/design/multimodal/multimodal_index.md",
    "docs/source/design/multiprocessing.md",
    "docs/source/design/v1/prefix_caching.md",
    "docs/source/dev/pooling_params.md",
    "docs/source/dev/sampling_params.md",
    "docs/source/features/automatic_prefix_caching.md",
    "docs/source/features/compatibility_matrix.md",
    "docs/source/features/disagg_prefill.md",
    "docs/source/features/lora.md",
    "docs/source/features/quantization/auto_awq.md",
    "docs/source/features/quantization/bnb.md",
    "docs/source/features/quantization/fp8.md",
    "docs/source/features/quantization/gguf.md",
    "docs/source/features/quantization/index.md",
    "docs/source/features/quantization/int4.md",
    "docs/source/features/quantization/int8.md",
    "docs/source/features/quantization/quantized_kvcache.md",
    "docs/source/features/quantization/supported_hardware.md",
    "docs/source/features/reasoning_outputs.md",
    "docs/source/features/spec_decode.md",
    "docs/source/features/structured_outputs.md",
    "docs/source/features/tool_calling.md",
    "docs/source/generate_examples.py",
    "docs/source/getting_started/amd-installation.md",
    "docs/source/getting_started/arm-installation.md",
    "docs/source/getting_started/examples/examples_index.template.md",
    "docs/source/getting_started/faq.md",
    "docs/source/getting_started/installation/ai_accelerator/hpu-gaudi.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/index.md",
    "docs/source/getting_started/installation/ai_accelerator/neuron.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/openvino.inc.md",
    "docs/source/getting_started/installation/ai_accelerator/tpu.inc.md",
    "docs/source/getting_started/installation/cpu/apple.inc.md",
    "docs/source/getting_started/installation/cpu/arm.inc.md",
    "docs/source/getting_started/installation/cpu/build.inc.md",
    "docs/source/getting_started/installation/cpu/index.md",
    "docs/source/getting_started/installation/cpu/x86.inc.md",
    "docs/source/getting_started/installation/device.template.md",
    "docs/source/getting_started/installation/gpu/cuda.inc.md",
    "docs/source/getting_started/installation/gpu/index.md",
    "docs/source/getting_started/installation/gpu/rocm.inc.md",
    "docs/source/getting_started/installation/gpu/xpu.inc.md",
    "docs/source/getting_started/installation/index.md",
    "docs/source/getting_started/installation/python_env_setup.inc.md",
    "docs/source/getting_started/quickstart.md",
    "docs/source/getting_started/troubleshooting.md",
    "docs/source/index.md",
    "docs/source/models/adding_model.md",
    "docs/source/models/enabling_multimodal_inputs.md",
    "docs/source/models/extensions/index.md",
    "docs/source/models/extensions/runai_model_streamer.md",
    "docs/source/models/extensions/tensorizer.md",
    "docs/source/models/generative_models.md",
    "docs/source/models/pooling_models.md",
    "docs/source/models/supported_models.md",
    "docs/source/performance/optimization.md",
    "docs/source/quantization/fp8_e4m3_kvcache.md",
    "docs/source/quantization/fp8_e5m2_kvcache.md",
    "docs/source/quantization/supported_hardware.md",
    "docs/source/serving/deploying_with_helm.md",
    "docs/source/serving/deploying_with_k8s.md",
    "docs/source/serving/distributed_serving.md",
    "docs/source/serving/engine_args.md",
    "docs/source/serving/env_vars.md",
    "docs/source/serving/integrations.md",
    "docs/source/serving/integrations/index.md",
    "docs/source/serving/integrations/langchain.md",
    "docs/source/serving/integrations/llamaindex.md",
    "docs/source/serving/metrics.md",
    "docs/source/serving/multimodal_inputs.md",
    "docs/source/serving/offline_inference.md",
    "docs/source/serving/openai_compatible_server.md",
    "docs/source/serving/usage_stats.md",
    "docs/source/usage/compatibility_matrix.md",
    "examples/fp8/README.md",
    "examples/fp8/extract_scales.py",
    "examples/fp8/quantizer/README.md",
    "examples/fp8/quantizer/quantize.py",
    "examples/gguf_inference.py",
    "examples/offline_inference/aqlm_example.py",
    "examples/offline_inference/arctic.py",
    "examples/offline_inference/audio_language.py",
    "examples/offline_inference/basic.py",
    "examples/offline_inference/basic_with_model_default_sampling.py",
    "examples/offline_inference/chat.py",
    "examples/offline_inference/chat_with_tools.py",
    "examples/offline_inference/classification.py",
    "examples/offline_inference/cli.py",
    "examples/offline_inference/cpu_offload.py",
    "examples/offline_inference/distributed.py",
    "examples/offline_inference/embedding.py",
    "examples/offline_inference/encoder_decoder.py",
    "examples/offline_inference/florence2_inference.py",
    "examples/offline_inference/gguf_inference.py",
    "examples/offline_inference/llm_engine_example.py",
    "examples/offline_inference/lora_with_quantization_inference.py",
    "examples/offline_inference/mlpspeculator.py",
    "examples/offline_inference/multilora_inference.py",
    "examples/offline_inference/neuron.py",
    "examples/offline_inference/neuron_int8_quantization.py",
    "examples/offline_inference/openai/openai_batch.md",
    "examples/offline_inference/openai/openai_example_batch.jsonl",
    "examples/offline_inference/pixtral.py",
    "examples/offline_inference/prefix_caching.py",
    "examples/offline_inference/profiling.py",
    "examples/offline_inference/profiling_tpu/README.md",
    "examples/offline_inference/profiling_tpu/profiling.py",
    "examples/offline_inference/rlhf.py",
    "examples/offline_inference/save_sharded_state.py",
    "examples/offline_inference/scoring.py",
    "examples/offline_inference/simple_profiling.py",
    "examples/offline_inference/structured_outputs.py",
    "examples/offline_inference/torchrun_example.py",
    "examples/offline_inference/tpu.py",
    "examples/offline_inference/vision_language.py",
    "examples/offline_inference/vision_language_embedding.py",
    "examples/offline_inference/vision_language_multi_image.py",
    "examples/offline_inference/whisper.py",
    "examples/online_serving/api_client.py",
    "examples/online_serving/chart-helm/.helmignore",
    "examples/online_serving/chart-helm/Chart.yaml",
    "examples/online_serving/chart-helm/README.md",
    "examples/online_serving/chart-helm/ct.yaml",
    "examples/online_serving/chart-helm/lintconf.yaml",
    "examples/online_serving/chart-helm/templates/_helpers.tpl",
    "examples/online_serving/chart-helm/templates/configmap.yaml",
    "examples/online_serving/chart-helm/templates/custom-objects.yaml",
    "examples/online_serving/chart-helm/templates/deployment.yaml",
    "examples/online_serving/chart-helm/templates/hpa.yaml",
    "examples/online_serving/chart-helm/templates/job.yaml",
    "examples/online_serving/chart-helm/templates/poddisruptionbudget.yaml",
    "examples/online_serving/chart-helm/templates/pvc.yaml",
    "examples/online_serving/chart-helm/templates/secrets.yaml",
    "examples/online_serving/chart-helm/templates/service.yaml",
    "examples/online_serving/chart-helm/values.schema.json",
    "examples/online_serving/chart-helm/values.yaml",
    "examples/online_serving/cohere_rerank_client.py",
    "examples/online_serving/disaggregated_prefill.sh",
    "examples/online_serving/gradio_openai_chatbot_webserver.py",
    "examples/online_serving/gradio_webserver.py",
    "examples/online_serving/jinaai_rerank_client.py",
    "examples/online_serving/openai_chat_completion_client.py",
    "examples/online_serving/openai_chat_completion_client_for_multimodal.py",
    "examples/online_serving/openai_chat_completion_client_with_tools.py",
    "examples/online_serving/openai_chat_completion_structured_outputs.py",
    "examples/online_serving/openai_chat_completion_with_reasoning.py",
    "examples/online_serving/openai_chat_completion_with_reasoning_streaming.py",
    "examples/online_serving/openai_chat_embedding_client_for_multimodal.py",
    "examples/online_serving/openai_completion_client.py",
    "examples/online_serving/openai_cross_encoder_score.py",
    "examples/online_serving/openai_embedding_client.py",
    "examples/online_serving/openai_pooling_client.py",
    "examples/online_serving/opentelemetry/Otel.md",
    "examples/online_serving/opentelemetry/dummy_client.py",
    "examples/online_serving/prometheus_grafana/README.md",
    "examples/online_serving/prometheus_grafana/docker-compose.yaml",
    "examples/online_serving/prometheus_grafana/grafana.json",
    "examples/online_serving/prometheus_grafana/prometheus.yaml",
    "examples/online_serving/run_cluster.sh",
    "examples/online_serving/sagemaker-entrypoint.sh",
    "examples/other/logging_configuration.md",
    "examples/other/tensorize_vllm_model.py",
    "examples/template_deepseek_vl2.jinja",
    "examples/template_pixtral_hf.jinja",
    "format.sh",
    "model_patch.diff",
    "pyproject.toml",
    "python_only_dev.py",
    "requirements-common.txt",
    "requirements-cpu.txt",
    "requirements-cuda.txt",
    "requirements-hpu.txt",
    "requirements-lint.txt",
    "requirements-test.in",
    "requirements-test.txt",
    "requirements-tpu.txt",
    "setup.py",
    "tests/async_engine/test_api_server.py",
    "tests/basic_correctness/test_basic_correctness.py",
    "tests/basic_correctness/test_cumem.py",
    "tests/basic_correctness/test_preemption.py",
    "tests/compile/test_basic_correctness.py",
    "tests/conftest.py",
    "tests/core/block/test_prefix_caching_block.py",
    "tests/distributed/test_custom_all_reduce.py",
    "tests/distributed/test_pynccl.py",
    "tests/distributed/test_torchrun_example.py",
    "tests/engine/test_custom_executor.py",
    "tests/engine/test_multiproc_workers.py",
    "tests/entrypoints/llm/test_collective_rpc.py",
    "tests/entrypoints/llm/test_encode.py",
    "tests/entrypoints/openai/reasoning_parsers/__init__.py",
    "tests/entrypoints/openai/reasoning_parsers/test_deepseekr1_reasoning_parser.py",
    "tests/entrypoints/openai/reasoning_parsers/utils.py",
    "tests/entrypoints/openai/test_cli_args.py",
    "tests/entrypoints/openai/test_lora_adapters.py",
    "tests/entrypoints/openai/test_lora_lineage.py",
    "tests/entrypoints/openai/test_metrics.py",
    "tests/entrypoints/openai/test_rerank.py",
    "tests/entrypoints/openai/test_run_batch.py",
    "tests/entrypoints/openai/test_score.py",
    "tests/entrypoints/openai/test_serving_chat.py",
    "tests/entrypoints/openai/test_serving_models.py",
    "tests/entrypoints/openai/test_shutdown.py",
    "tests/entrypoints/test_chat_utils.py",
    "tests/fp8_kv/llama2-70b-fp8-kv/kv_cache_scales.json",
    "tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json",
    "tests/kernels/test_activation.py",
    "tests/kernels/test_attention.py",
    "tests/kernels/test_attention_selector.py",
    "tests/kernels/test_block_fp8.py",
    "tests/kernels/test_blocksparse_attention.py",
    "tests/kernels/test_cache.py",
    "tests/kernels/test_cascade_flash_attn.py",
    "tests/kernels/test_cutlass.py",
    "tests/kernels/test_cutlass_2of4_sparse.py",
    "tests/kernels/test_encoder_decoder_attn.py",
    "tests/kernels/test_flash_attn.py",
    "tests/kernels/test_flashinfer.py",
    "tests/kernels/test_mha_attn.py",
    "tests/kernels/test_moe.py",
    "tests/kernels/test_prefix_prefill.py",
    "tests/kernels/test_semi_structured.py",
    "tests/kernels/test_triton_decode_attention.py",
    "tests/kernels/test_triton_scaled_mm.py",
    "tests/kernels/utils.py",
    "tests/kv_transfer/test_lookup_buffer.py",
    "tests/kv_transfer/test_send_recv.py",
    "tests/lora/conftest.py",
    "tests/lora/test_layers.py",
    "tests/lora/test_lora_checkpoints.py",
    "tests/lora/test_lora_huggingface.py",
    "tests/lora/test_lora_manager.py",
    "tests/lora/test_minicpmv.py",
    "tests/lora/test_minicpmv_tp.py",
    "tests/lora/test_mixtral.py",
    "tests/lora/test_peft_helper.py",
    "tests/lora/test_punica_ops_sizes.py",
    "tests/lora/test_punica_ops_variation.py",
    "tests/lora/test_quant_model.py",
    "tests/lora/test_qwen2vl.py",
    "tests/lora/utils.py",
    "tests/model_executor/test_model_load_with_params.py",
    "tests/models/decoder_only/audio_language/test_ultravox.py",
    "tests/models/decoder_only/language/test_fp8.py",
    "tests/models/decoder_only/language/test_gguf.py",
    "tests/models/decoder_only/language/test_jamba.py",
    "tests/models/decoder_only/language/test_mamba.py",
    "tests/models/decoder_only/language/test_models.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_phi3v.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen.py",
    "tests/models/decoder_only/vision_language/mm_processor_kwargs/test_qwen2_vl.py",
    "tests/models/decoder_only/vision_language/test_models.py",
    "tests/models/decoder_only/vision_language/test_pixtral.py",
    "tests/models/decoder_only/vision_language/test_qwen2_vl.py",
    "tests/models/decoder_only/vision_language/vlm_utils/model_utils.py",
    "tests/models/embedding/language/test_cls_models.py",
    "tests/models/embedding/language/test_embedding.py",
    "tests/models/embedding/language/test_scoring.py",
    "tests/models/encoder_decoder/audio_language/__init__.py",
    "tests/models/encoder_decoder/audio_language/test_whisper.py",
    "tests/models/encoder_decoder/vision_language/test_mllama.py",
    "tests/models/multimodal/__init__.py",
    "tests/models/multimodal/processing/__init__.py",
    "tests/models/multimodal/processing/test_common.py",
    "tests/models/multimodal/processing/test_idefics3.py",
    "tests/models/multimodal/processing/test_internvl.py",
    "tests/models/multimodal/processing/test_llava_next.py",
    "tests/models/multimodal/processing/test_llava_onevision.py",
    "tests/models/multimodal/processing/test_phi3v.py",
    "tests/models/multimodal/processing/test_qwen2_vl.py",
    "tests/models/registry.py",
    "tests/models/test_initialization.py",
    "tests/models/test_registry.py",
    "tests/multi_step/test_correctness_async_llm.py",
    "tests/multi_step/test_correctness_llm.py",
    "tests/multimodal/test_processing.py",
    "tests/multimodal/test_utils.py",
    "tests/multimodal/utils.py",
    "tests/neuron/test_prefix_prefill.py",
    "tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/my_llava.py",
    "tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_attention_backend.py",
    "tests/plugins/vllm_add_dummy_platform/vllm_add_dummy_platform/dummy_platform.py",
    "tests/plugins_tests/test_platform_plugins.py",
    "tests/quantization/test_compressed_tensors.py",
    "tests/quantization/test_fp8.py",
    "tests/quantization/test_lm_head.py",
    "tests/quantization/test_quark.py",
    "tests/quantization/test_register_quantization_config.py",
    "tests/samplers/test_rejection_sampler.py",
    "tests/samplers/test_seeded_generate.py",
    "tests/spec_decode/e2e/conftest.py",
    "tests/spec_decode/e2e/test_integration_dist_tp2.py",
    "tests/spec_decode/e2e/test_integration_dist_tp4.py",
    "tests/spec_decode/e2e/test_logprobs.py",
    "tests/spec_decode/e2e/test_medusa_correctness.py",
    "tests/spec_decode/e2e/test_mlp_correctness.py",
    "tests/spec_decode/e2e/test_multistep_correctness.py",
    "tests/spec_decode/e2e/test_ngram_correctness.py",
    "tests/spec_decode/test_scorer.py",
    "tests/spec_decode/test_spec_decode_worker.py",
    "tests/spec_decode/utils.py",
    "tests/tensorizer_loader/test_tensorizer.py",
    "tests/test_config.py",
    "tests/test_utils.py",
    "tests/tpu/test_quantization_accuracy.py",
    "tests/tracing/test_tracing.py",
    "tests/utils.py",
    "tests/v1/core/test_kv_cache_utils.py",
    "tests/v1/core/test_prefix_caching.py",
    "tests/v1/engine/test_async_llm.py",
    "tests/v1/engine/test_engine_core.py",
    "tests/v1/engine/test_engine_core_client.py",
    "tests/v1/engine/test_output_processor.py",
    "tests/v1/test_stats.py",
    "tests/v1/test_utils.py",
    "tests/vllm_test_utils/vllm_test_utils/__init__.py",
    "tests/vllm_test_utils/vllm_test_utils/monitor.py",
    "tests/weight_loading/models.txt",
    "tests/weight_loading/run_model_weight_loading_test.sh",
    "tests/weight_loading/test_weight_loading.py",
    "tests/worker/test_model_input.py",
    "tools/actionlint.sh",
    "tools/mypy.sh",
    "tools/profiler/print_layerwise_table.py",
    "tools/profiler/visualize_layerwise_profile.py",
    "tools/report_build_time_ninja.py",
    "tools/shellcheck.sh",
    "tools/sphinx-lint.sh",
    "vllm/__init__.py",
    "vllm/_custom_ops.py",
    "vllm/assets/image.py",
    "vllm/attention/backends/abstract.py",
    "vllm/attention/backends/blocksparse_attn.py",
    "vllm/attention/backends/flash_attn.py",
    "vllm/attention/backends/flashinfer.py",
    "vllm/attention/backends/hpu_attn.py",
    "vllm/attention/backends/ipex_attn.py",
    "vllm/attention/backends/mla/__init__.py",
    "vllm/attention/backends/mla/utils.py",
    "vllm/attention/backends/pallas.py",
    "vllm/attention/backends/placeholder_attn.py",
    "vllm/attention/backends/rocm_flash_attn.py",
    "vllm/attention/backends/torch_sdpa.py",
    "vllm/attention/backends/triton_mla.py",
    "vllm/attention/backends/utils.py",
    "vllm/attention/backends/xformers.py",
    "vllm/attention/layer.py",
    "vllm/attention/ops/ipex_attn.py",
    "vllm/attention/ops/nki_flash_attn.py",
    "vllm/attention/ops/paged_attn.py",
    "vllm/attention/ops/prefix_prefill.py",
    "vllm/attention/ops/triton_decode_attention.py",
    "vllm/attention/ops/triton_flash_attention.py",
    "vllm/attention/selector.py",
    "vllm/compilation/backends.py",
    "vllm/compilation/decorators.py",
    "vllm/compilation/wrapper.py",
    "vllm/config.py",
    "vllm/connections.py",
    "vllm/core/block/block_table.py",
    "vllm/core/block/common.py",
    "vllm/core/block/cpu_gpu_block_allocator.py",
    "vllm/core/block/interfaces.py",
    "vllm/core/block/naive_block.py",
    "vllm/core/block/prefix_caching_block.py",
    "vllm/core/block_manager.py",
    "vllm/core/interfaces.py",
    "vllm/core/placeholder_block_space_manager.py",
    "vllm/core/scheduler.py",
    "vllm/device_allocator/__init__.py",
    "vllm/device_allocator/cumem.py",
    "vllm/distributed/device_communicators/pynccl.py",
    "vllm/distributed/device_communicators/shm_broadcast.py",
    "vllm/distributed/kv_transfer/README.md",
    "vllm/distributed/kv_transfer/kv_connector/simple_connector.py",
    "vllm/distributed/parallel_state.py",
    "vllm/engine/arg_utils.py",
    "vllm/engine/async_llm_engine.py",
    "vllm/engine/llm_engine.py",
    "vllm/engine/metrics.py",
    "vllm/engine/multiprocessing/__init__.py",
    "vllm/engine/multiprocessing/client.py",
    "vllm/engine/multiprocessing/engine.py",
    "vllm/engine/output_processor/multi_step.py",
    "vllm/engine/output_processor/single_step.py",
    "vllm/engine/protocol.py",
    "vllm/entrypoints/chat_utils.py",
    "vllm/entrypoints/llm.py",
    "vllm/entrypoints/openai/api_server.py",
    "vllm/entrypoints/openai/cli_args.py",
    "vllm/entrypoints/openai/protocol.py",
    "vllm/entrypoints/openai/reasoning_parsers/__init__.py",
    "vllm/entrypoints/openai/reasoning_parsers/abs_reasoning_parsers.py",
    "vllm/entrypoints/openai/reasoning_parsers/deepseek_r1_reasoning_parser.py",
    "vllm/entrypoints/openai/run_batch.py",
    "vllm/entrypoints/openai/serving_chat.py",
    "vllm/entrypoints/openai/serving_completion.py",
    "vllm/entrypoints/openai/serving_engine.py",
    "vllm/entrypoints/openai/serving_models.py",
    "vllm/entrypoints/openai/serving_rerank.py",
    "vllm/entrypoints/openai/serving_score.py",
    "vllm/entrypoints/openai/tool_parsers/granite_20b_fc_tool_parser.py",
    "vllm/envs.py",
    "vllm/executor/cpu_executor.py",
    "vllm/executor/distributed_gpu_executor.py",
    "vllm/executor/executor_base.py",
    "vllm/executor/gpu_executor.py",
    "vllm/executor/hpu_executor.py",
    "vllm/executor/mp_distributed_executor.py",
    "vllm/executor/multiproc_worker_utils.py",
    "vllm/executor/multiproc_xpu_executor.py",
    "vllm/executor/neuron_executor.py",
    "vllm/executor/openvino_executor.py",
    "vllm/executor/ray_distributed_executor.py",
    "vllm/executor/ray_hpu_executor.py",
    "vllm/executor/ray_tpu_executor.py",
    "vllm/executor/ray_utils.py",
    "vllm/executor/ray_xpu_executor.py",
    "vllm/executor/tpu_executor.py",
    "vllm/executor/uniproc_executor.py",
    "vllm/executor/xpu_executor.py",
    "vllm/forward_context.py",
    "vllm/inputs/__init__.py",
    "vllm/inputs/data.py",
    "vllm/inputs/preprocess.py",
    "vllm/inputs/registry.py",
    "vllm/logger.py",
    "vllm/lora/layers.py",
    "vllm/lora/models.py",
    "vllm/lora/ops/sgmv_expand.py",
    "vllm/lora/ops/sgmv_expand_slice.py",
    "vllm/lora/ops/torch_ops/__init__.py",
    "vllm/lora/ops/torch_ops/lora_ops.py",
    "vllm/lora/ops/triton_ops/__init__.py",
    "vllm/lora/ops/triton_ops/bgmv_expand.py",
    "vllm/lora/ops/triton_ops/bgmv_expand_slice.py",
    "vllm/lora/ops/triton_ops/bgmv_shrink.py",
    "vllm/lora/ops/triton_ops/sgmv_expand.py",
    "vllm/lora/ops/triton_ops/sgmv_shrink.py",
    "vllm/lora/ops/triton_ops/utils.py",
    "vllm/lora/ops/utils.py",
    "vllm/lora/peft_helper.py",
    "vllm/lora/punica_wrapper/punica_cpu.py",
    "vllm/lora/punica_wrapper/punica_gpu.py",
    "vllm/lora/punica_wrapper/punica_selector.py",
    "vllm/lora/worker_manager.py",
    "vllm/model_executor/custom_op.py",
    "vllm/model_executor/guided_decoding/utils.py",
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py",
    "vllm/model_executor/layers/activation.py",
    "vllm/model_executor/layers/fused_moe/configs/E=256,N=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/fused_moe/configs/E=256,N=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=16384,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=1792,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=2048,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=4096,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X,dtype=fp8_w8a8.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=8192,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/fused_marlin_moe.py",
    "vllm/model_executor/layers/fused_moe/fused_moe.py",
    "vllm/model_executor/layers/fused_moe/layer.py",
    "vllm/model_executor/layers/fused_moe/moe_torch_iterative.py",
    "vllm/model_executor/layers/linear.py",
    "vllm/model_executor/layers/logits_processor.py",
    "vllm/model_executor/layers/quantization/__init__.py",
    "vllm/model_executor/layers/quantization/awq_marlin.py",
    "vllm/model_executor/layers/quantization/base_config.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_24.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w4a16_24.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_w8a8_int8.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/schemes/compressed_tensors_wNa16.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/utils.py",
    "vllm/model_executor/layers/quantization/experts_int8.py",
    "vllm/model_executor/layers/quantization/fp8.py",
    "vllm/model_executor/layers/quantization/gptq_marlin.py",
    "vllm/model_executor/layers/quantization/kernels/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/MPLinearKernel.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/exllama.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/machete.py",
    "vllm/model_executor/layers/quantization/kernels/mixed_precision/marlin.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/ScaledMMLinearKernel.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/__init__.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/cutlass.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/triton.py",
    "vllm/model_executor/layers/quantization/kernels/scaled_mm/xla.py",
    "vllm/model_executor/layers/quantization/kv_cache.py",
    "vllm/model_executor/layers/quantization/moe_wna16.py",
    "vllm/model_executor/layers/quantization/quark/__init__.py",
    "vllm/model_executor/layers/quantization/quark/quark.py",
    "vllm/model_executor/layers/quantization/quark/quark_moe.py",
    "vllm/model_executor/layers/quantization/quark/schemes/__init__.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_scheme.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_fp8.py",
    "vllm/model_executor/layers/quantization/quark/schemes/quark_w8a8_int8.py",
    "vllm/model_executor/layers/quantization/quark/utils.py",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=1536,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=1536,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2048,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=2304,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=24576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=256,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=1536,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=3072,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=32768,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=36864,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4096,K=512,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=4608,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=512,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=576,K=7168,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1024,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=1152,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=128,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=16384,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=18432,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2048,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H100_80GB_HBM3,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=2304,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/configs/N=7168,K=256,device_name=NVIDIA_H200,dtype=fp8_w8a8,block_shape=[128, 128].json",
    "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
    "vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py",
    "vllm/model_executor/layers/quantization/utils/quant_utils.py",
    "vllm/model_executor/layers/quantization/utils/w8a8_utils.py",
    "vllm/model_executor/layers/rejection_sampler.py",
    "vllm/model_executor/layers/resampler.py",
    "vllm/model_executor/layers/rotary_embedding.py",
    "vllm/model_executor/layers/sampler.py",
    "vllm/model_executor/layers/vocab_parallel_embedding.py",
    "vllm/model_executor/model_loader/loader.py",
    "vllm/model_executor/model_loader/tensorizer.py",
    "vllm/model_executor/model_loader/utils.py",
    "vllm/model_executor/model_loader/weight_utils.py",
    "vllm/model_executor/models/aria.py",
    "vllm/model_executor/models/baichuan.py",
    "vllm/model_executor/models/bart.py",
    "vllm/model_executor/models/bert.py",
    "vllm/model_executor/models/blip2.py",
    "vllm/model_executor/models/chameleon.py",
    "vllm/model_executor/models/chatglm.py",
    "vllm/model_executor/models/clip.py",
    "vllm/model_executor/models/commandr.py",
    "vllm/model_executor/models/dbrx.py",
    "vllm/model_executor/models/deepseek_v2.py",
    "vllm/model_executor/models/deepseek_v3.py",
    "vllm/model_executor/models/deepseek_vl2.py",
    "vllm/model_executor/models/eagle.py",
    "vllm/model_executor/models/exaone.py",
    "vllm/model_executor/models/fairseq2_llama.py",
    "vllm/model_executor/models/falcon.py",
    "vllm/model_executor/models/fuyu.py",
    "vllm/model_executor/models/gemma.py",
    "vllm/model_executor/models/gemma2.py",
    "vllm/model_executor/models/glm4_vision_encoder.py",
    "vllm/model_executor/models/gpt2.py",
    "vllm/model_executor/models/gpt_j.py",
    "vllm/model_executor/models/granite.py",
    "vllm/model_executor/models/granitemoe.py",
    "vllm/model_executor/models/idefics3.py",
    "vllm/model_executor/models/interfaces.py",
    "vllm/model_executor/models/interfaces_base.py",
    "vllm/model_executor/models/intern_vit.py",
    "vllm/model_executor/models/llama.py",
    "vllm/model_executor/models/llava.py",
    "vllm/model_executor/models/llava_next.py",
    "vllm/model_executor/models/llava_next_video.py",
    "vllm/model_executor/models/llava_onevision.py",
    "vllm/model_executor/models/minicpm.py",
    "vllm/model_executor/models/minicpm3.py",
    "vllm/model_executor/models/minicpmo.py",
    "vllm/model_executor/models/minicpmv.py",
    "vllm/model_executor/models/mixtral.py",
    "vllm/model_executor/models/mllama.py",
    "vllm/model_executor/models/mlp_speculator.py",
    "vllm/model_executor/models/molmo.py",
    "vllm/model_executor/models/nemotron.py",
    "vllm/model_executor/models/olmoe.py",
    "vllm/model_executor/models/opt.py",
    "vllm/model_executor/models/paligemma.py",
    "vllm/model_executor/models/phi.py",
    "vllm/model_executor/models/phi3.py",
    "vllm/model_executor/models/phi3_small.py",
    "vllm/model_executor/models/phi3v.py",
    "vllm/model_executor/models/phimoe.py",
    "vllm/model_executor/models/pixtral.py",
    "vllm/model_executor/models/qwen.py",
    "vllm/model_executor/models/qwen2.py",
    "vllm/model_executor/models/qwen2_audio.py",
    "vllm/model_executor/models/qwen2_moe.py",
    "vllm/model_executor/models/qwen2_rm.py",
    "vllm/model_executor/models/qwen2_vl.py",
    "vllm/model_executor/models/registry.py",
    "vllm/model_executor/models/roberta.py",
    "vllm/model_executor/models/siglip.py",
    "vllm/model_executor/models/solar.py",
    "vllm/model_executor/models/stablelm.py",
    "vllm/model_executor/models/starcoder2.py",
    "vllm/model_executor/models/ultravox.py",
    "vllm/model_executor/models/utils.py",
    "vllm/model_executor/models/vision.py",
    "vllm/model_executor/models/whisper.py",
    "vllm/model_executor/parameter.py",
    "vllm/model_executor/sampling_metadata.py",
    "vllm/multimodal/__init__.py",
    "vllm/multimodal/base.py",
    "vllm/multimodal/hasher.py",
    "vllm/multimodal/inputs.py",
    "vllm/multimodal/parse.py",
    "vllm/multimodal/processing.py",
    "vllm/multimodal/profiling.py",
    "vllm/multimodal/registry.py",
    "vllm/multimodal/utils.py",
    "vllm/outputs.py",
    "vllm/platforms/__init__.py",
    "vllm/platforms/cpu.py",
    "vllm/platforms/cuda.py",
    "vllm/platforms/hpu.py",
    "vllm/platforms/interface.py",
    "vllm/platforms/neuron.py",
    "vllm/platforms/openvino.py",
    "vllm/platforms/rocm.py",
    "vllm/platforms/tpu.py",
    "vllm/platforms/xpu.py",
    "vllm/plugins/__init__.py",
    "vllm/pooling_params.py",
    "vllm/profiler/layerwise_profile.py",
    "vllm/prompt_adapter/utils.py",
    "vllm/scalar_type.py",
    "vllm/scripts.py",
    "vllm/sequence.py",
    "vllm/spec_decode/batch_expansion.py",
    "vllm/spec_decode/interfaces.py",
    "vllm/spec_decode/medusa_worker.py",
    "vllm/spec_decode/mqa_scorer.py",
    "vllm/spec_decode/multi_step_worker.py",
    "vllm/spec_decode/ngram_worker.py",
    "vllm/spec_decode/smaller_tp_proposer_worker.py",
    "vllm/spec_decode/spec_decode_worker.py",
    "vllm/spec_decode/top1_proposer.py",
    "vllm/spec_decode/util.py",
    "vllm/tracing.py",
    "vllm/transformers_utils/config.py",
    "vllm/transformers_utils/configs/__init__.py",
    "vllm/transformers_utils/configs/aria.py",
    "vllm/transformers_utils/configs/deepseek_vl2.py",
    "vllm/transformers_utils/configs/nemotron.py",
    "vllm/transformers_utils/processors/__init__.py",
    "vllm/transformers_utils/processors/deepseek_vl2.py",
    "vllm/transformers_utils/s3_utils.py",
    "vllm/transformers_utils/tokenizer.py",
    "vllm/transformers_utils/tokenizer_group/__init__.py",
    "vllm/transformers_utils/tokenizer_group/base_tokenizer_group.py",
    "vllm/transformers_utils/tokenizer_group/ray_tokenizer_group.py",
    "vllm/transformers_utils/tokenizer_group/tokenizer_group.py",
    "vllm/transformers_utils/tokenizers/mistral.py",
    "vllm/usage/usage_lib.py",
    "vllm/utils.py",
    "vllm/v1/attention/backends/flash_attn.py",
    "vllm/v1/core/encoder_cache_manager.py",
    "vllm/v1/core/kv_cache_manager.py",
    "vllm/v1/core/kv_cache_utils.py",
    "vllm/v1/core/scheduler.py",
    "vllm/v1/engine/__init__.py",
    "vllm/v1/engine/async_llm.py",
    "vllm/v1/engine/core.py",
    "vllm/v1/engine/core_client.py",
    "vllm/v1/engine/detokenizer.py",
    "vllm/v1/engine/llm_engine.py",
    "vllm/v1/engine/mm_input_mapper.py",
    "vllm/v1/engine/output_processor.py",
    "vllm/v1/engine/processor.py",
    "vllm/v1/executor/abstract.py",
    "vllm/v1/executor/multiproc_executor.py",
    "vllm/v1/executor/ray_executor.py",
    "vllm/v1/executor/ray_utils.py",
    "vllm/v1/executor/uniproc_executor.py",
    "vllm/v1/kv_cache_interface.py",
    "vllm/v1/metrics/__init__.py",
    "vllm/v1/metrics/loggers.py",
    "vllm/v1/metrics/stats.py",
    "vllm/v1/outputs.py",
    "vllm/v1/request.py",
    "vllm/v1/sample/sampler.py",
    "vllm/v1/stats/__init__.py",
    "vllm/v1/stats/common.py",
    "vllm/v1/utils.py",
    "vllm/v1/worker/block_table.py",
    "vllm/v1/worker/gpu_worker.py",
    "vllm/worker/cache_engine.py",
    "vllm/worker/cpu_enc_dec_model_runner.py",
    "vllm/worker/cpu_model_runner.py",
    "vllm/worker/cpu_pooling_model_runner.py",
    "vllm/worker/cpu_worker.py",
    "vllm/worker/enc_dec_model_runner.py",
    "vllm/worker/hpu_model_runner.py",
    "vllm/worker/hpu_worker.py",
    "vllm/worker/model_runner.py",
    "vllm/worker/model_runner_base.py",
    "vllm/worker/multi_step_model_runner.py",
    "vllm/worker/neuron_model_runner.py",
    "vllm/worker/neuron_worker.py",
    "vllm/worker/openvino_model_runner.py",
    "vllm/worker/openvino_worker.py",
    "vllm/worker/pooling_model_runner.py",
    "vllm/worker/tpu_model_runner.py",
    "vllm/worker/tpu_worker.py",
    "vllm/worker/utils.py",
    "vllm/worker/worker.py",
    "vllm/worker/worker_base.py",
    "vllm/worker/xpu_model_runner.py",
    "vllm/worker/xpu_worker.py"
  ],
  "ok": false
}