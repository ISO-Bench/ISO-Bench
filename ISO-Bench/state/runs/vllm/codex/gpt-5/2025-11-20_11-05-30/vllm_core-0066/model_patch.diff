diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py
index da09bb70b..892d6ee44 100644
--- a/vllm/attention/backends/triton_mla.py
+++ b/vllm/attention/backends/triton_mla.py
@@ -106,7 +106,7 @@ class TritonMLAState(AttentionState):
         self._graph_block_tables = torch.from_numpy(
             self.runner.graph_block_tables).to(device=self.runner.device)
 
-        self._positions = torch.zeros((max_batch_size, ),
+        self._positions = torch.empty((max_batch_size, ),
                                       dtype=torch.long,
                                       device=self.runner.device)
 
@@ -714,7 +714,7 @@ class TritonMLAImpl(MLACommonImpl[TritonMLAMetadata]):
         B = q_nope.shape[0]
 
         q = torch.cat([q_nope, q_pe], dim=-1)
-        o = torch.zeros(B,
+        o = torch.empty(B,
                         self.num_heads,
                         self.kv_lora_rank,
                         dtype=q.dtype,
diff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py
index 83055d600..53765a7b9 100644
--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py
+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py
@@ -56,7 +56,7 @@ def unpack_quantized_values_into_int32(w_q: torch.Tensor,
     new_shape_perm = list(w_q_perm.shape)
     new_shape_perm[-1] *= pack_factor
 
-    res = torch.zeros(new_shape_perm, dtype=torch.int32, device=w_q.device)
+    res = torch.empty(new_shape_perm, dtype=torch.int32, device=w_q.device)
     for i in range(pack_factor):
         res[..., i::pack_factor] = (w_q_perm >> wtype.size_bits * i) & mask
 
@@ -105,9 +105,7 @@ def permute_rows(q_w: torch.Tensor,
     orig_device = q_w.device
     k_size, _ = q_w.shape
 
-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)
-    for i in range(k_size):
-        g_idx[i] = i // group_size
+    g_idx = (torch.arange(k_size, dtype=torch.int32) // group_size)
 
     # Simulate act_order by doing a random permutation on K
     rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index 28d8e3905..b5d737138 100644
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -63,7 +63,7 @@ class InputBatch:
         # Find a way to reduce the CPU memory usage.
         # This buffer is not directly transferred to the GPU, so it does not
         # need to be pinned.
-        self.token_ids_cpu_tensor = torch.zeros(
+        self.token_ids_cpu_tensor = torch.empty(
             (max_num_reqs, max_model_len),
             device="cpu",
             dtype=torch.int32,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index a00c00c30..3e871944f 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -136,10 +136,10 @@ class GPUModelRunner:
         self.num_sms = self.device_properties.multi_processor_count
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
 
@@ -155,16 +155,16 @@ class GPUModelRunner:
             # identical position IDs, making M-RoPE functionally equivalent to
             # 1D-RoPE.
             # See page 5 of https://arxiv.org/abs/2409.12191
-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
                 pin_memory=self.pin_memory)
 
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -177,27 +177,27 @@ class GPUModelRunner:
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
