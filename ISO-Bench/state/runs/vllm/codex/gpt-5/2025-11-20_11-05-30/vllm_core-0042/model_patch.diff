diff --git a/docs/design/v1/p2p_nccl_connector.md b/docs/design/v1/p2p_nccl_connector.md
index b1df93cfc..ffe0cd122 100644
--- a/docs/design/v1/p2p_nccl_connector.md
+++ b/docs/design/v1/p2p_nccl_connector.md
@@ -29,23 +29,23 @@ Currently, to quickly verify whether xPyD can work, a round-robin selection of 1
 
 Each P/D instance periodically sends a heartbeat packet to the Proxy/Router (currently every 3 seconds) to register (i.e., report `http_addr -> zmq_addr`) and keep the connection alive. If an instance crashes and fails to send a ping for a certain period of time, the Proxy/Router will remove the timed-out instance (this feature has not yet been developed).
 
-## KV Cache Transfer Methods
+## KVCache Transfer Methods
 
-There are three methods for KVcache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVcache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVcache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVcache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVcache from the P instance once it has allocated space for the KVcache.
+There are three methods for KVCache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVCache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVCache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVCache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVCache from the P instance once it has allocated space for the KVCache.
 
 Experimental results have shown that the performance of these methods, from highest to lowest, is as follows: PUT_ASYNC ‚Üí GET ‚Üí PUT.
 
 ## P2P Communication via ZMQ & NCCL
 
-As long as the address of the counterpart is known, point-to-point KV cache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.
+As long as the address of the counterpart is known, point-to-point KVCache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.
 
-Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVcache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVcache data itself.
+Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVCache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVCache data itself.
 
-When a P instance and a D instance transmit KVcache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVcache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVcache transmission can be performed, without being restricted by rank or world size.
+When a P instance and a D instance transmit KVCache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVCache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVCache transmission can be performed, without being restricted by rank or world size.
 
 ## NCCL Group Topology
 
-Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVcache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.
+Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVCache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.
 
 ![image2](https://github.com/user-attachments/assets/837e61d6-365e-4cbf-8640-6dd7ab295b36)
 
diff --git a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
index 4e82424d6..28e7dc318 100644
--- a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
+++ b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
@@ -66,6 +66,16 @@ def start_service_discovery(hostname, port):
 
 
 AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
+# Reuse a single aiohttp session to reduce connection overhead
+CLIENT_SESSION: aiohttp.ClientSession | None = None
+AUTH_BEARER = f"Bearer {os.environ.get('OPENAI_API_KEY', '')}"
+
+async def _get_client_session() -> aiohttp.ClientSession:
+    global CLIENT_SESSION
+    if CLIENT_SESSION is None or CLIENT_SESSION.closed:
+        CLIENT_SESSION = aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT)
+    return CLIENT_SESSION
+
 
 app = Quart(__name__)
 
@@ -75,19 +85,15 @@ def random_uuid() -> str:
 
 
 async def forward_request(url, data, request_id):
-    async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:
-        headers = {
-            "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
-            "X-Request-Id": request_id,
-        }
-        async with session.post(url=url, json=data, headers=headers) as response:
-            if response.status == 200:
-                if True:
-                    async for chunk_bytes in response.content.iter_chunked(1024):
-                        yield chunk_bytes
-                else:
-                    content = await response.read()
-                    yield content
+    session = await _get_client_session()
+    headers = {
+        "Authorization": AUTH_BEARER,
+        "X-Request-Id": request_id,
+    }
+    async with session.post(url=url, json=data, headers=headers) as response:
+        if response.status == 200:
+            async for chunk_bytes in response.content.iter_chunked(1024):
+                yield chunk_bytes
 
 
 @app.route("/v1/completions", methods=["POST"])
diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..1133036ab
--- /dev/null
+++ b/model_patch.diff
@@ -0,0 +1,68 @@
+diff --git a/docs/design/v1/p2p_nccl_connector.md b/docs/design/v1/p2p_nccl_connector.md
+index b1df93cfc..ffe0cd122 100644
+--- a/docs/design/v1/p2p_nccl_connector.md
++++ b/docs/design/v1/p2p_nccl_connector.md
+@@ -29,23 +29,23 @@ Currently, to quickly verify whether xPyD can work, a round-robin selection of 1
+ 
+ Each P/D instance periodically sends a heartbeat packet to the Proxy/Router (currently every 3 seconds) to register (i.e., report `http_addr -> zmq_addr`) and keep the connection alive. If an instance crashes and fails to send a ping for a certain period of time, the Proxy/Router will remove the timed-out instance (this feature has not yet been developed).
+ 
+-## KV Cache Transfer Methods
++## KVCache Transfer Methods
+ 
+-There are three methods for KVcache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVcache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVcache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVcache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVcache from the P instance once it has allocated space for the KVcache.
++There are three methods for KVCache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVCache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVCache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVCache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVCache from the P instance once it has allocated space for the KVCache.
+ 
+ Experimental results have shown that the performance of these methods, from highest to lowest, is as follows: PUT_ASYNC ‚Üí GET ‚Üí PUT.
+ 
+ ## P2P Communication via ZMQ & NCCL
+ 
+-As long as the address of the counterpart is known, point-to-point KV cache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.
++As long as the address of the counterpart is known, point-to-point KVCache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.
+ 
+-Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVcache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVcache data itself.
++Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVCache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVCache data itself.
+ 
+-When a P instance and a D instance transmit KVcache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVcache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVcache transmission can be performed, without being restricted by rank or world size.
++When a P instance and a D instance transmit KVCache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVCache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVCache transmission can be performed, without being restricted by rank or world size.
+ 
+ ## NCCL Group Topology
+ 
+-Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVcache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.
++Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVCache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.
+ 
+ ![image2](https://github.com/user-attachments/assets/837e61d6-365e-4cbf-8640-6dd7ab295b36)
+ 
+diff --git a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
+index 4e82424d6..28e7dc318 100644
+--- a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
++++ b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py
+@@ -66,6 +66,16 @@ def start_service_discovery(hostname, port):
+ 
+ 
+ AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
++# Reuse a single aiohttp session to reduce connection overhead
++CLIENT_SESSION: aiohttp.ClientSession | None = None
++AUTH_BEARER = f"Bearer {os.environ.get('OPENAI_API_KEY', '')}"
++
++async def _get_client_session() -> aiohttp.ClientSession:
++    global CLIENT_SESSION
++    if CLIENT_SESSION is None or CLIENT_SESSION.closed:
++        CLIENT_SESSION = aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT)
++    return CLIENT_SESSION
++
+ 
+ app = Quart(__name__)
+ 
+@@ -75,19 +85,15 @@ def random_uuid() -> str:
+ 
+ 
+ async def forward_request(url, data, request_id):
+-    async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:
+-        headers = {
+-            "Authorization": f"Bearer {os.environ.get('OPENAI_API_KEY')}",
+-            "X-Request-Id": request_id,
+-        }
+-        async with session.post(url=url, json=data, headers=headers) as response:
+-            if response.status == 200:
+-                if True:
+-                    async for chunk_bytes in response.content.iter_chunked(102
\ No newline at end of file
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
index 52f589a6d..cbd035eab 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py
@@ -6,6 +6,10 @@ from typing import TYPE_CHECKING, Any, Optional
 
 import regex as re
 import torch
+# Precompile regex patterns to avoid recompilation on every call
+_PREFILL_RE = re.compile(r"___prefill_addr_(.*):(\d+)___")
+_DECODE_RE = re.compile(r"___decode_addr_(.*):(\d+)")
+
 
 from vllm.config import VllmConfig
 from vllm.distributed.kv_transfer.kv_connector.v1.base import (
@@ -40,10 +44,10 @@ class ReqMeta:
     def make_meta(request_id: str, token_ids: list[int], block_ids: list[int],
                   block_size: int) -> "ReqMeta":
         valid_num_tokens = len(token_ids)
-        token_ids_tensor = torch.tensor(token_ids)
-        block_ids_tensor = torch.tensor(block_ids)
+        token_ids_tensor = torch.as_tensor(token_ids)
+        block_ids_tensor = torch.as_tensor(block_ids)
         num_blocks = block_ids_tensor.shape[0]
-        block_offsets = torch.arange(0, block_size)
+        block_offsets = torch.arange(block_size)
         slot_mapping = block_offsets.reshape((1, block_size)) + \
                 block_ids_tensor.reshape((num_blocks, 1)) * block_size
         slot_mapping = slot_mapping.flatten()[:valid_num_tokens]
@@ -455,19 +459,12 @@ class P2pNcclConnector(KVConnectorBase_V1):
 
     @staticmethod
     def parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:
-        # Regular expression to match the string hostname and integer port
-        if is_prefill:
-            pattern = r"___decode_addr_(.*):(\d+)"
-        else:
-            pattern = r"___prefill_addr_(.*):(\d+)___"
-
-        # Use re.search to find the pattern in the request_id
-        match = re.search(pattern, request_id)
+        # Use precompiled regex patterns for performance
+        pattern = _DECODE_RE if is_prefill else _PREFILL_RE
+        match = pattern.search(request_id)
         if match:
-            # Extract the ranks
             ip = match.group(1)
             port = int(match.group(2))
-
             return ip, port
         raise ValueError(
             f"Request id {request_id} does not contain hostname and port")
diff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
index 6c9ccb2e3..6025d1657 100644
--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py
@@ -239,7 +239,7 @@ class P2pNcclEngine:
         tensor_id: str,
         remote_address: typing.Optional[str] = None,
     ) -> torch.Tensor:
-        if self.send_type == "PUT" or self.send_type == "PUT_ASYNC":
+        if self.send_type in ("PUT", "PUT_ASYNC"):
             start_time = time.time()
             with self.recv_store_cv:
                 while tensor_id not in self.recv_store:
@@ -295,6 +295,7 @@ class P2pNcclEngine:
             socks = dict(self.poller.poll())
             if self.router_socket in socks:
                 remote_address, message = self.router_socket.recv_multipart()
+                remote = remote_address.decode()
                 data = msgpack.loads(message)
                 if data["cmd"] == "NEW":
                     unique_id = self.nccl.unique_id_from_bytes(
@@ -304,10 +305,10 @@ class P2pNcclEngine:
                         with set_p2p_nccl_context(self.nccl_num_channels):
                             comm: ncclComm_t = self.nccl.ncclCommInitRank(
                                 2, unique_id, rank)
-                        self.comms[remote_address.decode()] = (comm, rank)
+                        self.comms[remote] = (comm, rank)
                         logger.info(
                             "ü§ùncclCommInitRank Success, %süëà%s, MyRank:%s",
-                            self.zmq_address, remote_address.decode(), rank)
+                            self.zmq_address, remote, rank)
                 elif data["cmd"] == "PUT":
                     tensor_id = data["tensor_id"]
                     try:
@@ -318,7 +319,7 @@ class P2pNcclEngine:
                                                  device=self.device)
                         self.router_socket.send_multipart(
                             [remote_address, b"0"])
-                        comm, rank = self.comms[remote_address.decode()]
+                        comm, rank = self.comms[remote]
                         self._recv(comm, tensor, rank ^ 1, self.recv_stream)
                         tensor_size = tensor.element_size() * tensor.numel()
                         if (self.buffer_size + tensor_size
@@ -329,7 +330,7 @@ class P2pNcclEngine:
                             logger.warning(
                                 "üî¥[PUT]Recv Tensor, Out Of Threshold, "
                                 "%süëà%s, data:%s, addr:%d", self.zmq_address,
-                                remote_address.decode(), data, addr)
+                                remote, data, addr)
                         else:
                             self.buffer_size += tensor_size
 
@@ -340,7 +341,7 @@ class P2pNcclEngine:
                         logger.warning(
                             "üî¥[PUT]Recv Tensor, Out Of Memory, %süëà%s, "
                             "data:%s", self.zmq_address,
-                            remote_address.decode(), data)
+                            remote, data)
 
                     with self.recv_store_cv:
                         self.recv_store[tensor_id] = tensor
@@ -368,7 +369,7 @@ class P2pNcclEngine:
                         [remote_address, msgpack.dumps(data)])
 
                     if data["ret"] == 0:
-                        comm, rank = self.comms[remote_address.decode()]
+                        comm, rank = self.comms[remote]
                         self._send(comm, tensor.to(self.device), rank ^ 1,
                                    self.send_stream)
                 else:
diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py
index 7a1a0e56d..acda1e3c0 100644
--- a/vllm/spec_decode/ngram_worker.py
+++ b/vllm/spec_decode/ngram_worker.py
@@ -99,7 +99,11 @@ class NGramWorker(NonLLMProposerWorkerBase):
             input_ids = torch.as_tensor(seq_data.get_token_ids(),
                                         dtype=torch.long,
                                         device=cur_device)
-            input_length = seq_data.get_len()
+            input_length = seq_len
+
+            # Precompute a reusable arange for index computation within
+            # this sequence group. This avoids re-allocating per n-gram size.
+            arange_idx = torch.arange(sample_len, device=cur_device)
 
             for ngram_size in range(
                     min(self.ngram_prompt_lookup_max, input_length - 1),
@@ -117,15 +121,13 @@ class NGramWorker(NonLLMProposerWorkerBase):
                     # Do not match itself
                     matches = (windows[:-1] == ngram_tensor).all(dim=-1)
 
-                # first_match includes "values" (bool), indicating whether
-                # the match is found, and "indices", indicating the index
-                # of the first match.
-                first_match = matches.max(dim=-1)
-                if first_match.values.item():
-                    proposal_start_idx = first_match.indices.add_(ngram_size)
-                    spec_indices = (
-                        proposal_start_idx).repeat(sample_len) + torch.arange(
-                            sample_len, device=cur_device)
+                # Find indices of matches without triggering device-host sync.
+                # Using nonzero + numel() avoids .item() which can cause
+                # synchronization overhead on CUDA devices.
+                match_indices = torch.nonzero(matches, as_tuple=False).flatten()
+                if match_indices.numel() != 0:
+                    proposal_start_idx = match_indices[0].add_(ngram_size)
+                    spec_indices = proposal_start_idx.repeat(sample_len) + arange_idx
                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)
                     res = input_ids.gather(dim=-1,
                                            index=spec_indices).to(self.device)
@@ -148,13 +150,18 @@ class NGramWorker(NonLLMProposerWorkerBase):
             if token_id_list[idx] is None:
                 outputs.append(None)
             else:
+                # Avoid allocating and zero-filling a full 2D tensor when a
+                # broadcasted view suffices. Create a single zero row and
+                # expand in the batch dimension. This reduces memory writes
+                # and can improve performance without changing semantics.
+                zero_row = torch.zeros((1, self.vocab_size),
+                                       dtype=torch.float32,
+                                       device=self.device)
                 outputs.append(
                     SamplerOutput(
                         outputs=None,
                         sampled_token_probs=token_prob_list[idx],
-                        logprobs=torch.zeros((sample_len, self.vocab_size),
-                                             dtype=torch.float32,
-                                             device=self.device),
+                        logprobs=zero_row.expand(sample_len, -1),
                         sampled_token_ids=token_id_list[idx],
                     ))
 
