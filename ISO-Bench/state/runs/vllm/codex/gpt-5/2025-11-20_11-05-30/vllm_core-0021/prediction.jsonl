{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-64172a9", "model_patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex d07527304..375528f43 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -146,10 +146,13 @@ def _apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n     output_bin_counts, output_mask = _get_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n \n-    repetition_penalties = repetition_penalties[:, None].repeat(1, vocab_size)\n-    repetition_penalties[~(prompt_mask | output_mask)] = 1.0\n-    logits = torch.where(logits > 0, logits / repetition_penalties,\n-                         logits * repetition_penalties)\n+    # Apply repetition penalties without materializing a full (N, V) tensor.\n+    # Broadcast the per-sequence penalty and select only masked positions.\n+    use_mask = (prompt_mask | output_mask)\n+    rp = repetition_penalties.unsqueeze(1)\n+    pos = logits > 0\n+    logits = torch.where(use_mask, torch.where(pos, logits / rp, logits * rp),\n+                         logits)\n \n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n@@ -506,22 +509,23 @@ def _sample(\n     #                                   sampling_tensors)\n \n \n-def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n+def _get_ranks(x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     This function calculates the ranks of the chosen tokens in a logprob tensor.\n \n     Args:\n         x (torch.Tensor): 2D logprob tensor of shape (N, M)\n                         where N is the no. of tokens and M is the vocab dim.\n-        indices (List[int]): List of chosen token indices.\n+        indices (torch.Tensor): List of chosen token indices.\n \n     Returns:\n         torch.Tensor: 1D tensor of shape (N,) where N is the no. of tokens.\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[range(len(x)), indices]\n-    return (x > vals[:, None]).long().sum(1) + 1\n+    vals = x[torch.arange(0, len(x), device=x.device, dtype=indices.dtype),\n+             indices]\n+    return (x > vals[:, None]).long().sum(1).add_(1)\n \n \n def _get_logprobs(\n@@ -561,11 +565,16 @@ def _get_logprobs(\n         sample_idx += num_parent_seqs\n     assert sample_idx == logprobs.size(0)\n \n+    # Convert indices to tensors on the same device for efficient indexing.\n+    seq_idx_tensor = torch.as_tensor(batched_logprobs_query_seq_indices,\n+                                     device=logprobs.device,\n+                                     dtype=torch.long)\n+    tok_idx_tensor = torch.as_tensor(batched_logprobs_query_token_indices,\n+                                     device=logprobs.device,\n+                                     dtype=torch.long)\n+\n     # Batched query for logprobs of selected token\n-    batched_logprobs_query_result = logprobs[[\n-        batched_logprobs_query_seq_indices,\n-        batched_logprobs_query_token_indices\n-    ]]\n+    batched_logprobs_query_result = logprobs[(seq_idx_tensor, tok_idx_tensor)]\n \n     # Batched query for logprobs of topk tokens\n     if largest_num_logprobs > 0:\n@@ -580,8 +589,7 @@ def _get_logprobs(\n     batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n \n     batched_ranks_query_result = _get_ranks(\n-        logprobs[batched_logprobs_query_seq_indices],\n-        batched_logprobs_query_token_indices)\n+        logprobs[seq_idx_tensor], tok_idx_tensor)\n \n     # Gather results\n     result_prompt_logprobs: List[Optional[PromptLogprobs]] = []\n", "model_name_or_path": "gpt-5-2025-08-07"}
