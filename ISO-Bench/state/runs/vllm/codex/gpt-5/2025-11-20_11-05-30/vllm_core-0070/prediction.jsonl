{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3e1c76c", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex e06f7d54e..1350b1fcc 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -79,7 +79,8 @@ class TritonMLAImpl(MLACommonImpl[MLACommonMetadata]):\n         B = q_nope.shape[0]\n \n         q = torch.cat([q_nope, q_pe], dim=-1)\n-        o = torch.zeros(B,\n+        # Allocate output without zero-initialization; kernel fully writes it.\n+        o = torch.empty(B,\n                         self.num_heads,\n                         self.kv_lora_rank,\n                         dtype=q.dtype,\ndiff --git a/vllm/attention/layer.py b/vllm/attention/layer.py\nindex f9c2d4f49..a4391a295 100644\n--- a/vllm/attention/layer.py\n+++ b/vllm/attention/layer.py\n@@ -205,7 +205,8 @@ class Attention(nn.Module):\n         if self.use_output:\n             output_shape = (output_shape\n                             if output_shape is not None else query.shape)\n-            output = torch.zeros(output_shape,\n+            # Avoid zero-initialization; attention kernels write full output.\n+            output = torch.empty(output_shape,\n                                  dtype=query.dtype,\n                                  device=query.device)\n             hidden_size = output_shape[-1]\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\nindex 61247e930..cc34415cc 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n@@ -534,7 +534,7 @@ class BatchedPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         else:\n             b_type = quant_config.quant_dtype\n \n-        b_a1 = torch.zeros(\n+        b_a1 = torch.empty(\n             (num_local_experts, self.max_num_tokens, hidden_dim),\n             dtype=b_type,\n             device=a1.device)\ndiff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py\nindex d0d8c7d6f..e6baf4c58 100644\n--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py\n+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py\n@@ -679,7 +679,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n         \"\"\"\n \n         a1 = hidden_states\n-        output = a1 if inplace else torch.zeros_like(a1)\n+        output = a1 if inplace else torch.empty_like(a1)\n \n         local_num_experts = w1.size(0)\n         if global_num_experts == -1:\ndiff --git a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\nindex 9a5315b8b..785b284a9 100644\n--- a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n+++ b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n@@ -115,11 +115,11 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):\n         K = fused_expert_output.size(-1)\n \n         if output is None:\n-            output = torch.zeros((num_tokens, K),\n+            output = torch.empty((num_tokens, K),\n                                  device=fused_expert_output.device,\n                                  dtype=fused_expert_output.dtype)\n-        else:\n-            output.fill_(0)\n+\n+        initialized = torch.zeros((num_tokens,), dtype=torch.bool, device=fused_expert_output.device)\n \n         assert output.size() == (num_tokens, K), (\n             f\"Expected output size {(num_tokens, K)}, but got {output.size()}\")\n@@ -132,8 +132,20 @@ class TopKWeightAndReduceNaiveBatched(mk.TopKWeightAndReduce):\n             topks = torch.any(matching_tokens, dim=1).flatten()\n             rows = torch.count_nonzero(topks)\n             rhs = fused_expert_output[expert_id - first_expert, :rows, :]\n-            if not apply_router_weight_on_input:\n-                rhs.mul_(topk_weights[matching_tokens].view(rhs.size(0), 1))\n-            output[topks] = output[topks] + rhs\n+            if rows > 0:\n+                idx = torch.nonzero(topks, as_tuple=False).squeeze(1)\n+                new_rows_mask = ~initialized[idx]\n+                if not apply_router_weight_on_input:\n+                    rhs.mul_(topk_weights[matching_tokens].view(rhs.size(0), 1))\n+                if new_rows_mask.any():\n+                    output[idx[new_rows_mask]] = rhs[new_rows_mask]\n+                old_rows_mask = ~new_rows_mask\n+                if old_rows_mask.any():\n+                    output[idx[old_rows_mask]] = output[idx[old_rows_mask]] + rhs[old_rows_mask]\n+                initialized[idx] = True\n+\n+        # Zero rows that received no contributions from local experts\n+        if (~initialized).any():\n+            output[~initialized] = 0\n \n         return output\ndiff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 54361a232..7d03d0832 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -241,9 +241,8 @@ def permute_rows(q_w: torch.Tensor,\n     orig_device = q_w.device\n     k_size, _ = q_w.shape\n \n-    g_idx = torch.zeros((k_size, ), dtype=torch.int32)\n-    for i in range(k_size):\n-        g_idx[i] = i // group_size\n+    # Vectorized group index computation; avoids Python loop and zero-init\n+    g_idx = (torch.arange(k_size, dtype=torch.int32) // group_size)\n \n     # Simulate act_order by doing a random permutation on K\n     rand_perm = test_perm if test_perm is not None else torch.randperm(k_size)\n", "model_name_or_path": "gpt-5-2025-08-07"}
