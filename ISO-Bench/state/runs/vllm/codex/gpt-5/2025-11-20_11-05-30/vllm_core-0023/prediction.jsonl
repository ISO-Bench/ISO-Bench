{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c8d70e2", "model_patch": "diff --git a/requirements-common.txt b/requirements-common.txt\nindex b7c94cbdb..65bc4bd7b 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -1,6 +1,7 @@\n psutil\n sentencepiece  # Required for LLaMA tokenizer.\n numpy < 2.0.0\n+numba == 0.60.0 # Required for N-gram speculative decoding; 0.61 drops Python 3.9\n requests >= 2.26.0\n tqdm\n blake3\ndiff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py\nindex 9b116e00a..7dd3868d4 100644\n--- a/vllm/v1/spec_decode/ngram_proposer.py\n+++ b/vllm/v1/spec_decode/ngram_proposer.py\n@@ -2,6 +2,65 @@\n from typing import List, Optional\n \n import numpy as np\n+try:\n+    from numba import njit  # type: ignore\n+    _NUMBA_AVAILABLE = True\n+except Exception:  # pragma: no cover - fallback when numba unavailable\n+    _NUMBA_AVAILABLE = False\n+\n+\n+def _maybe_to_int64(arr: np.ndarray) -> np.ndarray:\n+    \"\"\"Ensure array is a contiguous 1D int64 for Numba, if possible.\"\"\"\n+    if arr.dtype != np.int64:\n+        return arr.astype(np.int64, copy=False)\n+    return arr\n+\n+\n+if _NUMBA_AVAILABLE:\n+\n+    @njit(cache=True)\n+    def _kmp_lps_array_numba(pattern: np.ndarray) -> np.ndarray:\n+        lps = np.empty(pattern.shape[0], dtype=np.int64)\n+        if pattern.shape[0] == 0:\n+            return lps\n+        lps[0] = 0\n+        prev_lps = 0\n+        i = 1\n+        while i < pattern.shape[0]:\n+            if pattern[i] == pattern[prev_lps]:\n+                prev_lps += 1\n+                lps[i] = prev_lps\n+                i += 1\n+            else:\n+                if prev_lps != 0:\n+                    prev_lps = lps[prev_lps - 1]\n+                else:\n+                    lps[i] = 0\n+                    i += 1\n+        return lps\n+\n+    @njit(cache=True)\n+    def _find_match_end_idx_numba(context: np.ndarray, n: int) -> int:\n+        context_len = context.shape[0]\n+        if n <= 0 or n > context_len:\n+            return -1\n+        pattern = context[context_len - n:]\n+        lps = _kmp_lps_array_numba(pattern)\n+        i = 0\n+        j = 0\n+        limit = context_len - n\n+        while i < limit:\n+            if context[i] == pattern[j]:\n+                i += 1\n+                j += 1\n+                if j == n:\n+                    return i\n+            else:\n+                if j != 0:\n+                    j = lps[j - 1]\n+                else:\n+                    i += 1\n+        return -1\n \n \n class NgramProposer:\n@@ -41,8 +100,15 @@ class NgramProposer:\n               followed that pattern. Here we will return [4,2,3] because \n               we only have three tokens after the match.\n         \"\"\"\n-        # TODO: Use c++ to implement the _find_subarray_kmp to\n-        # improve the efficiency\n+        # Fast path: Use Numba-accelerated KMP to find the match index,\n+        # then slice via NumPy. Fall back to pure Python if Numba is absent.\n+        if _NUMBA_AVAILABLE:\n+            arr = _maybe_to_int64(np.asarray(context_token_ids))\n+            match_end = _find_match_end_idx_numba(arr, n)\n+            if match_end < 0:\n+                return None\n+            return arr[match_end:match_end + k]\n+        # Fallback: Pure Python KMP.\n         return self._find_subarray_kmp(context_token_ids, n, k)\n \n     @staticmethod\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0ecc00acc..60b0284dd 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -154,10 +154,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -175,16 +175,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -197,27 +197,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1353,7 +1353,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,\n                     layer_spec.head_size)\n                 dtype = layer_spec.dtype\n-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n+                # Allocate without zero-initialization; unused regions are not\n+                # read before being written by the attention kernels.\n+                kv_caches[layer_name] = torch.empty(kv_cache_shape,\n                                                     dtype=dtype,\n                                                     device=self.device)\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
