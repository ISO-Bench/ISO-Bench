diff --git a/requirements-common.txt b/requirements-common.txt
index b7c94cbdb..65bc4bd7b 100644
--- a/requirements-common.txt
+++ b/requirements-common.txt
@@ -1,6 +1,7 @@
 psutil
 sentencepiece  # Required for LLaMA tokenizer.
 numpy < 2.0.0
+numba == 0.60.0 # Required for N-gram speculative decoding; 0.61 drops Python 3.9
 requests >= 2.26.0
 tqdm
 blake3
diff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py
index 9b116e00a..7dd3868d4 100644
--- a/vllm/v1/spec_decode/ngram_proposer.py
+++ b/vllm/v1/spec_decode/ngram_proposer.py
@@ -2,6 +2,65 @@
 from typing import List, Optional
 
 import numpy as np
+try:
+    from numba import njit  # type: ignore
+    _NUMBA_AVAILABLE = True
+except Exception:  # pragma: no cover - fallback when numba unavailable
+    _NUMBA_AVAILABLE = False
+
+
+def _maybe_to_int64(arr: np.ndarray) -> np.ndarray:
+    """Ensure array is a contiguous 1D int64 for Numba, if possible."""
+    if arr.dtype != np.int64:
+        return arr.astype(np.int64, copy=False)
+    return arr
+
+
+if _NUMBA_AVAILABLE:
+
+    @njit(cache=True)
+    def _kmp_lps_array_numba(pattern: np.ndarray) -> np.ndarray:
+        lps = np.empty(pattern.shape[0], dtype=np.int64)
+        if pattern.shape[0] == 0:
+            return lps
+        lps[0] = 0
+        prev_lps = 0
+        i = 1
+        while i < pattern.shape[0]:
+            if pattern[i] == pattern[prev_lps]:
+                prev_lps += 1
+                lps[i] = prev_lps
+                i += 1
+            else:
+                if prev_lps != 0:
+                    prev_lps = lps[prev_lps - 1]
+                else:
+                    lps[i] = 0
+                    i += 1
+        return lps
+
+    @njit(cache=True)
+    def _find_match_end_idx_numba(context: np.ndarray, n: int) -> int:
+        context_len = context.shape[0]
+        if n <= 0 or n > context_len:
+            return -1
+        pattern = context[context_len - n:]
+        lps = _kmp_lps_array_numba(pattern)
+        i = 0
+        j = 0
+        limit = context_len - n
+        while i < limit:
+            if context[i] == pattern[j]:
+                i += 1
+                j += 1
+                if j == n:
+                    return i
+            else:
+                if j != 0:
+                    j = lps[j - 1]
+                else:
+                    i += 1
+        return -1
 
 
 class NgramProposer:
@@ -41,8 +100,15 @@ class NgramProposer:
               followed that pattern. Here we will return [4,2,3] because 
               we only have three tokens after the match.
         """
-        # TODO: Use c++ to implement the _find_subarray_kmp to
-        # improve the efficiency
+        # Fast path: Use Numba-accelerated KMP to find the match index,
+        # then slice via NumPy. Fall back to pure Python if Numba is absent.
+        if _NUMBA_AVAILABLE:
+            arr = _maybe_to_int64(np.asarray(context_token_ids))
+            match_end = _find_match_end_idx_numba(arr, n)
+            if match_end < 0:
+                return None
+            return arr[match_end:match_end + k]
+        # Fallback: Pure Python KMP.
         return self._find_subarray_kmp(context_token_ids, n, k)
 
     @staticmethod
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 0ecc00acc..60b0284dd 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -154,10 +154,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         self.num_sms = self.device_properties.multi_processor_count
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
         # None in the first PP rank. The rest are set after load_model.
@@ -175,16 +175,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             # identical position IDs, making M-RoPE functionally equivalent to
             # 1D-RoPE.
             # See page 5 of https://arxiv.org/abs/2409.12191
-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
                 pin_memory=self.pin_memory)
 
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -197,27 +197,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
@@ -1353,7 +1353,9 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,
                     layer_spec.head_size)
                 dtype = layer_spec.dtype
-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,
+                # Allocate without zero-initialization; unused regions are not
+                # read before being written by the attention kernels.
+                kv_caches[layer_name] = torch.empty(kv_cache_shape,
                                                     dtype=dtype,
                                                     device=self.device)
             else:
