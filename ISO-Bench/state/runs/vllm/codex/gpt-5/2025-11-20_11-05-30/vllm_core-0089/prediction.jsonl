{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-90f1e55", "model_patch": "diff --git a/benchmarks/kernels/benchmark_marlin.py b/benchmarks/kernels/benchmark_marlin.py\nindex 8fb44e3a3..ef79d7f1a 100644\n--- a/benchmarks/kernels/benchmark_marlin.py\n+++ b/benchmarks/kernels/benchmark_marlin.py\n@@ -58,7 +58,6 @@ def bench_run(results: List[benchmark.Measurement], model: str,\n     (marlin_24_w_ref, marlin_24_q_w_comp, marlin_24_meta,\n      marlin_24_s) = marlin_24_quantize(b, quant_type, group_size)\n \n-    marlin_zp = torch.empty(0, dtype=torch.int, device=b.device)\n \n     # GPTQ quant\n     (w_ref, q_w, s, g_idx,\n@@ -77,6 +76,8 @@ def bench_run(results: List[benchmark.Measurement], model: str,\n \n     marlin_24_workspace = MarlinWorkspace(size_n, GPTQ_MARLIN_24_MIN_THREAD_N,\n                                           GPTQ_MARLIN_24_MAX_PARALLEL)\n+    # Allocate directly to the required shape and dtype to avoid a redundant\n+    # temporary buffer.\n     marlin_zp = torch.zeros_like(marlin_s, dtype=torch.int)\n \n     globals = {\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex e6f9f01ef..39484d2ba 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -217,10 +217,12 @@ def moe_align_block_size(\n         by block_size for proper block matrix operations.\n     \"\"\"\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n-    sorted_ids = torch.empty((max_num_tokens_padded, ),\n-                             dtype=torch.int32,\n-                             device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n+    # Pre-initialize with padding sentinel in a single allocation to avoid\n+    # an extra device-wide fill after allocation.\n+    sorted_ids = torch.full((max_num_tokens_padded, ),\n+                            fill_value=topk_ids.numel(),\n+                            dtype=torch.int32,\n+                            device=topk_ids.device)\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     expert_ids = torch.empty((max_num_m_blocks, ),\n                              dtype=torch.int32,\n@@ -438,12 +440,13 @@ def grouped_topk(hidden_states: torch.Tensor,\n                                -1).max(dim=-1).values  # [n, n_group]\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,\n                            sorted=False)[1]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    # Build a boolean mask directly to avoid dtype conversions.\n+    group_mask = torch.zeros_like(group_scores, dtype=torch.bool)  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = group_mask.unsqueeze(-1).expand(\n         num_token, num_expert_group,\n         scores.shape[-1] // num_expert_group).reshape(num_token, -1)  # [n, e]\n-    tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+    tmp_scores = scores.masked_fill(~score_mask, 0.0)  # [n, e]\n     topk_weights, topk_ids = torch.topk(tmp_scores,\n                                         k=topk,\n                                         dim=-1,\ndiff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex 25d95ac6e..f6c6d7651 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -53,15 +53,20 @@ class InputBatch:\n         self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n         self.req_id_to_index: Dict[str, int] = {}\n \n-        self.token_ids_cpu = np.empty((max_num_reqs, max_model_len),\n-                                      dtype=np.int32)\n+        self.token_ids_cpu_tensor = torch.empty(\n+            (max_num_reqs, max_model_len),\n+            device=\"cpu\",\n+            dtype=torch.int32,\n+            pin_memory=pin_memory,\n+        )\n+        self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),\n+        self.block_table = torch.empty((max_num_reqs, max_num_blocks_per_req),\n                                        device=self.device,\n                                        dtype=torch.int32)\n-        self.block_table_cpu_tensor = torch.zeros(\n+        self.block_table_cpu_tensor = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex e75be21ef..3d5e9f979 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -627,7 +627,7 @@ class GPUModelRunner:\n             num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n         for _ in range(self.num_attn_layers):\n             self.kv_caches.append(\n-                torch.zeros(kv_cache_shape,\n+                torch.empty(kv_cache_shape,\n                             dtype=self.kv_cache_dtype,\n                             device=self.device))\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
