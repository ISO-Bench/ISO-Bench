{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5eda21e", "model_patch": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 398fdc5f0..cadb03c25 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -77,8 +77,8 @@ steps:\n   - vllm/\n   - tests/basic_correctness/test_chunked_prefill\n   commands:\n-  - VLLM_ATTENTION_BACKEND=XFORMERS VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n-  - VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py\n \n - label: Core Test # 10min\n   mirror_hardwares: [amd]\n@@ -88,10 +88,10 @@ steps:\n   - vllm/distributed\n   - tests/core\n   commands:\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core/test_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/test_chunked_prefill_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n+  - pytest -v -s core/test_scheduler.py\n+  - pytest -v -s core core/test_chunked_prefill_scheduler.py\n+  - pytest -v -s core core/block/e2e/test_correctness.py\n+  - pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n   - pytest -v -s core --ignore=core/block/e2e/test_correctness.py --ignore=core/test_scheduler.py --ignore=core/test_chunked_prefill_scheduler.py --ignore=core/block/e2e/test_correctness.py --ignore=core/block/e2e/test_correctness_sliding_window.py\n \n - label: Entrypoints Test # 40min\n@@ -192,7 +192,7 @@ steps:\n   - vllm/\n   - tests/prefix_caching\n   commands:\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s prefix_caching/test_prefix_caching.py\n+    - pytest -v -s prefix_caching/test_prefix_caching.py\n     - pytest -v -s prefix_caching --ignore=prefix_caching/test_prefix_caching.py\n \n - label: Samplers Test # 36min\n@@ -217,7 +217,7 @@ steps:\n   - tests/spec_decode\n   commands:\n     - pytest -v -s spec_decode/e2e/test_multistep_correctness.py\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s spec_decode/e2e/test_compatibility.py\n+    - pytest -v -s spec_decode/e2e/test_compatibility.py\n     - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py --ignore=spec_decode/e2e/test_compatibility.py\n \n - label: LoRA Test %N # 15min each\ndiff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py\nindex 8457bde06..b24284afc 100644\n--- a/vllm/attention/backends/flash_attn.py\n+++ b/vllm/attention/backends/flash_attn.py\n@@ -444,12 +444,15 @@ class FlashAttentionMetadataBuilder(\n                                              self.runner.pin_memory)\n         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                                device, self.runner.pin_memory)\n-        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n+        # Use empty + explicit first-element init; cumsum writes the rest.\n+        query_start_loc = torch.empty(query_lens_tensor.shape[0] + 1,\n                                       dtype=torch.int32,\n                                       device=device)\n-        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n+        query_start_loc[0] = 0\n+        seq_start_loc = torch.empty(seq_lens_tensor.shape[0] + 1,\n                                     dtype=torch.int32,\n                                     device=device)\n+        seq_start_loc[0] = 0\n         torch.cumsum(seq_lens_tensor,\n                      dim=0,\n                      dtype=seq_start_loc.dtype,\ndiff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py\nindex ba9b2d043..31817ffdf 100644\n--- a/vllm/attention/backends/flashinfer.py\n+++ b/vllm/attention/backends/flashinfer.py\n@@ -642,12 +642,15 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                                              self.runner.pin_memory)\n         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                                device, self.runner.pin_memory)\n-        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n+        # Avoid full zero-fill; cumsum populates [1:], set index 0 explicitly.\n+        query_start_loc = torch.empty(query_lens_tensor.shape[0] + 1,\n                                       dtype=torch.int32,\n                                       device=device)\n-        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n+        query_start_loc[0] = 0\n+        seq_start_loc = torch.empty(seq_lens_tensor.shape[0] + 1,\n                                     dtype=torch.int32,\n                                     device=device)\n+        seq_start_loc[0] = 0\n         torch.cumsum(seq_lens_tensor,\n                      dim=0,\n                      dtype=seq_start_loc.dtype,\n@@ -670,10 +673,11 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                                                   dtype=torch.int)\n             paged_kv_last_page_len_tensor = torch.tensor(\n                 self.paged_kv_last_page_len, device=\"cpu\", dtype=torch.int)\n-            block_table_bound_tensor = torch.zeros(len(self.paged_kv_indptr) -\n-                                                   1,\n-                                                   device=\"cpu\",\n-                                                   dtype=torch.int)\n+            # Bound tensor is written by downstream kernels; init first val.\n+            block_table_bound_tensor = torch.empty(\n+                len(self.paged_kv_indptr) - 1, device=\"cpu\", dtype=torch.int)\n+            if block_table_bound_tensor.numel() > 0:\n+                block_table_bound_tensor[0] = 0\n         else:\n             paged_kv_indices_tensor = None\n             paged_kv_indptr_tensor = None\ndiff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py\nindex 53e3a53ba..1e679edde 100644\n--- a/vllm/attention/backends/utils.py\n+++ b/vllm/attention/backends/utils.py\n@@ -241,12 +241,15 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n                                              self.runner.pin_memory)\n         slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                                device, self.runner.pin_memory)\n-        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n+        # Avoid zero-fill; cumsum populates indices [1:], set [0] explicitly.\n+        query_start_loc = torch.empty(query_lens_tensor.shape[0] + 1,\n                                       dtype=torch.int32,\n                                       device=device)\n-        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n+        query_start_loc[0] = 0\n+        seq_start_loc = torch.empty(seq_lens_tensor.shape[0] + 1,\n                                     dtype=torch.int32,\n                                     device=device)\n+        seq_start_loc[0] = 0\n         torch.cumsum(seq_lens_tensor,\n                      dim=0,\n                      dtype=seq_start_loc.dtype,\n", "model_name_or_path": "gpt-5-2025-08-07"}
