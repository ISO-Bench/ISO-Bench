{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f168b85", "model_patch": "diff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..405ee9022 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -29,7 +29,7 @@ from vllm.distributed.parallel_state import get_pp_group\n from vllm.forward_context import get_forward_context\n from vllm.model_executor.layers.activation import ReLUSquaredActivation\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -63,26 +63,33 @@ class NemotronHMLP(nn.Module):\n         config: NemotronHConfig,\n         quant_config: Optional[QuantizationConfig] = None,\n         bias: bool = False,\n+        prefix: str = \"\",\n     ) -> None:\n         super().__init__()\n-        self.up_proj = MergedColumnParallelLinear(\n+        # Use non-merged column-parallel linear for a single output matrix\n+        # and avoid returning bias to reduce Python overhead.\n+        self.up_proj = ColumnParallelLinear(\n             input_size=config.hidden_size,\n-            output_sizes=[config.intermediate_size],\n+            output_size=config.intermediate_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.up_proj\",\n+            return_bias=False,\n         )\n         self.down_proj = RowParallelLinear(\n             input_size=config.intermediate_size,\n             output_size=config.hidden_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.down_proj\",\n+            return_bias=False,\n         )\n         self.act_fn = ReLUSquaredActivation()\n \n     def forward(self, x: torch.Tensor):\n-        x, _ = self.up_proj(x)\n+        x = self.up_proj(x)\n         x = self.act_fn(x)\n-        x, _ = self.down_proj(x)\n+        x = self.down_proj(x)\n         return x\n \n \n@@ -101,7 +108,8 @@ class NemotronHMLPDecoderLayer(nn.Module):\n \n         self.mixer = NemotronHMLP(config,\n                                   quant_config=quant_config,\n-                                  bias=config.mlp_bias)\n+                                  bias=config.mlp_bias,\n+                                  prefix=f\"{prefix}.mlp\")\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -207,12 +215,16 @@ class NemotronHAttention(nn.Module):\n             self.total_num_kv_heads,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.qkv_proj\",\n+            return_bias=False,\n         )\n         self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             config.hidden_size,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.o_proj\",\n+            return_bias=False,\n         )\n \n         self.attn = Attention(\n@@ -229,10 +241,10 @@ class NemotronHAttention(nn.Module):\n         hidden_states: torch.Tensor,\n         **kwargs,\n     ) -> torch.Tensor:\n-        qkv, _ = self.qkv_proj(hidden_states)\n+        qkv = self.qkv_proj(hidden_states)\n         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n         attn_output = self.attn(q, k, v)\n-        output, _ = self.o_proj(attn_output)\n+        output = self.o_proj(attn_output)\n         return output\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
