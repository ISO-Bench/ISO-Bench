diff --git a/vllm/sequence.py b/vllm/sequence.py
index 13746cef2..24e7b3334 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -39,46 +39,52 @@ PromptLogprobs = List[Optional[Dict[int, Logprob]]]
 SampleLogprobs = List[Dict[int, Logprob]]
 
 
-class SequenceStatus(enum.Enum):
-    """Status of a sequence."""
-    WAITING = enum.auto()
-    RUNNING = enum.auto()
-    SWAPPED = enum.auto()
-    FINISHED_STOPPED = enum.auto()
-    FINISHED_LENGTH_CAPPED = enum.auto()
-    FINISHED_ABORTED = enum.auto()
-    FINISHED_IGNORED = enum.auto()
+class SequenceStatus(enum.IntEnum):
+    """Status of a sequence.
+
+    Note: values are ordered so that any status value greater than
+    ``SWAPPED`` is considered finished. This enables a fast numeric check
+    in ``is_finished``.
+    """
+    WAITING = 0
+    RUNNING = 1
+    SWAPPED = 2
+    # Anything after SWAPPED will be considered finished.
+    FINISHED_STOPPED = 3
+    FINISHED_LENGTH_CAPPED = 4
+    FINISHED_ABORTED = 5
+    FINISHED_IGNORED = 6
+
 
     @staticmethod
     def is_finished(status: "SequenceStatus") -> bool:
-        return status in [
-            SequenceStatus.FINISHED_STOPPED,
-            SequenceStatus.FINISHED_LENGTH_CAPPED,
-            SequenceStatus.FINISHED_ABORTED,
-            SequenceStatus.FINISHED_IGNORED,
-        ]
+        # Fast numeric comparison instead of list membership checks.
+        return status > SequenceStatus.SWAPPED
 
     @staticmethod
     def get_finished_reason(status: "SequenceStatus") -> Union[str, None]:
-        if status == SequenceStatus.FINISHED_STOPPED:
-            finish_reason = "stop"
-        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:
-            finish_reason = "length"
-        elif status == SequenceStatus.FINISHED_ABORTED:
-            finish_reason = "abort"
-        elif status == SequenceStatus.FINISHED_IGNORED:
-            # The ignored sequences are the sequences whose prompt lengths
-            # are longer than the model's length cap. Therefore, the stop
-            # reason should also be "length" as in OpenAI API.
-            finish_reason = "length"
-        else:
-            finish_reason = None
-        return finish_reason
-
-
-class SequenceStage(enum.Enum):
-    PREFILL = enum.auto()
-    DECODE = enum.auto()
+        # The ignored sequences are those whose prompts exceed the model's
+        # length cap, which aligns with the "length" finish reason.
+        idx = int(status)
+        if 0 <= idx < len(SequenceStatus._FINISHED_REASONS):
+            return SequenceStatus._FINISHED_REASONS[idx]
+        return None
+
+# Map status value to finish reason for quick lookup.
+SequenceStatus._FINISHED_REASONS = (
+    None,     # WAITING
+    None,     # RUNNING
+    None,     # SWAPPED
+    "stop",   # FINISHED_STOPPED
+    "length", # FINISHED_LENGTH_CAPPED
+    "abort",  # FINISHED_ABORTED
+    "length", # FINISHED_IGNORED
+)
+
+
+class SequenceStage(enum.IntEnum):
+    PREFILL = 0
+    DECODE = 1
 
 
 @dataclass
