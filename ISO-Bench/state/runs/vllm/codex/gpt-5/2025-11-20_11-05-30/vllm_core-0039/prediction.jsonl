{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-51e971d", "model_patch": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 13746cef2..24e7b3334 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -39,46 +39,52 @@ PromptLogprobs = List[Optional[Dict[int, Logprob]]]\n SampleLogprobs = List[Dict[int, Logprob]]\n \n \n-class SequenceStatus(enum.Enum):\n-    \"\"\"Status of a sequence.\"\"\"\n-    WAITING = enum.auto()\n-    RUNNING = enum.auto()\n-    SWAPPED = enum.auto()\n-    FINISHED_STOPPED = enum.auto()\n-    FINISHED_LENGTH_CAPPED = enum.auto()\n-    FINISHED_ABORTED = enum.auto()\n-    FINISHED_IGNORED = enum.auto()\n+class SequenceStatus(enum.IntEnum):\n+    \"\"\"Status of a sequence.\n+\n+    Note: values are ordered so that any status value greater than\n+    ``SWAPPED`` is considered finished. This enables a fast numeric check\n+    in ``is_finished``.\n+    \"\"\"\n+    WAITING = 0\n+    RUNNING = 1\n+    SWAPPED = 2\n+    # Anything after SWAPPED will be considered finished.\n+    FINISHED_STOPPED = 3\n+    FINISHED_LENGTH_CAPPED = 4\n+    FINISHED_ABORTED = 5\n+    FINISHED_IGNORED = 6\n+\n \n     @staticmethod\n     def is_finished(status: \"SequenceStatus\") -> bool:\n-        return status in [\n-            SequenceStatus.FINISHED_STOPPED,\n-            SequenceStatus.FINISHED_LENGTH_CAPPED,\n-            SequenceStatus.FINISHED_ABORTED,\n-            SequenceStatus.FINISHED_IGNORED,\n-        ]\n+        # Fast numeric comparison instead of list membership checks.\n+        return status > SequenceStatus.SWAPPED\n \n     @staticmethod\n     def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n-        if status == SequenceStatus.FINISHED_STOPPED:\n-            finish_reason = \"stop\"\n-        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n-            finish_reason = \"length\"\n-        elif status == SequenceStatus.FINISHED_ABORTED:\n-            finish_reason = \"abort\"\n-        elif status == SequenceStatus.FINISHED_IGNORED:\n-            # The ignored sequences are the sequences whose prompt lengths\n-            # are longer than the model's length cap. Therefore, the stop\n-            # reason should also be \"length\" as in OpenAI API.\n-            finish_reason = \"length\"\n-        else:\n-            finish_reason = None\n-        return finish_reason\n-\n-\n-class SequenceStage(enum.Enum):\n-    PREFILL = enum.auto()\n-    DECODE = enum.auto()\n+        # The ignored sequences are those whose prompts exceed the model's\n+        # length cap, which aligns with the \"length\" finish reason.\n+        idx = int(status)\n+        if 0 <= idx < len(SequenceStatus._FINISHED_REASONS):\n+            return SequenceStatus._FINISHED_REASONS[idx]\n+        return None\n+\n+# Map status value to finish reason for quick lookup.\n+SequenceStatus._FINISHED_REASONS = (\n+    None,     # WAITING\n+    None,     # RUNNING\n+    None,     # SWAPPED\n+    \"stop\",   # FINISHED_STOPPED\n+    \"length\", # FINISHED_LENGTH_CAPPED\n+    \"abort\",  # FINISHED_ABORTED\n+    \"length\", # FINISHED_IGNORED\n+)\n+\n+\n+class SequenceStage(enum.IntEnum):\n+    PREFILL = 0\n+    DECODE = 1\n \n \n @dataclass\n", "model_name_or_path": "gpt-5-2025-08-07"}
