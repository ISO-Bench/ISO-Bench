{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c5932e5", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 442173939..62440d001 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -41,7 +41,7 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n         if prev_tokens is None:\n             prev_tokens = new_tokens\n         else:\n-            prev_tokens += new_tokens\n+            prev_tokens.extend(new_tokens)\n     return decoded_text\n \n \ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 724782841..d0eab1cc4 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -718,12 +718,18 @@ class LLMEngine:\n                          all_input_ids: List[int]) -> None:\n         if not logprobs:\n             return\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+        special_ids = tokenizer.all_special_ids\n         for token_id, sample_logprob in logprobs.items():\n             if (sample_logprob.decoded_token is None and token_id != -1):\n+                # Fast-path skip if this is a special token and we are skipping them.\n+                if prms.skip_special_tokens and token_id in special_ids:\n+                    sample_logprob.decoded_token = \"\"\n+                    continue\n                 all_input_ids_with_logprob = all_input_ids[:-1] + [token_id]\n                 (_, new_text, prefix_offset,\n                  read_offset) = detokenize_incrementally(\n-                     self.get_tokenizer_for_seq(seq),\n+                     tokenizer,\n                      all_input_ids=all_input_ids_with_logprob,\n                      prev_tokens=seq.tokens,\n                      prefix_offset=seq.prefix_offset,\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 1ec09f0cd..93ee313e3 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -190,17 +190,22 @@ def moe_align_block_size(\n     - The padding ensures that the total number of tokens is now divisible\n         by block_size for proper block matrix operations.\n     \"\"\"\n+    # Precompute values and avoid redundant size/dtype queries.\n+    total_tokens = topk_ids.numel()\n+    device = topk_ids.device\n+    # Allocate without zero-initialization for improved performance.\n     sorted_ids = torch.empty(\n-        (topk_ids.numel() + num_experts * (block_size - 1), ),\n+        (total_tokens + num_experts * (block_size - 1), ),\n         dtype=torch.int32,\n-        device=topk_ids.device)\n-    expert_ids = torch.empty((topk_ids.numel() + num_experts, ),\n+        device=device)\n+    expert_ids = torch.empty((total_tokens + num_experts, ),\n                              dtype=torch.int32,\n-                             device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n-    num_tokens_post_pad = torch.empty((1),\n-                                      dtype=torch.int32,\n-                                      device=topk_ids.device)\n+                             device=device)\n+    # Fill padding sentinel once; kernel writes all valid positions.\n+    sorted_ids.fill_(total_tokens)\n+    # Use 0-dim int32 tensor to reduce overhead while remaining compatible\n+    # with downstream kernels that read from a pointer.\n+    num_tokens_post_pad = torch.empty((), dtype=torch.int32, device=device)\n     ops.moe_align_block_size(topk_ids, num_experts, block_size, sorted_ids,\n                              expert_ids, num_tokens_post_pad)\n     return sorted_ids, expert_ids, num_tokens_post_pad\n@@ -348,7 +353,8 @@ def fused_moe(\n             topk_weights,\n             topk_ids,\n             token_expert_indicies,\n-            gating_output.float(),  # TODO(woosuk): Optimize this.\n+            # Avoid an unnecessary copy if already float32\n+            gating_output.to(torch.float32, copy=False),\n         )\n         del token_expert_indicies  # Not used. Will be used in the future.\n     if renormalize:\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19a8..04cb1f753 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -137,21 +137,24 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n-    all_special_tokens = set(tokenizer.all_special_tokens)\n+    # tokenizer.all_special_tokens is already a set via get_cached_tokenizer.\n+    all_special_tokens = tokenizer.all_special_tokens\n+    added_vocab = tokenizer.get_added_vocab()\n+    append_sub_text = sub_texts.append\n+    convert_sub = tokenizer.convert_tokens_to_string\n+\n     for token in output_tokens:\n         if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.get_added_vocab():\n+        if token in added_vocab:\n             if current_sub_text:\n-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-                sub_texts.append(sub_text)\n+                sub_texts.append(convert_sub(current_sub_text))\n                 current_sub_text = []\n-            sub_texts.append(token)\n+            append_sub_text(token)\n         else:\n             current_sub_text.append(token)\n     if current_sub_text:\n-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-        sub_texts.append(sub_text)\n+        sub_texts.append(convert_sub(current_sub_text))\n     if spaces_between_special_tokens:\n         return \" \".join(sub_texts)\n     else:\n@@ -171,6 +174,12 @@ def detokenize_incrementally(\n     spaces_between_special_tokens: bool = True,\n ) -> Tuple[List[str], str, int, int]:\n     new_token_id = all_input_ids[-1]\n+    # Fast path when we already have previous tokens and the new token is a\n+    # special token to be skipped: avoid any string conversion work.\n+    if prev_tokens is not None and skip_special_tokens and (\n+            new_token_id in tokenizer.all_special_ids):\n+        return [], \"\", prefix_offset, read_offset\n+\n     # This is the first iteration for this sequence\n     if prev_tokens is None:\n         new_tokens = tokenizer.convert_ids_to_tokens(\n@@ -195,10 +204,9 @@ def detokenize_incrementally(\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n     if tokenizer.is_fast or not tokenizer.get_added_vocab():\n-        prefix_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:read_offset])\n-        new_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:])\n+        convert = tokenizer.convert_tokens_to_string\n+        prefix_text = convert(output_tokens[prefix_offset:read_offset])\n+        new_text = convert(output_tokens[prefix_offset:])\n     else:\n         prefix_text = _convert_tokens_to_string_with_added_encoders(\n             tokenizer,\n", "model_name_or_path": "gpt-5-2025-08-07"}
