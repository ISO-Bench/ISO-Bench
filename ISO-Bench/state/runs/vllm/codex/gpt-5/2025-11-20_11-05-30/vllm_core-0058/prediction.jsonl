{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-51f8aa9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex 444bb25f2..174ffd043 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -287,15 +287,15 @@ class BlockTable:\n             extra_hash: Optional[int] = None) -> List[Block]:\n         blocks: List[Block] = []\n \n-        block_token_ids = []\n-        tail_token_ids = []\n-        for cur_token_ids in chunk_list(token_ids, self._block_size):\n-            if len(cur_token_ids) == self._block_size:\n-                block_token_ids.append(cur_token_ids)\n-            else:\n-                tail_token_ids.append(cur_token_ids)\n-\n-        if block_token_ids:\n+        num_tokens = len(token_ids)\n+        bs = self._block_size\n+        full_blocks = num_tokens // bs\n+        rem = num_tokens - full_blocks * bs\n+\n+        if full_blocks:\n+            block_token_ids = [\n+                token_ids[i * bs:(i + 1) * bs] for i in range(full_blocks)\n+            ]\n             blocks.extend(\n                 self._allocator.allocate_immutable_blocks(\n                     prev_block,\n@@ -304,9 +304,8 @@ class BlockTable:\n                     extra_hash=extra_hash))\n             prev_block = blocks[-1]\n \n-        if tail_token_ids:\n-            assert len(tail_token_ids) == 1\n-            cur_token_ids = tail_token_ids[0]\n+        if rem:\n+            cur_token_ids = token_ids[-rem:]\n \n             block = self._allocator.allocate_mutable_block(\n                 prev_block=prev_block, device=device, extra_hash=extra_hash)\n@@ -318,22 +317,12 @@ class BlockTable:\n \n     def _get_all_token_ids(self) -> List[int]:\n         # NOTE: This function is O(seq_len); use sparingly.\n-        token_ids: List[int] = []\n-\n         if not self._is_allocated:\n-            return token_ids\n-\n-        for block in self.blocks:\n-            token_ids.extend(block.token_ids)\n-\n-        return token_ids\n+            return []\n+        return [tid for block in self.blocks for tid in block.token_ids]\n \n     def _get_num_token_ids(self) -> int:\n-        res = 0\n-        for block in self.blocks:\n-            res += len(block.token_ids)\n-\n-        return res\n+        return sum(len(block.token_ids) for block in self.blocks)\n \n     @property\n     def _is_allocated(self) -> bool:\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 2913a01bf..e59ac8094 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -117,6 +117,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             block_pool=self._block_pool,  # Share block pool here\n         )\n \n+        # Cache an absolute->physical id lookup to avoid repeated sorting\n+        # and linear searches when mapping to device-local ids.\n+        sorted_ids = sorted(self.all_block_ids)\n+        self._phys_id_lut = {abs_id: idx for idx, abs_id in enumerate(sorted_ids)}\n+\n         # Evitor used to maintain how we want to handle those computed blocks\n         # if we find memory pressure is high.\n         self.eviction_policy = eviction_policy\n@@ -310,18 +315,16 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         raise BlockAllocator.NoFreeBlocksError()\n \n     def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:\n+        # Fast path: directly get a free physical block id from the\n+        # underlying allocator without creating a temporary Block object.\n         try:\n-            # Allocate mutable block and extract its block_id\n-            block = self._hashless_allocator.allocate_mutable_block(\n-                prev_block=None)\n-            block_id = block.block_id\n-            self._block_pool.free_block(block)\n-\n-            self._track_block_id(block_id, computed=False)\n-            return block_id\n+            block_id = self._hashless_allocator._allocate_block_id()  # type: ignore[attr-defined]\n         except BlockAllocator.NoFreeBlocksError:\n             return None\n \n+        self._track_block_id(block_id, computed=False)\n+        return block_id\n+\n     def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:\n         if self.evictor.num_blocks == 0:\n             return None\n@@ -433,7 +436,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         Returns:\n             int: The rzero-offset block id on certain device.\n         \"\"\"\n-        return sorted(self.all_block_ids).index(absolute_id)\n+        return self._phys_id_lut[absolute_id]\n \n     @property\n     def all_block_ids(self) -> FrozenSet[int]:\n@@ -569,19 +572,25 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         info in evictor's metadata.\n         \"\"\"\n \n+        block_tracker = self._block_tracker\n+        evictor = self.evictor\n         for block_id in block_ids:\n-            if self._block_tracker[block_id].active:\n-                self._block_tracker[block_id].last_accessed = now\n-            elif block_id in self.evictor:\n-                self.evictor.update(block_id, now)\n+            bt = block_tracker[block_id]\n+            if bt.active:\n+                bt.last_accessed = now\n+            elif block_id in evictor:\n+                evictor.update(block_id, now)\n             else:\n                 raise ValueError(\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n         # Mark all touched blocks as computed.\n+        if not self._touched_blocks:\n+            return\n+        block_tracker = self._block_tracker\n         for block_id in self._touched_blocks:\n-            self._block_tracker[block_id].computed = True\n+            block_tracker[block_id].computed = True\n         self._touched_blocks.clear()\n \n     def _track_block_id(self, block_id: Optional[BlockId],\ndiff --git a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\nindex 7016ff34c..bedfc5276 100644\n--- a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n+++ b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n@@ -31,7 +31,12 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         self.handle = None\n \n         # From https://github.com/deepseek-ai/DeepEP/blob/9fe9021f29c9083cd1808ab36b740208524d9f63/deep_ep/buffer.py#L164\n-        self.available_rank_configs = [2, 4, 8, 16, 24, 32, 64, 128, 144, 160]\n+        self.available_rank_configs = (2, 4, 8, 16, 24, 32, 64, 128, 144, 160)\n+        # Cache DeepEP configs once per instance to avoid repeated lookups.\n+        self._dispatch_config = (deep_ep.Buffer.get_dispatch_config(dp_size)\n+                                 if dp_size in self.available_rank_configs else None)\n+        self._combine_config = (deep_ep.Buffer.get_combine_config(dp_size)\n+                                if dp_size in self.available_rank_configs else None)\n \n     def num_dispatchers(self) -> int:\n         return self.num_dispatchers_\n@@ -47,14 +52,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         return torch.int64\n \n     def _get_dispatch_config(self) -> Optional[deep_ep.Config]:\n-        if self.dp_size not in self.available_rank_configs:\n-            return None\n-        return deep_ep.Buffer.get_dispatch_config(self.dp_size)\n+        return self._dispatch_config\n \n     def _get_combine_config(self) -> Optional[deep_ep.Config]:\n-        if self.dp_size not in self.available_rank_configs:\n-            return None\n-        return deep_ep.Buffer.get_combine_config(self.dp_size)\n+        return self._combine_config\n \n     def _do_dispatch(self, tokens: torch.Tensor,\n                      token_scales: Optional[torch.Tensor],\n@@ -112,10 +113,15 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # DeepEP's topk_ids output refers to the local experts directly. Offset\n         # the topk_ids to move it back to the global experts space so it aligns\n         # with existing vLLM interfaces.\n-        expert_topk_ids = torch.where(\n-            expert_topk_ids == -1,\n-            num_experts - 1 if self.rank_expert_offset == 0 else 0,\n-            expert_topk_ids + self.rank_expert_offset)\n+        invalid_mask = (expert_topk_ids == -1)\n+        # Offset only valid local expert ids to global space.\n+        if self.rank_expert_offset != 0:\n+            expert_topk_ids[~invalid_mask] += self.rank_expert_offset\n+        # Set tokens not in this rank to a valid global expert id so that\n+        # expert_map can later remap them to -1 safely.\n+        expert_topk_ids[invalid_mask] = (\n+            num_experts - 1 if self.rank_expert_offset == 0 else 0\n+        )\n \n         # Makes a GPU-CPU copy.\n         # TODO (varun): Maybe it is better to re-compute the expert_num_tokens\n@@ -142,14 +148,16 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n             # TODO: this only works for topK=1, will need to update for topK>1\n             assert topk == 1, (\n                 \"apply_router_weight_on_input is only implemented for topk=1\")\n-            a1 = a1 * topk_weights.to(a1.dtype)\n+            w = topk_weights if topk_weights.dtype == a1.dtype else topk_weights.to(a1.dtype)\n+            a1 = a1 * w\n \n-        if quant_config.per_act_token_quant:\n+        if quant_config.is_block_quantized:\n+            # Quant and Dispatch\n             a1q, a1q_scale = moe_kernel_quantize_input(\n                 a1,\n                 a1_scale,\n                 quant_dtype=quant_config.quant_dtype,\n-                per_act_token_quant=True,\n+                per_act_token_quant=quant_config.per_act_token_quant,\n                 block_shape=quant_config.block_shape,\n             )\n             if a1q_scale is not None and a1q_scale.numel() == 1:\n@@ -162,8 +170,10 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n                  rank_topk_weights=topk_weights,\n                  num_experts=num_experts)\n         else:\n-            # DeepEP kernels only support dispatching per-token-quant\n-            # quantization. dispatch in bfloat16.\n+            # Dispatch and Quant\n+            # DeepEP kernels only support dispatching block-quantized\n+            # activation scales.\n+            # Dispatch in bfloat16\n             (expert_x, _, expert_tokens_meta, expert_topk_ids,\n              expert_topk_weights) = self._do_dispatch(\n                  tokens=a1,\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex fe87b52f9..a67e90acd 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1288,9 +1288,9 @@ class HiddenStates(msgspec.Struct, array_like=True,\n             # Adding dummy hidden_states to this to maintain same shape\n             self.second_last_token_hidden_states = torch.cat([\n                 self.second_last_token_hidden_states,\n-                torch.zeros_like(hidden_states)\n-                if second_last_token_hidden_states is None else\n-                second_last_token_hidden_states\n+                (torch.empty_like(hidden_states)\n+                 if second_last_token_hidden_states is None else\n+                 second_last_token_hidden_states)\n             ])\n \n     def prune(self,\n", "model_name_or_path": "gpt-5-2025-08-07"}
