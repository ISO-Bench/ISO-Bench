{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-70363bc", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a40b..0f56f708a 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -345,8 +345,8 @@ def estimate_rejection_sampling_pdf(\n                                             num_samples, k)\n \n     # Bonus tokens not used but required.\n-    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n-                                  device=DEVICE).repeat(num_samples, 1)\n+    bonus_token_ids = torch.zeros((num_samples, 1), dtype=torch.int64,\n+                                  device=DEVICE)\n \n     sampling_metadata = create_sampling_metadata(all_greedy=False)\n     output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62e9..6b7910c46 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -248,13 +248,12 @@ class RejectionSampler(nn.Module):\n         if sampling_metadata.all_greedy:\n             return logits\n         assert sampling_metadata.temperature is not None\n-        # We should optimize the following code as\n-        # it will cause CPU -> GPU synchronization.\n-        temperature = torch.repeat_interleave(\n-            sampling_metadata.temperature,\n-            torch.tensor(sample_lens,\n-                         device=sampling_metadata.temperature.device))\n-        temperature = temperature.unsqueeze(dim=1)\n+        temps = sampling_metadata.temperature\n+        # Move temps to logits device if needed\n+        if temps.device != logits.device:\n+            temps = temps.to(logits.device)\n+        counts = torch.as_tensor(sample_lens, device=temps.device)\n+        temperature = torch.repeat_interleave(temps, counts).unsqueeze(1)\n         logits = logits / temperature\n         return logits.softmax(dim=-1, dtype=torch.float32)\n \n@@ -292,9 +291,7 @@ def _convert_2d_probs(\n         [num_total_tokens, vocab_size] -> \n             [batch_size, max_spec_len + 1, vocab_size]\n     \"\"\"\n-    cumulative_lens = torch.cumsum(torch.tensor(sample_lens,\n-                                                device=probs.device),\n-                                   dim=0)\n+    cumulative_lens = torch.as_tensor(sample_lens, device=probs.device).cumsum(0)\n     split_indices = cumulative_lens[:-1].tolist()  # Exclude last index\n \n     # Split into chunks without loops\ndiff --git a/vllm/v1/spec_decode/utils.py b/vllm/v1/spec_decode/utils.py\nindex 584140136..045a6cc84 100644\n--- a/vllm/v1/spec_decode/utils.py\n+++ b/vllm/v1/spec_decode/utils.py\n@@ -1,9 +1,9 @@\n # SPDX-License-Identifier: Apache-2.0\n from vllm.v1.sample.ops.topk_topp_sampler import random_sample  # noqa\n-from vllm.v1.worker.gpu_input_batch import InputBatch\n+from typing import Any\n \n \n-def is_spec_decode_supported(req_id: str, input_batch: InputBatch) -> bool:\n+def is_spec_decode_supported(req_id: str, input_batch: Any) -> bool:\n     if req_id in input_batch.top_k_reqs or req_id in input_batch.top_p_reqs:\n         # Spec decode doesn't support top_p/top_k sampling.\n         return False\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 66015382b..080f9178d 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -191,10 +191,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        # Use empty() to avoid zero-initialization; slices are fully overwritten\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -212,16 +213,17 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            # Avoid zero-init; only the used columns get populated before use\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -234,27 +236,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -471,14 +473,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.input_batch.block_table.commit(num_reqs)\n \n         # Get the number of scheduled tokens for each request.\n-        # TODO: The Python loop can be slow. Optimize.\n-        num_scheduled_tokens = np.empty(num_reqs, dtype=np.int32)\n-        max_num_scheduled_tokens = 0\n-        for i, req_id in enumerate(self.input_batch.req_ids):\n-            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n-            num_scheduled_tokens[i] = num_tokens\n-            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n-                                           num_tokens)\n+        # Use list-comp + np.asarray to reduce Python overhead.\n+        req_ids = self.input_batch.req_ids\n+        _tokens_list = [scheduler_output.num_scheduled_tokens[i]\n+                        for i in req_ids]\n+        num_scheduled_tokens = np.asarray(_tokens_list, dtype=np.int32)\n+        max_num_scheduled_tokens = max(_tokens_list)\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n@@ -737,10 +737,13 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n     ) -> torch.Tensor:\n         # Get the number of spec decode tokens for each request.\n         num_reqs = self.input_batch.num_reqs\n-        num_spec_decode_tokens = np.empty(num_reqs, dtype=np.int32)\n-        for i, req_id in enumerate(self.input_batch.req_ids):\n-            num_spec_decode_tokens[i] = len(\n-                scheduler_output.scheduled_spec_decode_tokens.get(req_id, ()))\n+        req_ids = self.input_batch.req_ids\n+        _spec_tokens_list = [\n+            len(scheduler_output.scheduled_spec_decode_tokens.get(req_id, ()))\n+            for req_id in req_ids\n+        ]\n+        num_spec_decode_tokens = np.asarray(_spec_tokens_list,\n+                                            dtype=np.int32)\n \n         # Get spec decode logits indices.\n         # E.g.,   num_scheduled_tokens: [4, 100, 3,   100, 2]\n", "model_name_or_path": "gpt-5-2025-08-07"}
