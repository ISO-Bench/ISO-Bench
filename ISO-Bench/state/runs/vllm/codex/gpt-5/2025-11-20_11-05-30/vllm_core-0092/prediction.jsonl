{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-88faa46", "model_patch": "diff --git a/benchmarks/kernels/benchmark_reshape_and_cache_flash.py b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\nnew file mode 100644\nindex 000000000..4ab002c38\n--- /dev/null\n+++ b/benchmarks/kernels/benchmark_reshape_and_cache_flash.py\n@@ -0,0 +1,163 @@\n+# SPDX-License-Identifier: Apache-2.0\n+# Benchmark for reshape_and_cache_flash kernel\n+from __future__ import annotations\n+\n+import time\n+from dataclasses import dataclass\n+from typing import Optional\n+\n+import torch\n+\n+from vllm import _custom_ops as ops\n+from vllm.logger import init_logger\n+from vllm.platforms import current_platform\n+from vllm.utils import (\n+    STR_DTYPE_TO_TORCH_DTYPE,\n+    FlexibleArgumentParser,\n+    create_kv_caches_with_random_flash,\n+)\n+\n+logger = init_logger(__name__)\n+\n+\n+@dataclass\n+class BenchmarkConfig:\n+    # problem sizes\n+    num_tokens: int = 4096\n+    num_heads: int = 32\n+    head_size: int = 128\n+    block_size: int = 16\n+    num_layers: int = 1\n+\n+    # dtypes\n+    model_dtype: str = \"bfloat16\"\n+    cache_dtype: str = \"auto\"\n+\n+    # runtime\n+    iters: int = 50\n+    warmup: int = 10\n+    seed: int = 0\n+    device: str = \"cuda\"\n+\n+\n+def _make_slot_mapping(num_tokens: int, block_size: int) -> torch.Tensor:\n+    # map tokens to slots in blocks [0..num_blocks*block_size)\n+    # keep it simple and contiguous for bandwidth stress\n+    num_blocks = (num_tokens + block_size - 1) // block_size\n+    slots = torch.arange(num_tokens, dtype=torch.long, device=\"cuda\")\n+    # in flash layout, slot==token for contiguous case\n+    return slots\n+\n+\n+@torch.inference_mode()\n+def run_benchmark(cfg: BenchmarkConfig) -> dict:\n+    torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cfg.model_dtype]\n+    current_platform.seed_everything(cfg.seed)\n+\n+    # inputs\n+    key = torch.empty((cfg.num_tokens, cfg.num_heads, cfg.head_size),\n+                      dtype=torch_dtype,\n+                      device=cfg.device).uniform_(-0.5, 0.5)\n+    value = torch.empty_like(key).uniform_(-0.5, 0.5)\n+\n+    # kv caches in flash layout\n+    num_blocks = (cfg.num_tokens + cfg.block_size - 1) // cfg.block_size\n+    key_caches, value_caches = create_kv_caches_with_random_flash(\n+        num_blocks=num_blocks,\n+        block_size=cfg.block_size,\n+        num_layers=cfg.num_layers,\n+        num_heads=cfg.num_heads,\n+        head_size=cfg.head_size,\n+        cache_dtype=cfg.cache_dtype,\n+        model_dtype=torch_dtype,\n+        device=cfg.device,\n+        seed=cfg.seed,\n+    )\n+    key_cache = key_caches[0]\n+    value_cache = value_caches[0]\n+\n+    slot_mapping = _make_slot_mapping(cfg.num_tokens, cfg.block_size)\n+\n+    # fake scales (ignored unless fp8)\n+    k_scale = torch.ones((), dtype=torch.float32, device=cfg.device)\n+    v_scale = torch.ones((), dtype=torch.float32, device=cfg.device)\n+\n+    # warmup\n+    for _ in range(cfg.warmup):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                    slot_mapping, cfg.cache_dtype, k_scale,\n+                                    v_scale)\n+    torch.cuda.synchronize()\n+\n+    # measure\n+    t0 = time.perf_counter()\n+    for _ in range(cfg.iters):\n+        ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n+                                    slot_mapping, cfg.cache_dtype, k_scale,\n+                                    v_scale)\n+    torch.cuda.synchronize()\n+    t1 = time.perf_counter()\n+\n+    elapsed_s = (t1 - t0) / cfg.iters\n+    bytes_moved = (\n+        # read key/value\n+        key.numel() * key.element_size() * 2\n+        + key_cache.numel() * key_cache.element_size()\n+        + value_cache.numel() * value_cache.element_size()\n+    )\n+    gbps = bytes_moved / elapsed_s / 1e9\n+\n+    return {\n+        \"time_ms\": elapsed_s * 1e3,\n+        \"gbps\": gbps,\n+        \"tokens\": cfg.num_tokens,\n+        \"heads\": cfg.num_heads,\n+        \"head_size\": cfg.head_size,\n+        \"block_size\": cfg.block_size,\n+        \"dtype\": str(torch_dtype).replace(\"torch.\", \"\"),\n+        \"cache_dtype\": cfg.cache_dtype,\n+    }\n+\n+\n+def main(argv: Optional[list[str]] = None) -> None:\n+    parser = FlexibleArgumentParser(\n+        description=\"Benchmark reshape_and_cache_flash kernel\",\n+    )\n+    parser.add_argument(\"--num-tokens\", type=int, default=4096)\n+    parser.add_argument(\"--num-heads\", type=int, default=32)\n+    parser.add_argument(\"--head-size\", type=int, default=128)\n+    parser.add_argument(\"--block-size\", type=int, default=16)\n+    parser.add_argument(\"--iters\", type=int, default=50)\n+    parser.add_argument(\"--warmup\", type=int, default=10)\n+    parser.add_argument(\"--model-dtype\",\n+                        type=str,\n+                        default=\"bfloat16\",\n+                        choices=[\"float\", \"half\", \"bfloat16\"])\n+    parser.add_argument(\"--cache-dtype\",\n+                        type=str,\n+                        default=\"auto\",\n+                        choices=[\"auto\", \"float\", \"half\", \"bfloat16\", \"fp8\"])  # noqa: E501\n+    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n+    parser.add_argument(\"--seed\", type=int, default=0)\n+    args = parser.parse_args(argv)\n+\n+    cfg = BenchmarkConfig(num_tokens=args.num_tokens,\n+                          num_heads=args.num_heads,\n+                          head_size=args.head_size,\n+                          block_size=args.block_size,\n+                          iters=args.iters,\n+                          warmup=args.warmup,\n+                          model_dtype=args.model_dtype,\n+                          cache_dtype=args.cache_dtype,\n+                          device=args.device,\n+                          seed=args.seed)\n+\n+    logger.info(\"Running reshape_and_cache_flash benchmark: %s\", cfg)\n+    stats = run_benchmark(cfg)\n+    logger.info(\"Result: time_per_iter_ms=%.3f, GB/s=%.2f\", stats[\"time_ms\"],\n+                stats[\"gbps\"])\n+\n+\n+if __name__ == \"__main__\":\n+    main()\n+\ndiff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex d06eac2b3..5c221a6fd 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -230,23 +230,38 @@ __global__ void reshape_and_cache_kernel(\n   const int64_t block_offset = slot_idx % block_size;\n \n   const int n = num_heads * head_size;\n+  const int64_t src_key_base = static_cast<int64_t>(token_idx) * key_stride;\n+  const int64_t src_value_base = static_cast<int64_t>(token_idx) * value_stride;\n+  const int head_size_div_x = head_size / x;\n+  const int per_head_stride = head_size_div_x * block_size * x;\n+  const int64_t key_block_base = static_cast<int64_t>(block_idx) *\n+                                 static_cast<int64_t>(num_heads) *\n+                                 static_cast<int64_t>(per_head_stride);\n+  const int64_t key_block_offset_base = static_cast<int64_t>(block_offset) * x;\n+  const int64_t value_block_base = static_cast<int64_t>(block_idx) *\n+                                   static_cast<int64_t>(num_heads) *\n+                                   static_cast<int64_t>(head_size) *\n+                                   static_cast<int64_t>(block_size) +\n+                                   static_cast<int64_t>(block_offset);\n+  const float k_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 1.0f : *k_scale;\n+  const float v_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 1.0f : *v_scale;\n+  \n+  #pragma unroll 4\n   for (int i = threadIdx.x; i < n; i += blockDim.x) {\n-    const int64_t src_key_idx = token_idx * key_stride + i;\n-    const int64_t src_value_idx = token_idx * value_stride + i;\n+    const int64_t src_key_idx = src_key_base + i;\n+    const int64_t src_value_idx = src_value_base + i;\n \n     const int head_idx = i / head_size;\n     const int head_offset = i % head_size;\n     const int x_idx = head_offset / x;\n     const int x_offset = head_offset % x;\n \n-    const int64_t tgt_key_idx =\n-        block_idx * num_heads * (head_size / x) * block_size * x +\n-        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +\n-        block_offset * x + x_offset;\n-    const int64_t tgt_value_idx =\n-        block_idx * num_heads * head_size * block_size +\n-        head_idx * head_size * block_size + head_offset * block_size +\n-        block_offset;\n+    const int64_t tgt_key_idx = key_block_base +\n+                                static_cast<int64_t>(head_idx) * per_head_stride +\n+                                static_cast<int64_t>(x_idx) * (block_size * x) +\n+                                key_block_offset_base + x_offset;\n+    const int64_t tgt_value_idx = value_block_base +\n+                                  static_cast<int64_t>(i) * block_size;\n     scalar_t tgt_key = key[src_key_idx];\n     scalar_t tgt_value = value[src_value_idx];\n     if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {\n@@ -254,9 +269,9 @@ __global__ void reshape_and_cache_kernel(\n       value_cache[tgt_value_idx] = tgt_value;\n     } else {\n       key_cache[tgt_key_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, *k_scale);\n+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, k_scale_val);\n       value_cache[tgt_value_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, *v_scale);\n+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, v_scale_val);\n     }\n   }\n }\n@@ -282,24 +297,28 @@ __global__ void reshape_and_cache_flash_kernel(\n   const int64_t block_idx = slot_idx / block_size;\n   const int64_t block_offset = slot_idx % block_size;\n   const int n = num_heads * head_size;\n+  const int64_t src_key_base = static_cast<int64_t>(token_idx) * key_stride;\n+  const int64_t src_value_base = static_cast<int64_t>(token_idx) * value_stride;\n+  const int64_t dst_base = static_cast<int64_t>(block_idx) * block_stride +\n+                           static_cast<int64_t>(block_offset) * n;\n+  const float k_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 1.0f : *k_scale;\n+  const float v_scale_val = (kv_dt == Fp8KVCacheDataType::kAuto) ? 1.0f : *v_scale;\n+\n+  #pragma unroll 4\n   for (int i = threadIdx.x; i < n; i += blockDim.x) {\n-    const int64_t src_key_idx = token_idx * key_stride + i;\n-    const int64_t src_value_idx = token_idx * value_stride + i;\n-    const int head_idx = i / head_size;\n-    const int head_offset = i % head_size;\n-    const int64_t tgt_key_value_idx = block_idx * block_stride +\n-                                      block_offset * num_heads * head_size +\n-                                      head_idx * head_size + head_offset;\n+    const int64_t src_key_idx = src_key_base + i;\n+    const int64_t src_value_idx = src_value_base + i;\n+    const int64_t dst_idx = dst_base + i;\n     scalar_t tgt_key = key[src_key_idx];\n     scalar_t tgt_value = value[src_value_idx];\n     if constexpr (kv_dt == Fp8KVCacheDataType::kAuto) {\n-      key_cache[tgt_key_value_idx] = tgt_key;\n-      value_cache[tgt_key_value_idx] = tgt_value;\n+      key_cache[dst_idx] = tgt_key;\n+      value_cache[dst_idx] = tgt_value;\n     } else {\n-      key_cache[tgt_key_value_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, *k_scale);\n-      value_cache[tgt_key_value_idx] =\n-          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, *v_scale);\n+      key_cache[dst_idx] =\n+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_key, k_scale_val);\n+      value_cache[dst_idx] =\n+          fp8::scaled_convert<cache_t, scalar_t, kv_dt>(tgt_value, v_scale_val);\n     }\n   }\n }\n@@ -438,7 +457,7 @@ void reshape_and_cache_flash(\n   TORCH_CHECK(key_cache.stride(0) == value_cache.stride(0));\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  dim3 block(std::min(num_heads * head_size, 1024));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(key));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
