{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-36fb68f", "model_patch": "diff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py\nindex 046f11d95..b43e81d6e 100644\n--- a/tests/kernels/test_moe.py\n+++ b/tests/kernels/test_moe.py\n@@ -15,18 +15,19 @@ from vllm.model_executor.models.mixtral import MixtralMoE\n def torch_moe(a, w1, w2, score, topk):\n     B, D = a.shape\n     a = a.view(B, -1, D).repeat(1, topk, 1).reshape(-1, D)\n-    out = torch.zeros(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n+    # No need to zero-initialize since every row is written exactly once\n+    out = torch.empty(B * topk, w2.shape[1], dtype=a.dtype, device=a.device)\n     score = torch.softmax(score, dim=-1, dtype=torch.float32)\n     topk_weight, topk_ids = torch.topk(score, topk)\n-    topk_weight = topk_weight.view(-1)\n-    topk_ids = topk_ids.view(-1)\n+    # Flatten once and cast weights early to avoid repeated dtype conversions\n+    topk_weight = topk_weight.reshape(-1).to(out.dtype)\n+    topk_ids = topk_ids.reshape(-1)\n     for i in range(w1.shape[0]):\n         mask = topk_ids == i\n         if mask.sum():\n             out[mask] = SiluAndMul()(\n                 a[mask] @ w1[i].transpose(0, 1)) @ w2[i].transpose(0, 1)\n-    return (out.view(B, -1, w2.shape[1]) *\n-            topk_weight.view(B, -1, 1).to(out.dtype)).sum(dim=1)\n+    return (out.view(B, -1, w2.shape[1]) * topk_weight.view(B, -1, 1)).sum(dim=1)\n \n \n @pytest.mark.parametrize(\"m\", [512, 222, 33, 1])\n@@ -75,13 +76,14 @@ def test_mixtral_moe(dtype: torch.dtype):\n     # Load the weights\n     vllm_moe.gate.weight.data[:] = hf_moe.gate.weight.data\n     for i in range(config.num_local_experts):\n-        weights = (hf_moe.experts[i].w1.weight.data,\n-                   hf_moe.experts[i].w3.weight.data)\n-        vllm_moe.ws[i][:] = torch.cat(weights, dim=0)\n-        vllm_moe.w2s[i][:] = hf_moe.experts[i].w2.weight.data\n+        # Avoid intermediate concat by assigning slices directly\n+        shard = vllm_moe.intermediate_size\n+        vllm_moe.ws[i][:shard, :] = hf_moe.experts[i].w1.weight.data\n+        vllm_moe.ws[i][shard:2 * shard, :] = hf_moe.experts[i].w3.weight.data\n+        vllm_moe.w2s[i][:, :] = hf_moe.experts[i].w2.weight.data\n \n     # Generate input batch of dimensions [batch_size, seq_len, hidden_dim]\n-    hf_inputs = torch.randn((1, 64, config.hidden_size)).to(dtype).to(\"cuda\")\n+    hf_inputs = torch.randn((1, 64, config.hidden_size), device=\"cuda\", dtype=dtype)\n     # vLLM uses 1D query [num_tokens, hidden_dim]\n     vllm_inputs = hf_inputs.flatten(0, 1)\n \ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 9ff9ba298..252ebf1cb 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -78,6 +78,7 @@ class MixtralMoE(nn.Module):\n         self.top_k = top_k\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size // self.tp_size\n+        self.quant_config = quant_config\n         # FIXME(pcmoritz): Make this more general to support different\n         # quantization schemes\n         self.use_fp8 = isinstance(quant_config, Fp8Config)\n", "model_name_or_path": "gpt-5-2025-08-07"}
