{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-eb5741a", "model_patch": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 2d8594cb8..a58bf8170 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -240,6 +240,9 @@ class XGrammarLogitsProcessor:\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n+    # Cache for device-resident token bitmasks to avoid per-step allocation\n+    _device_bitmask_cache: dict[str, torch.Tensor] = field(\n+        default_factory=dict)\n \n     def __getstate__(self) -> dict[str, Any]:\n         return {'config': self.config}\n@@ -252,6 +255,7 @@ class XGrammarLogitsProcessor:\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n         self.prefilled = False\n+        self._device_bitmask_cache = {}\n \n     def _ensure_ctx(self):\n         \"\"\"Lazily initialize the processor in the worker process\"\"\"\n@@ -278,6 +282,14 @@ class XGrammarLogitsProcessor:\n             ]\n             self.token_bitmask = xgr.allocate_token_bitmask(\n                 self.batch_size, self.config.vocab_size)\n+            # Pin CPU memory to enable faster, non-blocking H2D copies\n+            # when running on CUDA. Safe no-op when CUDA is unavailable.\n+            if torch.cuda.is_available():\n+                try:\n+                    self.token_bitmask = self.token_bitmask.pin_memory()\n+                except RuntimeError:\n+                    # Some backends may not support pinning; continue safely.\n+                    pass\n \n         if not self.prefilled:\n             # Have not sampled a token yet\n@@ -299,18 +311,33 @@ class XGrammarLogitsProcessor:\n         # fill_next_token_bitmask so we move it to the device of scores\n         device_type = scores.device.type\n         dtype = scores.dtype\n-        if device_type != \"cuda\":\n-            # xgrammar on cpu only supports float32 scores\n+        if device_type == \"cuda\":\n+            # Avoid per-call allocation by reusing a device-side buffer.\n+            dev = scores.device\n+            dev_key = str(dev)\n+            t_dev = self._device_bitmask_cache.get(dev_key)\n+            if (t_dev is None or t_dev.shape != self.token_bitmask.shape\n+                    or t_dev.dtype != self.token_bitmask.dtype\n+                    or t_dev.device != dev):\n+                t_dev = torch.empty_like(self.token_bitmask, device=dev)\n+                self._device_bitmask_cache[dev_key] = t_dev\n+\n+            # Async H2D copy (requires pinned host memory for full benefit)\n+            t_dev.copy_(self.token_bitmask, non_blocking=True)\n+\n+            # On GPU, shape mismatches are tolerated by the kernel; no unsqueeze\n+            xgr.apply_token_bitmask_inplace(scores, t_dev)\n+        else:\n+            # xgrammar on CPU only supports float32 scores\n             # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            scores = scores.to(\"cpu\").float().unsqueeze(0)\n-\n-        # Note: In this method, if the tensors have different dimensions\n-        # on CPU device fails, but on GPU it runs without error. Hence the\n-        # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(scores,\n-                                        self.token_bitmask.to(scores.device))\n-        if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n+            scores = scores.to(\"cpu\", dtype=torch.float32).unsqueeze(0)\n+\n+            # Note: On CPU, tensor dims must match; hence unsqueeze above.\n+            # Pass the host bitmask directly.\n+            xgr.apply_token_bitmask_inplace(scores, self.token_bitmask)\n+\n+            # Single conversion back to original device and dtype.\n+            scores = scores.to(device_type, dtype=dtype).squeeze()\n \n         return scores\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
