{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-8c1e77f", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed86..fc585ee9e 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -244,6 +244,7 @@ class LLM:\n             engine_args, usage_context=UsageContext.LLM_CLASS)\n \n         self.request_counter = Counter()\n+        self.default_sampling_params: Union[dict[str, Any], None] = None\n \n     @staticmethod\n     def get_engine_class() -> type[LLMEngine]:\n@@ -268,10 +269,11 @@ class LLM:\n             tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n \n     def get_default_sampling_params(self) -> SamplingParams:\n-        diff_sampling_param = (\n-            self.llm_engine.model_config.get_diff_sampling_param())\n-        if diff_sampling_param:\n-            return SamplingParams.from_optional(**diff_sampling_param)\n+        if self.default_sampling_params is None:\n+            self.default_sampling_params = (\n+                self.llm_engine.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            return SamplingParams.from_optional(**self.default_sampling_params)\n         return SamplingParams()\n \n     @overload\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0fc..53e7444e5 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -105,10 +105,10 @@ class OpenAIServingChat(OpenAIServing):\n                                 \"been registered\") from e\n \n         self.enable_prompt_tokens_details = enable_prompt_tokens_details\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\"Overwriting default chat sampling param with: %s\",\n-                        diff_sampling_param)\n+                        self.default_sampling_params)\n \n     async def create_chat_completion(\n         self,\n@@ -211,8 +211,7 @@ class OpenAIServingChat(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n+                default_sampling_params = self.default_sampling_params\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af84f..e3dec37ee 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -51,11 +51,11 @@ class OpenAIServingCompletion(OpenAIServing):\n                          models=models,\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def create_completion(\n         self,\n@@ -120,8 +120,7 @@ class OpenAIServingCompletion(OpenAIServing):\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n                 # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n+                default_sampling_params = self.default_sampling_params\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a5e..a030ed42e 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -161,11 +161,11 @@ class OpenAIServingTranscription(OpenAIServing):\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n \n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = self.model_config.get_diff_sampling_param()\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def _preprocess_transcription(\n         self,\n@@ -273,7 +273,7 @@ class OpenAIServingTranscription(OpenAIServing):\n         try:\n             # TODO(rob): subtract len of tokenized prompt.\n             default_max_tokens = self.model_config.max_model_len\n-            default_params = self.model_config.get_diff_sampling_param()\n+            default_params = self.default_sampling_params\n             sampling_params = request.to_sampling_params(\n                 default_max_tokens, default_params)\n \ndiff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 8bf7f3587..bae93c446 100755\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -224,8 +224,8 @@ class FlashAttentionImpl(AttentionImpl):\n         \"\"\"\n         assert output is not None, \"Output tensor must be provided.\"\n \n+        # Fast path for profiling or dry-run: avoid any additional ops.\n         if attn_metadata is None:\n-            # Profiling run.\n             return output\n \n         # IMPORTANT!\n@@ -237,7 +237,13 @@ class FlashAttentionImpl(AttentionImpl):\n         # Whenever making a change in this method, please benchmark the\n         # performance to make sure it does not introduce any overhead.\n \n+        # Cache frequently used fields locally to reduce Python attribute\n+        # lookups in the non-CUDA-graph region.\n         num_actual_tokens = attn_metadata.num_actual_tokens\n+        if num_actual_tokens == 0:\n+            # No actual tokens to process. Avoid touching the cache or running\n+            # any kernels to minimize CPU overhead in this edge case.\n+            return output\n         # Reshape the input keys and values and store them in the cache.\n         # NOTE(woosuk): Here, key and value are padded while slot_mapping is\n         # not padded. However, we don't need to do key[:num_actual_tokens] and\n@@ -258,20 +264,29 @@ class FlashAttentionImpl(AttentionImpl):\n         # Compute attention and update output up to `num_actual_tokens`.\n         if not attn_metadata.use_cascade:\n             # Regular attention (common case).\n+            # Minimize view/slice ops by computing views once and reusing.\n+            q_n = query[:num_actual_tokens]\n+            o_n = output[:num_actual_tokens]\n+            cu_seqlens_q = attn_metadata.query_start_loc\n+            max_seqlen_q = attn_metadata.max_query_len\n+            seqused_k = attn_metadata.seq_lens\n+            max_seqlen_k = attn_metadata.max_seq_len\n+            block_table = attn_metadata.block_table\n+\n             flash_attn_varlen_func(\n-                q=query[:num_actual_tokens],\n+                q=q_n,\n                 k=key_cache,\n                 v=value_cache,\n-                out=output[:num_actual_tokens],\n-                cu_seqlens_q=attn_metadata.query_start_loc,\n-                max_seqlen_q=attn_metadata.max_query_len,\n-                seqused_k=attn_metadata.seq_lens,\n-                max_seqlen_k=attn_metadata.max_seq_len,\n+                out=o_n,\n+                cu_seqlens_q=cu_seqlens_q,\n+                max_seqlen_q=max_seqlen_q,\n+                seqused_k=seqused_k,\n+                max_seqlen_k=max_seqlen_k,\n                 softmax_scale=self.scale,\n                 causal=True,\n                 alibi_slopes=self.alibi_slopes,\n                 window_size=self.sliding_window,\n-                block_table=attn_metadata.block_table,\n+                block_table=block_table,\n                 softcap=self.logits_soft_cap,\n                 fa_version=self.vllm_flash_attn_version,\n             )\n@@ -393,6 +408,8 @@ def cascade_attention(\n         \"Cascade attention does not support sliding window.\")\n \n     num_tokens = query.shape[0]\n+    if num_tokens == 0:\n+        return output\n     block_size = key_cache.shape[-3]\n     assert common_prefix_len % block_size == 0\n     num_common_kv_blocks = common_prefix_len // block_size\n", "model_name_or_path": "gpt-5-2025-08-07"}
