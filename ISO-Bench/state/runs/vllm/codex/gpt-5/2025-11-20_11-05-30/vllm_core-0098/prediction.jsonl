{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-67abdbb", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca66a..a6733c38e 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -169,15 +169,15 @@ class UncachedBlockAllocator(BlockAllocatorBase):\n         self.block_size = block_size\n         self.num_blocks = num_blocks\n \n-        # Initialize the free blocks.\n-        self.free_blocks: List[PhysicalTokenBlock] = []\n-        for i in range(num_blocks):\n-            block = PhysicalTokenBlock(device=device,\n-                                       block_number=i,\n-                                       block_size=block_size,\n-                                       block_hash=-1,\n-                                       num_hashed_tokens=0)\n-            self.free_blocks.append(block)\n+        # Initialize the free blocks using a list comprehension to\n+        # avoid repeated append overhead in a loop.\n+        self.free_blocks: List[PhysicalTokenBlock] = [\n+            PhysicalTokenBlock(device=device,\n+                               block_number=i,\n+                               block_size=block_size,\n+                               block_hash=-1,\n+                               num_hashed_tokens=0) for i in range(num_blocks)\n+        ]\n \n     def allocate(self,\n                  block_hash: Optional[int] = None,\n@@ -336,9 +336,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n         # Assign the self-attention block tables for each sequence.\n         if len(wait_seqs) == 1:\n-            self.block_tables[wait_seqs[0].seq_id] = block_table\n+            # Fast path: avoid extra indexing/copy for single sequence.\n+            self.block_tables[seq.seq_id] = block_table\n         else:\n-            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n+            # Reuse the precomputed wait_seqs list to avoid repeated calls.\n+            for seq in wait_seqs:\n                 self.block_tables[seq.seq_id] = block_table.copy()\n \n         # Allocate encoder sequence\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477efc5..fd2dc9656 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -655,6 +655,9 @@ class SequenceGroup:\n         return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n+        if self.is_single_seq:\n+            return self.seqs if self.seqs[0].is_finished() else []\n+\n         return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n", "model_name_or_path": "gpt-5-2025-08-07"}
