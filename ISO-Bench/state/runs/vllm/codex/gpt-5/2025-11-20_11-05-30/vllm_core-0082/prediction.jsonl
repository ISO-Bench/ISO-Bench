{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-cc466a3", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 7344d59e9..365ccc083 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -205,7 +205,9 @@ def _get_bin_counts_and_mask(\n     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                              dtype=torch.long,\n                              device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n+    # Avoid allocating a full-sized ones_like tensor by using an expanded view\n+    ones = torch.ones(1, dtype=torch.long, device=tokens.device)\n+    bin_counts.scatter_add_(1, tokens, ones.expand_as(tokens))\n     bin_counts = bin_counts[:, :vocab_size]\n     mask = bin_counts > 0\n \n@@ -382,8 +384,9 @@ def _random_sample(\n         same as the length of selected_seq_groups. If the corresponding\n         seq_group has do_sample=False, tuple contains ([], [])\n     \"\"\"\n-    # Find the maximum best_of value of the prompt phase requests.\n-    random_samples = random_samples.cpu()\n+    # Move to CPU only if needed for list conversions below.\n+    if random_samples.device.type != 'cpu':\n+        random_samples = random_samples.to('cpu')\n     sample_idx = 0\n     results: SampleResultType = []\n     for seq_group in selected_seq_groups:\n@@ -910,9 +913,9 @@ def _get_logprobs(\n \n         # We need to compute top k only if there exists logprobs > 0.\n         if largest_num_logprobs > 0:\n-            # Logprobs of topk tokens for a batch of sequence groups.\n-            # (num_query_tokens_across_batch).\n-            top_logprobs, top_token_ids = torch.topk(logprobs,\n+            # Compute topk only for the queried rows to reduce work.\n+            queried_logprobs = logprobs[query_indices_gpu]\n+            top_logprobs, top_token_ids = torch.topk(queried_logprobs,\n                                                      largest_num_logprobs,\n                                                      dim=-1)\n             top_logprobs = top_logprobs.to('cpu')\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4ea1..b47adee61 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -501,9 +501,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                             and self.sliding_window is None\n                             and inter_data.is_prompt)\n         inter_data.prefix_cache_hit = prefix_cache_hit\n-        if self.chunked_prefill_enabled and prefix_cache_hit:\n-            raise RuntimeError(\n-                \"chunked prefill cannot be used with prefix caching now.\")\n+        # Allow chunked prefill together with prefix cache hit.\n \n         # If prefix cache is hit, advance context length to bypass\n         # hit blocks. Accordingly, input tokens, position and query length\n@@ -1197,8 +1195,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n+        input_tokens = torch.empty(max_batch_size, dtype=torch.long, device=self.device)\n+        input_positions = torch.empty(max_batch_size, dtype=torch.long, device=self.device)\n \n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\n", "model_name_or_path": "gpt-5-2025-08-07"}
