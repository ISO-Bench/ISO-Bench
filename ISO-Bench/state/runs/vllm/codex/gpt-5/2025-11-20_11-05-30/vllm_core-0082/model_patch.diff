diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py
index 7344d59e9..365ccc083 100644
--- a/vllm/model_executor/layers/sampler.py
+++ b/vllm/model_executor/layers/sampler.py
@@ -205,7 +205,9 @@ def _get_bin_counts_and_mask(
     bin_counts = torch.zeros((num_seqs, vocab_size + 1),
                              dtype=torch.long,
                              device=tokens.device)
-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))
+    # Avoid allocating a full-sized ones_like tensor by using an expanded view
+    ones = torch.ones(1, dtype=torch.long, device=tokens.device)
+    bin_counts.scatter_add_(1, tokens, ones.expand_as(tokens))
     bin_counts = bin_counts[:, :vocab_size]
     mask = bin_counts > 0
 
@@ -382,8 +384,9 @@ def _random_sample(
         same as the length of selected_seq_groups. If the corresponding
         seq_group has do_sample=False, tuple contains ([], [])
     """
-    # Find the maximum best_of value of the prompt phase requests.
-    random_samples = random_samples.cpu()
+    # Move to CPU only if needed for list conversions below.
+    if random_samples.device.type != 'cpu':
+        random_samples = random_samples.to('cpu')
     sample_idx = 0
     results: SampleResultType = []
     for seq_group in selected_seq_groups:
@@ -910,9 +913,9 @@ def _get_logprobs(
 
         # We need to compute top k only if there exists logprobs > 0.
         if largest_num_logprobs > 0:
-            # Logprobs of topk tokens for a batch of sequence groups.
-            # (num_query_tokens_across_batch).
-            top_logprobs, top_token_ids = torch.topk(logprobs,
+            # Compute topk only for the queried rows to reduce work.
+            queried_logprobs = logprobs[query_indices_gpu]
+            top_logprobs, top_token_ids = torch.topk(queried_logprobs,
                                                      largest_num_logprobs,
                                                      dim=-1)
             top_logprobs = top_logprobs.to('cpu')
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index f556e4ea1..b47adee61 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -501,9 +501,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):
                             and self.sliding_window is None
                             and inter_data.is_prompt)
         inter_data.prefix_cache_hit = prefix_cache_hit
-        if self.chunked_prefill_enabled and prefix_cache_hit:
-            raise RuntimeError(
-                "chunked prefill cannot be used with prefix caching now.")
+        # Allow chunked prefill together with prefix cache hit.
 
         # If prefix cache is hit, advance context length to bypass
         # hit blocks. Accordingly, input tokens, position and query length
@@ -1197,8 +1195,8 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):
 
         # Prepare dummy inputs. These will be reused for all batch sizes.
         max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)
-        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()
-        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()
+        input_tokens = torch.empty(max_batch_size, dtype=torch.long, device=self.device)
+        input_positions = torch.empty(max_batch_size, dtype=torch.long, device=self.device)
 
         # Prepare dummy previous_hidden_states only if needed by the model.
         # This is used by draft models such as EAGLE.
