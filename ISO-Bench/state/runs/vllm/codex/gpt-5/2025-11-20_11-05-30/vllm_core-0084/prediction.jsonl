{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-dd572c0", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex d025f1257..0017dabad 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -108,12 +108,16 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n+    # token counters must be zero-initialized as kernels increment values\n     tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    # cumsum is fully written during stage3 except index 0; avoid full memset\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n+    # Ensure cumsum[0] is a valid starting point for expert 0\n+    cumsum[0] = 0\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -200,14 +204,13 @@ def moe_align_block_size(\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     if pad_sorted_ids:\n         max_num_tokens_padded = round_up(max_num_tokens_padded, block_size)\n+    # Pre-allocate buffers without unnecessary initialization; we will fill\n+    # only the regions that are actually used once we know the post-pad size.\n     sorted_ids = torch.empty((max_num_tokens_padded, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n-    # Expert ids must be zeroed out to prevent index out of bounds error while\n-    # mapping global expert ids to local expert ids in expert parallelism.\n-    expert_ids = torch.zeros((max_num_m_blocks, ),\n+    expert_ids = torch.empty((max_num_m_blocks, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n     num_tokens_post_pad = torch.empty((1),\n@@ -215,13 +218,62 @@ def moe_align_block_size(\n                                       device=topk_ids.device)\n     if num_experts >= 224:\n         if envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON or num_experts != 256:\n-            moe_align_block_size_triton(\n+            # For the Triton path, compute cumsum first to know the exact\n+            # number of tokens after padding, then initialize only the used\n+            # regions of the output buffers to minimize memory traffic.\n+            numel = topk_ids.numel()\n+            grid = (num_experts, )\n+            tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n+                                      dtype=torch.int32,\n+                                      device=topk_ids.device)\n+            cumsum = torch.empty((num_experts + 1, ),\n+                                 dtype=torch.int32,\n+                                 device=topk_ids.device)\n+            cumsum[0] = 0\n+            tokens_per_thread = ceil_div(numel, num_experts)\n+\n+            moe_align_block_size_stage1[grid](\n                 topk_ids,\n+                tokens_cnts,\n+                num_experts,\n+                numel,\n+                tokens_per_thread,\n+            )\n+            moe_align_block_size_stage2[grid](\n+                tokens_cnts,\n+                num_experts,\n+            )\n+            moe_align_block_size_stage3[(1, )](\n+                num_tokens_post_pad,\n+                tokens_cnts,\n+                cumsum,\n                 num_experts,\n                 block_size,\n+            )\n+\n+            # Initialize only the used portion of sorted_ids to the padding\n+            # sentinel; stage4 will overwrite entries corresponding to real\n+            # tokens.\n+            used_tokens = int(num_tokens_post_pad.item())\n+            if used_tokens > 0:\n+                sorted_ids[:used_tokens].fill_(topk_ids.numel())\n+\n+            # Zero out the trailing expert_ids beyond the used M blocks so\n+            # that downstream expert_map indexing is always safe.\n+            used_m_blocks = (used_tokens + block_size - 1) // block_size\n+            if used_m_blocks < max_num_m_blocks:\n+                expert_ids[used_m_blocks:].zero_()\n+\n+            moe_align_block_size_stage4[grid](\n+                topk_ids,\n                 sorted_ids,\n                 expert_ids,\n-                num_tokens_post_pad,\n+                tokens_cnts,\n+                cumsum,\n+                num_experts,\n+                block_size,\n+                numel,\n+                tokens_per_thread,\n             )\n         else:\n             # Currently requires num_experts=256\n@@ -233,9 +285,21 @@ def moe_align_block_size(\n                 expert_ids,\n                 num_tokens_post_pad,\n             )\n+            used_tokens = int(num_tokens_post_pad.item())\n+            used_m_blocks = (used_tokens + block_size - 1) // block_size\n+            if used_m_blocks < max_num_m_blocks:\n+                expert_ids[used_m_blocks:].zero_()\n     else:\n+        # For the CUDA path, we do not know the used lengths a priori,\n+        # so keep the original behavior for sorted_ids, but avoid zeoring the\n+        # full expert_ids buffer by zeroing only the unused tail afterwards.\n+        sorted_ids[:].fill_(topk_ids.numel())\n         ops.moe_align_block_size(topk_ids, num_experts, block_size, sorted_ids,\n                                  expert_ids, num_tokens_post_pad)\n+        used_tokens = int(num_tokens_post_pad.item())\n+        used_m_blocks = (used_tokens + block_size - 1) // block_size\n+        if used_m_blocks < max_num_m_blocks:\n+            expert_ids[used_m_blocks:].zero_()\n     if expert_map is not None:\n         expert_ids = expert_map[expert_ids]\n \ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 5d8b3f423..eaaa48cdd 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -89,18 +89,18 @@ class TopKTopPSampler(nn.Module):\n         p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n         \"\"\"More optimized implementation for top-k and top-p sampling.\"\"\"\n-        probs = logits.softmax(dim=-1, dtype=torch.float32)\n         if k is None and p is None:\n             # We prefer `random_sample` over `flashinfer_sample` when sorting is\n             # not needed. This is because `random_sample` does not require\n             # CPU-GPU synchronization while `flashinfer_sample` does.\n+            probs = logits.softmax(dim=-1, dtype=torch.float32)\n             return random_sample(probs, generators)\n         if generators:\n             logger.warning(\"FlashInfer 0.2.3+ does not support \"\n                            \"per-request generators. Falling back to \"\n                            \"PyTorch-native implementation.\")\n             return self.forward_native(logits, generators, k, p)\n-        return flashinfer_sample(probs, k, p, generators)\n+        return flashinfer_sample(logits, k, p, generators)\n \n     def forward_tpu(\n         self,\n@@ -200,7 +200,7 @@ def apply_top_k_top_p(\n         logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    logits.scatter_(dim=-1, index=logits_idx, src=logits_sort)\n     return logits\n \n \n@@ -254,12 +254,12 @@ def random_sample(\n \n \n def flashinfer_sample(\n-    probs: torch.Tensor,\n+    logits: torch.Tensor,\n     k: Optional[torch.Tensor],\n     p: Optional[torch.Tensor],\n     generators: dict[int, torch.Generator],\n ) -> torch.Tensor:\n-    \"\"\"Sample from the probabilities using FlashInfer.\n+    \"\"\"Sample from the logits using FlashInfer.\n \n     Statistically, this function is equivalent to the `random_sample` function.\n     However, this function is faster because it avoids sorting the logits tensor\n@@ -275,17 +275,45 @@ def flashinfer_sample(\n     \"\"\"\n     assert not (k is None and p is None)\n \n+    # Prefer logits-based FlashInfer APIs when available to avoid redundant\n+    # softmax on the host side. Fallback to *_from_probs if necessary.\n+    fi = flashinfer.sampling\n+\n+    def _to_probs(x: torch.Tensor) -> torch.Tensor:\n+        return x.softmax(dim=-1, dtype=torch.float32)\n+\n     if k is None:\n         # Top-p only.\n-        next_token_ids = flashinfer.sampling.top_p_sampling_from_probs(\n-            probs, p, deterministic=True)\n+        if hasattr(fi, 'top_p_sampling_from_logits'):\n+            next_token_ids = fi.top_p_sampling_from_logits(\n+                logits, p, deterministic=True)\n+        elif hasattr(fi, 'top_p_sampling'):\n+            next_token_ids = fi.top_p_sampling(\n+                logits, p, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_p_sampling_from_probs(\n+                _to_probs(logits), p, deterministic=True)\n     elif p is None:\n         # Top-k only.\n-        next_token_ids = flashinfer.sampling.top_k_sampling_from_probs(\n-            probs, k, deterministic=True)\n+        if hasattr(fi, 'top_k_sampling_from_logits'):\n+            next_token_ids = fi.top_k_sampling_from_logits(\n+                logits, k, deterministic=True)\n+        elif hasattr(fi, 'top_k_sampling'):\n+            next_token_ids = fi.top_k_sampling(\n+                logits, k, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_k_sampling_from_probs(\n+                _to_probs(logits), k, deterministic=True)\n     else:\n         # Both top-k and top-p.\n-        next_token_ids = (flashinfer.sampling.top_k_top_p_sampling_from_probs(\n-            probs, k, p, deterministic=True))\n+        if hasattr(fi, 'top_k_top_p_sampling_from_logits'):\n+            next_token_ids = fi.top_k_top_p_sampling_from_logits(\n+                logits, k, p, deterministic=True)\n+        elif hasattr(fi, 'top_k_top_p_sampling'):\n+            next_token_ids = fi.top_k_top_p_sampling(\n+                logits, k, p, deterministic=True)\n+        else:\n+            next_token_ids = fi.top_k_top_p_sampling_from_probs(\n+                _to_probs(logits), k, p, deterministic=True)\n \n     return next_token_ids.view(-1)\n", "model_name_or_path": "gpt-5-2025-08-07"}
