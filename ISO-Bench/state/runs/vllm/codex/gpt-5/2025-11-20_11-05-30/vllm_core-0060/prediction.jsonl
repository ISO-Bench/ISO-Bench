{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-bc8a8ce", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..288b72381 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -1,5 +1,6 @@\n import asyncio\n import atexit\n+import gc\n import importlib\n import inspect\n import multiprocessing\n@@ -104,6 +105,10 @@ async def lifespan(app: FastAPI):\n             task.add_done_callback(_running_tasks.remove)\n         else:\n             task = None\n+\n+        # Mark the startup heap as static so that it's ignored by GC to reduce pause times.\n+        gc.collect()\n+        gc.freeze()\n         try:\n             yield\n         finally:\ndiff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..693314e03 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -19,6 +19,13 @@ from vllm.utils import random_uuid, resolve_obj_by_qualname\n \n logger = init_logger(__name__)\n \n+\n+# Fast path for integer epoch time used in default_factory\n+_time = time.time\n+\n+def _now_int() -> int:\n+    return int(_time())\n+\n # torch is mocked during docs generation,\n # so we have to provide the values as literals\n _MOCK_LONG_INFO = Namespace(min=-9223372036854775808, max=9223372036854775807)\n@@ -73,7 +80,7 @@ class ErrorResponse(OpenAIBaseModel):\n class ModelPermission(OpenAIBaseModel):\n     id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\n     object: str = \"model_permission\"\n-    created: int = Field(default_factory=lambda: int(time.time()))\n+    created: int = Field(default_factory=_now_int)\n     allow_create_engine: bool = False\n     allow_sampling: bool = True\n     allow_logprobs: bool = True\n@@ -88,7 +95,7 @@ class ModelPermission(OpenAIBaseModel):\n class ModelCard(OpenAIBaseModel):\n     id: str\n     object: str = \"model\"\n-    created: int = Field(default_factory=lambda: int(time.time()))\n+    created: int = Field(default_factory=_now_int)\n     owned_by: str = \"vllm\"\n     root: Optional[str] = None\n     parent: Optional[str] = None\n@@ -170,10 +177,12 @@ def get_logits_processors(processors: Optional[LogitsProcessors],\n                           pattern: Optional[str]) -> Optional[List[Any]]:\n     if processors and pattern:\n         logits_processors = []\n+        compiled = re.compile(pattern)\n+        match = compiled.match\n         for processor in processors:\n             qualname = processor if isinstance(processor,\n                                                str) else processor.qualname\n-            if not re.match(pattern, qualname):\n+            if not match(qualname):\n                 raise ValueError(\n                     f\"Logits processor '{qualname}' is not allowed by this \"\n                     \"server. See --logits-processor-pattern engine argument \"\ndiff --git a/vllm/logits_process.py b/vllm/logits_process.py\nindex 7716ccd27..6450c1954 100644\n--- a/vllm/logits_process.py\n+++ b/vllm/logits_process.py\n@@ -50,8 +50,22 @@ class NoBadWordsLogitsProcessor:\n     _NEUTRAL_LOGIT = 0.0\n \n     def __init__(self, bad_words_ids: List[List[int]]):\n+        # Store as provided for API compatibility\n         self.bad_words_ids = bad_words_ids\n-        self.word_bias: torch.FloatTensor = None\n+        # Lazily constructed tensor for single-token biases;\n+        # set to None when no single-token bad words exist to avoid allocations.\n+        self.word_bias: torch.FloatTensor | None = None\n+        # Precompute structures for faster matching and lazy allocations.\n+        self._single_token_ids: List[int] = []\n+        self._multi_token_rules: List[Tuple[Tuple[int, ...], int]] = []\n+        for ids in bad_words_ids:\n+            if not ids:\n+                continue\n+            if len(ids) == 1:\n+                self._single_token_ids.append(ids[0])\n+            else:\n+                # (prefix tuple, last token id)\n+                self._multi_token_rules.append((tuple(ids[:-1]), ids[-1]))\n \n     def __call__(\n         self,\n@@ -61,29 +75,49 @@ class NoBadWordsLogitsProcessor:\n         if self.word_bias is None:\n             self._init_word_bias(logits=logits)\n \n-        last_token_bias = torch.zeros_like(logits)\n-\n-        for bad_word_ids in self.bad_words_ids:\n-            if len(bad_word_ids) == 1:  # 1-token words already processed\n-                continue\n-\n-            if len(bad_word_ids) > len(past_tokens_ids) + 1:\n+        # Base logits to return; avoid unnecessary allocation when no single\n+        # token bad words.\n+        if self.word_bias is None:\n+            out = logits\n+        else:\n+            out = logits + self.word_bias\n+\n+        # Fast path: no multi-token rules\n+        if not self._multi_token_rules:\n+            return out\n+\n+        pt_len = len(past_tokens_ids)\n+        matched_indices: List[int] = []\n+        seen = set()\n+        # Cache suffix tuples by length to avoid repeated slicing/boxing.\n+        needed_lengths = set()\n+        for prefix, _ in self._multi_token_rules:\n+            k = len(prefix)\n+            if k <= pt_len:\n+                needed_lengths.add(k)\n+        suffix_by_k = {k: tuple(past_tokens_ids[-k:]) for k in needed_lengths}\n+\n+        for prefix, last_token_id in self._multi_token_rules:\n+            k = len(prefix)\n+            if k > pt_len:\n                 continue\n+            # Compare suffix with expected prefix\n+            if suffix_by_k[k] == prefix:\n+                if last_token_id not in seen:\n+                    seen.add(last_token_id)\n+                    matched_indices.append(last_token_id)\n \n-            prefix_length = len(bad_word_ids) - 1\n-            last_token_id = bad_word_ids[-1]\n-            actual_prefix = past_tokens_ids[-prefix_length:]\n-            expected_prefix = bad_word_ids[:prefix_length]\n-\n-            assert len(actual_prefix) == len(expected_prefix)\n-\n-            is_match = tuple(actual_prefix) == tuple(expected_prefix)\n-            last_token_bias[last_token_id] += (self._SMALLEST_LOGIT if is_match\n-                                               else self._NEUTRAL_LOGIT)\n+        if not matched_indices:\n+            return out\n \n-        logits = logits + self.word_bias + last_token_bias\n+        # Ensure we do not mutate input logits tensor if no base addition\n+        if out is logits:\n+            out = logits.clone()\n+        # Apply -inf to matched last token indices\n+        for idx in matched_indices:\n+            out[idx] = self._SMALLEST_LOGIT\n \n-        return logits\n+        return out\n \n     def _init_word_bias(self, logits: torch.FloatTensor) -> None:\n         # Code based on NoBadWordsLogitsProcessor and SequenceBiasLogitsProcessor  # noqa: E501\n@@ -93,14 +127,17 @@ class NoBadWordsLogitsProcessor:\n \n         self._check_token_ids_bounds(vocab_size=vocab_size)\n \n+        if not self._single_token_ids:\n+            # No single-token bad words; skip allocating a full-size bias tensor.\n+            self.word_bias = None\n+            return\n+\n         self.word_bias = torch.zeros((vocab_size, ),\n                                      dtype=torch.float,\n                                      device=logits.device)\n \n-        for bad_word_ids in self.bad_words_ids:\n-            if len(bad_word_ids) == 1:\n-                bad_word_id = bad_word_ids[-1]\n-                self.word_bias[bad_word_id] = self._SMALLEST_LOGIT\n+        for bad_word_id in self._single_token_ids:\n+            self.word_bias[bad_word_id] = self._SMALLEST_LOGIT\n \n     def _check_token_ids_bounds(self, vocab_size: int) -> None:\n         invalid_token_ids = []\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e4411..e50a1da7d 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -205,10 +205,13 @@ class AsyncLLM(EngineClient):\n \n             # The output_handler task pushes items into the queue.\n             # This task pulls from the queue and yields to caller.\n+            get_nowait = q.get_nowait\n+            get = q.get\n+            qsize = q.qsize\n             while True:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                out = get_nowait() if qsize() > 0 else await get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\n@@ -229,24 +232,26 @@ class AsyncLLM(EngineClient):\n         \"\"\"Background loop: pulls from EngineCore and pushes to AsyncStreams.\"\"\"\n \n         try:\n+            get_output_async = self.engine_core.get_output_async\n+            process_outputs = self.output_processor.process_outputs\n+            abort_requests_async = self.engine_core.abort_requests_async\n+            log_stats = self._log_stats\n             while True:\n                 # 1) Pull EngineCoreOutputs from the EngineCore.\n-                outputs = await self.engine_core.get_output_async()\n+                outputs = await get_output_async()\n \n                 # 2) Process EngineCoreOutputs.\n-                processed_outputs = self.output_processor.process_outputs(\n-                    outputs.outputs)\n+                processed_outputs = process_outputs(outputs.outputs)\n                 # NOTE: RequestOutputs are pushed to their queues.\n                 assert len(processed_outputs.request_outputs) == 0\n \n                 # 3) Abort any reqs that finished due to stop strings.\n-                await self.engine_core.abort_requests_async(\n-                    processed_outputs.reqs_to_abort)\n+                await abort_requests_async(processed_outputs.reqs_to_abort)\n \n                 # 4) Logging.\n                 # TODO(rob): make into a coroutine and launch it in\n                 # background thread once we add Prometheus.\n-                self._log_stats(\n+                log_stats(\n                     scheduler_stats=outputs.scheduler_stats,\n                     iteration_stats=processed_outputs.iteration_stats,\n                 )\ndiff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py\nindex 19b89003c..12d546ebe 100644\n--- a/vllm/v1/engine/core_client.py\n+++ b/vllm/v1/engine/core_client.py\n@@ -111,7 +111,7 @@ class InprocClient(EngineCoreClient):\n         self.engine_core.add_request(request)\n \n     def abort_requests(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             self.engine_core.abort_requests(request_ids)\n \n     def shutdown(self) -> None:\n@@ -231,7 +231,7 @@ class SyncMPClient(MPClient):\n         self._send_input(EngineCoreRequestType.ADD, request)\n \n     def abort_requests(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             self._send_input(EngineCoreRequestType.ABORT, request_ids)\n \n     def profile(self, is_start: bool = True) -> None:\n@@ -273,7 +273,7 @@ class AsyncMPClient(MPClient):\n         await self._send_input(EngineCoreRequestType.ADD, request)\n \n     async def abort_requests_async(self, request_ids: List[str]) -> None:\n-        if len(request_ids) > 0:\n+        if request_ids:\n             await self._send_input(EngineCoreRequestType.ABORT, request_ids)\n \n     async def profile_async(self, is_start: bool = True) -> None:\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..b1f35de96 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -21,6 +21,17 @@ class OutputProcessorOutput:\n \n class RequestState:\n \n+    __slots__ = (\n+        \"request_id\",\n+        \"prompt\",\n+        \"prompt_token_ids\",\n+        \"prompt_len\",\n+        \"detokenizer\",\n+        \"is_prefilling\",\n+        \"queue\",\n+    )\n+\n+\n     def __init__(\n         self,\n         request_id: str,\n@@ -134,17 +145,19 @@ class OutputProcessor:\n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n         iteration_stats = IterationStats(self.log_stats)\n+        request_states = self.request_states\n+        iteration_stats_update = iteration_stats.update_from_output\n+        make_request_output = self._make_request_output\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n-            req_state = self.request_states.get(req_id)\n+            req_state = request_states.get(req_id)\n             if req_state is None:\n                 # Ignore output for already-aborted request.\n                 continue\n \n             # 1) Compute stats for this iteration.\n-            iteration_stats.update_from_output(engine_core_output,\n-                                               req_state.is_prefilling,\n-                                               req_state.prompt_len)\n+            iteration_stats_update(engine_core_output, req_state.is_prefilling,\n+                                   req_state.prompt_len)\n             req_state.is_prefilling = False\n \n             # 2) Detokenize the token ids into text.\n@@ -152,8 +165,8 @@ class OutputProcessor:\n                 engine_core_output)\n \n             # 3) Create and handle RequestOutput objects.\n-            if request_output := self._make_request_output(\n-                    req_state, detokenizer_output):\n+            if request_output := make_request_output(req_state,\n+                                                     detokenizer_output):\n                 if req_state.queue is not None:\n                     # AsyncLLM: put into queue for handling by generate().\n                     req_state.queue.put_nowait(request_output)\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..436bd04cf 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -15,6 +15,28 @@ if TYPE_CHECKING:\n \n class Request:\n \n+    __slots__ = (\n+        \"request_id\",\n+        \"sampling_params\",\n+        \"eos_token_id\",\n+        \"metrics\",\n+        \"lora_request\",\n+        \"status\",\n+        \"stop_reason\",\n+        \"max_tokens\",\n+        \"prompt\",\n+        \"prompt_token_ids\",\n+        \"num_prompt_tokens\",\n+        \"_output_token_ids\",\n+        \"_all_token_ids\",\n+        \"num_computed_tokens\",\n+        \"mm_positions\",\n+        \"mm_inputs\",\n+        \"mm_hashes\",\n+        \"_kv_block_hashes\",\n+    )\n+\n+\n     def __init__(\n         self,\n         request_id: str,\n", "model_name_or_path": "gpt-5-2025-08-07"}
