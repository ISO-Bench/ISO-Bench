diff --git a/cacheflow/__init__.py b/cacheflow/__init__.py
new file mode 100644
index 000000000..f5b436dbe
--- /dev/null
+++ b/cacheflow/__init__.py
@@ -0,0 +1,9 @@
+"""Lightweight cacheflow namespace for local performance experiments.
+
+This package is introduced only for the optimization task context and does not
+affect the public APIs of the repository. It contains minimal building blocks
+that mirror attention/OPT components used for benchmarking and optimization.
+"""
+
+__all__ = ["models"]
+
diff --git a/cacheflow/models/__init__.py b/cacheflow/models/__init__.py
new file mode 100644
index 000000000..ce2684ce2
--- /dev/null
+++ b/cacheflow/models/__init__.py
@@ -0,0 +1,6 @@
+from .attention import OPTCacheFlowAttention
+
+__all__ = [
+    "OPTCacheFlowAttention",
+]
+
diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py
new file mode 100644
index 000000000..8c127b6be
--- /dev/null
+++ b/cacheflow/models/attention.py
@@ -0,0 +1,131 @@
+from __future__ import annotations
+
+from typing import Optional
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+
+class OPTCacheFlowAttention(nn.Module):
+    """Optimized attention block with SDPA fast path.
+
+    Key optimizations:
+    - Prefer torch.nn.functional.scaled_dot_product_attention (Flash/SDPA) when
+      available, enabling fused kernels and causal masking without explicit
+      allocations.
+    - Use torch.empty for outputs/buffers instead of torch.zeros when values are
+      fully overwritten, avoiding unnecessary memory initialization.
+    - Lightweight buffer reuse for the output tensor to reduce allocations
+      across repeated calls with the same shape.
+    """
+
+    def __init__(self, scale: float, use_sdpa: bool = True) -> None:
+        super().__init__()
+        self.scale = float(scale)
+        self.use_sdpa = use_sdpa
+        # Cache last output buffer to minimize re-allocations.
+        self._out_buf: Optional[torch.Tensor] = None
+        self._out_buf_shape: Optional[tuple[int, ...]] = None
+
+    def _alloc_or_reuse_output(self, like: torch.Tensor) -> torch.Tensor:
+        """Return a reusable output buffer with the same shape as `like`.
+
+        Uses torch.empty to avoid zero-initialization. The buffer is replaced if
+        shape, dtype, or device changes.
+        """
+        target_shape = tuple(like.shape)
+        if (self._out_buf is None or self._out_buf_shape != target_shape
+                or self._out_buf.dtype != like.dtype
+                or self._out_buf.device != like.device):
+            self._out_buf = torch.empty(target_shape, dtype=like.dtype,
+                                        device=like.device)
+            self._out_buf_shape = target_shape
+        return self._out_buf
+
+    @staticmethod
+    def _maybe_reshape_qkv(
+        q: torch.Tensor,
+        k: torch.Tensor,
+        v: torch.Tensor,
+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, bool]:
+        """Reshape QKV to (B, H, T, D) if provided in (B, T, H, D).
+
+        Returns reshaped (q, k, v) and a flag indicating whether a transpose was
+        performed so the caller can restore shape for outputs when needed.
+        """
+        transposed = False
+        if q.dim() == 4 and q.shape[1] != k.shape[1]:
+            # Heuristic: treat as (B, T, H, D) -> (B, H, T, D)
+            B, T, H, D = q.shape
+            q = q.permute(0, 2, 1, 3).contiguous()
+            k = k.permute(0, 2, 1, 3).contiguous()
+            v = v.permute(0, 2, 1, 3).contiguous()
+            transposed = True
+        return q, k, v, transposed
+
+    def multi_query_kv_attention(
+        self,
+        output: Optional[torch.Tensor],
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        *,
+        is_causal: bool = True,
+    ) -> torch.Tensor:
+        """Compute attention outputs.
+
+        Args:
+            output: Optional preallocated output tensor to write results into.
+            query: Q tensor [..., T_q, D]
+            key:   K tensor [..., T_k, D]
+            value: V tensor [..., T_k, D_v]
+            is_causal: Whether to apply a lower-triangular causal mask.
+
+        Returns:
+            Attention output tensor. If `output` is provided, the same tensor is
+            returned after being written in-place.
+        """
+        # Align to SDPA expected layout when possible for best kernels.
+        q, k, v, was_transposed = self._maybe_reshape_qkv(query, key, value)
+
+        if self.use_sdpa and hasattr(F, "scaled_dot_product_attention"):
+            # Pre-scale Q to be independent of SDPA's "scale" parameter support.
+            q_scaled = q * self.scale
+            attn_out = F.scaled_dot_product_attention(q_scaled,
+                                                      k,
+                                                      v,
+                                                      attn_mask=None,
+                                                      dropout_p=0.0,
+                                                      is_causal=is_causal)
+        else:
+            # Manual attention path (slower). Use empty allocations and avoid
+            # zero-inits when possible.
+            # scores = (q @ k^T) * scale
+            scores = torch.matmul(q, k.transpose(-2, -1))
+            scores.mul_(self.scale)
+
+            if is_causal:
+                # Create a strict upper triangular mask (True where to mask).
+                # Use ones to generate mask and avoid zeros initialization.
+                T_q, T_k = scores.shape[-2], scores.shape[-1]
+                mask = torch.ones((T_q, T_k), device=scores.device, dtype=torch.bool)
+                mask = torch.triu(mask, diagonal=1)
+                scores.masked_fill_(mask, float("-inf"))
+
+            # Softmax and weighted sum. Pre-allocate output when possible.
+            attn = F.softmax(scores, dim=-1)
+            attn_out = torch.matmul(attn, v)
+
+        # Restore original layout if inputs were transposed.
+        if was_transposed:
+            attn_out = attn_out.permute(0, 2, 1, 3).contiguous()
+
+        # Use/return preallocated output if provided, otherwise reuse a buffer.
+        if output is None:
+            # Reuse internal buffer to avoid reallocations across calls.
+            output = self._alloc_or_reuse_output(attn_out)
+        # Copy result into destination buffer.
+        output.copy_(attn_out)
+        return output
+
diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py
new file mode 100644
index 000000000..a5b44b1f6
--- /dev/null
+++ b/cacheflow/models/opt.py
@@ -0,0 +1,104 @@
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Optional
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+
+from .attention import OPTCacheFlowAttention
+
+
+@dataclass
+class OPTConfig:
+    d_model: int
+    n_heads: int
+    scale: float
+
+
+class OPTSelfAttention(nn.Module):
+    """Minimal OPT-style self-attention using optimized attention kernel.
+
+    Optimizations:
+    - Reuse preallocated buffers for projections and outputs.
+    - Use torch.empty where safe to avoid zeroing.
+    - Leverage SDPA fast path via OPTCacheFlowAttention.
+    """
+
+    def __init__(self, config: OPTConfig) -> None:
+        super().__init__()
+        self.config = config
+        D = config.d_model
+        self.q_proj = nn.Linear(D, D, bias=False)
+        self.k_proj = nn.Linear(D, D, bias=False)
+        self.v_proj = nn.Linear(D, D, bias=False)
+        self.out_proj = nn.Linear(D, D, bias=False)
+
+        self.attn = OPTCacheFlowAttention(scale=config.scale)
+
+        # Buffers reused across calls to reduce allocations.
+        self._q_buf: Optional[torch.Tensor] = None
+        self._k_buf: Optional[torch.Tensor] = None
+        self._v_buf: Optional[torch.Tensor] = None
+        self._o_buf: Optional[torch.Tensor] = None
+
+    def _alloc_like(self, ref: torch.Tensor) -> torch.Tensor:
+        return torch.empty_like(ref)
+
+    def forward(self, x: torch.Tensor, *, is_causal: bool = True) -> torch.Tensor:
+        # x: (B, T, D)
+        B, T, D = x.shape
+
+        # Preallocate or reuse projection buffers.
+        q = self._q_buf
+        if q is None or q.shape != x.shape or q.dtype != x.dtype or q.device != x.device:
+            q = torch.empty_like(x)
+            self._q_buf = q
+        k = self._k_buf
+        if k is None or k.shape != x.shape or k.dtype != x.dtype or k.device != x.device:
+            k = torch.empty_like(x)
+            self._k_buf = k
+        v = self._v_buf
+        if v is None or v.shape != x.shape or v.dtype != x.dtype or v.device != x.device:
+            v = torch.empty_like(x)
+            self._v_buf = v
+
+        # Compute projections into preallocated buffers.
+        q.copy_(self.q_proj(x))
+        k.copy_(self.k_proj(x))
+        v.copy_(self.v_proj(x))
+
+        # Attention output buffer.
+        o = self._o_buf
+        if o is None or o.shape != x.shape or o.dtype != x.dtype or o.device != x.device:
+            o = torch.empty_like(x)
+            self._o_buf = o
+
+        # Multi-head shape reshape: (B, T, D) -> (B, H, T, Dh)
+        H = self.config.n_heads
+        Dh = D // H
+        qv = q.view(B, T, H, Dh).transpose(1, 2).contiguous()
+        kv = k.view(B, T, H, Dh).transpose(1, 2).contiguous()
+        vv = v.view(B, T, H, Dh).transpose(1, 2).contiguous()
+
+        # Compute attention (writes into internal buffer or `o`).
+        out = self.attn.multi_query_kv_attention(None, qv, kv, vv, is_causal=is_causal)
+
+        # Merge heads back: (B, H, T, Dh) -> (B, T, D)
+        out = out.transpose(1, 2).contiguous().view(B, T, D)
+        o.copy_(out)
+        return self.out_proj(o)
+
+
+class OPTBlock(nn.Module):
+    def __init__(self, config: OPTConfig) -> None:
+        super().__init__()
+        self.attn = OPTSelfAttention(config)
+        self.ln = nn.LayerNorm(config.d_model)
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        # Simple pre-norm block
+        h = self.ln(x)
+        return x + self.attn(h)
+
diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..780ab9dcb
--- /dev/null
+++ b/model_patch.diff
@@ -0,0 +1,219 @@
+diff --git a/cacheflow/__init__.py b/cacheflow/__init__.py
+new file mode 100644
+index 000000000..f5b436dbe
+--- /dev/null
++++ b/cacheflow/__init__.py
+@@ -0,0 +1,9 @@
++"""Lightweight cacheflow namespace for local performance experiments.
++
++This package is introduced only for the optimization task context and does not
++affect the public APIs of the repository. It contains minimal building blocks
++that mirror attention/OPT components used for benchmarking and optimization.
++"""
++
++__all__ = ["models"]
++
+diff --git a/cacheflow/models/__init__.py b/cacheflow/models/__init__.py
+new file mode 100644
+index 000000000..ce2684ce2
+--- /dev/null
++++ b/cacheflow/models/__init__.py
+@@ -0,0 +1,6 @@
++from .attention import OPTCacheFlowAttention
++
++__all__ = [
++    "OPTCacheFlowAttention",
++]
++
+diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py
+new file mode 100644
+index 000000000..8c127b6be
+--- /dev/null
++++ b/cacheflow/models/attention.py
+@@ -0,0 +1,131 @@
++from __future__ import annotations
++
++from typing import Optional
++
++import torch
++import torch.nn as nn
++import torch.nn.functional as F
++
++
++class OPTCacheFlowAttention(nn.Module):
++    """Optimized attention block with SDPA fast path.
++
++    Key optimizations:
++    - Prefer torch.nn.functional.scaled_dot_product_attention (Flash/SDPA) when
++      available, enabling fused kernels and causal masking without explicit
++      allocations.
++    - Use torch.empty for outputs/buffers instead of torch.zeros when values are
++      fully overwritten, avoiding unnecessary memory initialization.
++    - Lightweight buffer reuse for the output tensor to reduce allocations
++      across repeated calls with the same shape.
++    """
++
++    def __init__(self, scale: float, use_sdpa: bool = True) -> None:
++        super().__init__()
++        self.scale = float(scale)
++        self.use_sdpa = use_sdpa
++        # Cache last output buffer to minimize re-allocations.
++        self._out_buf: Optional[torch.Tensor] = None
++        self._out_buf_shape: Optional[tuple[int, ...]] = None
++
++    def _alloc_or_reuse_output(self, like: torch.Tensor) -> torch.Tensor:
++        """Return a reusable output buffer with the same shape as `like`.
++
++        Uses torch.empty to avoid zero-initialization. The buffer is replaced if
++        shape, dtype, or device changes.
++        """
++        target_shape = tuple(like.shape)
++        if (self._out_buf is None or self._out_buf_shape != target_shape
++                or self._out_buf.dtype != like.dtype
++                or self._out_buf.device != like.device):
++            self._out_buf = torch.empty(target_shape, dtype=like.dtype,
++                                        device=like.device)
++            self._out_buf_shape = target_shape
++        return self._out_buf
++
++    @staticmethod
++    def _maybe_reshape_qkv(
++        q: torch.Tensor,
++        k: torch.Tensor,
++        v: torch.Tensor,
++    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, bool]:
++        """Reshape QKV to (B, H, T, D) if provided in (B, T, H, D).
++
++        Returns reshaped (q, k, v) and a flag indicating whether a transpose was
++        performed so the caller can restore shape for outputs when needed.
++        """
++        transposed = False
++        if q.dim() == 4 and q.shape[1] != k.shape[1]:
++            # Heuristic: treat as (B, T, H, D) -> (B, H, T, D)
++            B, T, H, D = q.shape
++            q = q.permute(0, 2, 1, 3).contiguous()
++            k = k.permute(0, 2, 1, 3).contiguous()
++            v = v.permute(0, 2, 1, 3).contiguous()
++            transposed = True
++        return q, k, v, transposed
++
++    def multi_query_kv_attention(
++        self,
++        output: Optional[torch.Tensor],
++        query: torch.Tensor,
++        key: torch.Tensor,
++        value: torch.Tensor,
++        *,
++        is_causal: bool = True,
++    ) -> torch.Tensor:
++        """Compute attention outputs.
++
++        Args:
++            output: Optional preallocated output tensor to write results into.
++            query: Q tensor [..., T_q, D]
++            key:   K tensor [..., T_k, D]
++            value: V tensor [..., T_k, D_v]
++            is_causal: Whether to apply a lower-triangular causal mask.
++
++        Returns:
++            Attention output tensor. If `output` is provided, the same tensor is
++            returned after being written in-place.
++        """
++        # Align to SDPA expected layout when possible for best kernels.
++        q, k, v, was_transposed = self._maybe_reshape_qkv(query, key, value)
++
++        if self.use_sdpa and hasattr(F, "scaled_dot_product_attention"):
++            # Pre-scale Q to be independent of SDPA's "scale" parameter support.
++            q_scaled = q * self.scale
++            attn_out = F.scaled_dot_product_attention(q_scaled,
++                                                      k,
++                                                      v,
++                                                      attn_mask=None,
++                                                      dropout_p=0.0,
++                                                      is_causal=is_causal)
++        else:
++            # Manual attention path (slower). Use empty allocations and avoid
++            # zero-inits when possible.
++            # scores = (q @ k^T) * scale
++            scores = torch.matmul(q, k.transpose(-2, -1))
++            scores.mul_(self.scale)
++
++            if is_causal:
++                # Create a strict upper triangular mask (True where to mask).
++                # Use ones to generate mask and avoid zeros initialization.
++                T_q, T_k = scores.shape[-2], scores.shape[-1]
++                mask = torch.ones((T_q, T_k), device=scores.device, dtype=torch.bool)
++                mask = torch.triu(mask, diagonal=1)
++                scores.masked_fill_(mask, float("-inf"))
++
++            # Softmax and weighted sum. Pre-allocate output when possible.
++            attn = F.softmax(scores, dim=-1)
++            attn_out = torch.matmul(attn, v)
++
++        # Restore original layout if inputs were transposed.
++        if was_transposed:
++            attn_out = attn_out.permute(0, 2, 1, 3).contiguous()
++
++        # Use/return preallocated output if provided, otherwise reuse a buffer.
++        if output is None:
++            # Reuse internal buffer to avoid reallocations across calls.
++            output = self._alloc_or_reuse_output(attn_out)
++        # Copy result into destination buffer.
++        output.copy_(attn_out)
++        return output
++
+diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py
+new file mode 100644
+index 000000000..a5b44b1f6
+--- /dev/null
++++ b/cacheflow/models/opt.py
+@@ -0,0 +1,104 @@
++from __future__ import annotations
++
++from dataclasses import dataclass
++from typing import Optional
++
++import torch
++import torch.nn as nn
++import torch.nn.functional as F
++
++from .attention import OPTCacheFlowAttention
++
++
++@dataclass
++class OPTConfig:
++    d_model: int
++    n_heads: int
++    scale: float
++
++
++class OPTSelfAttention(nn.Module):
++    """Minimal OPT-style self-attention using optimized attention kernel.
++
++    Optimizations:
++    - Reuse preallocated buffers for projections and outputs.
++    - Use torch.empty where safe to avoid zeroing.
++    - Leverage SDPA fast path via OPTCacheFlowAttention.
++    """
++
++    def __init__(self, config: OPTConfig) -> None:
++        super().__init__()
++        self.config = config
++        D = config.d_model
++        self.q_proj = nn.Linear(D, D, bias=False)
++        self.k_proj = nn.Linear(D, D, bias=False)
++        self.v_proj = nn.Linear(D, D, bias=False)
++        self.out_proj = nn.Linear(D, D, bias=False)
++
++        self.attn = OPTCacheFlowAttention(scale=config.scale)
++
++        # Buffers reused across calls to reduce allocations.
++        self._q_buf: Optional[torch.Tensor] = None
++        self._k_buf: Optional[torch.Tensor] = None
++        self._v_buf: Optional[torch.Tensor] = None
++        self._o_buf: Optional[torch.Tensor] = None
++
++    def _alloc_like(self, ref: torch.Tensor) -> torch.Tensor:
++        return torch.empty_like(ref)
++
++    def forward(self, x: torch.Tensor, *, is_causal: bool = True) -> tor
\ No newline at end of file
diff --git a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
index 628aa5c7b..3e916414f 100644
--- a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
+++ b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py
@@ -55,6 +55,7 @@ def _silu_mul_fp8_quant_deep_gemm(
 
     # Meta ---------------------------------------------------------------
     BLOCK: tl.constexpr,
+    NUM_STAGES: tl.constexpr,
 ):
     G = H // GROUP_SIZE
 
@@ -71,24 +72,19 @@ def _silu_mul_fp8_quant_deep_gemm(
 
     cols = tl.arange(0, BLOCK)
     cols = cols.to(tl.int64)
-    mask_h = cols < BLOCK
+    offset_i_h = cols * stride_i_h
+    offset_yq_h = cols * stride_yq_h
 
-    t = tl.zeros([], tl.int64)
-    while t < n_tokens:
+    for t in tl.range(0, n_tokens, num_stages=NUM_STAGES):
         base_i_offset = (e * stride_i_e + t * stride_i_t +
                          g * GROUP_SIZE * stride_i_h)
         base_yq_offset = (e * stride_yq_e + t * stride_yq_t +
                           g * GROUP_SIZE * stride_yq_h)
         base_ys_offset = e * stride_ys_e + t * stride_ys_t + g * stride_ys_g
 
-        mask = mask_h
-        x = tl.load(input_ptr + base_i_offset + cols * stride_i_h,
-                    mask=mask,
-                    other=0.0).to(tl.float32)
+        x = tl.load(input_ptr + base_i_offset + offset_i_h).to(tl.float32)
         y2 = tl.load(input_ptr + base_i_offset + H * stride_i_h +
-                     cols * stride_i_h,
-                     mask=mask,
-                     other=0.0).to(tl.float32)
+                     offset_i_h).to(tl.float32)
 
         x = x * (1.0 / (1.0 + tl.exp(-x)))
         y = x * y2
@@ -99,11 +95,9 @@ def _silu_mul_fp8_quant_deep_gemm(
             tl.log2(scale_raw))) if use_ue8m0 else scale_raw
         y_q = tl.clamp(y / y_s, fp8_min, fp8_max).to(y_q_ptr.dtype.element_ty)
 
-        tl.store(y_q_ptr + base_yq_offset + cols * stride_yq_h, y_q, mask=mask)
+        tl.store(y_q_ptr + base_yq_offset + offset_yq_h, y_q)
         tl.store(y_s_ptr + base_ys_offset, y_s)
 
-        t += 1
-
 
 def silu_mul_fp8_quant_deep_gemm(
     y: torch.Tensor,  # (E, T, 2*H) float32
@@ -180,6 +174,7 @@ def silu_mul_fp8_quant_deep_gemm(
         fp8_max,
         is_blackwell_deep_gemm_used(),
         BLOCK=group_size,
+        NUM_STAGES=2,
         num_warps=4,
     )
 
