{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-1d35662", "model_patch": "diff --git a/csrc/moe/moe_permute_unpermute_op.cu b/csrc/moe/moe_permute_unpermute_op.cu\nindex 13aecd800..e827d9b11 100644\n--- a/csrc/moe/moe_permute_unpermute_op.cu\n+++ b/csrc/moe/moe_permute_unpermute_op.cu\n@@ -49,7 +49,7 @@ void moe_permute(\n   auto permuted_experts_id = torch::empty_like(topk_ids);\n   auto dst_row_id2src_row_id_map = torch::empty_like(src_row_id2dst_row_id_map);\n   auto align_expert_first_token_offset =\n-      torch::zeros_like(expert_first_token_offset);\n+      torch::empty_like(expert_first_token_offset);\n \n   CubKeyValueSorter sorter{};\n   int64_t* valid_num_ptr = nullptr;\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex bdd3a1a9c..72d683508 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -35,6 +35,10 @@ if TYPE_CHECKING:\n logger = init_logger(__name__)\n \n \n+# Precompile EBNF escape regex to avoid recompilation overhead\n+_EBNF_ESCAPE_RE = re.compile(r'([\"\\\\])')\n+\n+\n def get_local_xgrammar_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer: PreTrainedTokenizer,\n@@ -271,7 +275,8 @@ class GrammarConfig:\n     def escape_ebnf_string(s: str) -> str:\n         \"\"\"Escape special characters in a EBNF string.\"\"\"\n         # Escape double quotes and backslashes\n-        return re.sub(r'([\"\\\\])', r'\\\\\\1', s)\n+        # Use precompiled regex to avoid per-call compile cost\n+        return _EBNF_ESCAPE_RE.sub(r'\\\\\\1', s)\n \n     @staticmethod\n     def choice_as_grammar(choice: list[str] | None) -> str:\n@@ -301,6 +306,9 @@ class XGrammarLogitsProcessor:\n     matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n     batch_size: int = field(default=1)\n     prefilled: bool = field(default=False)\n+    # Cache device-specific token bitmask to avoid repeated transfers\n+    _token_bitmask_cached: torch.Tensor | None = None\n+    _token_bitmask_cached_device: torch.device | None = None\n \n     def __post_init__(self):\n         if self.tokenizer_info is None:\n@@ -321,6 +329,8 @@ class XGrammarLogitsProcessor:\n         self.batch_size = 1\n         self.token_bitmask = None  # type: ignore[assignment]\n         self.prefilled = False\n+        self._token_bitmask_cached = None\n+        self._token_bitmask_cached_device = None\n \n     def _ensure_ctx(self):\n         \"\"\"Lazily initialize the processor in the worker process\"\"\"\n@@ -381,21 +391,37 @@ class XGrammarLogitsProcessor:\n                 matcher.fill_next_token_bitmask(self.token_bitmask, i)\n \n         # token_bitmask is a CPU tensor for use with accept_token and\n-        # fill_next_token_bitmask so we move it to the device of scores\n-        device_type = scores.device.type\n-        dtype = scores.dtype\n-        if device_type != \"cuda\":\n-            # xgrammar on cpu only supports float32 scores\n-            # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            scores = scores.to(\"cpu\").float().unsqueeze(0)\n-\n-        # Note: In this method, if the tensors have different dimensions\n-        # on CPU device fails, but on GPU it runs without error. Hence the\n-        # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(\n-            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n-        if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n+        # fill_next_token_bitmask. For GPU we cache a device copy to\n+        # avoid per-call transfers.\n+        orig_device = scores.device\n+        orig_dtype = scores.dtype\n+\n+        if orig_device.type != \"cuda\":\n+            # Ensure scores are on CPU and float32 for CPU kernels.\n+            if orig_device.type != \"cpu\":\n+                scores = scores.to(\"cpu\")\n+            if scores.dtype is not torch.float32:\n+                scores = scores.float()\n+            # Match expected shape\n+            scores = scores.unsqueeze(0)\n+            xgr.apply_token_bitmask_inplace(scores, self.token_bitmask)\n+            # Restore original dtype/device and shape\n+            if orig_device.type != \"cpu\":\n+                scores = scores.to(orig_device)\n+            if orig_dtype is not torch.float32:\n+                scores = scores.to(orig_dtype)\n+            scores = scores.squeeze(0)\n+        else:\n+            # GPU path: cache device-specific token bitmask\n+            if (self._token_bitmask_cached_device != orig_device\n+                    or self._token_bitmask_cached is None\n+                    or self._token_bitmask_cached.shape !=\n+                    self.token_bitmask.shape):\n+                self._token_bitmask_cached = self.token_bitmask.to(\n+                    orig_device, non_blocking=True)\n+                self._token_bitmask_cached_device = orig_device\n+            xgr.apply_token_bitmask_inplace(scores,\n+                                            self._token_bitmask_cached)\n \n         return scores\n \n@@ -414,9 +440,10 @@ class XGrammarLogitsProcessor:\n                 xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n             ]\n \n-        # Create a new token bitmask with the same size\n-        if hasattr(self, 'token_bitmask') and self.token_bitmask is not None:\n-            new_processor.token_bitmask = self.token_bitmask\n+        # Do not share mutable token bitmask buffers; allocate lazily on use\n+        new_processor.token_bitmask = None  # type: ignore[assignment]\n+        new_processor._token_bitmask_cached = None\n+        new_processor._token_bitmask_cached_device = None\n \n         # Copy simple attributes\n         new_processor.batch_size = self.batch_size\ndiff --git a/vllm/model_executor/layers/fused_moe/cutlass_moe.py b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\nindex ff49d7bb7..9f4b0cf12 100644\n--- a/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n@@ -136,18 +136,10 @@ def run_cutlass_moe_fp8(\n                                      dtype=torch.int32,\n                                      device=device)\n \n-        # With expert_map each Rank processes only a subset of experts. As\n-        # a result not all of a_map and c2 tensors are filled. We fill it\n-        # zeros for correctness.\n-        if expert_map is not None:\n-            a_map = torch.zeros((local_topk_ids.numel()),\n-                                dtype=torch.int32,\n-                                device=device)\n-        else:\n-            a_map = torch.empty((local_topk_ids.numel()),\n-                                dtype=torch.int32,\n-                                device=device)\n-\n+        # Always allocate without initialization; fill only where needed.\n+        a_map = torch.empty((local_topk_ids.numel()),\n+                            dtype=torch.int32,\n+                            device=device)\n         c_map = torch.empty((local_topk_ids.numel()),\n                             dtype=torch.int32,\n                             device=device)\n@@ -156,6 +148,12 @@ def run_cutlass_moe_fp8(\n                                     problem_sizes1, problem_sizes2, a_map,\n                                     c_map, global_num_experts, N, K)\n \n+        # Zero only invalid entries to avoid reading garbage during shuffle.\n+        if expert_map is not None:\n+            invalid_mask = (local_topk_ids.reshape(-1) < 0)\n+            if invalid_mask.any().item():\n+                a_map.masked_fill_(invalid_mask, 0)\n+\n         a1q = ops.shuffle_rows(a1q, a_map)\n         a1q_scale = (ops.shuffle_rows(a1q_scale, a_map)\n                      if per_act_token else a1q_scale)\n@@ -186,7 +184,10 @@ def run_cutlass_moe_fp8(\n         c2, a2_scale, use_per_token_if_dynamic=per_act_token)\n \n     if expert_map is not None:\n-        c3.fill_(0)\n+        # Zero only rows corresponding to invalid tokens to avoid stale data.\n+        invalid_rows = (local_topk_ids.reshape(-1) < 0).nonzero(as_tuple=False).reshape(-1).to(dtype=torch.int64, device=c3.device)\n+        if invalid_rows.numel() > 0:\n+            c3.index_fill_(0, invalid_rows, 0)\n \n     ops.cutlass_moe_mm(c3, a2q, w2, a2q_scale, w2_scale, expert_offsets,\n                        problem_sizes2, ab_strides2, ab_strides2, c_strides2,\ndiff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\nindex 1a31410c3..2c68ccd3e 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\n@@ -203,7 +203,7 @@ class CompressedTensorsW4A4MoeMethod(CompressedTensorsMoEMethod):\n         round_up_multiple = lambda x, m: (x + m - 1) // m * m\n         M_padded = round_up_multiple(M, 128)\n         K_padded = round_up_multiple(K, 4)\n-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)\n+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)\n         padded_scale[:B, :M, :K] = scale\n         batches, rows, cols = padded_scale.shape\n         assert rows % 128 == 0\n", "model_name_or_path": "gpt-5-2025-08-07"}
