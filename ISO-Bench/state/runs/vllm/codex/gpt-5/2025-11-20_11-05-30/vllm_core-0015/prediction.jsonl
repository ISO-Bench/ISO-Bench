{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3cd91dc", "model_patch": "diff --git a/tests/v1/test_serial_utils.py b/tests/v1/test_serial_utils.py\nindex bc0e0cbd8..e241633ba 100644\n--- a/tests/v1/test_serial_utils.py\n+++ b/tests/v1/test_serial_utils.py\n@@ -50,7 +50,9 @@ def test_encode_decode():\n         large_non_contig_tensor=torch.rand(1024, 512)[:, 10:20],\n     )\n \n-    encoder = MsgpackEncoder()\n+    # Use a smaller threshold to inline fewer arrays into the main buffer,\n+    # increasing zero-copy behavior during decoding.\n+    encoder = MsgpackEncoder(size_threshold=256)\n     decoder = MsgpackDecoder(MyType)\n \n     encoded = encoder.encode(obj)\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex f80bf878f..9e9af1687 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -90,6 +90,7 @@ if TYPE_CHECKING:\n     V_SCALE_CONSTANT: int = 100\n     VLLM_SERVER_DEV_MODE: bool = False\n     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n+    VLLM_V1_MSGPACK_MIN_NOCOPY_BUF_SIZE: int = 512\n     VLLM_MLA_DISABLE: bool = False\n     VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False\n     VLLM_RAY_PER_WORKER_GPUS: float = 1.0\n@@ -134,7 +135,7 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:\n \n # begin-env-vars-definition\n \n-environment_variables: dict[str, Callable[[], Any]] = {\n+    environment_variables: dict[str, Callable[[], Any]] = {\n \n     # ================== Installation Time Env Vars ==================\n \n@@ -316,6 +317,14 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_LOGGING_PREFIX\":\n     lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n \n+    # Minimum size in bytes for array/tensor payloads that will be kept\n+    # as separate buffers in v1 msgpack serialization, to avoid copying\n+    # into the main buffer. Lower values increase the number of aux buffers\n+    # (and reduce copying), while higher values inline more data.\n+    # Defaults to 512 bytes.\n+    \"VLLM_V1_MSGPACK_MIN_NOCOPY_BUF_SIZE\":\n+    lambda: int(os.getenv(\"VLLM_V1_MSGPACK_MIN_NOCOPY_BUF_SIZE\", \"512\")),\n+\n     # if set, vllm will call logits processors in a thread pool with this many\n     # threads. This is useful when using custom logits processors that either\n     # (a) launch additional CUDA kernels or (b) do significant CPU-bound work\n@@ -711,8 +720,17 @@ environment_variables: dict[str, Callable[[], Any]] = {\n \n def __getattr__(name: str):\n     # lazy evaluation of environment variables\n-    if name in environment_variables:\n-        return environment_variables[name]()\n+    # During module import, this function can be invoked before\n+    # `environment_variables` is defined. Handle that gracefully.\n+    envmap = globals().get(\"environment_variables\")\n+    if envmap is None:\n+        # Module is still initializing. Provide safe defaults for early access\n+        # and fallback to raw environment for others.\n+        if name == \"VLLM_CONFIGURE_LOGGING\":\n+            return 1\n+        return os.environ.get(name)\n+    if name in envmap:\n+        return envmap[name]()\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \ndiff --git a/vllm/v1/serial_utils.py b/vllm/v1/serial_utils.py\nindex 3af6793fd..8a6582851 100644\n--- a/vllm/v1/serial_utils.py\n+++ b/vllm/v1/serial_utils.py\n@@ -11,13 +11,16 @@ import numpy as np\n import torch\n import zmq\n from msgspec import msgpack\n+import os\n \n CUSTOM_TYPE_PICKLE = 1\n CUSTOM_TYPE_CLOUDPICKLE = 2\n CUSTOM_TYPE_RAW_VIEW = 3\n \n-# TODO calibrate this size\n-MIN_NOCOPY_BUF_SIZE = 512\n+# Default threshold for when to avoid copying array data into the main\n+# msgpack buffer. Can be tuned via encoder arg or env var.\n+_DEFAULT_MIN_NOCOPY_BUF_SIZE = int(\n+    os.getenv(\"VLLM_V1_MSGPACK_MIN_NOCOPY_BUF_SIZE\", \"512\"))\n \n bytestr = Union[bytes, bytearray, memoryview, zmq.Frame]\n \n@@ -29,7 +32,13 @@ class MsgpackEncoder:\n     not thread-safe when encoding tensors / numpy arrays.\n     \"\"\"\n \n-    def __init__(self):\n+    def __init__(self, *, size_threshold: Optional[int] = None):\n+        # Use a configurable size threshold for deciding when to inline\n+        # array/tensor payloads vs. when to keep them as separate buffers.\n+        # Lower values will inline fewer arrays (more aux buffers).\n+        # Higher values will inline more arrays (fewer aux buffers).\n+        self.size_threshold: int = (size_threshold if size_threshold is not None\n+                                    else _DEFAULT_MIN_NOCOPY_BUF_SIZE)\n         self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)\n         # This is used as a local stash of buffers that we can then access from\n         # our custom `msgspec` hook, `enc_hook`. We don't have a way to\n@@ -77,8 +86,9 @@ class MsgpackEncoder:\n         self, obj: np.ndarray\n     ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n         assert self.aux_buffers is not None\n-        arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()\n-        if not obj.shape or obj.nbytes < MIN_NOCOPY_BUF_SIZE:\n+        # Prefer zero-copy when C-contiguous; otherwise fall back to bytes.\n+        arr_data = memoryview(obj) if obj.flags.c_contiguous else obj.tobytes()\n+        if not obj.shape or obj.nbytes < self.size_threshold:\n             # Encode small arrays and scalars inline. Using this extension type\n             # ensures we can avoid copying when decoding.\n             data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)\n@@ -106,6 +116,8 @@ class MsgpackDecoder:\n                                        ext_hook=self.ext_hook,\n                                        dec_hook=self.dec_hook)\n         self.aux_buffers: Sequence[bytestr] = ()\n+        # Cache np.dtype objects to avoid repeated parsing of dtype strings\n+        self._dtype_cache: dict[str, np.dtype] = {}\n \n     def decode(self, bufs: Union[bytestr, Sequence[bytestr]]) -> Any:\n         if isinstance(bufs, (bytes, bytearray, memoryview, zmq.Frame)):\n@@ -131,7 +143,12 @@ class MsgpackDecoder:\n     def _decode_ndarray(self, arr: Any) -> np.ndarray:\n         dtype, shape, data = arr\n         buffer = self.aux_buffers[data] if isinstance(data, int) else data\n-        return np.ndarray(buffer=buffer, dtype=np.dtype(dtype), shape=shape)\n+        # Reuse cached dtype objects for lower overhead\n+        dt = self._dtype_cache.get(dtype)\n+        if dt is None:\n+            dt = np.dtype(dtype)\n+            self._dtype_cache[dtype] = dt\n+        return np.ndarray(buffer=buffer, dtype=dt, shape=shape)\n \n     def ext_hook(self, code: int, data: memoryview) -> Any:\n         if code == CUSTOM_TYPE_RAW_VIEW:\n", "model_name_or_path": "gpt-5-2025-08-07"}
