diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py
index 4ec5a775f..617b6ddac 100644
--- a/vllm/core/block_manager.py
+++ b/vllm/core/block_manager.py
@@ -322,6 +322,10 @@ class SelfAttnBlockSpaceManager(BlockSpaceManager):
         This method determines which blocks can be safely skipped for all
         sequences in the sequence group.
         """
+        # Fast-path: if caching disabled, nothing to skip.
+        if not self.enable_caching:
+            return []
+
         computed_seq_block_ids = []
         for seq in seqs:
             all_blocks = self.block_tables[seq.seq_id].physical_block_ids
diff --git a/vllm/core/evictor.py b/vllm/core/evictor.py
index 7ec4768e9..4b4365ebf 100644
--- a/vllm/core/evictor.py
+++ b/vllm/core/evictor.py
@@ -64,6 +64,8 @@ class BlockMetaData:
     blocks with the same content hash, but their physical id is unique.
     """
 
+    __slots__ = ("content_hash", "num_hashed_tokens", "last_accessed")
+
     def __init__(self, content_hash: int, num_hashed_tokens: int,
                  last_accessed: float):
         self.content_hash = content_hash
@@ -85,7 +87,9 @@ class LRUEvictor(Evictor):
     CLEANUP_THRESHOLD = 50
 
     def __init__(self):
+        # Mapping from physical block id -> BlockMetaData
         self.free_table: Dict[int, BlockMetaData] = {}
+        # Min-heap of tuples: (last_accessed, -num_hashed_tokens, block_id, content_hash)
         self.priority_queue = []
 
     def __contains__(self, block_id: int) -> bool:
@@ -95,55 +99,55 @@ class LRUEvictor(Evictor):
         if len(self.free_table) == 0:
             raise ValueError("No usable cache memory left")
 
-        while self.priority_queue:
+        free_table = self.free_table
+        pq = self.priority_queue
+        while pq:
             # We do not remove outdated entries from the priority queue at the
             # time of updating the last_accessed timestamp. Instead, outdated
             # entries are filtered out here during eviction. Outdated entries
             # would either not in the free table, or have older last accessed
             # time.
-            last_accessed, _, block_id, content_hash = heapq.heappop(
-                self.priority_queue)
-            if (block_id in self.free_table and
-                    self.free_table[block_id].last_accessed == last_accessed):
-                self.free_table.pop(block_id)
+            last_accessed, _, block_id, content_hash = heapq.heappop(pq)
+            meta = free_table.get(block_id)
+            if meta is not None and meta.last_accessed == last_accessed:
+                free_table.pop(block_id, None)
                 return block_id, content_hash
 
         raise ValueError("No usable cache memory left")
 
     def add(self, block_id: int, content_hash: int, num_hashed_tokens: int,
             last_accessed: float):
+        # Insert metadata and a corresponding heap entry.
         self.free_table[block_id] = BlockMetaData(content_hash,
                                                   num_hashed_tokens,
                                                   last_accessed)
-        heapq.heappush(
-            self.priority_queue,
-            (last_accessed, -num_hashed_tokens, block_id, content_hash))
+        heapq.heappush(self.priority_queue,
+                       (last_accessed, -num_hashed_tokens, block_id,
+                        content_hash))
         self._cleanup_if_necessary()
 
     def update(self, block_id: int, last_accessed: float):
         self.free_table[block_id].last_accessed = last_accessed
 
     def _cleanup_if_necessary(self):
-        if len(self.priority_queue) > LRUEvictor.CLEANUP_THRESHOLD * len(
-                self.free_table):
+        free_len = len(self.free_table)
+        if free_len and len(self.priority_queue) > LRUEvictor.CLEANUP_THRESHOLD * free_len:
             self._cleanup()
 
     def _cleanup(self):
-        new_priority_queue: List[Tuple[float, int, int, int]] = []
-
-        for block_id, block in self.free_table.items():
-            new_priority_queue.append(
-                (block.last_accessed, -block.num_hashed_tokens, block_id,
-                 block.content_hash))
+        # Rebuild the heap from current free_table metadata.
+        ft = self.free_table
+        new_priority_queue: List[Tuple[float, int, int, int]] = [
+            (meta.last_accessed, -meta.num_hashed_tokens, block_id,
+             meta.content_hash) for block_id, meta in ft.items()
+        ]
         heapq.heapify(new_priority_queue)
-
         self.priority_queue = new_priority_queue
 
     def remove(self, block_id: int):
-        if block_id not in self.free_table:
+        if self.free_table.pop(block_id, None) is None:
             raise ValueError(
                 "Attempting to remove block that's not in the evictor")
-        self.free_table.pop(block_id)
 
     @property
     def num_blocks(self) -> int:
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 9de233896..fbacac226 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -26,6 +26,7 @@
 import math
 from typing import Any, Optional, Union
 
+import numpy as np
 import torch
 import torch.nn as nn
 from transformers import PretrainedConfig
@@ -1468,6 +1469,17 @@ class MRotaryEmbedding(RotaryEmbedding):
             mrope_position_delta + seq_len,
         ).expand(3, -1)
 
+
+    @staticmethod
+    def get_next_input_positions_tensor_out(out: np.ndarray, out_offset: int,
+                                            mrope_position_delta: int,
+                                            context_len: int,
+                                            num_new_tokens: int) -> None:
+        start = mrope_position_delta + context_len
+        end = start + num_new_tokens
+        vals = np.arange(start, end, dtype=out.dtype)
+        out[:, out_offset:out_offset + num_new_tokens] = vals
+
     @classmethod
     def omni_get_updates_use_audio_in_video(
         cls,
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 520d8fb18..9214965f2 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -257,7 +257,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
@@ -280,21 +280,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
@@ -889,15 +889,13 @@ class GPUModelRunner(LoRAModelRunnerMixin):
                 dst_start = mrope_pos_ptr
                 dst_end = mrope_pos_ptr + completion_part_len
 
-                self.mrope_positions_cpu[:, dst_start:dst_end] = \
-                    MRotaryEmbedding.get_next_input_positions_tensor(
-                        req.mrope_position_delta,
-                        context_len=num_computed_tokens +
-                        prompt_part_len,
-                        seq_len=num_computed_tokens +
-                        prompt_part_len +
-                        completion_part_len,
-                    )
+                MRotaryEmbedding.get_next_input_positions_tensor_out(
+                    self.mrope_positions_cpu.numpy(),
+                    dst_start,
+                    req.mrope_position_delta,
+                    context_len=num_computed_tokens + prompt_part_len,
+                    num_new_tokens=completion_part_len,
+                )
 
                 mrope_pos_ptr += completion_part_len
 
