{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-2a0309a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex b53a540ed..b99dab49c 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -440,8 +440,6 @@ class MambaMixer2(CustomOp):\n                 query_start_loc=attn_metadata.query_start_loc).transpose(\n                     0, 1)[:seq_len]\n \n-            # TODO: Why is this needed?\n-            hidden_states_B_C = hidden_states_B_C.contiguous()\n         else:\n             hidden_states_B_C = causal_conv1d_update(\n                 hidden_states_B_C,\n@@ -466,10 +464,10 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-            if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+            if has_initial_states is not None and torch.any(has_initial_states):\n+                zero_idx = mamba_cache_params.state_indices_tensor[~has_initial_states]\n+                if zero_idx.numel() > 0:\n+                    mamba_cache_params.ssm_state[zero_idx] = 0\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -495,8 +493,7 @@ class MambaMixer2(CustomOp):\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            mamba_cache_params.ssm_state[mamba_cache_params.state_indices_tensor] = varlen_state\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n@@ -504,7 +501,7 @@ class MambaMixer2(CustomOp):\n \n             n_groups = self.n_groups // self.tp_size\n             A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n+                -1, self.head_dim, self.ssm_state_size)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n             D = self.D[:, None, ...].expand(-1, self.head_dim)\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 96f6d807b..a6f779c29 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -169,8 +169,9 @@ class Sampler(nn.Module):\n         indices = torch.cat((token_ids, topk_indices), dim=1)\n         logprobs = torch.cat((token_logprobs, topk_logprobs), dim=1)\n \n-        # Use int32 to reduce the tensor size.\n+        # Use int32 to reduce the tensor size of indices and token ranks.\n         indices = indices.to(torch.int32)\n+        token_ranks = token_ranks.to(torch.int32)\n \n         return LogprobsTensors(indices, logprobs, token_ranks)\n \ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex c2a976108..c16900408 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -32,8 +32,8 @@ from vllm.v1.core.encoder_cache_manager import compute_encoder_budget\n from vllm.v1.engine.mm_input_cache import MMInputCacheClient\n from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n                                         KVCacheSpec)\n-from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsTensors,\n-                             ModelRunnerOutput)\n+from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsLists,\n+                             LogprobsTensors, ModelRunnerOutput)\n from vllm.v1.sample.metadata import SamplingMetadata\n from vllm.v1.sample.rejection_sampler import INVALID_TOKEN_ID, RejectionSampler\n from vllm.v1.spec_decode.ngram_proposer import NgramProposer\n@@ -192,10 +192,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        # Allocate without initialization; we only use populated slices.\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -213,16 +214,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -235,27 +236,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1040,8 +1041,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE: GPU -> CPU Sync happens here.\n         # Move as many CPU operations as possible before this sync point.\n         logprobs_tensors = sampler_output.logprobs_tensors\n-        logprobs_lists = logprobs_tensors.tolists() \\\n-            if logprobs_tensors is not None else None\n+        if logprobs_tensors is not None:\n+            # Perform a single explicit sync per tensor to reduce overhead.\n+            _ids_cpu = logprobs_tensors.logprob_token_ids.cpu()\n+            _lp_cpu = logprobs_tensors.logprobs.cpu()\n+            _ranks_cpu = logprobs_tensors.selected_token_ranks.cpu()\n+            logprobs_lists = LogprobsLists(\n+                _ids_cpu.tolist(), _lp_cpu.tolist(), _ranks_cpu.tolist())\n+        else:\n+            logprobs_lists = None\n \n         # Compute prompt logprobs if needed.\n         prompt_logprobs_dict = self._get_prompt_logprobs_dict(\n@@ -1051,18 +1059,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n         # Get the valid generated tokens.\n         sampled_token_ids = sampler_output.sampled_token_ids\n-        max_gen_len = sampled_token_ids.shape[-1]\n+        # Reduce CPU-GPU synchronization by copying once to CPU.\n+        sampled_token_ids_cpu = sampled_token_ids.cpu()\n+        max_gen_len = sampled_token_ids_cpu.shape[-1]\n         if max_gen_len == 1:\n             # No spec decode tokens.\n-            valid_sampled_token_ids = sampled_token_ids.tolist()\n+            valid_sampled_token_ids = sampled_token_ids_cpu.tolist()\n         else:\n             # Includes spec decode tokens.\n-            valid_mask = sampled_token_ids != INVALID_TOKEN_ID\n-            gen_lens = valid_mask.sum(dim=1).tolist()\n-            # TODO(woosuk): Optimize this.\n+            valid_mask_cpu = sampled_token_ids_cpu.ne(INVALID_TOKEN_ID)\n+            gen_lens = valid_mask_cpu.sum(dim=1).tolist()\n+            rows = sampled_token_ids_cpu.tolist()\n             valid_sampled_token_ids = [\n-                seq.tolist()\n-                for seq in sampled_token_ids[valid_mask].split(gen_lens)\n+                row[:n] if n > 0 else []\n+                for row, n in zip(rows, gen_lens)\n             ]\n \n         if not self.use_spec_decode:\n", "model_name_or_path": "gpt-5-2025-08-07"}
