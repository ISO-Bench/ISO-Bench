{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-10904e6", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/core/test_kv_cache_utils.py b/tests/v1/core/test_kv_cache_utils.py\nindex f4081766e..702eec58d 100644\n--- a/tests/v1/core/test_kv_cache_utils.py\n+++ b/tests/v1/core/test_kv_cache_utils.py\n@@ -135,6 +135,94 @@ def test_free_kv_cache_block_queue_get_all_free_blocks():\n         blocks[1:2] + blocks[3:] + [block_to_remove]\n \n \n+def test_free_kv_cache_block_queue_append_n():\n+    # Create an empty FreeKVCacheBlockQueue with no blocks\n+    queue = FreeKVCacheBlockQueue([])\n+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]\n+\n+    # Append 0 block: fake_head->fake_tail, num_free_blocks stays 0\n+    queue.append_n([])\n+    assert queue.num_free_blocks == 0\n+    assert (queue.fake_free_list_head.next_free_block\n+            is queue.fake_free_list_tail)\n+    assert (queue.fake_free_list_tail.prev_free_block\n+            is queue.fake_free_list_head)\n+\n+    # Append 1 block: fake_head->b0->fake_tail\n+    queue.append_n(blocks[0:1])\n+    assert queue.num_free_blocks == 1\n+    assert queue.free_list_head is blocks[0]\n+    assert queue.free_list_tail is blocks[0]\n+    assert queue.fake_free_list_head.next_free_block is blocks[0]\n+    assert blocks[0].prev_free_block is queue.fake_free_list_head\n+    assert blocks[0].next_free_block is queue.fake_free_list_tail\n+    assert queue.fake_free_list_tail.prev_free_block is blocks[0]\n+\n+    # Append 2 blocks: fake_head->b0->b4->b5->fake_tail\n+    queue.append_n(blocks[4:6])\n+    assert queue.num_free_blocks == 3\n+    assert queue.free_list_head is blocks[0]\n+    assert queue.free_list_tail is blocks[5]\n+    assert blocks[0].prev_free_block is queue.fake_free_list_head\n+    assert blocks[0].next_free_block is blocks[4]\n+    assert blocks[4].prev_free_block is blocks[0]\n+    assert blocks[4].next_free_block is blocks[5]\n+    assert blocks[5].prev_free_block is blocks[4]\n+    assert blocks[5].next_free_block is queue.fake_free_list_tail\n+    assert queue.fake_free_list_tail.prev_free_block is blocks[5]\n+\n+    # Append 2 more blocks: fake_head->b0->b4->b5->b2->b3->fake_tail\n+    queue.append_n(blocks[2:4])\n+    assert queue.num_free_blocks == 5\n+    assert queue.free_list_head is blocks[0]\n+    assert queue.free_list_tail is blocks[3]\n+    # Validate a couple of links in the chain\n+    assert blocks[5].next_free_block is blocks[2]\n+    assert blocks[2].prev_free_block is blocks[5]\n+    assert blocks[3].next_free_block is queue.fake_free_list_tail\n+\n+\n+def test_free_kv_cache_block_queue_popleft_n():\n+    blocks = [KVCacheBlock(block_id=i) for i in range(6)]\n+    queue = FreeKVCacheBlockQueue(blocks)\n+\n+    # Pop 0 block\n+    popped0 = queue.popleft_n(0)\n+    assert popped0 == []\n+    assert queue.num_free_blocks == 6\n+    assert queue.free_list_head is blocks[0]\n+    assert queue.free_list_tail is blocks[5]\n+\n+    # Pop 1 block: returns [b0]\n+    popped1 = queue.popleft_n(1)\n+    assert [blk.block_id for blk in popped1] == [0]\n+    assert queue.num_free_blocks == 5\n+    assert queue.free_list_head is blocks[1]\n+    assert queue.free_list_tail is blocks[5]\n+    assert queue.fake_free_list_head.next_free_block is blocks[1]\n+    assert blocks[1].prev_free_block is queue.fake_free_list_head\n+\n+    # Pop 2 blocks: returns [b1, b2]\n+    popped2 = queue.popleft_n(2)\n+    assert [blk.block_id for blk in popped2] == [1, 2]\n+    assert queue.num_free_blocks == 3\n+    assert queue.free_list_head is blocks[3]\n+    assert queue.free_list_tail is blocks[5]\n+\n+    # Pop 3 more (all remaining): returns [b3, b4, b5]\n+    popped3 = queue.popleft_n(3)\n+    assert [blk.block_id for blk in popped3] == [3, 4, 5]\n+    assert queue.num_free_blocks == 0\n+    assert queue.free_list_head is None\n+    assert queue.free_list_tail is None\n+    assert queue.fake_free_list_head.next_free_block is queue.fake_free_list_tail\n+    assert queue.fake_free_list_tail.prev_free_block is queue.fake_free_list_head\n+\n+    # Pop from empty should raise\n+    with pytest.raises(ValueError):\n+        queue.popleft_n(1)\n+\n+\n def test_generate_block_hash_extra_keys():\n     request = make_request(\n         request_id=0,\ndiff --git a/vllm/v1/core/kv_cache_utils.py b/vllm/v1/core/kv_cache_utils.py\nindex bab99fe37..1712b5d98 100644\n--- a/vllm/v1/core/kv_cache_utils.py\n+++ b/vllm/v1/core/kv_cache_utils.py\n@@ -89,14 +89,34 @@ class FreeKVCacheBlockQueue:\n     def __init__(self, blocks: List[KVCacheBlock]) -> None:\n         self.num_free_blocks = len(blocks)\n \n-        # Initialize the doubly linked list of free blocks.\n-        self.free_list_head: Optional[KVCacheBlock] = blocks[0]\n-        self.free_list_tail: Optional[KVCacheBlock] = blocks[-1]\n-        for i in range(self.num_free_blocks):\n-            if i > 0:\n-                blocks[i].prev_free_block = blocks[i - 1]\n-            if i < self.num_free_blocks - 1:\n-                blocks[i].next_free_block = blocks[i + 1]\n+        # Sentinel nodes to simplify pointer manipulation and avoid\n+        # repeated None checks on hot paths. These are internal-only\n+        # and should not be exposed to callers.\n+        self.fake_free_list_head = KVCacheBlock(block_id=-1)\n+        self.fake_free_list_tail = KVCacheBlock(block_id=-2)\n+        # Link sentinels together initially.\n+        self.fake_free_list_head.next_free_block = self.fake_free_list_tail\n+        self.fake_free_list_tail.prev_free_block = self.fake_free_list_head\n+\n+        # Initialize the doubly linked list of free blocks between sentinels.\n+        if self.num_free_blocks > 0:\n+            # Stitch the provided blocks together.\n+            for i in range(self.num_free_blocks):\n+                prev_blk = blocks[i - 1] if i > 0 else self.fake_free_list_head\n+                next_blk = (blocks[i + 1]\n+                            if i < self.num_free_blocks - 1 else\n+                            self.fake_free_list_tail)\n+                blocks[i].prev_free_block = prev_blk\n+                blocks[i].next_free_block = next_blk\n+                prev_blk.next_free_block = blocks[i]\n+                next_blk.prev_free_block = blocks[i]\n+\n+            self.free_list_head = blocks[0]\n+            self.free_list_tail = blocks[-1]\n+        else:\n+            # Empty free list.\n+            self.free_list_head = None\n+            self.free_list_tail = None\n \n     def popleft(self) -> KVCacheBlock:\n         \"\"\"Pop the first free block and reduce num_free_blocks by 1.\n@@ -111,6 +131,63 @@ class FreeKVCacheBlockQueue:\n         self.remove(block)\n         return block\n \n+    def popleft_n(self, n: int) -> List[KVCacheBlock]:\n+        \"\"\"Pop the first n free blocks in a single pass.\n+\n+        Args:\n+            n: Number of blocks to pop. Must be >= 0 and <= num_free_blocks.\n+\n+        Returns:\n+            A list containing the popped blocks in order from head to tail.\n+        \"\"\"\n+        if n < 0:\n+            raise ValueError(f\"n must be non-negative, got {n}\")\n+        if n == 0:\n+            return []\n+        if n > self.num_free_blocks:\n+            raise ValueError(\n+                f\"Requested {n} blocks but only {self.num_free_blocks} are available\"\n+            )\n+\n+        # Fetch the head and locate the nth block.\n+        first = self.free_list_head\n+        assert first is not None  # guarded by checks above\n+        last = first\n+        for _ in range(n - 1):\n+            assert last.next_free_block is not None\n+            last = last.next_free_block\n+\n+        # Detach [first, last] from the list between sentinels.\n+        prev_node = first.prev_free_block\n+        next_node = last.next_free_block\n+        assert prev_node is not None and next_node is not None\n+        prev_node.next_free_block = next_node\n+        next_node.prev_free_block = prev_node\n+\n+        # Fix endpoints of the popped chain.\n+        first.prev_free_block = None\n+        last.next_free_block = None\n+\n+        # Update head/tail and counters.\n+        if next_node is self.fake_free_list_tail:\n+            # List becomes empty after popping everything.\n+            self.free_list_head = None\n+            self.free_list_tail = None\n+        else:\n+            self.free_list_head = next_node\n+        self.num_free_blocks -= n\n+\n+        # Collect popped blocks without additional traversals.\n+        popped: List[KVCacheBlock] = []\n+        cur = first\n+        while True:\n+            popped.append(cur)\n+            if cur is last:\n+                break\n+            assert cur.next_free_block is not None\n+            cur = cur.next_free_block\n+        return popped\n+\n     def remove(self, block: KVCacheBlock) -> None:\n         \"\"\"Remove a block in the free list and reduce num_free_blocks by 1.\n         \n@@ -126,10 +203,14 @@ class FreeKVCacheBlockQueue:\n \n         if block == self.free_list_head:\n             # Update the head if the block is the head.\n-            self.free_list_head = block.next_free_block\n+            self.free_list_head = (block.next_free_block\n+                                   if block.next_free_block is not\n+                                   self.fake_free_list_tail else None)\n         if block == self.free_list_tail:\n             # Update the tail if the block is the tail.\n-            self.free_list_tail = block.prev_free_block\n+            self.free_list_tail = (block.prev_free_block\n+                                   if block.prev_free_block is not\n+                                   self.fake_free_list_head else None)\n \n         # Remove the block from the linked list.\n         block.prev_free_block = block.next_free_block = None\n@@ -143,29 +224,72 @@ class FreeKVCacheBlockQueue:\n             block: The block to append.\n         \"\"\"\n         if self.free_list_tail is not None:\n-            # Link the last block to the new block.\n+            # Link the last real block to the new block.\n             self.free_list_tail.next_free_block = block\n             block.prev_free_block = self.free_list_tail\n+            # Link the new block to the tail sentinel.\n+            block.next_free_block = self.fake_free_list_tail\n+            self.fake_free_list_tail.prev_free_block = block\n             self.free_list_tail = block\n         else:\n-            # The free list is empty.\n-            assert self.free_list_head is None\n+            # The free list is empty. Attach between sentinels.\n+            self.fake_free_list_head.next_free_block = block\n+            block.prev_free_block = self.fake_free_list_head\n+            block.next_free_block = self.fake_free_list_tail\n+            self.fake_free_list_tail.prev_free_block = block\n             self.free_list_head = self.free_list_tail = block\n \n-        block.next_free_block = None\n         self.num_free_blocks += 1\n \n+    def append_n(self, blocks: List[KVCacheBlock]) -> None:\n+        \"\"\"Append multiple blocks in one pass. No-ops on empty input.\n+\n+        Args:\n+            blocks: The blocks to append, in order.\n+        \"\"\"\n+        if not blocks:\n+            return\n+\n+        # Stitch the provided blocks together as a chain first.\n+        for i, blk in enumerate(blocks):\n+            prev_blk = blocks[i - 1] if i > 0 else None\n+            next_blk = blocks[i + 1] if i + 1 < len(blocks) else None\n+            blk.prev_free_block = prev_blk\n+            blk.next_free_block = next_blk\n+\n+        first, last = blocks[0], blocks[-1]\n+        if self.free_list_tail is not None:\n+            # Connect existing tail -> first and last -> tail sentinel.\n+            tail = self.free_list_tail\n+            tail.next_free_block = first\n+            first.prev_free_block = tail\n+            last.next_free_block = self.fake_free_list_tail\n+            self.fake_free_list_tail.prev_free_block = last\n+            self.free_list_tail = last\n+        else:\n+            # Empty list: insert between sentinels.\n+            self.fake_free_list_head.next_free_block = first\n+            first.prev_free_block = self.fake_free_list_head\n+            last.next_free_block = self.fake_free_list_tail\n+            self.fake_free_list_tail.prev_free_block = last\n+            self.free_list_head = first\n+            self.free_list_tail = last\n+\n+        self.num_free_blocks += len(blocks)\n+\n     def get_all_free_blocks(self) -> List[KVCacheBlock]:\n         \"\"\"Get all free blocks in the free list. Mainly used for testing.\n         \n         Returns:\n             A list of free blocks.\n         \"\"\"\n-        ret = []\n+        ret: List[KVCacheBlock] = []\n         curr_block = self.free_list_head\n         while curr_block is not None:\n             ret.append(curr_block)\n-            curr_block = curr_block.next_free_block\n+            curr_block = (curr_block.next_free_block\n+                          if curr_block.next_free_block is not\n+                          self.fake_free_list_tail else None)\n         return ret\n \n \ndiff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex acc3a944e..32aee44e3 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -8,7 +8,7 @@ import torch\n class SamplerOutput:\n \n     # [num_reqs]\n-    sampled_token_ids: List[int]\n+    sampled_token_ids: torch.Tensor\n \n     # [num_reqs, max_num_logprobs + 1]\n     logprob_token_ids: Optional[torch.Tensor]\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 7cd42ca21..9ad665a64 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -50,9 +50,8 @@ class Sampler(nn.Module):\n         # Use int32 to reduce the tensor size.\n         sampled = sampled.to(torch.int32)\n \n-        # NOTE: CPU-GPU synchronization happens here.\n         sampler_output = SamplerOutput(\n-            sampled_token_ids=sampled.tolist(),\n+            sampled_token_ids=sampled,\n             logprob_token_ids=topk_indices,\n             logprobs=topk_logprobs,\n             prompt_logprob_token_ids=None,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4b3c325de..08f554a45 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -136,10 +136,10 @@ class GPUModelRunner:\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n \n@@ -155,16 +155,16 @@ class GPUModelRunner:\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -176,27 +176,27 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -775,7 +775,12 @@ class GPUModelRunner:\n             sampling_metadata=sampling_metadata,\n         )\n \n-        sampled_token_ids = sampler_output.sampled_token_ids\n+        sampled_token_ids_t = sampler_output.sampled_token_ids\n+        sampled_token_ids_cpu = (sampled_token_ids_t.cpu()\n+                                 if sampled_token_ids_t.is_cuda\n+                                 else sampled_token_ids_t)\n+        sampled_token_ids_list = sampled_token_ids_cpu.tolist()\n+\n         # TODO(woosuk): The following loop can be slow since it iterates over\n         # the requests one by one. Optimize.\n         num_reqs = self.input_batch.num_reqs\n@@ -787,7 +792,7 @@ class GPUModelRunner:\n             assert seq_len <= req_state.num_tokens\n             if seq_len == req_state.num_tokens:\n                 # Append the sampled token to the output token ids.\n-                token_id = sampled_token_ids[i]\n+                token_id = sampled_token_ids_list[i]\n                 self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                 self.input_batch.num_tokens[i] += 1\n                 req_state.output_token_ids.append(token_id)\n@@ -817,7 +822,7 @@ class GPUModelRunner:\n         model_runner_output = ModelRunnerOutput(\n             req_ids=req_ids,\n             req_id_to_index=self.input_batch.req_id_to_index,\n-            sampled_token_ids=sampled_token_ids,\n+            sampled_token_ids=sampled_token_ids_list,\n             logprob_token_ids_cpu=logprob_token_ids,\n             logprobs_cpu=logprobs,\n         )\n", "model_name_or_path": "gpt-5-2025-08-07"}
