{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-95baec8", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex b2aaeb33c..97a423d86 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -313,7 +313,10 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n         # Compute a new hash for the block so that it can be shared by other\n         # Sequences\n-        new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n+        # Avoid touching the (potentially large) logical block list; compute\n+        # the last full block index directly from sequence length.\n+        last_full_idx = seq.data.get_len() // seq.block_size - 1\n+        new_hash = seq.hash_of_block(last_full_idx)\n \n         # if new_hash is already in the cached table, then free last_block\n         # and return the cached version\n@@ -328,7 +331,8 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         self,\n         seq: Sequence,\n     ) -> bool:\n-        token_ids_len = len(seq.data.get_token_ids())\n+        # Avoid constructing token lists; use O(1) length query.\n+        token_ids_len = seq.data.get_len()\n         return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n \n     def _maybe_promote_last_block(\n@@ -353,10 +357,13 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         if not self.enable_caching:\n             return self.gpu_allocator.allocate()\n         block_hash: Optional[int] = None\n+        num_hashed_tokens: int = 0\n         if (self._is_last_block_full(seq)):\n-            block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n-        num_hashed_tokens = seq.num_hashed_tokens_of_block(\n-            len(seq.logical_token_blocks) - 1)\n+            # Compute last full block index without scanning logical blocks.\n+            last_full_idx = seq.data.get_len() // seq.block_size - 1\n+            block_hash = seq.hash_of_block(last_full_idx)\n+            # Each full block contributes exactly `block_size` hashed tokens.\n+            num_hashed_tokens = (last_full_idx + 1) * seq.block_size\n \n         # num_hashed_tokens is used to compute future hashes\n         # (e.g. in the hashing function, it is used to ask the sequence for\n", "model_name_or_path": "gpt-5-2025-08-07"}
