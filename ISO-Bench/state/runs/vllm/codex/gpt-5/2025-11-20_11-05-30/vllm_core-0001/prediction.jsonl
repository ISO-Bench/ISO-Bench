{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-fbefc8a", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\nindex 66f9106bd..4bf775347 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 16,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 64,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\nindex ed5b655d8..5a3f415d5 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 32,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 128,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\nindex 822f04e33..a799eb4b6 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n@@ -52,7 +52,7 @@\n         \"num_stages\": 2,\n         \"waves_per_eu\": 0,\n         \"matrix_instr_nonkdim\": 16,\n-        \"kpack\": 1\n+        \"kpack\": 2\n     },\n     \"24\": {\n         \"BLOCK_SIZE_M\": 16,\ndiff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nnew file mode 100644\nindex 000000000..185d0d34e\n--- /dev/null\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -0,0 +1,136 @@\n+# SPDX-License-Identifier: Apache-2.0\n+\n+\"\"\"Reasoning parser for the Qwen3 model family.\"\"\"\n+from collections.abc import Sequence\n+from typing import Optional, Tuple, Union\n+\n+from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\n+                                              DeltaMessage)\n+from vllm.entrypoints.openai.reasoning_parsers.abs_reasoning_parsers import (\n+    ReasoningParser, ReasoningParserManager)\n+\n+\n+@ReasoningParserManager.register_module(\"qwen3\")\n+class Qwen3ReasoningParser(ReasoningParser):\n+    \"\"\"Parser that extracts <think>...</think> reasoning spans.\"\"\"\n+\n+    def __init__(self, tokenizer):\n+        super().__init__(tokenizer)\n+        # Constant markers used to delimit reasoning blocks\n+        self.think_start_token = \"<think>\"\n+        self.think_end_token = \"</think>\"\n+        # Cache lengths to avoid repeated len() calls in hot paths\n+        self._think_start_len = len(self.think_start_token)\n+        self._think_end_len = len(self.think_end_token)\n+\n+        self.think_start_token_id = self.vocab.get(self.think_start_token)\n+        self.think_end_token_id = self.vocab.get(self.think_end_token)\n+        if (self.think_start_token_id is None\n+                or self.think_end_token_id is None):\n+            raise RuntimeError(\"Missing think tokens in tokenizer vocabulary.\")\n+\n+    def extract_reasoning_content_streaming(\n+        self,\n+        previous_text: str,\n+        current_text: str,\n+        delta_text: str,\n+        previous_token_ids: Sequence[int],\n+        current_token_ids: Sequence[int],\n+        delta_token_ids: Sequence[int],\n+    ) -> Union[DeltaMessage, None]:\n+        \"\"\"Fast-path streaming handler using minimal searches.\n+\n+        The logic mirrors the original behavior while reducing unnecessary\n+        allocations and regex usage. We only perform substring searches when\n+        the relevant start/end tokens appear in the token deltas.\n+        \"\"\"\n+\n+        if not delta_token_ids:\n+            return None\n+\n+        # We use local variables to avoid repeated attribute lookups\n+        end_tok = self.think_end_token\n+        start_tok = self.think_start_token\n+        end_len = self._think_end_len\n+\n+        if self.think_start_token_id in previous_token_ids:\n+            if self.think_end_token_id in delta_token_ids:\n+                end_index = delta_text.find(end_tok)\n+                if end_index == -1:\n+                    # Fallback: treat the delta as reasoning if malformed\n+                    return DeltaMessage(reasoning_content=delta_text)\n+                reasoning_content = delta_text[:end_index]\n+                content = delta_text[end_index + end_len:]\n+                return DeltaMessage(reasoning_content=reasoning_content,\n+                                    content=content or None)\n+            # Already inside a <think> block; everything is reasoning\n+            return DeltaMessage(reasoning_content=delta_text)\n+\n+        if self.think_start_token_id in delta_token_ids:\n+            if self.think_end_token_id in delta_token_ids:\n+                start_index = delta_text.find(start_tok)\n+                end_index = delta_text.find(end_tok, start_index + 1)\n+                if start_index == -1 or end_index == -1:\n+                    return DeltaMessage(reasoning_content=delta_text)\n+                reasoning_content = delta_text[start_index +\n+                                               self._think_start_len:end_index]\n+                content = delta_text[end_index + end_len:]\n+                return DeltaMessage(reasoning_content=reasoning_content,\n+                                    content=content or None)\n+            # Start token appeared; rest of delta belongs to reasoning\n+            return DeltaMessage(reasoning_content=delta_text)\n+\n+        if self.think_end_token_id in delta_token_ids:\n+            end_index = delta_text.find(end_tok)\n+            if end_index == -1:\n+                return DeltaMessage(reasoning_content=delta_text)\n+            reasoning_content = delta_text[:end_index]\n+            content = delta_text[end_index + end_len:]\n+            return DeltaMessage(reasoning_content=reasoning_content,\n+                                content=content or None)\n+\n+        return DeltaMessage(content=delta_text)\n+\n+    def extract_reasoning_content(\n+            self, model_output: str, request: ChatCompletionRequest\n+    ) -> Tuple[Optional[str], Optional[str]]:\n+        \"\"\"Efficiently extract reasoning content without regex.\n+\n+        For input like:\n+          pre<think>abc</think>post\n+        returns (\"abc\", \"prepost\").\n+\n+        If the end token is not present, we treat the whole text as reasoning\n+        (consistent with streaming where the start token may have appeared\n+        earlier). If the start token is not present but the end token is, we\n+        assume the reasoning started earlier and prepend the start token to\n+        parse the current chunk correctly.\n+        \"\"\"\n+\n+        # Fast path: no end token means everything is still within reasoning\n+        if self.think_end_token not in model_output:\n+            return model_output, None\n+\n+        # If we have an end token but no start token, assume it started earlier\n+        if self.think_start_token not in model_output:\n+            model_output = f\"{self.think_start_token}{model_output}\"\n+\n+        start_tok = self.think_start_token\n+        end_tok = self.think_end_token\n+        s_len = self._think_start_len\n+        e_len = self._think_end_len\n+\n+        s_idx = model_output.find(start_tok)\n+        if s_idx == -1:\n+            # Shouldn't happen due to insertion above, but be robust\n+            return None, model_output or None\n+        e_idx = model_output.find(end_tok, s_idx + s_len)\n+        if e_idx == -1:\n+            # End token missing; treat entire text as reasoning\n+            return model_output, None\n+\n+        reasoning_content = model_output[s_idx + s_len:e_idx]\n+        # Remove the think block while preserving any prefix/suffix text\n+        remaining = model_output[:s_idx] + model_output[e_idx + e_len:]\n+\n+        return (reasoning_content or None, remaining or None)\n", "model_name_or_path": "gpt-5-2025-08-07"}
