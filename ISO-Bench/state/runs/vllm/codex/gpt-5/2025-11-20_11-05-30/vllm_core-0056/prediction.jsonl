{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-beebf47", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py\nindex 8184b0732..2dca25b78 100644\n--- a/vllm/attention/backends/mla/common.py\n+++ b/vllm/attention/backends/mla/common.py\n@@ -961,10 +961,8 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):\n             chunk_seq_lens = (chunk_ends - context_chunk_starts).clamp(min=0)\n             _context_chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                 torch.int32)\n-            zero = torch.zeros(num_chunks, dtype=torch.int32, device=device)\\\n-                .unsqueeze(-1)\n-            context_chunk_cu_seq_lens = \\\n-                torch.cat([zero, _context_chunk_cu_seq_lens], dim=1)\n+            context_chunk_cu_seq_lens = torch.nn.functional.pad(\n+                _context_chunk_cu_seq_lens, (1, 0), value=0)\n             context_chunk_max_seq_lens = \\\n                 chunk_seq_lens.max(dim=1).values.tolist()\n             context_chunk_seq_tot = chunk_seq_lens.sum(dim=1).tolist()\n@@ -1307,8 +1305,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n                 seq_starts=prefill_metadata.context_chunk_starts[i],\n             )\n \n-            kv_c_normed = workspace[:toks]\\\n-                [..., :self.kv_lora_rank].unsqueeze(1)\n+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]\n             k_pe = workspace[:toks]\\\n                 [..., self.kv_lora_rank:].unsqueeze(1)\n \ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98b667972..1d0c5c12b 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -208,6 +208,9 @@ class OpenAIServingChat(OpenAIServing):\n         # Schedule the request and get the result generator.\n         generators: list[AsyncGenerator[RequestOutput, None]] = []\n         try:\n+            # Extract trace headers once per request instead of per prompt.\n+            trace_headers = (None if raw_request is None else await\n+                             self._get_trace_headers(raw_request.headers))\n             for i, engine_prompt in enumerate(engine_prompts):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n@@ -227,9 +230,6 @@ class OpenAIServingChat(OpenAIServing):\n                                  lora_request=lora_request,\n                                  prompt_adapter_request=prompt_adapter_request)\n \n-                trace_headers = (None if raw_request is None else await\n-                                 self._get_trace_headers(raw_request.headers))\n-\n                 if isinstance(sampling_params, BeamSearchParams):\n                     generator = self.engine_client.beam_search(\n                         prompt=engine_prompt,\n@@ -314,8 +314,9 @@ class OpenAIServingChat(OpenAIServing):\n         # all_previous_token_ids will not be used twice in the same iteration.\n         if tool_choice_auto or should_stream_with_reasoning_parsing:\n             # These are only required in \"auto\" tool choice case\n-            previous_texts = [\"\"] * num_choices\n-            all_previous_token_ids = [[]] * num_choices\n+            # Avoid shared references from list multiplication for lists.\n+            previous_texts = [\"\" for _ in range(num_choices)]\n+            all_previous_token_ids = [[] for _ in range(num_choices)]\n         else:\n             previous_texts, all_previous_token_ids = None, None\n \n@@ -337,9 +338,11 @@ class OpenAIServingChat(OpenAIServing):\n         # Prepare the tool parser if it's needed\n         try:\n             if tool_choice_auto and self.tool_parser:\n+                # Create an independent parser per choice to avoid\n+                # shared mutable state across indices.\n                 tool_parsers: list[Optional[ToolParser]] = [\n-                    self.tool_parser(tokenizer)\n-                ] * num_choices\n+                    self.tool_parser(tokenizer) for _ in range(num_choices)\n+                ]\n             else:\n                 tool_parsers = [None] * num_choices\n         except Exception as e:\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex 667ff448e..fa950b90a 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -117,6 +117,9 @@ class OpenAIServingCompletion(OpenAIServing):\n         # Schedule the request and get the result generator.\n         generators: list[AsyncGenerator[RequestOutput, None]] = []\n         try:\n+            # Extract trace headers once per request instead of per prompt.\n+            trace_headers = (None if raw_request is None else await\n+                             self._get_trace_headers(raw_request.headers))\n             for i, engine_prompt in enumerate(engine_prompts):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n@@ -138,9 +141,6 @@ class OpenAIServingCompletion(OpenAIServing):\n                                  lora_request=lora_request,\n                                  prompt_adapter_request=prompt_adapter_request)\n \n-                trace_headers = (None if raw_request is None else await\n-                                 self._get_trace_headers(raw_request.headers))\n-\n                 if isinstance(sampling_params, BeamSearchParams):\n                     generator = self.engine_client.beam_search(\n                         prompt=engine_prompt,\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex c98262eea..5c0e9f5cb 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,14 +532,11 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=torch.nn.functional.pad(\n+                        _chunk_cu_seq_lens, (1, 0), value=0),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -873,8 +870,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 seq_starts=prefill_metadata.chunked_context.starts[i],\n             )\n \n-            kv_c_normed = workspace[:toks]\\\n-                [..., :self.kv_lora_rank].unsqueeze(1)\n+            kv_c_normed = workspace[:toks][..., :self.kv_lora_rank]\n             k_pe = workspace[:toks]\\\n                 [..., self.kv_lora_rank:].unsqueeze(1)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
