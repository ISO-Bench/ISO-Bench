{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-cf2f084", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 5b19e3f35..3519cbad9 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -467,16 +467,10 @@ class MambaMixer2(CustomOp):\n \n             initial_states = None\n \n-            if has_initial_states is not None and torch.any(\n-                    has_initial_states):\n-\n-                # vectorized ssm_state zero init\n-                batched_zero_init_func = torch.vmap(\n-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n-                batched_zero_init_func(\n-                    mamba_cache_params.\n-                    state_indices_tensor[~has_initial_states].unsqueeze(\n-                        dim=-1), )\n+            if has_initial_states is not None and any(has_initial_states):\n+\n+                for idx in mamba_cache_params.state_indices_tensor[~has_initial_states]:\n+                    mamba_cache_params.ssm_state[idx].zero_()\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -505,12 +499,9 @@ class MambaMixer2(CustomOp):\n             # limitation which doesn't allow use of `item()`\n             # Note: the lambda capture can happen where ssm_state is initialized\n             #       instead of here\n-            batched_copy = torch.vmap(\n-                lambda idx, source_state: mamba_cache_params.ssm_state[\n-                    idx].copy_(source_state))\n-            batched_copy(\n-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n-                varlen_state)\n+            for idx, source_state in zip(\n+                mamba_cache_params.state_indices_tensor, varlen_state):\n+                mamba_cache_params.ssm_state[idx].copy_(source_state)\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n@@ -518,7 +509,7 @@ class MambaMixer2(CustomOp):\n \n             n_groups = self.n_groups // self.tp_size\n             A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n+                -1, self.head_dim, self.ssm_state_size)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n             D = self.D[:, None, ...].expand(-1, self.head_dim)\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nindex 9d1d4bb92..2665052b9 100644\n--- a/vllm/transformers_utils/detokenizer.py\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -45,12 +45,15 @@ class Detokenizer:\n         all_token_ids = seq.get_token_ids()\n         prompt_token_ids = all_token_ids[:-1]\n         tokenizer = self.get_tokenizer_for_seq(seq)\n+        # Initialize offsets and token buffer for incremental decode.\n+        # Using empty prev_tokens avoids an extra prompt conversion call in\n+        # the first iteration.\n         prefix_offset = 0\n         read_offset = 0\n         next_iter_prefix_offset = 0\n         next_iter_read_offset = 0\n         next_iter_tokens: List[str] = []\n-        prev_tokens = None\n+        prev_tokens: List[str] | None = []\n \n         for token_position_in_logprob, prompt_logprobs_for_token in enumerate(\n                 prompt_logprobs):\n@@ -64,12 +67,18 @@ class Detokenizer:\n             for token_id, sample_logprob in prompt_logprobs_for_token.items():\n                 if (sample_logprob.decoded_token is None\n                         and token_id != VLLM_INVALID_TOKEN_ID):\n-                    prompt_token_ids_with_token = (\n-                        prompt_token_ids[:token_position] + [token_id])\n+                    # Fast-path: avoid expensive decode for special tokens\n+                    # when we skip them.\n+                    if prms.skip_special_tokens and token_id in tokenizer.all_special_ids:  # type: ignore[attr-defined]\n+                        sample_logprob.decoded_token = \"\"\n+                        continue\n+\n                     (new_tokens, new_text, new_prefix_offset,\n                      new_read_offset) = detokenize_incrementally(\n                          tokenizer=tokenizer,\n-                         all_input_ids=prompt_token_ids_with_token,\n+                         # Only the new token id is needed when prev_tokens\n+                         # is provided.\n+                         all_input_ids=[token_id],\n                          prev_tokens=prev_tokens,\n                          prefix_offset=prefix_offset,\n                          read_offset=read_offset,\n@@ -125,7 +134,8 @@ class Detokenizer:\n         (new_tokens, new_decoded_token_text, prefix_offset,\n          read_offset) = detokenize_incrementally(\n              tokenizer=tokenizer,\n-             all_input_ids=all_input_ids,\n+             # When prev_tokens is provided, we only need the latest id.\n+             all_input_ids=[token_id_generated_this_iteration],\n              prev_tokens=seq.tokens,\n              prefix_offset=seq.prefix_offset,\n              read_offset=seq.read_offset,\n@@ -146,10 +156,17 @@ class Detokenizer:\n \n                 if (sample_logprob.decoded_token is None\n                         and token_id != VLLM_INVALID_TOKEN_ID):\n-                    all_input_ids_with_logprob = previous_tokens + [token_id]\n+                    # Fast-path: avoid decode for special tokens when\n+                    # skipping special tokens.\n+                    if prms.skip_special_tokens and token_id in tokenizer.all_special_ids:  # type: ignore[attr-defined]\n+                        sample_logprob.decoded_token = \"\"\n+                        continue\n+\n                     (_, new_text, _, _) = detokenize_incrementally(\n                         tokenizer=tokenizer,\n-                        all_input_ids=all_input_ids_with_logprob,\n+                        # Only the candidate id is needed when prev_tokens\n+                        # is provided.\n+                        all_input_ids=[token_id],\n                         prev_tokens=seq.tokens,\n                         prefix_offset=seq.prefix_offset,\n                         read_offset=seq.read_offset,\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 2c34f2f5d..38143c4ce 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -72,6 +72,11 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n         tokenizer.all_special_tokens_extended)\n     tokenizer_all_special_tokens = set(tokenizer.all_special_tokens)\n     tokenizer_vocab = tokenizer.get_vocab()\n+    # Cache added vocab as it may be recomputed on each access otherwise.\n+    try:\n+        tokenizer_added_vocab = tokenizer.get_added_vocab()\n+    except Exception:\n+        tokenizer_added_vocab = {}\n     tokenizer_len = len(tokenizer)\n \n     max_token_id = max(tokenizer_vocab.values())\n@@ -107,6 +112,10 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n         def __len__(self):\n             return tokenizer_len\n \n+        # Cache get_added_vocab to avoid repeated reconstruction.\n+        def get_added_vocab(self):  # type: ignore[override]\n+            return tokenizer_added_vocab\n+\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     tokenizer.__class__ = CachedTokenizer\n", "model_name_or_path": "gpt-5-2025-08-07"}
