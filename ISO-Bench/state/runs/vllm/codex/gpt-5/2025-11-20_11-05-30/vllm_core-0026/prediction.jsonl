{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-067c34a", "model_patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 24ddd35ab..076eb9be9 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -50,11 +50,14 @@ def decode_tokens(\n     `skip_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n+    # Prefer fast/private decode path when available to avoid extra\n+    # list conversions or Python-side overhead in some tokenizers.\n+    decode_method = getattr(tokenizer, \"_decode\", tokenizer.decode)\n     if skip_special_tokens is not None:\n-        return tokenizer.decode(token_ids,\n-                                skip_special_tokens=skip_special_tokens)\n+        return decode_method(token_ids,\n+                             skip_special_tokens=skip_special_tokens)\n \n-    return tokenizer.decode(token_ids)\n+    return decode_method(token_ids)\n \n \n def encode_tokens(\n@@ -73,6 +76,14 @@ def encode_tokens(\n     settings.\n     \"\"\"\n \n+    # Prefer fast/private encode path when available.\n+    encode_method = getattr(tokenizer, \"_encode\", tokenizer.encode)\n+\n+    # Fast path: no optional args provided.\n+    if truncation is None and max_length is None \\\n+            and add_special_tokens is None:\n+        return encode_method(text)\n+\n     kw_args: dict[str, Any] = {}\n     if max_length is not None:\n         kw_args[\"max_length\"] = max_length\n@@ -83,7 +94,7 @@ def encode_tokens(\n     if add_special_tokens is not None:\n         kw_args[\"add_special_tokens\"] = add_special_tokens\n \n-    return tokenizer.encode(text, **kw_args)\n+    return encode_method(text, **kw_args)\n \n \n def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n@@ -137,6 +148,16 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n         def __reduce__(self):\n             return get_cached_tokenizer, (tokenizer, )\n \n+        # Override encode/decode to prefer faster private methods if present\n+        # while preserving the original tokenizer's behavior.\n+        def decode(self, token_ids, *args, **kwargs):  # type: ignore[override]\n+            _decode = getattr(tokenizer, \"_decode\", tokenizer.decode)\n+            return _decode(token_ids, *args, **kwargs)\n+\n+        def encode(self, text, *args, **kwargs):  # type: ignore[override]\n+            _encode = getattr(tokenizer, \"_encode\", tokenizer.encode)\n+            return _encode(text, *args, **kwargs)\n+\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     cached_tokenizer.__class__ = CachedTokenizer\n", "model_name_or_path": "gpt-5-2025-08-07"}
