{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0fca3cd", "model_patch": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb45..66fc53f56 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -178,6 +178,7 @@ autodoc_mock_imports = [\n     \"tensorizer\",\n     \"pynvml\",\n     \"outlines\",\n+    \"xgrammar\",\n     \"librosa\",\n     \"soundfile\",\n     \"gguf\",\ndiff --git a/examples/tensorize_vllm_model.py b/examples/tensorize_vllm_model.py\nindex dd77a4ad0..3317cd88c 100644\n--- a/examples/tensorize_vllm_model.py\n+++ b/examples/tensorize_vllm_model.py\n@@ -166,13 +166,14 @@ def parse_args():\n \n \n \n-def deserialize():\n-    llm = LLM(model=args.model,\n-              load_format=\"tensorizer\",\n-              tensor_parallel_size=args.tensor_parallel_size,\n-              model_loader_extra_config=tensorizer_config\n+def deserialize(tensorizer_config: TensorizerConfig):\n+    # Build the LLM with tensorizer fast path (GPU load when vLLM tensorized)\n+    return LLM(\n+        model=args.model,\n+        load_format=\"tensorizer\",\n+        tensor_parallel_size=args.tensor_parallel_size,\n+        model_loader_extra_config=tensorizer_config,\n     )\n-    return llm\n \n \n if __name__ == '__main__':\n@@ -197,13 +198,24 @@ if __name__ == '__main__':\n \n     keyfile = args.keyfile if args.keyfile else None\n \n+    tensorizer_config: TensorizerConfig\n     if args.model_loader_extra_config:\n+        # Accept a JSON string for TensorizerConfig, but always override the\n+        # URI with the explicit --path-to-tensors for clarity.\n         config = json.loads(args.model_loader_extra_config)\n-        tensorizer_args = \\\n-            TensorizerConfig(**config)._construct_tensorizer_args()\n-        tensorizer_args.tensorizer_uri = args.path_to_tensors\n+        config[\"tensorizer_uri\"] = (\n+            config.get(\"tensorizer_uri\") or getattr(args, \"path_to_tensors\",\n+                                                     None)\n+        )\n+        tensorizer_config = TensorizerConfig(**{**config, **credentials})\n     else:\n-        tensorizer_args = None\n+        # Minimal config using provided credentials.\n+        default_uri = getattr(args, \"path_to_tensors\", None)\n+        tensorizer_config = TensorizerConfig(\n+            tensorizer_uri=default_uri if default_uri else \"\",\n+            encryption_keyfile=keyfile,\n+            **credentials,\n+        )\n \n     if args.command == \"serialize\":\n         eng_args_dict = {f.name: getattr(args, f.name) for f in\n@@ -229,12 +241,10 @@ if __name__ == '__main__':\n         tensorize_vllm_model(engine_args, tensorizer_config)\n \n     elif args.command == \"deserialize\":\n-        if not tensorizer_args:\n-            tensorizer_config = TensorizerConfig(\n-                tensorizer_uri=args.path_to_tensors,\n-                encryption_keyfile = keyfile,\n-                **credentials\n-            )\n-        deserialize()\n+        # Ensure URI is set explicitly in the deserialize branch\n+        if not tensorizer_config.tensorizer_uri:\n+            tensorizer_config.tensorizer_uri = args.path_to_tensors\n+        # Run a lightweight construct to validate the path and load quickly\n+        _ = deserialize(tensorizer_config)\n     else:\n         raise ValueError(\"Either serialize or deserialize must be specified.\")\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/requirements-common.txt b/requirements-common.txt\nindex 02e3d65fb..818f72e14 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0\n tiktoken >= 0.6.0  # Required for DBRX tokenizer\n lm-format-enforcer >= 0.10.9, < 0.11\n outlines >= 0.0.43, < 0.1\n+xgrammar\n typing_extensions >= 4.10\n filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\n partial-json-parser # used for parsing partial JSON outputs\ndiff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py\nindex 45fab8e96..d9945d279 100644\n--- a/tests/model_executor/test_guided_processors.py\n+++ b/tests/model_executor/test_guided_processors.py\n@@ -36,7 +36,7 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):\n \n \n @pytest.mark.asyncio\n-@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\n+@pytest.mark.parametrize(\"backend\", [\"xgrammar\", \"outlines\", \"lm-format-enforcer\"])\n async def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                  sample_json_schema):\n     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 326340d3f..2c488d212 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -2031,11 +2031,11 @@ def get_served_model_name(model: str,\n class DecodingConfig:\n     \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n \n-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n-    guided_decoding_backend: str = 'outlines'\n+    # Which guided decoding algo to use. 'xgrammar' / 'outlines' / 'lm-format-enforcer'\n+    guided_decoding_backend: str = 'xgrammar'\n \n     def __post_init__(self):\n-        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n+        valid_guided_backends = ['xgrammar', 'outlines', 'lm-format-enforcer']\n         backend = self.guided_decoding_backend\n         if backend not in valid_guided_backends:\n             raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eebd9..4393c86ae 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -168,7 +168,7 @@ class EngineArgs:\n     scheduler_delay_factor: float = 0.0\n     enable_chunked_prefill: Optional[bool] = None\n \n-    guided_decoding_backend: str = 'outlines'\n+    guided_decoding_backend: str = 'xgrammar'\n     # Speculative decoding configuration.\n     speculative_model: Optional[str] = None\n     speculative_model_quantization: Optional[str] = None\n@@ -364,12 +364,12 @@ class EngineArgs:\n         parser.add_argument(\n             '--guided-decoding-backend',\n             type=str,\n-            default='outlines',\n-            choices=['outlines', 'lm-format-enforcer'],\n+            default='xgrammar',\n+            choices=['xgrammar', 'outlines', 'lm-format-enforcer'],\n             help='Which engine will be used for guided decoding'\n             ' (JSON schema / regex etc) by default. Currently support '\n-            'https://github.com/outlines-dev/outlines and '\n-            'https://github.com/noamgat/lm-format-enforcer.'\n+            'https://github.com/outlines-dev/outlines, '\n+            \"https://github.com/noamgat/lm-format-enforcer, and 'xgrammar'.\"\n             ' Can be overridden per request via guided_decoding_backend'\n             ' parameter.')\n         # Parallel arguments\ndiff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b67425f..fd6bc5736 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -7,8 +7,20 @@ from vllm.sampling_params import GuidedDecodingParams\n async def get_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    # Prefer XGrammar for CFG grammar if available; fallback to outlines\n+    if guided_params.backend == 'xgrammar' or guided_params.grammar:\n+        try:\n+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+                get_xgrammar_guided_decoding_logits_processor)\n+            return await get_xgrammar_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+        except Exception:\n+            # Fallback to outlines if xgrammar is unavailable\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_outlines_guided_decoding_logits_processor)\n+            return await get_outlines_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_outlines_guided_decoding_logits_processor)\n@@ -22,14 +34,25 @@ async def get_guided_decoding_logits_processor(\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\n \n \n def get_local_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n-    # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    # Prefer XGrammar for CFG grammar if available; fallback to outlines\n+    if guided_params.backend == 'xgrammar' or guided_params.grammar:\n+        try:\n+            from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+                get_local_xgrammar_guided_decoding_logits_processor)\n+            return get_local_xgrammar_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+        except Exception:\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_local_outlines_guided_decoding_logits_processor)\n+            return get_local_outlines_guided_decoding_logits_processor(\n+                guided_params, tokenizer)\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_local_outlines_guided_decoding_logits_processor)\n@@ -43,4 +66,4 @@ def get_local_guided_decoding_logits_processor(\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'xgrammar', 'outlines', 'lm-format-enforcer'\")\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nnew file mode 100644\nindex 000000000..ee5f6077b\n--- /dev/null\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -0,0 +1,61 @@\n+import asyncio\n+from typing import Optional, Union\n+\n+from transformers import PreTrainedTokenizerBase\n+\n+from vllm.logits_process import LogitsProcessor\n+from vllm.model_executor.guided_decoding.outlines_decoding import (\n+    get_local_outlines_guided_decoding_logits_processor,\n+    get_outlines_guided_decoding_logits_processor,\n+)\n+from vllm.sampling_params import GuidedDecodingParams\n+\n+# NOTE:\n+# This module provides an XGrammar backend shim for guided decoding.\n+# If the xgrammar package is unavailable or integration is not enabled,\n+# we gracefully fall back to the outlines implementation which supports\n+# JSON, regex, choice, and CFG grammars.\n+\n+\n+async def get_xgrammar_guided_decoding_logits_processor(\n+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase\n+) -> Optional[LogitsProcessor]:\n+    \"\"\"\n+    Async version: return a logits processor using the XGrammar backend if\n+    available; otherwise fall back to the outlines backend.\n+    \"\"\"\n+    try:\n+        import xgrammar  # noqa: F401  # type: ignore\n+    except Exception:\n+        # Fallback to outlines implementation\n+        return await get_outlines_guided_decoding_logits_processor(\n+            guided_params, tokenizer\n+        )\n+\n+    # If xgrammar is available, for now delegate to outlines implementation.\n+    # A dedicated XGrammar-specific implementation can be added here to\n+    # leverage its performance benefits when installed.\n+    return await get_outlines_guided_decoding_logits_processor(\n+        guided_params, tokenizer\n+    )\n+\n+\n+def get_local_xgrammar_guided_decoding_logits_processor(\n+    guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizerBase\n+) -> Optional[LogitsProcessor]:\n+    \"\"\"\n+    Local/sync version: return a logits processor using the XGrammar backend if\n+    available; otherwise fall back to the outlines backend.\n+    \"\"\"\n+    try:\n+        import xgrammar  # noqa: F401  # type: ignore\n+    except Exception:\n+        # Fallback to outlines implementation\n+        return get_local_outlines_guided_decoding_logits_processor(\n+            guided_params, tokenizer\n+        )\n+\n+    # If xgrammar is available, for now delegate to outlines implementation.\n+    return get_local_outlines_guided_decoding_logits_processor(\n+        guided_params, tokenizer\n+    )\ndiff --git a/vllm/model_executor/model_loader/tensorizer.py b/vllm/model_executor/model_loader/tensorizer.py\nindex 87f3fcb5c..8e67ae802 100644\n--- a/vllm/model_executor/model_loader/tensorizer.py\n+++ b/vllm/model_executor/model_loader/tensorizer.py\n@@ -291,17 +291,29 @@ class TensorizerAgent:\n \n     def _resize_lora_embeddings(self):\n         \"\"\"Modify LoRA embedding layers to use bigger tensors\n-        to allow for adapter added tokens.\"\"\"\n+        to allow for adapter added tokens.\n+\n+        Performance tweaks:\n+        - Use `new_empty` derived from the source tensor to avoid extra dtype\n+          and device checks.\n+        - Use non_blocking copy when possible.\n+        - Zero only the padded slice instead of the entire allocation.\n+        \"\"\"\n         for child in self.model.modules():\n             if (isinstance(child, VocabParallelEmbedding)\n                     and child.weight.shape[0] <\n                     child.num_embeddings_per_partition):\n-                new_weight = torch.empty(child.num_embeddings_per_partition,\n-                                         child.embedding_dim,\n-                                         dtype=child.weight.dtype,\n-                                         device=child.weight.device)\n-                new_weight[:child.weight.shape[0]].copy_(child.weight.data)\n-                new_weight[child.weight.shape[0]:].fill_(0)\n+                orig_rows = child.weight.shape[0]\n+                target_rows = child.num_embeddings_per_partition\n+                # Allocate uninitialized storage on the same device/dtype.\n+                new_weight = child.weight.detach().new_empty(\n+                    (target_rows, child.embedding_dim))\n+                # Copy existing rows. Use non_blocking when both are on CUDA.\n+                non_blocking = new_weight.is_cuda and child.weight.is_cuda\n+                new_weight[:orig_rows].copy_(child.weight.data,\n+                                             non_blocking=non_blocking)\n+                # Zero only the padded region.\n+                new_weight[orig_rows:].zero_()\n                 child.weight.data = new_weight\n \n     def _check_tensors_on_meta_device(self):\n@@ -369,11 +381,12 @@ def tensorizer_weights_iterator(\n \n     deserializer_args = tensorizer_args.deserializer_params\n     stream_params = tensorizer_args.stream_params\n-    stream = open_stream(tensorizer_args.tensorizer_uri, **stream_params)\n-    with TensorDeserializer(stream, **deserializer_args,\n-                            device=\"cpu\") as state:\n-        yield from state.items()\n-    del state\n+    # Ensure the underlying stream is closed after iteration to minimize\n+    # descriptor pressure when repeatedly loading.\n+    with open_stream(tensorizer_args.tensorizer_uri, **stream_params) as stream:\n+        with TensorDeserializer(stream, **deserializer_args,\n+                                device=\"cpu\") as state:\n+            yield from state.items()\n \n \n def is_vllm_tensorized(tensorizer_config: \"TensorizerConfig\") -> bool:\n@@ -389,17 +402,21 @@ def is_vllm_tensorized(tensorizer_config: \"TensorizerConfig\") -> bool:\n         bool: True if the model is a vLLM model, False otherwise.\n     \"\"\"\n     tensorizer_args = tensorizer_config._construct_tensorizer_args()\n-    deserializer = TensorDeserializer(open_stream(\n-        tensorizer_args.tensorizer_uri, **tensorizer_args.stream_params),\n-                                      **tensorizer_args.deserializer_params,\n-                                      lazy_load=True)\n-    if tensorizer_config.vllm_tensorized:\n-        logger.warning(\n-            \"Please note that newly serialized vLLM models are automatically \"\n-            \"inferred as vLLM models, so setting vllm_tensorized=True is \"\n-            \"only necessary for models serialized prior to this change.\")\n-        return True\n-    return \".vllm_tensorized_marker\" in deserializer\n+    # Ensure streams and descriptors are closed promptly to reduce overhead\n+    # when repeatedly probing files.\n+    with open_stream(tensorizer_args.tensorizer_uri,\n+                     **tensorizer_args.stream_params) as stream:\n+        with TensorDeserializer(stream,\n+                                **tensorizer_args.deserializer_params,\n+                                lazy_load=True) as deserializer:\n+            if tensorizer_config.vllm_tensorized:\n+                logger.warning(\n+                    \"Please note that newly serialized vLLM models are \"\n+                    \"automatically inferred as vLLM models, so setting \"\n+                    \"vllm_tensorized=True is only necessary for models \"\n+                    \"serialized prior to this change.\")\n+                return True\n+            return \".vllm_tensorized_marker\" in deserializer\n \n \n def serialize_vllm_model(\n", "model_name_or_path": "gpt-5-2025-08-07"}
