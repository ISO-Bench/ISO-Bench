{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f721096", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex e93143c83..460a471de 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -174,12 +174,17 @@ class SchedulerOutputs:\n     def _sort_by_lora_ids(self):\n         assert 0 <= self.num_prefill_groups <= len(self.scheduled_seq_groups)\n \n+        # Hoist repeated checks out of key function for efficiency\n+        need_prefill_first = (0 < self.num_prefill_groups <\n+                              len(self.scheduled_seq_groups))\n+\n         def key_fn(group: ScheduledSequenceGroup):\n-            key = (group.seq_group.lora_int_id, group.seq_group.request_id)\n-            if 0 < self.num_prefill_groups < len(self.scheduled_seq_groups):\n+            sg = group.seq_group\n+            key = (sg.lora_int_id, sg.request_id)\n+            if need_prefill_first:\n                 # Sort sequence groups so that all prefills come before all\n                 # decodes as required by chunked prefill.\n-                return (not group.seq_group.is_prefill(), *key)\n+                return (not sg.is_prefill(), *key)\n             return key\n \n         self.scheduled_seq_groups = sorted(self.scheduled_seq_groups,\n@@ -1075,7 +1080,10 @@ class Scheduler:\n         waiting_queue = self.waiting\n \n         leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n-        while self._passed_delay(time.time()) and waiting_queue:\n+        # Check delay condition once per scheduling call to avoid repeated\n+        # system clock queries inside the loop.\n+        passed_delay = self._passed_delay(time.time())\n+        while passed_delay and waiting_queue:\n             seq_group = waiting_queue[0]\n \n             waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n@@ -1837,11 +1845,12 @@ class Scheduler:\n         self.prev_time, self.prev_prompt = now, False\n         # Delay scheduling prompts to let waiting queue fill up\n         if self.scheduler_config.delay_factor > 0 and self.waiting:\n-            earliest_arrival_time = min(\n-                [e.metrics.arrival_time for e in self.waiting])\n-            passed_delay = ((now - earliest_arrival_time)\n-                            > (self.scheduler_config.delay_factor *\n-                               self.last_prompt_latency) or not self.running)\n+            # Use a generator to avoid constructing a temporary list\n+            earliest_arrival_time = min(e.metrics.arrival_time\n+                                        for e in self.waiting)\n+            passed_delay = ((now - earliest_arrival_time) >\n+                            (self.scheduler_config.delay_factor *\n+                             self.last_prompt_latency) or not self.running)\n         else:\n             passed_delay = True\n         return passed_delay\ndiff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 64c2dac52..39833c3f2 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -161,8 +161,10 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm import _custom_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != query.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                       dtype=query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n@@ -184,8 +186,10 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm._ipex_ops import ipex_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != positions.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n+                                                       dtype=query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n@@ -290,8 +294,10 @@ class RotaryEmbedding(CustomOp):\n         if offsets is not None:\n             positions = positions + offsets\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                   dtype=query.dtype)\n+        if self.cos_sin_cache.device != query.device or \\\n+            self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                       dtype=query.dtype)\n \n         positions = positions.flatten()\n         num_tokens = positions.shape[0]\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b55854de..b9a4df689 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -200,7 +200,6 @@ from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n import torch\n from compressed_tensors.quantization import QuantizationStrategy\n \n-from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionBackend, AttentionLayer,\n                                               AttentionMetadata,\n@@ -222,8 +221,6 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n     apply_fp8_linear_generic, current_platform_fp8_dtype, is_fp8)\n from vllm.model_executor.layers.quantization.utils.quant_utils import (\n     scaled_quantize)\n-from vllm.model_executor.layers.rotary_embedding import (\n-    DeepseekScalingRotaryEmbedding, RotaryEmbedding)\n from vllm.utils import cdiv, round_down\n \n try:\n@@ -237,9 +234,13 @@ if TYPE_CHECKING:\n     from vllm.v1.worker.gpu_input_batch import InputBatch\n     from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n \n+    from vllm.model_executor.layers.rotary_embedding import (\n+        DeepseekScalingRotaryEmbedding, RotaryEmbedding)\n+\n logger = init_logger(__name__)\n \n \n+\n class MLACommonBackend(AttentionBackend):\n \n     accept_output_buffer: bool = True\n@@ -532,14 +533,11 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=torch.nn.functional.pad(\n+                        _chunk_cu_seq_lens, (1, 0, 0, 0)),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -605,7 +603,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         qk_rope_head_dim: int,\n         qk_head_dim: int,\n         v_head_dim: int,\n-        rotary_emb: RotaryEmbedding,\n+        rotary_emb: \"RotaryEmbedding\",\n         # q_proj should be q_b_proj if q_lora_rank is not None, but from an\n         # attention backend perspective we rely on the layer to pass in the\n         # correct matrix\n@@ -627,6 +625,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         self.v_head_dim = v_head_dim\n \n         self.rotary_emb = rotary_emb\n+        from vllm.model_executor.layers.rotary_embedding import DeepseekScalingRotaryEmbedding\n         self.use_yarn_rope = isinstance(rotary_emb,\n                                         DeepseekScalingRotaryEmbedding)\n         self.q_proj = q_proj\n@@ -864,6 +863,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         for i in range(iters):\n             toks = prefill_metadata.chunked_context.seq_tot[i]\n \n+            from vllm import _custom_ops as ops\n             ops.gather_cache(\n                 src_cache=kv_c_and_k_pe_cache,\n                 dst=workspace,\n@@ -1054,6 +1054,7 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n \n         # write the latent and rope to kv cache\n         if kv_cache.numel() > 0:\n+            from vllm import _custom_ops as ops\n             ops.concat_and_cache_mla(\n                 k_c_normed,\n                 k_pe.squeeze(1),\n", "model_name_or_path": "gpt-5-2025-08-07"}
