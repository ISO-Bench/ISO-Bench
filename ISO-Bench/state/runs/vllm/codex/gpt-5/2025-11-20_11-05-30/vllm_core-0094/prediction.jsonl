{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-8936316", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca66a..ad26d3c51 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -336,9 +336,9 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n         # Assign the self-attention block tables for each sequence.\n         if len(wait_seqs) == 1:\n-            self.block_tables[wait_seqs[0].seq_id] = block_table\n+            self.block_tables[seq.seq_id] = block_table\n         else:\n-            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n+            for seq in wait_seqs:\n                 self.block_tables[seq.seq_id] = block_table.copy()\n \n         # Allocate encoder sequence\ndiff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex 4a6825c01..c83b215ac 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -119,6 +119,9 @@ class RayGPUExecutor(DistributedGPUExecutor):\n         driver_ip = get_ip()\n         logger.info(\"driver_ip: %s\", driver_ip)\n         worker_wrapper_kwargs = self._get_worker_wrapper_args()\n+\n+        # First create all workers to minimize per-actor control plane cost.\n+        created_workers: List[RayWorkerWrapper] = []\n         for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n             if not bundle.get(\"GPU\", 0):\n                 continue\n@@ -134,20 +137,36 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 scheduling_strategy=scheduling_strategy,\n                 **ray_remote_kwargs,\n             )(RayWorkerWrapper).remote(**worker_wrapper_kwargs)\n+            created_workers.append(worker)\n \n-            if self.use_ray_spmd_worker:\n-                self.workers.append(worker)\n+        # Classify workers once with a single batched IP lookup.\n+        if self.use_ray_spmd_worker:\n+            self.workers = created_workers\n+        else:\n+            # Fetch all IPs in one go.\n+            ip_refs = [w.get_node_ip.remote() for w in created_workers]\n+            ips: List[str] = ray.get(ip_refs)\n+            # Pick the first worker colocated with the driver as the dummy.\n+            driver_idx: Optional[int] = None\n+            for idx, ip in enumerate(ips):\n+                if ip == driver_ip:\n+                    driver_idx = idx\n+                    break\n+            if driver_idx is not None:\n+                self.driver_dummy_worker = created_workers[driver_idx]\n+                self.driver_worker = RayWorkerWrapper(**worker_wrapper_kwargs)\n+                # All other workers become remote workers.\n+                self.workers = [\n+                    w for i, w in enumerate(created_workers) if i != driver_idx\n+                ]\n+                # Keep a parallel list of IPs for the remote workers only.\n+                worker_ips = [\n+                    ip for i, ip in enumerate(ips) if i != driver_idx\n+                ]\n             else:\n-                worker_ip = ray.get(worker.get_node_ip.remote())\n-                if worker_ip == driver_ip and self.driver_dummy_worker is None:\n-                    # If the worker is on the same node as the driver, we use it\n-                    # as the resource holder for the driver process.\n-                    self.driver_dummy_worker = worker\n-                    self.driver_worker = RayWorkerWrapper(\n-                        **worker_wrapper_kwargs)\n-                else:\n-                    # Else, added to the list of workers.\n-                    self.workers.append(worker)\n+                # No GPU on driver node; keep all as remote workers.\n+                self.workers = created_workers\n+                worker_ips = ips\n \n         logger.debug(\"workers: %s\", self.workers)\n         logger.debug(\"driver_dummy_worker: %s\", self.driver_dummy_worker)\n@@ -157,31 +176,22 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 \"adjusting the Ray placement group or running the driver on a \"\n                 \"GPU node.\")\n \n-        worker_ips = [\n-            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n-            for worker in self.workers\n-        ]\n+        # Prepare counts and sort using already-fetched IPs without extra RPCs.\n         ip_counts: Dict[str, int] = {}\n         for ip in worker_ips:\n             ip_counts[ip] = ip_counts.get(ip, 0) + 1\n \n-        def sort_by_driver_then_worker_ip(worker):\n-            \"\"\"\n-            Sort the workers based on 3 properties:\n-            1. If the worker is on the same node as the driver (vllm engine),\n-                it should be placed first.\n-            2. Then, if the worker is on a node with fewer workers, it should\n-                be placed first.\n-            3. Finally, if the work is on a node with smaller IP address, it\n-                should be placed first.\n-            \"\"\"\n-            ip = ray.get(worker.get_node_ip.remote())\n+        # Pair workers with their IPs to sort deterministically.\n+        worker_ip_pairs = list(zip(self.workers, worker_ips))\n+\n+        def sort_key(pair: Tuple[RayWorkerWrapper, str]):\n+            ip = pair[1]\n             return (ip != driver_ip, ip_counts[ip], ip)\n \n-        # After sorting, the workers on the same node will be\n-        # close to each other, and the workers on the driver\n-        # node will be placed first.\n-        self.workers = sorted(self.workers, key=sort_by_driver_then_worker_ip)\n+        # After sorting, colocated workers are grouped and driver-node workers\n+        # come first.\n+        worker_ip_pairs.sort(key=sort_key)\n+        self.workers = [w for (w, _) in worker_ip_pairs]\n \n         # Get the set of GPU IDs used on each node.\n         worker_node_and_gpu_ids = self._run_workers(\"get_node_and_gpu_ids\",\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477efc5..963e37b94 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -655,15 +655,26 @@ class SequenceGroup:\n         return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n+        if self.is_single_seq:\n+            return self.seqs if self.seqs[0].is_finished() else []\n+\n         return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n+        if self.is_single_seq:\n+            seq = self.seqs[0]\n+            if not seq.is_finished():\n+                seq.data.update_num_computed_tokens(num_new_computed_tokens)\n+            return\n         for seq in self.seqs:\n             if not seq.is_finished():\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n     def get_num_uncomputed_tokens(self) -> int:\n+        if self.is_single_seq:\n+            seq = self.seqs[0]\n+            return 0 if seq.is_finished() else seq.data.get_num_uncomputed_tokens()\n         num_uncomputed_tokens = 0\n         for seq in self.seqs:\n             if not seq.is_finished():\n", "model_name_or_path": "gpt-5-2025-08-07"}
