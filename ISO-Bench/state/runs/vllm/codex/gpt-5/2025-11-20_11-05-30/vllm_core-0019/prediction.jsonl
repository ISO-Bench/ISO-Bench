{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-733e7c9", "model_patch": "diff --git a/tests/v1/sample/test_topk_topp_sampler.py b/tests/v1/sample/test_topk_topp_sampler.py\nnew file mode 100644\nindex 000000000..7bf28ac4b\n--- /dev/null\n+++ b/tests/v1/sample/test_topk_topp_sampler.py\n@@ -0,0 +1,43 @@\n+# SPDX-License-Identifier: Apache-2.0\n+import torch\n+\n+from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p\n+\n+\n+def _apply_top_k_top_p_slow(logits: torch.Tensor, k: torch.Tensor | None,\n+                            p: torch.Tensor | None) -> torch.Tensor:\n+    if k is None and p is None:\n+        return logits\n+    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n+\n+    if k is not None:\n+        top_k_mask = logits_sort.size(1) - k.to(torch.long)\n+        top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n+        top_k_mask = logits_sort < top_k_mask\n+        logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n+\n+    if p is not None:\n+        probs_sort = logits_sort.softmax(dim=-1)\n+        probs_sum = probs_sort.cumsum(dim=-1)\n+        top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n+        top_p_mask[:, -1] = False\n+        logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n+\n+    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    return logits\n+\n+\n+def test_topk_only_equivalence_cpu():\n+    torch.manual_seed(0)\n+    B = 64\n+    V = 4096\n+    logits = torch.randn(B, V)\n+    k = torch.randint(1, 64, (B, ))\n+    # Disable top-k for ~50% of requests by setting k=vocab size\n+    mask = torch.rand(B) < 0.5\n+    k = k.masked_fill(mask, V)\n+\n+    fast = apply_top_k_top_p(logits.clone(), k, None)\n+    slow = _apply_top_k_top_p_slow(logits.clone(), k, None)\n+    assert torch.equal(fast, slow)\n+\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea71187..df8bb7e32 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -115,11 +115,17 @@ class TopKTopPSampler(nn.Module):\n         # If only top-k is specified, use pytorch's builtin topk op. This leads\n         # to significant speed up on TPU compared to using apply_top_k_top_p.\n         if k is not None and p is None:\n-            topk_values, topk_indices = torch.topk(logits, k, dim=-1)\n-\n-            mask = torch.ones_like(logits, dtype=torch.bool)\n-            mask.scatter_(-1, topk_indices, False)\n-            logits.masked_fill_(mask, float('-inf'))\n+            # Use threshold masking to avoid allocating a full boolean mask.\n+            # This significantly reduces memory bandwidth on large vocabularies.\n+            if k.numel() == 1:\n+                topk_values, _ = torch.topk(logits, int(k.item()), dim=-1)\n+                thresh = topk_values[..., -1:]\n+                logits.masked_fill_(logits < thresh, float('-inf'))\n+            else:\n+                k_max = int(k.max().item())\n+                topk_values, _ = torch.topk(logits, k_max, dim=-1)\n+                thresh = torch.gather(topk_values, -1, (k - 1).unsqueeze(-1))\n+                logits.masked_fill_(logits < thresh, float('-inf'))\n         else:\n             # TODO Placeholder for TPU optimized topp kernel\n             # logits = apply_top_k_top_p(logits, k, p)\n@@ -140,6 +146,30 @@ def apply_top_k_top_p(\n     \"\"\"\n     if k is None and p is None:\n         return logits\n+\n+    # Fast path: top-k only. Avoid full sort by computing a per-row\n+    # threshold using topk and masking values below the threshold.\n+    if k is not None and p is None:\n+        vocab_size = logits.size(1)\n+        k_long = k.to(torch.long)\n+        # Clamp to valid range for safety.\n+        k_long = k_long.clamp_(min=1, max=vocab_size)\n+        if k_long.numel() == 1:\n+            # Single k for the whole batch.\n+            topk_vals, _ = torch.topk(logits, int(k_long.item()), dim=-1)\n+            thresh = topk_vals[..., -1:].detach()\n+            logits.masked_fill_(logits < thresh, -float(\"inf\"))\n+            return logits\n+        else:\n+            # Per-row k. Use the maximum k to compute a shared topk and\n+            # gather the per-row threshold from it.\n+            k_max = int(k_long.max().item())\n+            topk_vals, _ = torch.topk(logits, k_max, dim=-1)\n+            gather_idx = (k_long - 1).clamp_min(0).unsqueeze(-1)\n+            thresh = torch.gather(topk_vals, dim=-1, index=gather_idx).detach()\n+            logits.masked_fill_(logits < thresh, -float(\"inf\"))\n+            return logits\n+\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n     if k is not None:\n@@ -159,8 +189,9 @@ def apply_top_k_top_p(\n         top_p_mask[:, -1] = False\n         logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n-    # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    # Re-sort the probabilities back to the original order.\n+    # Scatter directly into `logits` to reuse its storage.\n+    logits.scatter_(dim=-1, index=logits_idx, src=logits_sort)\n     return logits\n \n \ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049dc..83b4a1cde 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -212,8 +212,8 @@ class Sampler(nn.Module):\n         adjusted_min_p = min_p.unsqueeze(1) * max_probabilities\n         # Identify valid tokens using threshold comparison\n         valid_token_mask = probability_values >= adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[~valid_token_mask] = -float('inf')\n+        # Apply mask in-place to avoid indexing materialization\n+        logits.masked_fill_(~valid_token_mask, -float('inf'))\n         return logits\n \n     def apply_logits_bias(\n", "model_name_or_path": "gpt-5-2025-08-07"}
