diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py
index 5f0eb0019..37cedff48 100644
--- a/vllm/worker/neuron_worker.py
+++ b/vllm/worker/neuron_worker.py
@@ -1,9 +1,6 @@
 # SPDX-License-Identifier: Apache-2.0
 """A Neuron worker class."""
-from typing import List, Optional, Tuple
-
-import torch
-import torch.distributed
+from typing import List, Optional, Tuple, TYPE_CHECKING
 
 from vllm.config import VllmConfig
 from vllm.distributed import (ensure_model_parallel_initialized,
@@ -16,6 +13,9 @@ from vllm.worker.worker_base import (LocalOrDistributedWorkerBase,
                                      LoraNotSupportedWorkerBase, WorkerBase,
                                      WorkerInput)
 
+if TYPE_CHECKING:  # Avoid importing torch at runtime for non-essential paths
+    import torch  # type: ignore
+
 
 class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
     """A worker class that executes the model on a group of neuron cores.
@@ -71,12 +71,13 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
 
         Swapping is not yet supported, so always return num_cpu_blocks=0.
 
-        We configure num_gpu_blocks to be equal to max_num_seqs.
+        We configure num_gpu_blocks to be equal to max_num_seqs + 1.
         """
         # Set the number of GPU blocks to be the same as the maximum number of
         # sequences that can be processed in a single batch. This is equivalent
         # to schedule without PagedAttention.
-        num_gpu_blocks = self.scheduler_config.max_num_seqs
+        max_num_seqs = self.scheduler_config.max_num_seqs
+        num_gpu_blocks = max_num_seqs + 1
 
         # Swap not yet supported with Neuron backend.
         num_cpu_blocks = 0
@@ -90,7 +91,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
 
         # Different values are not tested.
         assert num_cpu_blocks == 0
-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs
+        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1
 
         self.cache_config.num_gpu_blocks = num_gpu_blocks
         self.cache_config.num_cpu_blocks = num_cpu_blocks
@@ -100,14 +101,14 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):
         return False
 
     @property
-    def kv_cache(self) -> Optional[List[List[torch.Tensor]]]:
+    def kv_cache(self) -> Optional[List[List["torch.Tensor"]]]:
         return None
 
-    @torch.inference_mode()
     def prepare_worker_input(
             self, execute_model_req: ExecuteModelRequest) -> WorkerInput:
-        return WorkerInput(num_seq_groups=len(
-            execute_model_req.seq_group_metadata_list), )
+        # Micro-optimization: minimize attribute lookups
+        num_seq_groups = len(execute_model_req.seq_group_metadata_list)
+        return WorkerInput(num_seq_groups=num_seq_groups)
 
     def execute_worker(self, worker_input: WorkerInput) -> None:
         pass
