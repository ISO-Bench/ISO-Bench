diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
index b53a540ed..0f0c05bca 100644
--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
@@ -466,10 +466,15 @@ class MambaMixer2(CustomOp):
         if has_prefill:
 
             initial_states = None
-            if has_initial_states is not None and any(has_initial_states):
-                for idx in mamba_cache_params.state_indices_tensor[
-                        ~has_initial_states]:
-                    mamba_cache_params.ssm_state[idx].zero_()
+            if has_initial_states is not None and torch.any(
+                    has_initial_states):
+                # Vectorized zero init for ssm_state on the subset without
+                # initial states to avoid per-index kernel launches.
+                zero_indices = mamba_cache_params.state_indices_tensor[
+                    ~has_initial_states]
+                if zero_indices.numel() > 0:
+                    mamba_cache_params.ssm_state[zero_indices].zero_()
+                # Gather initial states in a single advanced-indexing op.
                 initial_states = mamba_cache_params.ssm_state[
                     mamba_cache_params.state_indices_tensor]
 
@@ -494,9 +499,10 @@ class MambaMixer2(CustomOp):
             )
 
             # update ssm states
-            # - varlen state is a (batch, nheads, headdim, dstate) tensor
-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):
-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])
+            # - varlen_state is (batch, nheads, headdim, dstate); perform
+            #   a single batched copy using advanced indexing.
+            mamba_cache_params.ssm_state[
+                mamba_cache_params.state_indices_tensor].copy_(varlen_state)
 
             # - reshape
             hidden_states = scan_output.view(seq_len, -1)
