{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-9bde5ba", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py\nindex 00c1a2911..a7fffe34e 100644\n--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py\n+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py\n@@ -163,7 +163,7 @@ def generate_continuous_batched_examples(example_lens_by_batch,\n \n         # get the metadata\n         cu_seqlens = torch.tensor((0, ) + spec, device=device).cumsum(dim=0)\n-        seq_idx = torch.zeros(cu_seqlens[-1],\n+        seq_idx = torch.empty(cu_seqlens[-1],\n                               dtype=torch.int32,\n                               device=cu_seqlens.device)\n         for i, (srt, end) in enumerate(zip(\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 36edac237..e1fa2ccb7 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -583,7 +583,11 @@ class MambaMixer2(MambaBase, CustomOp):\n                                                                1]\n                                  if has_prefill else None)\n \n-        ssd_output_list = []\n+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim\n+        ssd_output_combined = torch.empty(num_actual_tokens,\n+                                          feature_dim,\n+                                          device=hidden_states_B_C.device,\n+                                          dtype=hidden_states_B_C.dtype)\n \n         # Process prefill requests\n         if has_prefill:\n@@ -653,7 +657,13 @@ class MambaMixer2(MambaBase, CustomOp):\n             ssm_state[state_indices_tensor_p] = varlen_state\n \n             # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n+            prefill_flat = scan_output.view(num_prefill_tokens, -1)\n+            if envs.VLLM_USE_V1:\n+                start = num_decodes\n+            else:\n+                start = 0\n+            ssd_output_combined[start:start + num_prefill_tokens].copy_(\n+                prefill_flat)\n \n         # Process decode requests\n         if has_decode:\n@@ -699,18 +709,16 @@ class MambaMixer2(MambaBase, CustomOp):\n                 state_batch_indices=state_indices_tensor_d,\n             )\n \n+            decode_flat = hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n+                                         self.head_dim)\n             if envs.VLLM_USE_V1:\n-                ssd_output_list.insert(\n-                    0,\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n+                ssd_output_combined[:num_decodes].copy_(decode_flat)\n             else:\n-                ssd_output_list.append(\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n+                ssd_output_combined[num_prefill_tokens:num_prefill_tokens +\n+                                    num_decodes].copy_(decode_flat)\n \n         # Merge prefill and decode outputs before passing to gated MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n+        hidden_states = ssd_output_combined\n \n         # 4. gated MLP\n         # GatedRMSNorm internally applying SiLU to the gate\ndiff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py\nindex a4ded2b7a..27dcb4c11 100644\n--- a/vllm/model_executor/models/phi4flash.py\n+++ b/vllm/model_executor/models/phi4flash.py\n@@ -129,16 +129,16 @@ class SambaYAttention(nn.Module):\n \n         self.lambda_init = self.lambda_init_fn(layer_idx)\n         self.lambda_q1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k1 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_q2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.lambda_k2 = nn.Parameter(\n-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,\n+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,\n                                                                     std=0.1))\n         self.subln = nn.RMSNorm(2 * self.head_dim,\n                                 eps=1e-5,\ndiff --git a/vllm/model_executor/models/plamo2.py b/vllm/model_executor/models/plamo2.py\nindex 9bc577cfe..787ae1668 100644\n--- a/vllm/model_executor/models/plamo2.py\n+++ b/vllm/model_executor/models/plamo2.py\n@@ -257,7 +257,12 @@ class Plamo2MambaMixer(nn.Module):\n         query_start_loc_p = (attn_metadata.query_start_loc[:num_prefills + 1]\n                              if has_prefill else None)\n \n-        ssd_output_list = []\n+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim\n+        num_actual_tokens = num_prefill_tokens + num_decodes\n+        ssd_output_combined = torch.empty(num_actual_tokens,\n+                                          feature_dim,\n+                                          device=hidden_states.device,\n+                                          dtype=hidden_states.dtype)\n \n         # Process prefill requests\n         if has_prefill:\n@@ -319,7 +324,8 @@ class Plamo2MambaMixer(nn.Module):\n             mamba_cache_params.ssm_state[state_indices_tensor_p] = varlen_state\n \n             # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n+            prefill_flat = scan_output.view(num_prefill_tokens, -1)\n+            ssd_output_combined[:num_prefill_tokens].copy_(prefill_flat)\n \n         # Process decode requests\n         if has_decode:\n@@ -363,13 +369,14 @@ class Plamo2MambaMixer(nn.Module):\n                 dt_softplus=True,\n                 state_batch_indices=state_indices_tensor_d,\n             )\n-            assert self.num_heads % self.tp_size == 0\n-            ssd_output_list.append(\n-                hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                     self.head_dim))\n+            ssd_output_combined[num_prefill_tokens:num_prefill_tokens +\n+                                num_decodes].copy_(\n+                                    hidden_states_d.view(-1, (self.num_heads //\n+                                                             self.tp_size) *\n+                                                         self.head_dim))\n \n         # Merge prefill and decode outputs before passing to MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n+        hidden_states = ssd_output_combined\n \n         # 4. Final linear projection\n         out = self.out_proj(hidden_states)\ndiff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex ad9854dd2..9a3307152 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -84,8 +84,9 @@ class BlockPool:\n             The cached blocks if exists, or None.\n         \"\"\"\n         cached_blocks = []\n+        cache_map = self.cached_block_hash_to_block\n         for group_id in kv_cache_group_ids:\n-            cached_blocks_one_group = self.cached_block_hash_to_block.get(\n+            cached_blocks_one_group = cache_map.get(\n                 BlockHashWithGroupId(block_hash, group_id))\n             if not cached_blocks_one_group:\n                 return None\n@@ -180,8 +181,9 @@ class BlockPool:\n             block_hash_with_group_id = BlockHashWithGroupId(\n                 block_hash, kv_cache_group_id)\n             blk.block_hash = block_hash_with_group_id\n-            self.cached_block_hash_to_block[block_hash_with_group_id][\n-                blk.block_id] = blk\n+            cached = self.cached_block_hash_to_block\n+            blk_id = blk.block_id\n+            cached[block_hash_with_group_id][blk_id] = blk\n             if new_hashes is not None:\n                 new_hashes.append(block_hash.hash_value)\n             prev_block_hash_value = block_hash.hash_value\n@@ -214,7 +216,8 @@ class BlockPool:\n             raise ValueError(\n                 f\"Cannot get {num_blocks} free blocks from the pool\")\n \n-        ret: list[KVCacheBlock] = self.free_block_queue.popleft_n(num_blocks)\n+        fqueue = self.free_block_queue\n+        ret: list[KVCacheBlock] = fqueue.popleft_n(num_blocks)\n \n         # In order to only iterate the list once, we duplicated code a bit\n         if self.enable_caching:\n@@ -244,13 +247,15 @@ class BlockPool:\n             # The block doesn't have hash, eviction is not needed\n             return False\n         blocks_by_id = self.cached_block_hash_to_block.get(block_hash)\n-        if blocks_by_id is None:\n+        if not blocks_by_id:\n             # block_hash not found in cached_block_hash_to_block,\n             # eviction is not needed\n             return False\n+        block_id = block.block_id\n         block.reset_hash()\n-        blocks_by_id.pop(block.block_id, None)\n-        if len(blocks_by_id) == 0:\n+        if block_id is not None:\n+            blocks_by_id.pop(block_id, None)\n+        if not blocks_by_id:\n             del self.cached_block_hash_to_block[block_hash]\n \n         if self.enable_kv_cache_events:\n@@ -286,14 +291,15 @@ class BlockPool:\n             ordered_blocks: A list of blocks to free ordered by their eviction\n                 priority.\n         \"\"\"\n-        # Materialize the iterable to allow multiple passes.\n-        blocks_list = list(ordered_blocks)\n-        for block in blocks_list:\n+        # Single pass: decrement and collect zero-ref blocks\n+        to_append: list[KVCacheBlock] = []\n+        fqueue = self.free_block_queue\n+        for block in ordered_blocks:\n             block.ref_cnt -= 1\n-        self.free_block_queue.append_n([\n-            block for block in blocks_list\n-            if block.ref_cnt == 0 and not block.is_null\n-        ])\n+            if block.ref_cnt == 0 and not block.is_null:\n+                to_append.append(block)\n+        if to_append:\n+            fqueue.append_n(to_append)\n \n     def reset_prefix_cache(self) -> bool:\n         \"\"\"Reset prefix cache. This function may be used in RLHF\n", "model_name_or_path": "gpt-5-2025-08-07"}
