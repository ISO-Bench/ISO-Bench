diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py
index 00c1a2911..a7fffe34e 100644
--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py
+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py
@@ -163,7 +163,7 @@ def generate_continuous_batched_examples(example_lens_by_batch,
 
         # get the metadata
         cu_seqlens = torch.tensor((0, ) + spec, device=device).cumsum(dim=0)
-        seq_idx = torch.zeros(cu_seqlens[-1],
+        seq_idx = torch.empty(cu_seqlens[-1],
                               dtype=torch.int32,
                               device=cu_seqlens.device)
         for i, (srt, end) in enumerate(zip(
diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
index 36edac237..e1fa2ccb7 100644
--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
@@ -583,7 +583,11 @@ class MambaMixer2(MambaBase, CustomOp):
                                                                1]
                                  if has_prefill else None)
 
-        ssd_output_list = []
+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim
+        ssd_output_combined = torch.empty(num_actual_tokens,
+                                          feature_dim,
+                                          device=hidden_states_B_C.device,
+                                          dtype=hidden_states_B_C.dtype)
 
         # Process prefill requests
         if has_prefill:
@@ -653,7 +657,13 @@ class MambaMixer2(MambaBase, CustomOp):
             ssm_state[state_indices_tensor_p] = varlen_state
 
             # - reshape
-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))
+            prefill_flat = scan_output.view(num_prefill_tokens, -1)
+            if envs.VLLM_USE_V1:
+                start = num_decodes
+            else:
+                start = 0
+            ssd_output_combined[start:start + num_prefill_tokens].copy_(
+                prefill_flat)
 
         # Process decode requests
         if has_decode:
@@ -699,18 +709,16 @@ class MambaMixer2(MambaBase, CustomOp):
                 state_batch_indices=state_indices_tensor_d,
             )
 
+            decode_flat = hidden_states_d.view(-1, (self.num_heads // self.tp_size) *
+                                         self.head_dim)
             if envs.VLLM_USE_V1:
-                ssd_output_list.insert(
-                    0,
-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *
-                                         self.head_dim))
+                ssd_output_combined[:num_decodes].copy_(decode_flat)
             else:
-                ssd_output_list.append(
-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *
-                                         self.head_dim))
+                ssd_output_combined[num_prefill_tokens:num_prefill_tokens +
+                                    num_decodes].copy_(decode_flat)
 
         # Merge prefill and decode outputs before passing to gated MLP
-        hidden_states = torch.vstack(ssd_output_list)
+        hidden_states = ssd_output_combined
 
         # 4. gated MLP
         # GatedRMSNorm internally applying SiLU to the gate
diff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py
index a4ded2b7a..27dcb4c11 100644
--- a/vllm/model_executor/models/phi4flash.py
+++ b/vllm/model_executor/models/phi4flash.py
@@ -129,16 +129,16 @@ class SambaYAttention(nn.Module):
 
         self.lambda_init = self.lambda_init_fn(layer_idx)
         self.lambda_q1 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_k1 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_q2 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.lambda_k2 = nn.Parameter(
-            torch.zeros(self.head_dim, dtype=torch.float32).normal_(mean=0,
+            torch.empty(self.head_dim, dtype=torch.float32).normal_(mean=0,
                                                                     std=0.1))
         self.subln = nn.RMSNorm(2 * self.head_dim,
                                 eps=1e-5,
diff --git a/vllm/model_executor/models/plamo2.py b/vllm/model_executor/models/plamo2.py
index 9bc577cfe..787ae1668 100644
--- a/vllm/model_executor/models/plamo2.py
+++ b/vllm/model_executor/models/plamo2.py
@@ -257,7 +257,12 @@ class Plamo2MambaMixer(nn.Module):
         query_start_loc_p = (attn_metadata.query_start_loc[:num_prefills + 1]
                              if has_prefill else None)
 
-        ssd_output_list = []
+        feature_dim = (self.num_heads // self.tp_size) * self.head_dim
+        num_actual_tokens = num_prefill_tokens + num_decodes
+        ssd_output_combined = torch.empty(num_actual_tokens,
+                                          feature_dim,
+                                          device=hidden_states.device,
+                                          dtype=hidden_states.dtype)
 
         # Process prefill requests
         if has_prefill:
@@ -319,7 +324,8 @@ class Plamo2MambaMixer(nn.Module):
             mamba_cache_params.ssm_state[state_indices_tensor_p] = varlen_state
 
             # - reshape
-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))
+            prefill_flat = scan_output.view(num_prefill_tokens, -1)
+            ssd_output_combined[:num_prefill_tokens].copy_(prefill_flat)
 
         # Process decode requests
         if has_decode:
@@ -363,13 +369,14 @@ class Plamo2MambaMixer(nn.Module):
                 dt_softplus=True,
                 state_batch_indices=state_indices_tensor_d,
             )
-            assert self.num_heads % self.tp_size == 0
-            ssd_output_list.append(
-                hidden_states_d.view(-1, (self.num_heads // self.tp_size) *
-                                     self.head_dim))
+            ssd_output_combined[num_prefill_tokens:num_prefill_tokens +
+                                num_decodes].copy_(
+                                    hidden_states_d.view(-1, (self.num_heads //
+                                                             self.tp_size) *
+                                                         self.head_dim))
 
         # Merge prefill and decode outputs before passing to MLP
-        hidden_states = torch.vstack(ssd_output_list)
+        hidden_states = ssd_output_combined
 
         # 4. Final linear projection
         out = self.out_proj(hidden_states)
diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py
index ad9854dd2..9a3307152 100644
--- a/vllm/v1/core/block_pool.py
+++ b/vllm/v1/core/block_pool.py
@@ -84,8 +84,9 @@ class BlockPool:
             The cached blocks if exists, or None.
         """
         cached_blocks = []
+        cache_map = self.cached_block_hash_to_block
         for group_id in kv_cache_group_ids:
-            cached_blocks_one_group = self.cached_block_hash_to_block.get(
+            cached_blocks_one_group = cache_map.get(
                 BlockHashWithGroupId(block_hash, group_id))
             if not cached_blocks_one_group:
                 return None
@@ -180,8 +181,9 @@ class BlockPool:
             block_hash_with_group_id = BlockHashWithGroupId(
                 block_hash, kv_cache_group_id)
             blk.block_hash = block_hash_with_group_id
-            self.cached_block_hash_to_block[block_hash_with_group_id][
-                blk.block_id] = blk
+            cached = self.cached_block_hash_to_block
+            blk_id = blk.block_id
+            cached[block_hash_with_group_id][blk_id] = blk
             if new_hashes is not None:
                 new_hashes.append(block_hash.hash_value)
             prev_block_hash_value = block_hash.hash_value
@@ -214,7 +216,8 @@ class BlockPool:
             raise ValueError(
                 f"Cannot get {num_blocks} free blocks from the pool")
 
-        ret: list[KVCacheBlock] = self.free_block_queue.popleft_n(num_blocks)
+        fqueue = self.free_block_queue
+        ret: list[KVCacheBlock] = fqueue.popleft_n(num_blocks)
 
         # In order to only iterate the list once, we duplicated code a bit
         if self.enable_caching:
@@ -244,13 +247,15 @@ class BlockPool:
             # The block doesn't have hash, eviction is not needed
             return False
         blocks_by_id = self.cached_block_hash_to_block.get(block_hash)
-        if blocks_by_id is None:
+        if not blocks_by_id:
             # block_hash not found in cached_block_hash_to_block,
             # eviction is not needed
             return False
+        block_id = block.block_id
         block.reset_hash()
-        blocks_by_id.pop(block.block_id, None)
-        if len(blocks_by_id) == 0:
+        if block_id is not None:
+            blocks_by_id.pop(block_id, None)
+        if not blocks_by_id:
             del self.cached_block_hash_to_block[block_hash]
 
         if self.enable_kv_cache_events:
@@ -286,14 +291,15 @@ class BlockPool:
             ordered_blocks: A list of blocks to free ordered by their eviction
                 priority.
         """
-        # Materialize the iterable to allow multiple passes.
-        blocks_list = list(ordered_blocks)
-        for block in blocks_list:
+        # Single pass: decrement and collect zero-ref blocks
+        to_append: list[KVCacheBlock] = []
+        fqueue = self.free_block_queue
+        for block in ordered_blocks:
             block.ref_cnt -= 1
-        self.free_block_queue.append_n([
-            block for block in blocks_list
-            if block.ref_cnt == 0 and not block.is_null
-        ])
+            if block.ref_cnt == 0 and not block.is_null:
+                to_append.append(block)
+        if to_append:
+            fqueue.append_n(to_append)
 
     def reset_prefix_cache(self) -> bool:
         """Reset prefix cache. This function may be used in RLHF
