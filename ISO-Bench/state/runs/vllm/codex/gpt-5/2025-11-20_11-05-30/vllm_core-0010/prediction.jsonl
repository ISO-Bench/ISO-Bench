{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f728ab8", "model_patch": "diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex a9ef97391..c8bdecf41 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -10,13 +10,22 @@ def get_token_bin_counts_and_mask(\n     vocab_size: int,\n     num_seqs: int,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    # Compute the bin counts for the tokens.\n-    # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n+    # Compute per-token bin counts and mask.\n+    # tokens may contain padding value `vocab_size`; exclude those from counts\n+    # to avoid allocating an extra column and then slicing it out.\n+    bin_counts = torch.empty((num_seqs, vocab_size),\n                              dtype=torch.long,\n                              device=tokens.device)\n-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n-    bin_counts = bin_counts[:, :vocab_size]\n+    bin_counts.zero_()\n+\n+    # Mark valid (non-padding) positions and accumulate counts.\n+    valid = tokens != vocab_size\n+    # Use a safe index for padding positions; their src contribution is zero.\n+    safe_tokens = torch.where(\n+        valid, tokens,\n+        torch.zeros((), dtype=tokens.dtype, device=tokens.device))\n+    src = valid.to(torch.long)\n+    bin_counts.scatter_add_(1, safe_tokens, src)\n     mask = bin_counts > 0\n \n     return bin_counts, mask\n@@ -45,12 +54,18 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                                    vocab_size, num_seqs)\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n-        1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n+    # Precompute whether a token appears in prompt or output.\n+    cond = prompt_mask | output_mask\n+\n+    # Broadcast repetition penalties across vocab dimension without material\n+    # repeat. If token appears in prompt/output, apply penalty; otherwise 1.0.\n+    rep = repetition_penalties.unsqueeze(1)\n+    one = torch.ones((), device=logits.device, dtype=logits.dtype)\n+    penalties = torch.where(cond, rep, one)\n+\n+    # If logits are positive, divide by penalty; otherwise multiply by penalty.\n+    scaling = torch.where(logits > 0, one / penalties, penalties)\n+    logits.mul_(scaling)\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n     logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts\n", "model_name_or_path": "gpt-5-2025-08-07"}
