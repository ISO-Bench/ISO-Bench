diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py
index a9ef97391..c8bdecf41 100644
--- a/vllm/model_executor/layers/utils.py
+++ b/vllm/model_executor/layers/utils.py
@@ -10,13 +10,22 @@ def get_token_bin_counts_and_mask(
     vocab_size: int,
     num_seqs: int,
 ) -> Tuple[torch.Tensor, torch.Tensor]:
-    # Compute the bin counts for the tokens.
-    # vocab_size + 1 for padding.
-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),
+    # Compute per-token bin counts and mask.
+    # tokens may contain padding value `vocab_size`; exclude those from counts
+    # to avoid allocating an extra column and then slicing it out.
+    bin_counts = torch.empty((num_seqs, vocab_size),
                              dtype=torch.long,
                              device=tokens.device)
-    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))
-    bin_counts = bin_counts[:, :vocab_size]
+    bin_counts.zero_()
+
+    # Mark valid (non-padding) positions and accumulate counts.
+    valid = tokens != vocab_size
+    # Use a safe index for padding positions; their src contribution is zero.
+    safe_tokens = torch.where(
+        valid, tokens,
+        torch.zeros((), dtype=tokens.dtype, device=tokens.device))
+    src = valid.to(torch.long)
+    bin_counts.scatter_add_(1, safe_tokens, src)
     mask = bin_counts > 0
 
     return bin_counts, mask
@@ -45,12 +54,18 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,
                                                    vocab_size, num_seqs)
     output_bin_counts, output_mask = get_token_bin_counts_and_mask(
         output_tokens_tensor, vocab_size, num_seqs)
-    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(
-        1, vocab_size)
-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,
-                                      repetition_penalties, 1.0)[logits > 0]
-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,
-                                       repetition_penalties, 1.0)[logits <= 0]
+    # Precompute whether a token appears in prompt or output.
+    cond = prompt_mask | output_mask
+
+    # Broadcast repetition penalties across vocab dimension without material
+    # repeat. If token appears in prompt/output, apply penalty; otherwise 1.0.
+    rep = repetition_penalties.unsqueeze(1)
+    one = torch.ones((), device=logits.device, dtype=logits.dtype)
+    penalties = torch.where(cond, rep, one)
+
+    # If logits are positive, divide by penalty; otherwise multiply by penalty.
+    scaling = torch.where(logits > 0, one / penalties, penalties)
+    logits.mul_(scaling)
     # We follow the definition in OpenAI API.
     # Refer to https://platform.openai.com/docs/api-reference/parameter-details
     logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts
