{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-56fe4c2", "model_patch": "diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\nindex 3ff162170..b15f12cda 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n@@ -82,27 +82,29 @@ def scaled_mm_kernel(a_ptr, b_ptr, scale_a_ptr, scale_b_ptr, c_ptr, bias_ptr,\n         b_ptrs += BLOCK_SIZE_K * stride_bk\n \n     # Apply scale at end.\n-    masks_scale_a = masks_scale_am[:, None] & (tl.arange(0, 1) < 1)[:, None]\n+    # Load and broadcast scales; avoid redundant, always-true mask terms.\n+    masks_scale_a = masks_scale_am[:, None]\n     scale_a = tl.load(scale_a_ptrs[:, None], masks_scale_a)\n     # Need to broadcast to the appropriate size, if scale_a is already\n     # (BLOCK_SIZE_M, 1) then it will broadcast to its own shape. Same goes\n     # for scale_b below.\n     scale_a = scale_a.broadcast_to((BLOCK_SIZE_M, 1))\n-    accumulator = scale_a * accumulator.to(tl.float32)\n+    # Convert once to fp32 and apply scales.\n+    accumulator = accumulator.to(tl.float32)\n+    accumulator = scale_a * accumulator\n \n-    masks_scale_b = masks_scale_bn[:, None] & (tl.arange(0, 1) < 1)[None, :]\n+    masks_scale_b = masks_scale_bn[:, None]\n     scale_b = tl.load(scale_b_ptrs[:, None], masks_scale_b)\n     scale_b = scale_b.broadcast_to((BLOCK_SIZE_N, 1))\n-    accumulator = scale_b.T * accumulator.to(tl.float32)\n+    accumulator = scale_b.T * accumulator\n \n     # Convert to output format.\n     c = accumulator.to(c_ptr.type.element_ty)\n \n     # Add bias, it's already in output format, so add it after conversion.\n     if bias_ptr:\n-        offsets_bias = offsets_bn\n-        bias_ptrs = bias_ptr + offsets_bias\n-        bias_mask = offsets_bias < N\n+        bias_ptrs = bias_ptr + offsets_bn\n+        bias_mask = masks_bn\n         bias = tl.load(bias_ptrs, bias_mask)\n         c += bias\n \n@@ -128,7 +130,8 @@ def triton_scaled_mm(input: torch.Tensor,\n                      bias: Optional[torch.Tensor] = None,\n                      block_size_m: int = 32,\n                      block_size_n: int = 32,\n-                     block_size_k: int = 32) -> torch.Tensor:\n+                     block_size_k: int = 32,\n+                     use_heuristic: bool = True) -> torch.Tensor:\n     M, K = input.shape\n     N = weight.shape[1]\n \n@@ -148,6 +151,20 @@ def triton_scaled_mm(input: torch.Tensor,\n     grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(\n         N, META['BLOCK_SIZE_N']), )\n \n+    # Heuristically tune tile sizes for improved performance on common shapes.\n+    if use_heuristic:\n+        is_small_N = N < 8192\n+        next_pow2_m = max(32, triton.next_power_of_2(M))\n+        if next_pow2_m <= 32:\n+            tile_shape = (64, 64, 256) if is_small_N else (64, 128, 256)\n+        elif next_pow2_m <= 64:\n+            tile_shape = (64, 64, 256)\n+        elif next_pow2_m <= 128:\n+            tile_shape = (64, 128, 128)\n+        else:\n+            tile_shape = (128, 128, 128)\n+        block_size_m, block_size_n, block_size_k = tile_shape\n+\n     result = torch.empty((M, N), dtype=out_dtype, device=input.device)\n \n     has_scalar = lambda x: x.shape[0] == 1 and x.shape[1] == 1\n@@ -180,5 +197,4 @@ def triton_scaled_mm(input: torch.Tensor,\n                            BLOCK_SIZE_K=block_size_k,\n                            BLOCK_SIZE_SCALE_A=block_size_sa,\n                            BLOCK_SIZE_SCALE_B=block_size_sb)\n-\n-    return result.to(out_dtype)\n+    return result\n", "model_name_or_path": "gpt-5-2025-08-07"}
