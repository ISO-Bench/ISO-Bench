I’ve scanned the repo and set up a quick micro-benchmark; now I’ll optimize llama.py to reduce peak memory and overhead.
