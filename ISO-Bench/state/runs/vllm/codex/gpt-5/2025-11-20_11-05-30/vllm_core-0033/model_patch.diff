diff --git a/vllm/envs.py b/vllm/envs.py
index 261cc7855..3d3d673c7 100755
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -120,6 +120,8 @@ if TYPE_CHECKING:
     VLLM_TPU_MOST_MODEL_LEN: Optional[int] = None
     VLLM_USE_DEEP_GEMM: bool = False
     VLLM_USE_FLASHINFER_MOE: bool = False
+    VLLM_USE_FLASHINFER_MOE_FP8: bool = False
+    VLLM_USE_FLASHINFER_MOE_FP4: bool = False
     VLLM_XGRAMMAR_CACHE_MB: int = 0
     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256
     VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False
@@ -854,10 +856,19 @@ environment_variables: dict[str, Callable[[], Any]] = {
     "VLLM_USE_DEEP_GEMM":
     lambda: bool(int(os.getenv("VLLM_USE_DEEP_GEMM", "0"))),
 
-    # Allow use of FlashInfer CUTLASS kernels for fused moe ops.
+    # Allow use of FlashInfer CUTLASS kernels for fused MoE ops (legacy flag).
+    # Prefer the more specific flags below when possible.
     "VLLM_USE_FLASHINFER_MOE":
     lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE", "0"))),
 
+    # Allow use of FlashInfer MoE kernels for fused MoE ops (FP8 path).
+    "VLLM_USE_FLASHINFER_MOE_FP8":
+    lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE_FP8", "0"))),
+
+    # Allow use of FlashInfer MoE kernels for fused MoE ops (FP4 path).
+    "VLLM_USE_FLASHINFER_MOE_FP4":
+    lambda: bool(int(os.getenv("VLLM_USE_FLASHINFER_MOE_FP4", "0"))),
+
     # Control the cache sized used by the xgrammar compiler. The default
     # of 512 MB should be enough for roughly 1000 JSON schemas.
     # It can be changed with this variable if needed for some reason.
diff --git a/vllm/model_executor/layers/fused_moe/config.py b/vllm/model_executor/layers/fused_moe/config.py
index 9bebb6a65..5a4e3b5a7 100644
--- a/vllm/model_executor/layers/fused_moe/config.py
+++ b/vllm/model_executor/layers/fused_moe/config.py
@@ -191,8 +191,11 @@ class FusedMoEParallelConfig:
 
     @property
     def use_flashinfer_cutlass_kernels(self):
-        return (envs.VLLM_USE_FLASHINFER_MOE
-                and has_flashinfer_cutlass_fused_moe())
+        # Support legacy flag and new granular flags (FP8 / FP4)
+        use_flashinfer = (envs.VLLM_USE_FLASHINFER_MOE
+                          or envs.VLLM_USE_FLASHINFER_MOE_FP8
+                          or envs.VLLM_USE_FLASHINFER_MOE_FP4)
+        return use_flashinfer and has_flashinfer_cutlass_fused_moe()
 
     @staticmethod
     def make(tp_size_: int, dp_size_: int,
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index aec5d7b25..0caefb1ba 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -904,7 +904,9 @@ def fused_topk(
                                        dtype=torch.int32,
                                        device=hidden_states.device)
 
-    gating_output_float = gating_output.float()  # TODO(woosuk): Optimize this.
+    # Avoid unnecessary cast if already float32
+    gating_output_float = (gating_output if gating_output.dtype == torch.float32
+                           else gating_output.float())
 
     topk_func = dispatch_topk_func()
     topk_weights, topk_ids = topk_func(topk_weights, topk_ids,
@@ -950,12 +952,15 @@ def grouped_topk(
                                    -1).max(dim=-1).values  # [n, n_group]
     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,
                            sorted=False)[1]  # [n, top_k_group]
-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]
-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
+    # Build boolean mask directly to avoid dtype conversions
+    group_mask = torch.zeros(group_scores.shape,
+                             dtype=torch.bool,
+                             device=group_scores.device)  # [n, n_group]
+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]
     score_mask = group_mask.unsqueeze(-1).expand(
         num_token, num_expert_group,
         scores.size(-1) // num_expert_group).reshape(num_token, -1)  # [n, e]
-    tmp_scores = scores.masked_fill(~score_mask.bool(),
+    tmp_scores = scores.masked_fill(~score_mask,
                                     float("-inf"))  # [n, e]
 
     if e_score_correction_bias is not None:
diff --git a/vllm/model_executor/layers/quantization/modelopt.py b/vllm/model_executor/layers/quantization/modelopt.py
index 3807899fc..2af24e979 100644
--- a/vllm/model_executor/layers/quantization/modelopt.py
+++ b/vllm/model_executor/layers/quantization/modelopt.py
@@ -625,7 +625,10 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):
         round_up_multiple = lambda x, m: (x + m - 1) // m * m
         M_padded = round_up_multiple(M, 128)
         K_padded = round_up_multiple(K, 4)
-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
+        # Allocate directly on the same device to avoid host->device copy
+        padded_scale = torch.zeros((B, M_padded, K_padded),
+                                   dtype=scale.dtype,
+                                   device=scale.device)
         padded_scale[:B, :M, :K] = scale
         batches, rows, cols = padded_scale.shape
         assert rows % 128 == 0
@@ -633,7 +636,8 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):
         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,
                                             cols // 4, 4)
         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
-        swizzled_scale = swizzled_scale.contiguous().cuda()
+        # Ensure correct device without redundant transfers
+        swizzled_scale = swizzled_scale.contiguous().to(scale.device)
         return (swizzled_scale.reshape(M, K)
                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))
 
@@ -914,7 +918,10 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
         round_up_multiple = lambda x, m: (x + m - 1) // m * m
         M_padded = round_up_multiple(M, 128)
         K_padded = round_up_multiple(K, 4)
-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
+        # Allocate directly on the same device to avoid host->device copy
+        padded_scale = torch.zeros((B, M_padded, K_padded),
+                                   dtype=scale.dtype,
+                                   device=scale.device)
         padded_scale[:B, :M, :K] = scale
         batches, rows, cols = padded_scale.shape
         assert rows % 128 == 0
@@ -922,7 +929,8 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):
         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,
                                             cols // 4, 4)
         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))
-        swizzled_scale = swizzled_scale.contiguous().cuda()
+        # Ensure correct device without redundant transfers
+        swizzled_scale = swizzled_scale.contiguous().to(scale.device)
         return (swizzled_scale.reshape(M, K)
                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))
 
diff --git a/vllm/utils/flashinfer.py b/vllm/utils/flashinfer.py
index dbd2dc393..afd42d0b0 100644
--- a/vllm/utils/flashinfer.py
+++ b/vllm/utils/flashinfer.py
@@ -64,8 +64,14 @@ def _lazy_import_wrapper(module_name: str,
 
 
 # Create lazy wrappers for each function
-flashinfer_cutlass_fused_moe = _lazy_import_wrapper("flashinfer.fused_moe",
-                                                    "cutlass_fused_moe")
+# CUTLASS fused MoE (FP4/FP8 backends may be provided by the package)
+flashinfer_cutlass_fused_moe = _lazy_import_wrapper(
+    "flashinfer.fused_moe", "cutlass_fused_moe")
+
+# Optional FP8 blockscale fused MoE entrypoint (if present)
+flashinfer_cutlass_fused_moe_fp8 = _lazy_import_wrapper(
+    "flashinfer.fused_moe", "cutlass_fused_moe_fp8")
+
 fp4_quantize = _lazy_import_wrapper("flashinfer", "fp4_quantize")
 fp4_swizzle_blockscale = _lazy_import_wrapper("flashinfer",
                                               "fp4_swizzle_blockscale")
@@ -94,6 +100,10 @@ def has_flashinfer_cutlass_fused_moe() -> bool:
         mod = _get_submodule(module_name)
         if not mod or not hasattr(mod, attr_name):
             return False
+    # If FP8 entrypoint exists, we also consider it supported
+    mod_fp8 = _get_submodule("flashinfer.fused_moe")
+    if mod_fp8 and hasattr(mod_fp8, "cutlass_fused_moe_fp8"):
+        return True
     return True
 
 
@@ -103,5 +113,6 @@ __all__ = [
     "flashinfer_cutlass_fused_moe",
     "fp4_quantize",
     "fp4_swizzle_blockscale",
+    "flashinfer_cutlass_fused_moe_fp8",
     "autotune",
 ]
