{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-7d94577", "model_patch": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 261cc7855..3d3d673c7 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -120,6 +120,8 @@ if TYPE_CHECKING:\n     VLLM_TPU_MOST_MODEL_LEN: Optional[int] = None\n     VLLM_USE_DEEP_GEMM: bool = False\n     VLLM_USE_FLASHINFER_MOE: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP8: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP4: bool = False\n     VLLM_XGRAMMAR_CACHE_MB: int = 0\n     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256\n     VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False\n@@ -854,10 +856,19 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_USE_DEEP_GEMM\":\n     lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"0\"))),\n \n-    # Allow use of FlashInfer CUTLASS kernels for fused moe ops.\n+    # Allow use of FlashInfer CUTLASS kernels for fused MoE ops (legacy flag).\n+    # Prefer the more specific flags below when possible.\n     \"VLLM_USE_FLASHINFER_MOE\":\n     lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE\", \"0\"))),\n \n+    # Allow use of FlashInfer MoE kernels for fused MoE ops (FP8 path).\n+    \"VLLM_USE_FLASHINFER_MOE_FP8\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP8\", \"0\"))),\n+\n+    # Allow use of FlashInfer MoE kernels for fused MoE ops (FP4 path).\n+    \"VLLM_USE_FLASHINFER_MOE_FP4\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP4\", \"0\"))),\n+\n     # Control the cache sized used by the xgrammar compiler. The default\n     # of 512 MB should be enough for roughly 1000 JSON schemas.\n     # It can be changed with this variable if needed for some reason.\ndiff --git a/vllm/model_executor/layers/fused_moe/config.py b/vllm/model_executor/layers/fused_moe/config.py\nindex 9bebb6a65..5a4e3b5a7 100644\n--- a/vllm/model_executor/layers/fused_moe/config.py\n+++ b/vllm/model_executor/layers/fused_moe/config.py\n@@ -191,8 +191,11 @@ class FusedMoEParallelConfig:\n \n     @property\n     def use_flashinfer_cutlass_kernels(self):\n-        return (envs.VLLM_USE_FLASHINFER_MOE\n-                and has_flashinfer_cutlass_fused_moe())\n+        # Support legacy flag and new granular flags (FP8 / FP4)\n+        use_flashinfer = (envs.VLLM_USE_FLASHINFER_MOE\n+                          or envs.VLLM_USE_FLASHINFER_MOE_FP8\n+                          or envs.VLLM_USE_FLASHINFER_MOE_FP4)\n+        return use_flashinfer and has_flashinfer_cutlass_fused_moe()\n \n     @staticmethod\n     def make(tp_size_: int, dp_size_: int,\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex aec5d7b25..0caefb1ba 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -904,7 +904,9 @@ def fused_topk(\n                                        dtype=torch.int32,\n                                        device=hidden_states.device)\n \n-    gating_output_float = gating_output.float()  # TODO(woosuk): Optimize this.\n+    # Avoid unnecessary cast if already float32\n+    gating_output_float = (gating_output if gating_output.dtype == torch.float32\n+                           else gating_output.float())\n \n     topk_func = dispatch_topk_func()\n     topk_weights, topk_ids = topk_func(topk_weights, topk_ids,\n@@ -950,12 +952,15 @@ def grouped_topk(\n                                    -1).max(dim=-1).values  # [n, n_group]\n     group_idx = torch.topk(group_scores, k=topk_group, dim=-1,\n                            sorted=False)[1]  # [n, top_k_group]\n-    group_mask = torch.zeros_like(group_scores)  # [n, n_group]\n-    group_mask.scatter_(1, group_idx, 1)  # [n, n_group]\n+    # Build boolean mask directly to avoid dtype conversions\n+    group_mask = torch.zeros(group_scores.shape,\n+                             dtype=torch.bool,\n+                             device=group_scores.device)  # [n, n_group]\n+    group_mask.scatter_(1, group_idx, True)  # [n, n_group]\n     score_mask = group_mask.unsqueeze(-1).expand(\n         num_token, num_expert_group,\n         scores.size(-1) // num_expert_group).reshape(num_token, -1)  # [n, e]\n-    tmp_scores = scores.masked_fill(~score_mask.bool(),\n+    tmp_scores = scores.masked_fill(~score_mask,\n                                     float(\"-inf\"))  # [n, e]\n \n     if e_score_correction_bias is not None:\ndiff --git a/vllm/model_executor/layers/quantization/modelopt.py b/vllm/model_executor/layers/quantization/modelopt.py\nindex 3807899fc..2af24e979 100644\n--- a/vllm/model_executor/layers/quantization/modelopt.py\n+++ b/vllm/model_executor/layers/quantization/modelopt.py\n@@ -625,7 +625,10 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):\n         round_up_multiple = lambda x, m: (x + m - 1) // m * m\n         M_padded = round_up_multiple(M, 128)\n         K_padded = round_up_multiple(K, 4)\n-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)\n+        # Allocate directly on the same device to avoid host->device copy\n+        padded_scale = torch.zeros((B, M_padded, K_padded),\n+                                   dtype=scale.dtype,\n+                                   device=scale.device)\n         padded_scale[:B, :M, :K] = scale\n         batches, rows, cols = padded_scale.shape\n         assert rows % 128 == 0\n@@ -633,7 +636,8 @@ class ModelOptNvFp4LinearMethod(LinearMethodBase):\n         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,\n                                             cols // 4, 4)\n         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))\n-        swizzled_scale = swizzled_scale.contiguous().cuda()\n+        # Ensure correct device without redundant transfers\n+        swizzled_scale = swizzled_scale.contiguous().to(scale.device)\n         return (swizzled_scale.reshape(M, K)\n                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))\n \n@@ -914,7 +918,10 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         round_up_multiple = lambda x, m: (x + m - 1) // m * m\n         M_padded = round_up_multiple(M, 128)\n         K_padded = round_up_multiple(K, 4)\n-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)\n+        # Allocate directly on the same device to avoid host->device copy\n+        padded_scale = torch.zeros((B, M_padded, K_padded),\n+                                   dtype=scale.dtype,\n+                                   device=scale.device)\n         padded_scale[:B, :M, :K] = scale\n         batches, rows, cols = padded_scale.shape\n         assert rows % 128 == 0\n@@ -922,7 +929,8 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         padded_scale = padded_scale.reshape(batches, rows // 128, 4, 32,\n                                             cols // 4, 4)\n         swizzled_scale = padded_scale.permute((0, 1, 4, 3, 2, 5))\n-        swizzled_scale = swizzled_scale.contiguous().cuda()\n+        # Ensure correct device without redundant transfers\n+        swizzled_scale = swizzled_scale.contiguous().to(scale.device)\n         return (swizzled_scale.reshape(M, K)\n                 if scale_ndim == 2 else swizzled_scale.reshape(B, M, K))\n \ndiff --git a/vllm/utils/flashinfer.py b/vllm/utils/flashinfer.py\nindex dbd2dc393..afd42d0b0 100644\n--- a/vllm/utils/flashinfer.py\n+++ b/vllm/utils/flashinfer.py\n@@ -64,8 +64,14 @@ def _lazy_import_wrapper(module_name: str,\n \n \n # Create lazy wrappers for each function\n-flashinfer_cutlass_fused_moe = _lazy_import_wrapper(\"flashinfer.fused_moe\",\n-                                                    \"cutlass_fused_moe\")\n+# CUTLASS fused MoE (FP4/FP8 backends may be provided by the package)\n+flashinfer_cutlass_fused_moe = _lazy_import_wrapper(\n+    \"flashinfer.fused_moe\", \"cutlass_fused_moe\")\n+\n+# Optional FP8 blockscale fused MoE entrypoint (if present)\n+flashinfer_cutlass_fused_moe_fp8 = _lazy_import_wrapper(\n+    \"flashinfer.fused_moe\", \"cutlass_fused_moe_fp8\")\n+\n fp4_quantize = _lazy_import_wrapper(\"flashinfer\", \"fp4_quantize\")\n fp4_swizzle_blockscale = _lazy_import_wrapper(\"flashinfer\",\n                                               \"fp4_swizzle_blockscale\")\n@@ -94,6 +100,10 @@ def has_flashinfer_cutlass_fused_moe() -> bool:\n         mod = _get_submodule(module_name)\n         if not mod or not hasattr(mod, attr_name):\n             return False\n+    # If FP8 entrypoint exists, we also consider it supported\n+    mod_fp8 = _get_submodule(\"flashinfer.fused_moe\")\n+    if mod_fp8 and hasattr(mod_fp8, \"cutlass_fused_moe_fp8\"):\n+        return True\n     return True\n \n \n@@ -103,5 +113,6 @@ __all__ = [\n     \"flashinfer_cutlass_fused_moe\",\n     \"fp4_quantize\",\n     \"fp4_swizzle_blockscale\",\n+    \"flashinfer_cutlass_fused_moe_fp8\",\n     \"autotune\",\n ]\n", "model_name_or_path": "gpt-5-2025-08-07"}
