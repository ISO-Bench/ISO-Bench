{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-6368e77", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex 4bf5cbbd1..eecb15938 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -123,6 +123,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n \n         # Create the workers.\n         driver_ip = get_ip()\n+        workers = []\n         for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n             if not bundle.get(\"GPU\", 0):\n                 continue\n@@ -138,20 +139,33 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 scheduling_strategy=scheduling_strategy,\n                 **ray_remote_kwargs,\n             )(RayWorkerWrapper).remote(vllm_config=self.vllm_config)\n+            workers.append(worker)\n \n-            if self.use_ray_spmd_worker:\n-                self.workers.append(worker)\n+        # Fetch worker IPs in batch to avoid per-worker ray.get overhead.\n+        worker_ip_refs = [\n+            worker.get_node_ip.remote()  # type: ignore[attr-defined]\n+            for worker in workers\n+        ]\n+        workers_ips_all = ray.get(worker_ip_refs)\n+\n+        if self.use_ray_spmd_worker:\n+            self.workers = workers\n+        else:\n+            driver_index: Optional[int] = None\n+            for idx, ip in enumerate(workers_ips_all):\n+                if ip == driver_ip:\n+                    driver_index = idx\n+                    break\n+            if driver_index is not None:\n+                self.driver_dummy_worker = workers[driver_index]\n+                self.driver_worker = RayWorkerWrapper(vllm_config=self.vllm_config)\n+                self.workers = [w for i, w in enumerate(workers) if i != driver_index]\n+                worker_ips = [\n+                    ip for i, ip in enumerate(workers_ips_all) if i != driver_index\n+                ]\n             else:\n-                worker_ip = ray.get(worker.get_node_ip.remote())\n-                if worker_ip == driver_ip and self.driver_dummy_worker is None:\n-                    # If the worker is on the same node as the driver, we use it\n-                    # as the resource holder for the driver process.\n-                    self.driver_dummy_worker = worker\n-                    self.driver_worker = RayWorkerWrapper(\n-                        vllm_config=self.vllm_config)\n-                else:\n-                    # Else, added to the list of workers.\n-                    self.workers.append(worker)\n+                self.workers = workers\n+                worker_ips = workers_ips_all\n \n         logger.debug(\"workers: %s\", self.workers)\n         logger.debug(\"driver_dummy_worker: %s\", self.driver_dummy_worker)\n@@ -161,41 +175,28 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 \"adjusting the Ray placement group or running the driver on a \"\n                 \"GPU node.\")\n \n-        worker_ips = [\n-            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n-            for worker in self.workers\n-        ]\n+        # Precompute ip counts and sort workers without additional ray.get calls.\n+        if self.use_ray_spmd_worker:\n+            # In SPMD mode we haven't filtered out a driver; use all IPs.\n+            worker_ips = workers_ips_all\n+\n         ip_counts: Dict[str, int] = {}\n         for ip in worker_ips:\n             ip_counts[ip] = ip_counts.get(ip, 0) + 1\n \n-        def sort_by_driver_then_worker_ip(worker):\n-            \"\"\"\n-            Sort the workers based on 3 properties:\n-            1. If the worker is on the same node as the driver (vllm engine),\n-                it should be placed first.\n-            2. Then, if the worker is on a node with fewer workers, it should\n-                be placed first.\n-            3. Finally, if the work is on a node with smaller IP address, it\n-                should be placed first.\n-            \"\"\"\n-            ip = ray.get(worker.get_node_ip.remote())\n-            return (ip != driver_ip, ip_counts[ip], ip)\n-\n-        # After sorting, the workers on the same node will be\n-        # close to each other, and the workers on the driver\n-        # node will be placed first.\n-        self.workers = sorted(self.workers, key=sort_by_driver_then_worker_ip)\n+        workers_with_ips = list(zip(self.workers, worker_ips))\n+        self.workers = [w for w, _ in sorted(\n+            workers_with_ips,\n+            key=lambda wi: (wi[1] != driver_ip, ip_counts[wi[1]], wi[1]),\n+        )]\n \n         # Get the set of GPU IDs used on each node.\n-        worker_node_and_gpu_ids = []\n-        for worker in [self.driver_dummy_worker] + self.workers:\n-            if worker is None:\n-                # driver_dummy_worker can be None when using ray spmd worker.\n-                continue\n-            worker_node_and_gpu_ids.append(\n-                ray.get(worker.get_node_and_gpu_ids.remote()) \\\n-            ) # type: ignore\n+        workers_all = [w for w in [self.driver_dummy_worker] + self.workers if w is not None]\n+        node_and_gpu_id_refs = [\n+            w.get_node_and_gpu_ids.remote()  # type: ignore[attr-defined]\n+            for w in workers_all\n+        ]\n+        worker_node_and_gpu_ids = ray.get(node_and_gpu_id_refs)\n \n         node_workers = defaultdict(list)  # node id -> list of worker ranks\n         node_gpus = defaultdict(list)  # node id -> list of gpu ids\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex e6701f4c4..c515e7c76 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -35,7 +35,20 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n         tokenizer.all_special_tokens_extended)\n     tokenizer_all_special_tokens = set(tokenizer.all_special_tokens)\n     tokenizer_len = len(tokenizer)\n-    max_token_id = max(tokenizer.get_vocab().values())\n+    # Cache frequently accessed tokenizer attributes/methods to avoid\n+    # repeated, potentially expensive computations inside HF tokenizers.\n+    tokenizer_is_fast = getattr(tokenizer, \"is_fast\", False)\n+    tokenizer_vocab = tokenizer.get_vocab()\n+    max_token_id = max(tokenizer_vocab.values())\n+    # get_added_vocab exists for HF tokenizers; cache it for faster membership\n+    # checks used during detokenization.\n+    try:\n+        tokenizer_added_vocab = tokenizer.get_added_vocab()\n+    except Exception:  # pragma: no cover - extremely defensive\n+        tokenizer_added_vocab = {}\n+    # Cache BOS/EOS ids when available (frequently queried attributes).\n+    tokenizer_bos_token_id = getattr(tokenizer, \"bos_token_id\", None)\n+    tokenizer_eos_token_id = getattr(tokenizer, \"eos_token_id\", None)\n \n     class CachedTokenizer(tokenizer.__class__):  # type: ignore\n \n@@ -58,6 +71,24 @@ def get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n         def __len__(self):\n             return tokenizer_len\n \n+        @property\n+        def is_fast(self):\n+            return tokenizer_is_fast\n+\n+        def get_vocab(self):  # type: ignore[override]\n+            return tokenizer_vocab\n+\n+        def get_added_vocab(self):  # type: ignore[override]\n+            return tokenizer_added_vocab\n+\n+        @property\n+        def bos_token_id(self):\n+            return tokenizer_bos_token_id\n+\n+        @property\n+        def eos_token_id(self):\n+            return tokenizer_eos_token_id\n+\n     CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n \n     tokenizer.__class__ = CachedTokenizer\n", "model_name_or_path": "gpt-5-2025-08-07"}
