{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c34eeec", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 70463ecd9..82a656d7b 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -205,8 +205,11 @@ class RotaryEmbedding(CustomOp):\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         from vllm._ipex_ops import ipex_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n-                                                   dtype=query.dtype)\n+        # Avoid expensive __setattr__ calls by only updating when needed\n+        if self.cos_sin_cache.device != positions.device or \\\n+                self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n+                                                       dtype=query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if key is None:\n@@ -320,8 +323,11 @@ class RotaryEmbedding(CustomOp):\n         if offsets is not None:\n             positions = positions + offsets\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                   dtype=query.dtype)\n+        # Avoid redundant dtype/device casts on every call\n+        if self.cos_sin_cache.device != query.device or \\\n+                self.cos_sin_cache.dtype != query.dtype:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                       dtype=query.dtype)\n \n         positions = positions.flatten()\n         num_tokens = positions.shape[0]\n@@ -966,7 +972,9 @@ class Llama4VisionRotaryEmbedding(RotaryEmbedding):\n         key: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         assert key is not None\n-        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(query.device)\n+        # Avoid reassigning buffer unless device actually changes\n+        if self.cos_sin_cache.device != query.device:\n+            self.cos_sin_cache = self.cos_sin_cache.to(query.device)\n         query_ = torch.view_as_complex(query.float().reshape(\n             *query.shape[:-1], -1, 2))\n         key_ = torch.view_as_complex(key.float().reshape(\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 83e181116..21e62d83f 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -515,10 +515,13 @@ class MLACommonMetadataBuilder(Generic[M]):\n                                        chunk_starts + max_context_chunk)\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n \n-                cu_seq_lens_cpu = torch.zeros(num_chunks,\n+                # Use empty + explicit initialization to avoid zeroing the\n+                # entire tensor, which is unnecessary work.\n+                cu_seq_lens_cpu = torch.empty(num_chunks,\n                                               self._num_prefills + 1,\n                                               dtype=torch.int32,\n                                               pin_memory=True)\n+                cu_seq_lens_cpu[:, 0] = 0\n                 torch.cumsum(chunk_seq_lens,\n                              dim=1,\n                              out=cu_seq_lens_cpu[:, 1:],\n", "model_name_or_path": "gpt-5-2025-08-07"}
