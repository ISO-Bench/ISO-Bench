{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-029c71d", "model_patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..639aeae04 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -732,3 +732,13 @@ class TestPrefixCachingBlockAllocator:\n             blocks.append(prev_block)\n \n         return blocks\n+\n+    @staticmethod\n+    def test_mark_blocks_as_computed_noop_empty():\n+        \"\"\"Ensure calling mark_blocks_as_computed with an empty list is a no-op\n+        and does not raise exceptions (fast path).\"\"\"\n+        block_size = 16\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+        # Should not raise\n+        allocator.mark_blocks_as_computed([])\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..ca7083441 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -108,6 +108,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n \n         self.metric_data = CacheMetricData()\n \n+        # Cached mapping from absolute block id to physical index on this\n+        # allocator for faster lookups in get_physical_block_id.\n+        # Build once since block id set is static for the allocator lifetime.\n+        self._abs_to_phys_map = {\n+            abs_id: idx\n+            for idx, abs_id in enumerate(sorted(self.all_block_ids))\n+        }\n+\n     # Implements Block.Factory.\n     def _create_block(\n         self,\n@@ -401,7 +409,8 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         Returns:\n             int: The rzero-offset block id on certain device.\n         \"\"\"\n-        return sorted(self.all_block_ids).index(absolute_id)\n+        # Use a cached mapping for O(1) lookup instead of sorting each call.\n+        return self._abs_to_phys_map[absolute_id]\n \n     @property\n     def all_block_ids(self) -> FrozenSet[int]:\n@@ -507,7 +516,36 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        \"\"\"Mark the given blocks as computed.\n+\n+        This updates the internal tracker so subsequent scheduling passes can\n+        treat these blocks as reusable cached prefixes. Fast-path for the\n+        common case where the input is empty.\n+\n+        Args:\n+            block_ids: Physical block ids to mark as computed.\n+        \"\"\"\n+        # Fast path: nothing to mark.\n+        if not block_ids:\n+            return\n+\n+        tracker = self._block_tracker\n+        evictor = self.evictor\n+\n+        # Mark only active blocks. Blocks already in the evictor are implicitly\n+        # considered computed.\n+        for block_id in block_ids:\n+            state = tracker.get(block_id)\n+            if state is None:\n+                continue\n+            if state.active:\n+                state.computed = True\n+            elif block_id in evictor:\n+                # Already accounted for via evictor metadata.\n+                continue\n+            else:\n+                # Not active and not in evictor: nothing to do.\n+                continue\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..838d398d5 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,27 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        # Mark full prompt blocks as computed for prefix caching so\n+        # subsequent scheduling rounds can skip them. Skip the last block to\n+        # avoid caching an entire prompt, which can cause incorrect behavior\n+        # in some runners.\n+        if not self.enable_caching:\n+            return\n+\n+        block_ids_to_mark: List[int] = []\n+        for seq in seq_group.get_seqs():\n+            if seq.seq_id not in self.block_tables:\n+                continue\n+            ids = self.block_tables[seq.seq_id].physical_block_ids\n+            if not ids:\n+                continue\n+            # Exclude the last block id per prefix-caching convention.\n+            block_ids_to_mark.extend([bid for bid in ids[:-1] if bid is not None])\n+\n+        if block_ids_to_mark:\n+            # Deduplicate to reduce tracker updates.\n+            uniq_ids = list(dict.fromkeys(block_ids_to_mark))\n+            self.block_allocator.mark_blocks_as_computed(uniq_ids)\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:\n", "model_name_or_path": "gpt-5-2025-08-07"}
