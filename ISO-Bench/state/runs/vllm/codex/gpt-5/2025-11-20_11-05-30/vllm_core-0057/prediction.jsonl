{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-6dd55af", "model_patch": "diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b8ba36f35..c78c17475 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -6,7 +6,6 @@ from collections.abc import AsyncGenerator, Mapping\n from copy import copy\n from typing import Any, Optional, Union\n \n-import numpy as np\n \n import vllm.envs as envs\n from vllm.config import ModelConfig, VllmConfig\n@@ -27,7 +26,7 @@ from vllm.transformers_utils.config import (\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\n from vllm.usage.usage_lib import UsageContext\n-from vllm.utils import Device, cdiv\n+from vllm.utils import Device\n from vllm.v1.engine import EngineCoreRequest\n from vllm.v1.engine.core_client import EngineCoreClient\n from vllm.v1.engine.exceptions import EngineDeadError, EngineGenerateError\n@@ -382,28 +381,36 @@ class AsyncLLM(EngineClient):\n \n                     # Split outputs into chunks of at most\n                     # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the\n-                    # event loop for too long.\n-                    if num_outputs <= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:\n-                        slices = (outputs.outputs, )\n-                    else:\n-                        slices = np.array_split(\n-                            outputs.outputs,\n-                            cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n-\n-                    for i, outputs_slice in enumerate(slices):\n+                    # event loop for too long. Use pure Python slicing to avoid\n+                    # unnecessary NumPy overhead/allocations.\n+                    chunk_size = VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n+                    total = num_outputs\n+\n+                    # Accumulate aborts across chunks to reduce RPCs.\n+                    reqs_to_abort_all: list[str] = []\n+\n+                    i = 0\n+                    while i < total:\n+                        outputs_slice = outputs.outputs[i:i + chunk_size]\n                         # 2) Process EngineCoreOutputs.\n                         processed_outputs = output_processor.process_outputs(\n                             outputs_slice, outputs.timestamp, iteration_stats)\n                         # NOTE: RequestOutputs are pushed to their queues.\n                         assert not processed_outputs.request_outputs\n \n-                        # Allow other asyncio tasks to run between chunks\n-                        if i + 1 < len(slices):\n+                        # Accumulate aborts and allow other asyncio tasks to run\n+                        if processed_outputs.reqs_to_abort:\n+                            reqs_to_abort_all.extend(\n+                                processed_outputs.reqs_to_abort)\n+                        if i + chunk_size < total:\n                             await asyncio.sleep(0)\n+                        i += chunk_size\n \n-                        # 3) Abort any reqs that finished due to stop strings.\n+                    # 3) Abort any reqs that finished due to stop strings.\n+                    if reqs_to_abort_all:\n+                        # De-duplicate to minimize redundant aborts.\n                         await engine_core.abort_requests_async(\n-                            processed_outputs.reqs_to_abort)\n+                            list(dict.fromkeys(reqs_to_abort_all)))\n \n                     # 4) Logging.\n                     # TODO(rob): make into a coroutine and launch it in\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 2bcd61d1f..023390178 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -49,10 +49,13 @@ class RequestOutputCollector:\n \n     async def get(self) -> Union[RequestOutput, PoolingRequestOutput]:\n         \"\"\"Get operation blocks on put event.\"\"\"\n-        while (output := self.output) is None:\n-            await self.ready.wait()\n+        ready = self.ready\n+        output = self.output\n+        while output is None:\n+            await ready.wait()\n+            output = self.output\n         self.output = None\n-        self.ready.clear()\n+        ready.clear()\n         if isinstance(output, Exception):\n             raise output\n         return output\n@@ -368,12 +371,15 @@ class OutputProcessor:\n         within the loop below.\n         \"\"\"\n \n+        request_states = self.request_states\n+        parent_requests = self.parent_requests\n+\n         request_outputs: Union[list[RequestOutput],\n                                list[PoolingRequestOutput]] = []\n         reqs_to_abort: list[str] = []\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n-            req_state = self.request_states.get(req_id)\n+            req_state = request_states.get(req_id)\n             if req_state is None:\n                 # Ignore output for already-aborted request.\n                 continue\n@@ -383,18 +389,16 @@ class OutputProcessor:\n                                            engine_core_timestamp,\n                                            iteration_stats)\n \n-            new_token_ids = engine_core_output.new_token_ids\n             pooling_output = engine_core_output.pooling_output\n             finish_reason = engine_core_output.finish_reason\n             stop_reason = engine_core_output.stop_reason\n-            kv_transfer_params = engine_core_output.kv_transfer_params\n-            num_cached_tokens = engine_core_output.num_cached_tokens\n             req_state.is_prefilling = False\n \n             if pooling_output is None:\n                 assert req_state.detokenizer is not None\n                 assert req_state.logprobs_processor is not None\n                 # 2) Detokenize the token ids into text and perform stop checks.\n+                new_token_ids = engine_core_output.new_token_ids\n                 stop_string = req_state.detokenizer.update(\n                     new_token_ids, finish_reason == FinishReason.STOP)\n                 if stop_string:\n@@ -405,6 +409,13 @@ class OutputProcessor:\n                 # if required.\n                 req_state.logprobs_processor.update_from_output(\n                     engine_core_output)\n+                kv_transfer_params = engine_core_output.kv_transfer_params\n+                num_cached_tokens = engine_core_output.num_cached_tokens\n+            else:\n+                # Avoid unnecessary attribute reads when handling pooling.\n+                new_token_ids = []\n+                kv_transfer_params = None\n+                num_cached_tokens = 0\n \n             # 4) Create and handle RequestOutput objects.\n             if request_output := req_state.make_request_output(\n@@ -419,11 +430,11 @@ class OutputProcessor:\n \n             # Free completed requests.\n             if finish_reason is not None:\n-                self.request_states.pop(req_id)\n+                request_states.pop(req_id)\n                 # Remove parent request if applicable.\n                 parent_req = req_state.parent_req\n                 if parent_req and not parent_req.child_requests:\n-                    self.parent_requests.pop(parent_req.request_id, None)\n+                    parent_requests.pop(parent_req.request_id, None)\n                 if not engine_core_output.finished:\n                     # If req not finished in EngineCore, but Detokenizer\n                     # detected stop string, abort needed in EngineCore.\n", "model_name_or_path": "gpt-5-2025-08-07"}
