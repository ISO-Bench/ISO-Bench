{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d666466", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 07d51acf9..ea7aefc8d 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -1,5 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n-from typing import Optional, Tuple\n+from typing import Optional, Tuple, Dict, Tuple as Tup\n \n import torch\n import triton\n@@ -14,6 +14,41 @@ def ceil_div(a, b):\n     return (a + b - 1) // b\n \n \n+# Lightweight buffer cache to reduce frequent GPU allocations for\n+# per-expert temporary buffers used by the Triton path.\n+# Keyed by (device, num_experts) since shapes depend only on num_experts.\n+_BUFFER_CACHE: Dict[Tup[torch.device, int], Tup[torch.Tensor, torch.Tensor]] = {}\n+\n+\n+def _get_moe_temp_buffers(topk_ids: torch.Tensor,\n+                          num_experts: int) -> Tup[torch.Tensor, torch.Tensor]:\n+    device = topk_ids.device\n+    key = (device, num_experts)\n+    tokens_cnts: Optional[torch.Tensor]\n+    cumsum: Optional[torch.Tensor]\n+    pair = _BUFFER_CACHE.get(key)\n+    if pair is None:\n+        tokens_cnts = topk_ids.new_empty((num_experts + 1, num_experts),\n+                                         dtype=torch.int32)\n+        cumsum = topk_ids.new_empty((num_experts + 1, ), dtype=torch.int32)\n+        _BUFFER_CACHE[key] = (tokens_cnts, cumsum)\n+    else:\n+        tokens_cnts, cumsum = pair\n+        # In case buffers were moved/freed externally, re-create.\n+        if tokens_cnts.device != device or not tokens_cnts.is_contiguous():\n+            tokens_cnts = topk_ids.new_empty((num_experts + 1, num_experts),\n+                                             dtype=torch.int32)\n+            _BUFFER_CACHE[key] = (tokens_cnts, cumsum)\n+        if cumsum.device != device or not cumsum.is_contiguous():\n+            cumsum = topk_ids.new_empty((num_experts + 1, ),\n+                                        dtype=torch.int32)\n+            _BUFFER_CACHE[key] = (tokens_cnts, cumsum)\n+    # Reset contents for this invocation.\n+    tokens_cnts.zero_()\n+    cumsum.zero_()\n+    return tokens_cnts, cumsum\n+\n+\n @triton.jit\n def moe_align_block_size_stage1(\n     topk_ids_ptr,\n@@ -109,12 +144,8 @@ def moe_align_block_size_triton(\n ) -> None:\n     numel = topk_ids.numel()\n     grid = (num_experts, )\n-    tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n-                              dtype=torch.int32,\n-                              device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n-                         dtype=torch.int32,\n-                         device=topk_ids.device)\n+    # Reuse zeroed buffers to reduce allocation overhead.\n+    tokens_cnts, cumsum = _get_moe_temp_buffers(topk_ids, num_experts)\n     tokens_per_thread = ceil_div(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -201,19 +232,16 @@ def moe_align_block_size(\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     if pad_sorted_ids:\n         max_num_tokens_padded = round_up(max_num_tokens_padded, block_size)\n-    sorted_ids = torch.empty((max_num_tokens_padded, ),\n-                             dtype=torch.int32,\n-                             device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n+    # Initialize to a sentinel in a single op.\n+    sorted_ids = torch.full((max_num_tokens_padded, ),\n+                            int(topk_ids.numel()),\n+                            dtype=torch.int32,\n+                            device=topk_ids.device)\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     # Expert ids must be zeroed out to prevent index out of bounds error while\n     # mapping global expert ids to local expert ids in expert parallelism.\n-    expert_ids = torch.zeros((max_num_m_blocks, ),\n-                             dtype=torch.int32,\n-                             device=topk_ids.device)\n-    num_tokens_post_pad = torch.empty((1),\n-                                      dtype=torch.int32,\n-                                      device=topk_ids.device)\n+    expert_ids = topk_ids.new_zeros((max_num_m_blocks, ), dtype=torch.int32)\n+    num_tokens_post_pad = topk_ids.new_empty((1, ), dtype=torch.int32)\n     if num_experts >= 224:\n         if envs.VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON or num_experts != 256:\n             moe_align_block_size_triton(\ndiff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex 43f30f710..f2beb091d 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -67,11 +67,11 @@ class BlockPool:\n         Returns:\n             The cached block if it exists, or None.\n         \"\"\"\n-        if block_hash in self.cached_block_hash_to_block:\n-            first_block_id = list(\n-                self.cached_block_hash_to_block[block_hash].keys())[0]\n-            return self.cached_block_hash_to_block[block_hash][first_block_id]\n-        return None\n+        cached_blocks = self.cached_block_hash_to_block.get(block_hash)\n+        if not cached_blocks:\n+            return None\n+        first_block_id = next(iter(cached_blocks))\n+        return cached_blocks[first_block_id]\n \n     def cache_full_blocks(\n         self,\n@@ -102,11 +102,11 @@ class BlockPool:\n             block_size: Number of tokens in each block.\n             hash_fn: The hash function to use for block hashes.\n         \"\"\"\n-        if num_cached_blocks == num_full_blocks:\n-            return\n+        cdict = self.cached_block_hash_to_block\n         new_full_blocks = blocks[num_cached_blocks:num_full_blocks]\n         assert len(block_hashes) >= num_cached_blocks\n         new_block_hashes = block_hashes[num_cached_blocks:]\n+        new_block_hashes_len = len(new_block_hashes)\n \n         # Update the new blocks with the block hashes through the chain.\n         if num_cached_blocks == 0:\n@@ -116,10 +116,11 @@ class BlockPool:\n             assert prev_block.block_hash is not None\n             prev_block_hash_value = prev_block.block_hash.hash_value\n \n+        all_token_ids = request.all_token_ids\n         for i, blk in enumerate(new_full_blocks):\n             assert blk.block_hash is None\n \n-            if i < len(new_block_hashes):\n+            if i < new_block_hashes_len:\n                 # The block hash may already be computed in\n                 # \"get_computed_blocks\" if the tokens are not generated by\n                 # this request (either the prompt tokens or the previously\n@@ -132,8 +133,7 @@ class BlockPool:\n                 blk_idx = num_cached_blocks + i\n                 start_token_idx = blk_idx * block_size\n                 end_token_idx = (blk_idx + 1) * block_size\n-                block_tokens = request.all_token_ids[\n-                    start_token_idx:end_token_idx]\n+                block_tokens = all_token_ids[start_token_idx:end_token_idx]\n                 assert len(block_tokens) == block_size, (\n                     f\"Expected {block_size} tokens, got \"\n                     f\"{len(block_tokens)} at {blk_idx}th block for request \"\n@@ -152,7 +152,7 @@ class BlockPool:\n \n             # Update and added the full block to the cache.\n             blk.block_hash = block_hash\n-            self.cached_block_hash_to_block[block_hash][blk.block_id] = blk\n+            cdict[block_hash][blk.block_id] = blk\n             prev_block_hash_value = block_hash.hash_value\n \n     def get_new_blocks(self, num_blocks: int) -> list[KVCacheBlock]:\n@@ -171,19 +171,19 @@ class BlockPool:\n                 f\"Cannot get {num_blocks} free blocks from the pool\")\n \n         ret: list[KVCacheBlock] = []\n-        idx = 0\n-        while idx < num_blocks:\n+        free_q = self.free_block_queue\n+        maybe_evict = self._maybe_evict_cached_block\n+        for _ in range(num_blocks):\n             # First allocate blocks.\n-            curr_block = self.free_block_queue.popleft()\n+            curr_block = free_q.popleft()\n             assert curr_block.ref_cnt == 0\n \n             # If the block is cached, evict it.\n             if self.enable_caching:\n-                self._maybe_evict_cached_block(curr_block)\n+                maybe_evict(curr_block)\n \n             curr_block.incr_ref()\n             ret.append(curr_block)\n-            idx += 1\n \n         return ret\n \n@@ -199,15 +199,18 @@ class BlockPool:\n             True if the block is evicted, False otherwise.\n         \"\"\"\n         block_hash = block.block_hash\n-        if block_hash and block_hash in self.cached_block_hash_to_block:\n-            block.reset_hash()\n-            del self.cached_block_hash_to_block[block_hash][block.block_id]\n+        if not block_hash:\n+            return False\n+        cached_blocks = self.cached_block_hash_to_block.get(block_hash)\n+        if not cached_blocks:\n+            return False\n+        block.reset_hash()\n+        cached_blocks.pop(block.block_id, None)\n \n-            if len(self.cached_block_hash_to_block[block_hash]) == 0:\n-                del self.cached_block_hash_to_block[block_hash]\n+        if not cached_blocks:\n+            self.cached_block_hash_to_block.pop(block_hash, None)\n \n-            return True\n-        return False\n+        return True\n \n     def touch(self, blocks: list[KVCacheBlock]) -> None:\n         \"\"\"Touch a block increases its reference count by 1, and may remove\n", "model_name_or_path": "gpt-5-2025-08-07"}
