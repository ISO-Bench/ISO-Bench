{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-51c31bc", "model_patch": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex c9cd099b8..21a2d0bc1 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -99,9 +99,9 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n       \"Failed to determine torch nvcc compiler flags\")\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n+      # Enable FP8 paths and allow half/bfloat intrinsics for better kernels\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8\")\n-    endif()\n-    if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0)\n+      list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n       list(REMOVE_ITEM GPU_FLAGS\n         \"-D__CUDA_NO_HALF_OPERATORS__\"\n         \"-D__CUDA_NO_HALF_CONVERSIONS__\"\ndiff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex fb6882f3e..52330f54c 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -1,13 +1,20 @@\n+// Optimized layernorm kernels with lighter-weight reductions and better\n+// occupancy tuning.\n #include \"type_convert.cuh\"\n #include \"dispatch_utils.h\"\n+#include \"reduction_utils.cuh\"\n \n #include <torch/cuda.h>\n #include <c10/cuda/CUDAGuard.h>\n \n #ifndef USE_ROCM\n   #include <cub/cub.cuh>\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n #else\n   #include <hipcub/hipcub.hpp>\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n #endif\n \n namespace vllm {\n@@ -27,9 +34,8 @@ __global__ void rms_norm_kernel(\n     variance += x * x;\n   }\n \n-  using BlockReduce = cub::BlockReduce<float, 1024>;\n-  __shared__ typename BlockReduce::TempStorage reduceStore;\n-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n+  // Reduce across the entire block and make the sum available to all threads.\n+  variance = vllm::block_reduce_sum_all<1024>(variance);\n \n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n@@ -79,9 +85,7 @@ fused_add_rms_norm_kernel(\n     residual_v[id] = temp;\n   }\n \n-  using BlockReduce = cub::BlockReduce<float, 1024>;\n-  __shared__ typename BlockReduce::TempStorage reduceStore;\n-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n+  variance = vllm::block_reduce_sum_all<1024>(variance);\n \n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n@@ -118,9 +122,7 @@ fused_add_rms_norm_kernel(\n     residual[blockIdx.x * hidden_size + idx] = z;\n   }\n \n-  using BlockReduce = cub::BlockReduce<float, 1024>;\n-  __shared__ typename BlockReduce::TempStorage reduceStore;\n-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n+  variance = vllm::block_reduce_sum_all<1024>(variance);\n \n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n@@ -144,7 +146,9 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  // Tune block size for better occupancy in memory-bound regimes.\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\ndiff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nnew file mode 100644\nindex 000000000..36e170884\n--- /dev/null\n+++ b/csrc/reduction_utils.cuh\n@@ -0,0 +1,47 @@\n+// Lightweight warp/block reduction utilities shared across kernels.\n+#pragma once\n+\n+#include <cuda.h>\n+\n+namespace vllm {\n+\n+// Warp-level sum using shuffles.\n+__device__ __forceinline__ float warp_reduce_sum(float val) {\n+#ifndef USE_ROCM\n+  unsigned mask = 0xffffffffu;\n+  for (int offset = 16; offset > 0; offset >>= 1) {\n+    val += __shfl_down_sync(mask, val, offset);\n+  }\n+#else\n+  // HIP does not require an explicit mask argument.\n+  for (int offset = 16; offset > 0; offset >>= 1) {\n+    val += __shfl_down(val, offset);\n+  }\n+#endif\n+  return val;\n+}\n+\n+// Block-level sum reduction that returns the block-wide sum to all threads.\n+template <int MaxBlockSize>\n+__device__ __forceinline__ float block_reduce_sum_all(float val) {\n+  // Enough for up to 1024 threads (32 warps)\n+  __shared__ float shared[MaxBlockSize / 32];\n+  int lane = threadIdx.x & 31;\n+  int wid = threadIdx.x >> 5;\n+\n+  float sum = warp_reduce_sum(val);\n+  if (lane == 0) shared[wid] = sum;\n+  __syncthreads();\n+\n+  // Number of active warps in the block\n+  int nwarps = (blockDim.x + 31) >> 5;\n+  sum = (threadIdx.x < nwarps) ? shared[lane] : 0.0f;\n+  if (wid == 0) sum = warp_reduce_sum(sum);\n+\n+  if (wid == 0 && lane == 0) shared[0] = sum;\n+  __syncthreads();\n+  return shared[0];\n+}\n+\n+}  // namespace vllm\n+\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..9a2f46fb9\n--- /dev/null\n+++ b/model_patch.diff\n@@ -0,0 +1,116 @@\n+diff --git a/cmake/utils.cmake b/cmake/utils.cmake\n+index c9cd099b8..21a2d0bc1 100644\n+--- a/cmake/utils.cmake\n++++ b/cmake/utils.cmake\n+@@ -99,9 +99,9 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n+       \"Failed to determine torch nvcc compiler flags\")\n+ \n+     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n++      # Enable FP8 paths and allow half/bfloat intrinsics for better kernels\n+       list(APPEND GPU_FLAGS \"-DENABLE_FP8\")\n+-    endif()\n+-    if (CUDA_VERSION VERSION_GREATER_EQUAL 12.0)\n++      list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+       list(REMOVE_ITEM GPU_FLAGS\n+         \"-D__CUDA_NO_HALF_OPERATORS__\"\n+         \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\n+index fb6882f3e..52330f54c 100644\n+--- a/csrc/layernorm_kernels.cu\n++++ b/csrc/layernorm_kernels.cu\n+@@ -1,13 +1,20 @@\n++// Optimized layernorm kernels with lighter-weight reductions and better\n++// occupancy tuning.\n+ #include \"type_convert.cuh\"\n+ #include \"dispatch_utils.h\"\n++#include \"reduction_utils.cuh\"\n+ \n+ #include <torch/cuda.h>\n+ #include <c10/cuda/CUDAGuard.h>\n+ \n+ #ifndef USE_ROCM\n+   #include <cub/cub.cuh>\n++  #include <cuda_bf16.h>\n++  #include <cuda_fp16.h>\n+ #else\n+   #include <hipcub/hipcub.hpp>\n++  #include <hip/hip_bf16.h>\n++  #include <hip/hip_fp16.h>\n+ #endif\n+ \n+ namespace vllm {\n+@@ -27,9 +34,8 @@ __global__ void rms_norm_kernel(\n+     variance += x * x;\n+   }\n+ \n+-  using BlockReduce = cub::BlockReduce<float, 1024>;\n+-  __shared__ typename BlockReduce::TempStorage reduceStore;\n+-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n++  // Reduce across the entire block and make the sum available to all threads.\n++  variance = vllm::block_reduce_sum_all<1024>(variance);\n+ \n+   if (threadIdx.x == 0) {\n+     s_variance = rsqrtf(variance / hidden_size + epsilon);\n+@@ -79,9 +85,7 @@ fused_add_rms_norm_kernel(\n+     residual_v[id] = temp;\n+   }\n+ \n+-  using BlockReduce = cub::BlockReduce<float, 1024>;\n+-  __shared__ typename BlockReduce::TempStorage reduceStore;\n+-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n++  variance = vllm::block_reduce_sum_all<1024>(variance);\n+ \n+   if (threadIdx.x == 0) {\n+     s_variance = rsqrtf(variance / hidden_size + epsilon);\n+@@ -118,9 +122,7 @@ fused_add_rms_norm_kernel(\n+     residual[blockIdx.x * hidden_size + idx] = z;\n+   }\n+ \n+-  using BlockReduce = cub::BlockReduce<float, 1024>;\n+-  __shared__ typename BlockReduce::TempStorage reduceStore;\n+-  variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n++  variance = vllm::block_reduce_sum_all<1024>(variance);\n+ \n+   if (threadIdx.x == 0) {\n+     s_variance = rsqrtf(variance / hidden_size + epsilon);\n+@@ -144,7 +146,9 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n+   int num_tokens = input.numel() / hidden_size;\n+ \n+   dim3 grid(num_tokens);\n+-  dim3 block(std::min(hidden_size, 1024));\n++  // Tune block size for better occupancy in memory-bound regimes.\n++  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n++  dim3 block(std::min(hidden_size, max_block_size));\n+   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n+   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\n+diff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\n+new file mode 100644\n+index 000000000..36e170884\n+--- /dev/null\n++++ b/csrc/reduction_utils.cuh\n+@@ -0,0 +1,47 @@\n++// Lightweight warp/block reduction utilities shared across kernels.\n++#pragma once\n++\n++#include <cuda.h>\n++\n++namespace vllm {\n++\n++// Warp-level sum using shuffles.\n++__device__ __forceinline__ float warp_reduce_sum(float val) {\n++#ifndef USE_ROCM\n++  unsigned mask = 0xffffffffu;\n++  for (int offset = 16; offset > 0; offset >>= 1) {\n++    val += __shfl_down_sync(mask, val, offset);\n++  }\n++#else\n++  // HIP does not require an explicit mask argument.\n++  for (int offset = 16; offset > 0; offset >>= 1) {\n++    val += __shfl_down(val, offset);\n++  }\n++#endif\n++  return val;\n++}\n++\n++// Block-level sum reduction that returns the block-w\n\\ No newline at end of file\ndiff --git a/tests/kernels/test_layernorm.py b/tests/kernels/test_layernorm.py\nindex fa4bbe458..c4eb66371 100644\n--- a/tests/kernels/test_layernorm.py\n+++ b/tests/kernels/test_layernorm.py\n@@ -71,6 +71,7 @@ def test_rms_norm(\n @pytest.mark.parametrize(\"quant_scale\", [1.0, 0.01, 10.0])\n @pytest.mark.parametrize(\"seed\", SEEDS)\n @pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n+@torch.inference_mode()\n def test_fused_rms_norm_quant(\n     num_tokens: int,\n     hidden_size: int,\ndiff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9dcf..51efbfe20 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -37,7 +37,7 @@ from vllm.model_executor.layers.rotary_embedding import get_rope\n from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n \n from .llama import LlamaForCausalLM, LlamaMLP, LlamaModel\n-from .utils import (AutoWeightsLoader, extract_layer_index,\n+from .utils import (AutoWeightsLoader, extract_layer_index, fast_topk,\n                     is_pp_missing_parameter)\n \n \n@@ -50,7 +50,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores, router_indices = torch.topk(gating_output, topk, dim=-1)\n+        router_scores, router_indices = fast_topk(gating_output, topk, dim=-1)\n         router_scores = torch.sigmoid(router_scores.float()).to(\n             hidden_states.dtype)\n         return (router_scores, router_indices.to(torch.int32))\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434f3..8893f00db 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -654,7 +654,7 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):\n     ) -> IntermediateTensors:\n         return IntermediateTensors({\n             key:\n-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)\n+            torch.empty((batch_size, hidden_size), dtype=dtype, device=device)\n             for key in keys\n         })\n \n@@ -695,6 +695,18 @@ def extract_layer_index(layer_name: str) -> int:\n     return int_vals[0]\n \n \n+\n+def fast_topk(x: torch.Tensor, k: int, dim: int = -1) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Optimized top-k for the common k==1 case used by some MoE routers.\n+    Falls back to torch.topk for k>1.\n+    \"\"\"\n+    if k == 1:\n+        values, indices = torch.max(x, dim=dim, keepdim=True)\n+        return values, indices\n+    return torch.topk(x, k, dim=dim)\n+\n+\n def cast_overflow_tensors(\n     tensors: torch.Tensor,\n     offset: float = 1000,\n", "model_name_or_path": "gpt-5-2025-08-07"}
