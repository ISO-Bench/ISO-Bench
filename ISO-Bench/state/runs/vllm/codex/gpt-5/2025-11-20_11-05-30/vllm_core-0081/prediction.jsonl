{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-ec82c3e", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 329b03a57..f9d300529 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -3,7 +3,6 @@\n # noqa: UP007\n from __future__ import annotations\n \n-import copy\n import json\n import re\n from dataclasses import dataclass, field\n@@ -117,11 +116,11 @@ class GrammarCompilerCache:\n     This cache reduces the overhead of creating new compiler instances when\n     using the same tokenizer configuration.\n     \"\"\"\n-    _cache: dict[str, xgr.GrammarCompiler] = {}\n+    _cache: dict[int, xgr.GrammarCompiler] = {}\n \n     @classmethod\n     def get_compiler(cls, config: GrammarConfig) -> xgr.GrammarCompiler:\n-        cache_key = str(config.tokenizer_hash)\n+        cache_key = config.tokenizer_hash\n \n         if cache_key not in cls._cache:\n             assert config.tokenizer_data is not None\n@@ -319,7 +318,7 @@ class XGrammarLogitsProcessor:\n             for i, matcher in enumerate(self.matchers):\n                 if not matcher.is_terminated():\n                     sampled_token = input_ids[-1]\n-                    assert self.matchers[i].accept_token(sampled_token)\n+                    assert matcher.accept_token(sampled_token)\n \n         for i, matcher in enumerate(self.matchers):\n             if not matcher.is_terminated():\n@@ -334,8 +333,12 @@ class XGrammarLogitsProcessor:\n         dtype = scores.dtype\n         if device_type != \"cuda\":\n             # xgrammar on cpu only supports float32 scores\n-            # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n-            scores = scores.to(\"cpu\").float().unsqueeze(0)\n+            # If already on CPU with float32, avoid redundant copies.\n+            if device_type == \"cpu\" and dtype == torch.float32:\n+                scores = scores.unsqueeze(0)\n+            else:\n+                # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n+                scores = scores.to(\"cpu\").float().unsqueeze(0)\n \n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n@@ -343,10 +346,21 @@ class XGrammarLogitsProcessor:\n         xgr.apply_token_bitmask_inplace(\n             scores, self.token_bitmask.to(scores.device, non_blocking=True))\n         if device_type != \"cuda\":\n-            scores = scores.to(dtype).to(device_type).squeeze()\n+            if device_type == \"cpu\" and dtype == torch.float32:\n+                scores = scores.squeeze()\n+            else:\n+                scores = scores.to(dtype).to(device_type).squeeze()\n \n         return scores\n \n     def clone(self) -> XGrammarLogitsProcessor:\n-        \"\"\"Deepcopy due to per-sequence state in the matchers\"\"\"\n-        return copy.deepcopy(self)\n+        \"\"\"Create a new instance with shared compiled grammar but separate state.\"\"\"\n+        new_processor = XGrammarLogitsProcessor(self.config)\n+        # Share the compiled grammar context (immutable after compilation)\n+        new_processor.ctx = self.ctx\n+        # Preserve batch size; lazily reinitialize matchers/bitmask on first call\n+        new_processor.batch_size = self.batch_size\n+        new_processor.matchers = []\n+        new_processor.token_bitmask = None  # type: ignore[assignment]\n+        new_processor.prefilled = False\n+        return new_processor\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4d0ae9a20..1fdc7b96b 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -164,10 +164,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        # Use uninitialized buffers; values are set before every use.\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -185,16 +186,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -207,27 +208,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
