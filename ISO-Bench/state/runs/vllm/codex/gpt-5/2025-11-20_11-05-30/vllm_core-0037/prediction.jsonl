{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-63d635d", "model_patch": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ef3d28c80..ceb576808 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -259,6 +259,8 @@ class Qwen2_5_VisionAttention(nn.Module):\n         x: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: torch.Tensor,\n+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+        seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n         # [s, b, c] --> [s, b, head * 3 * head_dim]\n         x, _ = self.qkv(x)\n@@ -284,8 +286,9 @@ class Qwen2_5_VisionAttention(nn.Module):\n             from flash_attn import flash_attn_varlen_func\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n-\n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            # Avoid GPU->CPU sync by allowing caller to pass max_seqlen.\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -320,8 +323,9 @@ class Qwen2_5_VisionAttention(nn.Module):\n         elif self.attn_backend == _Backend.XFORMERS:\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n-\n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            # Allow caller to pass seqlens list to avoid .tolist() syncs.\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -364,11 +368,18 @@ class Qwen2_5_VisionBlock(nn.Module):\n                                      quant_config=quant_config,\n                                      prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+    def forward(self,\n+                x: torch.Tensor,\n+                cu_seqlens: torch.Tensor,\n+                rotary_pos_emb: torch.Tensor,\n+                *,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[list[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -611,6 +622,20 @@ class Qwen2_5_VisionTransformer(nn.Module):\n \n         # windows attention\n         window_index, cu_window_seqlens = self.get_window_index(grid_thw)\n+        # Precompute window seqlens/max on CPU to avoid sync later.\n+        if len(cu_window_seqlens) >= 2:\n+            _cu_win_unique = [cu_window_seqlens[0]]\n+            for _v in cu_window_seqlens[1:]:\n+                if _v != _cu_win_unique[-1]:\n+                    _cu_win_unique.append(_v)\n+            _seqlens_win = [\n+                _cu_win_unique[i] - _cu_win_unique[i - 1]\n+                for i in range(1, len(_cu_win_unique))\n+            ]\n+            _max_seqlen_win = max(_seqlens_win) if _seqlens_win else 0\n+        else:\n+            _seqlens_win = []\n+            _max_seqlen_win = 0\n         cu_window_seqlens = torch.tensor(\n             cu_window_seqlens,\n             device=hidden_states.device,\n@@ -630,17 +655,25 @@ class Qwen2_5_VisionTransformer(nn.Module):\n                                              grid_thw[:, 0]).cumsum(\n                                                  dim=0, dtype=torch.int32)\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), \"constant\", 0)\n+        _seqlens_full = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+        _max_seqlen_full = max(_seqlens_full) if _seqlens_full else 0\n \n         # transformers\n         hidden_states = hidden_states.unsqueeze(1)\n         for layer_num, blk in enumerate(self.blocks):\n             if layer_num in self.fullatt_block_indexes:\n                 cu_seqlens_now = cu_seqlens\n+                _max_now = _max_seqlen_full\n+                _seqlens_now = _seqlens_full\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n+                _max_now = _max_seqlen_win\n+                _seqlens_now = _seqlens_win\n             hidden_states = blk(hidden_states,\n                                 cu_seqlens=cu_seqlens_now,\n-                                rotary_pos_emb=rotary_pos_emb)\n+                                rotary_pos_emb=rotary_pos_emb,\n+                                max_seqlen=_max_now,\n+                                seqlens=_seqlens_now)\n \n         # For Qwen2.5-VL-3B, float16 will overflow at last block\n         # for long visual tokens sequences.\ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex ac3d154dd..cc327efd3 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -307,6 +307,8 @@ class Qwen2VisionAttention(nn.Module):\n         x: torch.Tensor,\n         cu_seqlens: torch.Tensor,\n         rotary_pos_emb: torch.Tensor,\n+        max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+        seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n \n         # [s, b, c] --> [s, b, 3 * head * head_dim]\n@@ -319,8 +321,13 @@ class Qwen2VisionAttention(nn.Module):\n         q, k, v = (rearrange(x, \"s b ... -> b s ...\").contiguous()\n                    for x in (q, k, v))\n         if rotary_pos_emb is not None:\n-            q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)\n-            k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)\n+            use_flash_attn = self.attn_backend == _Backend.FLASH_ATTN\n+            q = apply_rotary_pos_emb_vision(q,\n+                                            rotary_pos_emb,\n+                                            use_flash_attn=use_flash_attn)\n+            k = apply_rotary_pos_emb_vision(k,\n+                                            rotary_pos_emb,\n+                                            use_flash_attn=use_flash_attn)\n \n         if self.attn_backend == _Backend.FLASH_ATTN:\n             # from vllm_flash_attn.flash_attn_interface import (\n@@ -329,7 +336,9 @@ class Qwen2VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+            # Avoid GPU->CPU sync here by allowing caller to pass max_seqlen.\n+            if max_seqlen is None:\n+                max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -364,8 +373,9 @@ class Qwen2VisionAttention(nn.Module):\n         elif self.attn_backend == _Backend.XFORMERS:\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n-\n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+            # Allow the caller to provide Python list to avoid .tolist() syncs.\n+            if seqlens is None:\n+                seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -409,11 +419,18 @@ class Qwen2VisionBlock(nn.Module):\n                                   quant_config=quant_config,\n                                   prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+    def forward(self,\n+                x: torch.Tensor,\n+                cu_seqlens: torch.Tensor,\n+                rotary_pos_emb: torch.Tensor,\n+                *,\n+                max_seqlen: Optional[int] = None,\n+                seqlens: Optional[list[int]] = None) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -616,16 +633,24 @@ class Qwen2VisionTransformer(nn.Module):\n         # compute position embedding\n         rotary_pos_emb = self.rot_pos_emb(grid_thw)\n \n-        # compute cu_seqlens\n+        # compute cu_seqlens (CPU tensor) and its derived values to avoid\n+        # runtime GPU->CPU syncs inside attention kernels.\n         cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],\n                                              grid_thw[:, 0]).cumsum(\n                                                  dim=0, dtype=torch.int32)\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), \"constant\", 0)\n+        # Precompute sequence lengths and max length once.\n+        _seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n+        _max_seqlen = max(_seqlens) if _seqlens else 0\n \n         # transformers\n         x = x.unsqueeze(1)\n         for blk in self.blocks:\n-            x = blk(x, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n+            x = blk(x,\n+                    cu_seqlens=cu_seqlens,\n+                    rotary_pos_emb=rotary_pos_emb,\n+                    max_seqlen=_max_seqlen,\n+                    seqlens=_seqlens)\n \n         # adapter\n         x = self.merger(x)\n", "model_name_or_path": "gpt-5-2025-08-07"}
