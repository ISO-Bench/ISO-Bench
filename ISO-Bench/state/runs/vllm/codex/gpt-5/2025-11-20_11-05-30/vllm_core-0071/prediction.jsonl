{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f67e9e9", "model_patch": "diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\nnew file mode 100644\nindex 000000000..a0a77bd81\n--- /dev/null\n+++ b/benchmark/benchmark_latency.py\n@@ -0,0 +1,55 @@\n+import argparse\n+import time\n+from typing import List\n+\n+import torch\n+\n+from cacheflow.master.simple_frontend import SimpleFrontend\n+from cacheflow.master.server import (Server, add_server_arguments,\n+                                     initialize_ray_cluster)\n+from cacheflow.sampling_params import SamplingParams\n+from cacheflow.utils import get_gpu_memory, get_cpu_memory\n+\n+\n+def main(args: argparse.Namespace):\n+    assert args.pipeline_parallel_size == 1, (\n+        'Pipeline parallelism is not supported yet.')\n+\n+    (num_nodes, num_devices_per_node, distributed_init_method,\n+     all_stage_devices) = initialize_ray_cluster(address='local',\n+                                                 num_gpus=args.tensor_parallel_size)\n+\n+    server = Server(args.model,\n+                    tensor_parallel_size=args.tensor_parallel_size,\n+                    pipeline_parallel_size=args.pipeline_parallel_size,\n+                    seed=args.seed,\n+                    dtype=args.dtype,\n+                    worker_use_ray=True)\n+\n+    frontend = SimpleFrontend(server)\n+\n+    prompts: List[str] = [\"Hello world!\"] * args.num_prompts\n+    params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=args.max_tokens)\n+\n+    # Warmup\n+    frontend.generate(prompts[:2], params)\n+\n+    t0 = time.time()\n+    frontend.generate(prompts, params)\n+    t1 = time.time()\n+    print({\n+        \"num_prompts\": args.num_prompts,\n+        \"max_tokens\": args.max_tokens,\n+        \"latency_s\": round(t1 - t0, 3),\n+    })\n+\n+\n+if __name__ == '__main__':\n+    parser = argparse.ArgumentParser()\n+    add_server_arguments(parser)\n+    parser.add_argument('--num-prompts', type=int, default=8)\n+    parser.add_argument('--max-tokens', type=int, default=8)\n+    parser.add_argument('--dtype', type=str, default='float16')\n+    parser.add_argument('--seed', type=int, default=0)\n+    args = parser.parse_args()\n+    main(args)\ndiff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nindex 8b132e442..7d1353f56 100644\n--- a/cacheflow/models/attention.py\n+++ b/cacheflow/models/attention.py\n@@ -34,11 +34,16 @@ class GPTCacheFlowAttention(nn.Module):\n             raise ValueError('FlashAttention does not support head_size > 128.')\n \n         device = query.device\n-        prefix_sum = [0]\n-        for prompt_len in prompt_lens:\n-            prefix_sum.append(prefix_sum[-1] + prompt_len)\n-        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n-        max_prompt_len = max(prompt_lens)\n+        # Compute prefix_sum more efficiently on device using cumsum.\n+        if prompt_lens:\n+            lens = torch.tensor(prompt_lens, dtype=torch.int, device=device)\n+            prefix_sum = torch.empty(lens.numel() + 1, dtype=torch.int, device=device)\n+            prefix_sum[0] = 0\n+            torch.cumsum(lens, dim=0, out=prefix_sum[1:])\n+            max_prompt_len = int(lens.max().item())\n+        else:\n+            prefix_sum = torch.zeros(1, dtype=torch.int, device=device)\n+            max_prompt_len = 0\n \n         # FIXME(woosuk): Unnecessary copy. Optimize this.\n         qkv = torch.stack([query, key, value], dim=1)\ndiff --git a/cacheflow/models/utils.py b/cacheflow/models/utils.py\nindex 84e7fbce6..bd7b49bee 100644\n--- a/cacheflow/models/utils.py\n+++ b/cacheflow/models/utils.py\n@@ -20,5 +20,6 @@ def get_torch_dtype(dtype: Union[torch.dtype, str]) -> torch.dtype:\n \n def get_dtype_size(dtype: Union[torch.dtype, str]) -> int:\n     torch_dtype = get_torch_dtype(dtype)\n-    return torch.tensor([], dtype=torch_dtype).element_size()\n-\n+    # Use empty tensor construction to avoid Python-list to tensor overhead.\n+    # Shape-0 empty tensor suffices for querying element size.\n+    return torch.empty((), dtype=torch_dtype).element_size()\ndiff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py\nindex 978ca04e6..63c647002 100644\n--- a/cacheflow/parallel_utils/tensor_parallel/layers.py\n+++ b/cacheflow/parallel_utils/tensor_parallel/layers.py\n@@ -198,8 +198,8 @@ class VocabParallelEmbedding(torch.nn.Module):\n             # Build the mask.\n             input_mask = (input_ < self.vocab_start_index) | \\\n                          (input_ >= self.vocab_end_index)\n-            # Mask the input.\n-            masked_input = input_.clone() - self.vocab_start_index\n+            # Mask the input without redundant clone.\n+            masked_input = input_ - self.vocab_start_index\n             masked_input[input_mask] = 0\n         else:\n             masked_input = input_\n@@ -210,7 +210,7 @@ class VocabParallelEmbedding(torch.nn.Module):\n                                       self.sparse)\n         # Mask the output embedding.\n         if self.tensor_model_parallel_size > 1:\n-            output_parallel[input_mask, :] = 0.0\n+            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0.0)\n         # Reduce across all the model parallel GPUs.\n         output = reduce_from_tensor_model_parallel_region(output_parallel)\n         return output\n@@ -270,7 +270,9 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n             total_input = all_gather_buffer\n         else:\n             total_input = input\n-        grad_input = grad_output.matmul(weight)\n+        # Preallocate grad_input buffer and compute in-place to reduce allocations\n+        grad_input = get_global_memory_buffer().get_tensor(list(input.size()), input.dtype, \"mpu_grad_input\")\n+        torch.matmul(grad_output, weight, out=grad_input)\n \n         if ctx.sequence_parallel:\n             handle.wait()\n@@ -279,7 +281,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n         grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1],\n                                        grad_output.shape[2])\n         total_input = total_input.view(total_input.shape[0] * total_input.shape[1],\n-\t\t\t\t       total_input.shape[2])\n+                                       total_input.shape[2])\n \n         if ctx.async_grad_allreduce:\n             # Asynchronous all-reduce\n@@ -291,9 +293,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n         if ctx.sequence_parallel:\n             assert not ctx.async_grad_allreduce\n             dim_size = list(input.size())\n-            sub_grad_input = torch.empty(dim_size, dtype=input.dtype,\n-                                         device=torch.cuda.current_device(),\n-                                         requires_grad=False)\n+            sub_grad_input = get_global_memory_buffer().get_tensor(dim_size, input.dtype, \"mpu_sub_grad_input\")\n             # reduce_scatter\n             handle = torch.distributed._reduce_scatter_base(sub_grad_input, grad_input,\n                                                             group=get_tensor_model_parallel_group(),\ndiff --git a/cacheflow/worker/cache_engine.py b/cacheflow/worker/cache_engine.py\nindex 164b2a2a6..7064f741d 100644\n--- a/cacheflow/worker/cache_engine.py\n+++ b/cacheflow/worker/cache_engine.py\n@@ -32,6 +32,10 @@ class CacheEngine:\n         self.num_cpu_blocks = num_cpu_blocks\n         self.dtype = dtype\n \n+        # Cache the element size in bytes to avoid recomputation.\n+        # Using empty((), ...) avoids Python list to tensor overhead.\n+        self._dtype_element_size = torch.empty((), dtype=self.dtype).element_size()\n+\n         # Initialize the cache.\n         self.gpu_cache = self.allocate_gpu_cache()\n         self.cpu_cache = self.allocate_cpu_cache()\n@@ -43,7 +47,7 @@ class CacheEngine:\n         self.events = [torch.cuda.Event() for _ in range(num_layers)]\n \n     def get_key_block_shape(self) -> Tuple[int, int, int, int]:\n-        element_size = torch.tensor([], dtype=self.dtype).element_size()\n+        element_size = self._dtype_element_size\n         x = 16 // element_size\n         return (\n             self.num_heads,\ndiff --git a/cacheflow/worker/worker.py b/cacheflow/worker/worker.py\nindex f309cf12f..7beb210b9 100644\n--- a/cacheflow/worker/worker.py\n+++ b/cacheflow/worker/worker.py\n@@ -80,8 +80,8 @@ class Worker:\n             world_size=world_size,\n             rank=rank,\n         )\n-        # A small all_reduce for warmup.\n-        torch.distributed.all_reduce(torch.zeros(1).cuda())\n+        # A small all_reduce for warmup. Avoid zero-initialization cost.\n+        torch.distributed.all_reduce(torch.empty(1, device='cuda'))\n         initialize_model_parallel(tensor_parallel_size,\n                                   pipeline_parallel_size)\n \ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..31919469f\n--- /dev/null\n+++ b/model_patch.diff\n@@ -0,0 +1,180 @@\n+diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\n+new file mode 100644\n+index 000000000..a0a77bd81\n+--- /dev/null\n++++ b/benchmark/benchmark_latency.py\n+@@ -0,0 +1,55 @@\n++import argparse\n++import time\n++from typing import List\n++\n++import torch\n++\n++from cacheflow.master.simple_frontend import SimpleFrontend\n++from cacheflow.master.server import (Server, add_server_arguments,\n++                                     initialize_ray_cluster)\n++from cacheflow.sampling_params import SamplingParams\n++from cacheflow.utils import get_gpu_memory, get_cpu_memory\n++\n++\n++def main(args: argparse.Namespace):\n++    assert args.pipeline_parallel_size == 1, (\n++        'Pipeline parallelism is not supported yet.')\n++\n++    (num_nodes, num_devices_per_node, distributed_init_method,\n++     all_stage_devices) = initialize_ray_cluster(address='local',\n++                                                 num_gpus=args.tensor_parallel_size)\n++\n++    server = Server(args.model,\n++                    tensor_parallel_size=args.tensor_parallel_size,\n++                    pipeline_parallel_size=args.pipeline_parallel_size,\n++                    seed=args.seed,\n++                    dtype=args.dtype,\n++                    worker_use_ray=True)\n++\n++    frontend = SimpleFrontend(server)\n++\n++    prompts: List[str] = [\"Hello world!\"] * args.num_prompts\n++    params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=args.max_tokens)\n++\n++    # Warmup\n++    frontend.generate(prompts[:2], params)\n++\n++    t0 = time.time()\n++    frontend.generate(prompts, params)\n++    t1 = time.time()\n++    print({\n++        \"num_prompts\": args.num_prompts,\n++        \"max_tokens\": args.max_tokens,\n++        \"latency_s\": round(t1 - t0, 3),\n++    })\n++\n++\n++if __name__ == '__main__':\n++    parser = argparse.ArgumentParser()\n++    add_server_arguments(parser)\n++    parser.add_argument('--num-prompts', type=int, default=8)\n++    parser.add_argument('--max-tokens', type=int, default=8)\n++    parser.add_argument('--dtype', type=str, default='float16')\n++    parser.add_argument('--seed', type=int, default=0)\n++    args = parser.parse_args()\n++    main(args)\n+diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\n+index 8b132e442..7d1353f56 100644\n+--- a/cacheflow/models/attention.py\n++++ b/cacheflow/models/attention.py\n+@@ -34,11 +34,16 @@ class GPTCacheFlowAttention(nn.Module):\n+             raise ValueError('FlashAttention does not support head_size > 128.')\n+ \n+         device = query.device\n+-        prefix_sum = [0]\n+-        for prompt_len in prompt_lens:\n+-            prefix_sum.append(prefix_sum[-1] + prompt_len)\n+-        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n+-        max_prompt_len = max(prompt_lens)\n++        # Compute prefix_sum more efficiently on device using cumsum.\n++        if prompt_lens:\n++            lens = torch.tensor(prompt_lens, dtype=torch.int, device=device)\n++            prefix_sum = torch.empty(lens.numel() + 1, dtype=torch.int, device=device)\n++            prefix_sum[0] = 0\n++            torch.cumsum(lens, dim=0, out=prefix_sum[1:])\n++            max_prompt_len = int(lens.max().item())\n++        else:\n++            prefix_sum = torch.zeros(1, dtype=torch.int, device=device)\n++            max_prompt_len = 0\n+ \n+         # FIXME(woosuk): Unnecessary copy. Optimize this.\n+         qkv = torch.stack([query, key, value], dim=1)\n+diff --git a/cacheflow/models/utils.py b/cacheflow/models/utils.py\n+index 84e7fbce6..bd7b49bee 100644\n+--- a/cacheflow/models/utils.py\n++++ b/cacheflow/models/utils.py\n+@@ -20,5 +20,6 @@ def get_torch_dtype(dtype: Union[torch.dtype, str]) -> torch.dtype:\n+ \n+ def get_dtype_size(dtype: Union[torch.dtype, str]) -> int:\n+     torch_dtype = get_torch_dtype(dtype)\n+-    return torch.tensor([], dtype=torch_dtype).element_size()\n+-\n++    # Use empty tensor construction to avoid Python-list to tensor overhead.\n++    # Shape-0 empty tensor suffices for querying element size.\n++    return torch.empty((), dtype=torch_dtype).element_size()\n+diff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py\n+index 978ca04e6..63c647002 100644\n+--- a/cacheflow/parallel_utils/tensor_parallel/layers.py\n++++ b/cacheflow/parallel_utils/tensor_parallel/layers.py\n+@@ -198,8 +198,8 @@ class VocabParallelEmbedding(torch.nn.Module):\n+             # Build the mask.\n+             input_mask = (input_ < self.vocab_start_index) | \\\n+                          (input_ >= self.vocab_end_index)\n+-            # Mask the input.\n+-            masked_input = input_.clone() - self.vocab_start_index\n++            # Mask the input without redundant clone.\n++            masked_input = input_ - self.vocab_start_index\n+             masked_input[input_mask] = 0\n+         else:\n+             masked_input = input_\n+@@ -210,7 +210,7 @@ class VocabParallelEmbedding(torch.nn.Module):\n+                                       self.sparse)\n+         # Mask the output embedding.\n+         if self.tensor_model_parallel_size > 1:\n+-            output_parallel[input_mask, :] = 0.0\n++            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0.0)\n+         # Reduce across all the model parallel GPUs.\n+         output = reduce_from_tensor_model_parallel_region(output_parallel)\n+         return output\n+@@ -270,7 +270,9 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n+             total_input = all_gather_buffer\n+         else:\n+             total_input = input\n+-        grad_input = grad_output.matmul(weight)\n++        # Preallocate grad_input buffer and compute in-place to reduce allocations\n++        grad_input = get_global_memory_buffer().get_tensor(list(input.size()), input.dtype, \"mpu_grad_input\")\n++        torch.matmul(grad_output, weight, out=grad_input)\n+ \n+         if ctx.sequence_parallel:\n+             handle.wait()\n+@@ -279,7 +281,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n+         grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1],\n+                                        grad_output.shape[2])\n+         total_input = total_input.view(total_input.shape[0] * total_input.shape[1],\n+-\t\t\t\t       total_input.shape[2])\n++                                       total_input.shape[2])\n+ \n+         if ctx.async_grad_allreduce:\n+             # Asynchronous all-reduce\n+@@ -291,9 +293,7 @@ class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n+         if ctx.sequence_parallel:\n+             assert not ctx.async_grad_allreduce\n+             dim_size = list(input.size())\n+-            sub_grad_input = torch.empty(dim_size, dtype=input.dtype,\n+-                                         device=torch.cuda.current_device(),\n+-                                         requires_grad=False)\n++            sub_grad_input = get_global_memory_buffer().get_tensor(dim_size, input.dtype, \"mpu_sub_grad_input\")\n+             # reduce_scatter\n+             handle = torch.distributed._reduce_scatter_base(sub_grad_input, grad_input,\n+                                                             group=get_tensor_model_parallel_group(),\n+diff --git a/cacheflow/worker/cache_engine.py b/cacheflow/worker/cache_engine.py\n+index 164b2a2a6..7064f741d 100644\n+--- a/cacheflow/worker/cache_engine.py\n++++ b/cacheflow/worker/cache_engine.py\n+@@ -32,6 +32,10 @@ class CacheEngine:\n+         self.num_cpu_blocks = num_cpu_blocks\n+         self.dtype = dtype\n+ \n++        # Cache the element size in bytes to avoid recomputation.\n++        # Using empty((), ...) avoids Python list to tensor overhead.\n++        self._dtype_element_size = torch.empty((), dtype=self.dtype).element_size()\n++\n+         # Initialize the cache.\n+         self.gpu_cache = self.allocate_gpu_cache()\n+         self.cpu_cache = self.allocate_cpu_cache()\n+@@ -43,7 +47,7 @@ class CacheEngine:\n+         self.events = [torch.cuda.Event() for _ in range(num_layers)]\n+ \n+     def get_key_block_shape(self) -> Tuple[int, int, int, int]:\n+-        element_size = torch.tensor([], dtype=self.dtype).element_size()\n++        element_size = self._dtype_element_size\n+         x = 16 // element_size\n+         return (\n+             self.num_heads,\n+diff --git a/cacheflow/worker/worker.py b/cacheflow/w\n\\ No newline at end of file\n", "model_name_or_path": "gpt-5-2025-08-07"}
