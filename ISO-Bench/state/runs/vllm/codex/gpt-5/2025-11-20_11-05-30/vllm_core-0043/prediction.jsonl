{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-6d917d0", "model_patch": "diff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex ed7e06cab..041f5bcd5 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,6 +1,7 @@\n import enum\n+import heapq\n from abc import ABC, abstractmethod\n-from typing import OrderedDict, Tuple\n+from typing import Dict, List, Tuple\n \n \n class EvictionPolicy(enum.Enum):\n@@ -75,8 +76,17 @@ class LRUEvictor(Evictor):\n     highest num_hashed_tokens value, then one will be chose arbitrarily\n     \"\"\"\n \n+    # CLEANUP_THRESHOLD determines the maximum allowable size of the priority\n+    # queue relative to the free table size. When this threshold is exceeded,\n+    # a cleanup operation is triggered to reduce memory usage caused by lazy\n+    # deletion of stale heap entries.\n+    CLEANUP_THRESHOLD = 50\n+\n     def __init__(self):\n-        self.free_table: OrderedDict[int, BlockMetaData] = OrderedDict()\n+        # Map of block_id -> BlockMetaData\n+        self.free_table: Dict[int, BlockMetaData] = {}\n+        # Min-heap of (last_accessed, -num_hashed_tokens, block_id)\n+        self.priority_queue: List[Tuple[float, int, int]] = []\n \n     def __contains__(self, block_id: int) -> bool:\n         return block_id in self.free_table\n@@ -85,44 +95,83 @@ class LRUEvictor(Evictor):\n         if len(self.free_table) == 0:\n             raise ValueError(\"No usable cache memory left\")\n \n-        evicted_block, evicted_block_id = None, None\n-        # The blocks with the lowest timestamps should be placed consecutively\n-        # at the start of OrderedDict. Loop through all these blocks to\n-        # find the one with maximum number of hashed tokens.\n-        for _id, block in self.free_table.items():\n-            if evicted_block is None:\n-                evicted_block, evicted_block_id = block, _id\n+        # Pop from the heap until we find a valid, non-stale entry.\n+        while self.priority_queue:\n+            last_accessed, neg_num_tokens, block_id = heapq.heappop(\n+                self.priority_queue)\n+            meta = self.free_table.get(block_id)\n+            if meta is None:\n+                # Block was removed; stale heap entry.\n+                continue\n+            # Validate that the heap entry matches current metadata. If the\n+            # metadata has changed since this entry was pushed (updated\n+            # last_accessed or num_hashed_tokens), this entry is stale.\n+            if (meta.last_accessed != last_accessed or\n+                    -meta.num_hashed_tokens != neg_num_tokens):\n                 continue\n-            if evicted_block.last_accessed < block.last_accessed:\n-                break\n-            if evicted_block.num_hashed_tokens < block.num_hashed_tokens:\n-                evicted_block, evicted_block_id = block, _id\n \n-        assert evicted_block is not None\n-        assert evicted_block_id is not None\n-        self.free_table.pop(evicted_block_id)\n+            # Found a valid entry to evict.\n+            self.free_table.pop(block_id)\n+            return block_id, meta.content_hash\n \n-        return evicted_block_id, evicted_block.content_hash\n+        # If we exhausted the heap but free_table isn't empty (shouldn't\n+        # normally happen), rebuild the heap and try again.\n+        self._rebuild_heap()\n+        if not self.priority_queue:\n+            raise ValueError(\"No usable cache memory left\")\n+        last_accessed, neg_num_tokens, block_id = heapq.heappop(\n+            self.priority_queue)\n+        meta = self.free_table.pop(block_id)\n+        return block_id, meta.content_hash\n \n     def add(self, block_id: int, content_hash: int, num_hashed_tokens: int,\n             last_accessed: float):\n-        self.free_table[block_id] = BlockMetaData(content_hash,\n-                                                  num_hashed_tokens,\n-                                                  last_accessed)\n+        meta = BlockMetaData(content_hash, num_hashed_tokens, last_accessed)\n+        self.free_table[block_id] = meta\n+        self._push_heap(block_id, meta)\n+        self._maybe_cleanup()\n \n     def update(self, block_id: int, last_accessed: float):\n-        self.free_table[block_id].last_accessed = last_accessed\n+        meta = self.free_table[block_id]\n+        meta.last_accessed = last_accessed\n+        # Push a new entry to the heap; old one becomes stale (lazy deletion)\n+        self._push_heap(block_id, meta)\n+        self._maybe_cleanup()\n \n     def remove(self, block_id: int):\n         if block_id not in self.free_table:\n             raise ValueError(\n                 \"Attempting to remove block that's not in the evictor\")\n+        # Remove from the table. Any existing heap entries are lazily discarded.\n         self.free_table.pop(block_id)\n \n     @property\n     def num_blocks(self) -> int:\n         return len(self.free_table)\n \n+    def _push_heap(self, block_id: int, meta: BlockMetaData) -> None:\n+        # Use negative num_hashed_tokens so that larger token counts are\n+        # preferred when last_accessed ties (min-heap behavior). \n+        heapq.heappush(self.priority_queue,\n+                       (meta.last_accessed, -meta.num_hashed_tokens, block_id))\n+\n+    def _rebuild_heap(self) -> None:\n+        self.priority_queue = [\n+            (m.last_accessed, -m.num_hashed_tokens, bid)\n+            for bid, m in self.free_table.items()\n+        ]\n+        heapq.heapify(self.priority_queue)\n+\n+    def _maybe_cleanup(self) -> None:\n+        # Trigger heap cleanup when number of heap entries is too large\n+        # compared to live entries in the table.\n+        table_size = len(self.free_table)\n+        if table_size == 0:\n+            self.priority_queue.clear()\n+            return\n+        if len(self.priority_queue) > (self.CLEANUP_THRESHOLD * table_size):\n+            self._rebuild_heap()\n+\n \n def make_evictor(eviction_policy: EvictionPolicy) -> Evictor:\n     if eviction_policy == EvictionPolicy.LRU:\n", "model_name_or_path": "gpt-5-2025-08-07"}
