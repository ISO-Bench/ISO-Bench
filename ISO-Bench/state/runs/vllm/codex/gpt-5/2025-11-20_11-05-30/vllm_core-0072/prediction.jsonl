{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-802329d", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 48cdebee9..a26257b04 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -161,12 +161,8 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm import _custom_ops as ops\n \n-        # __setattr__ in nn.Module (called by `self.cos_sin_cache = ...`)\n-        # is expensive, so avoid calling it if possible\n-        if self.cos_sin_cache.device != query.device or \\\n-            self.cos_sin_cache.dtype != query.dtype:\n-            self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n-                                                       dtype=query.dtype)\n+        self.cos_sin_cache = self.cos_sin_cache.to(query.device,\n+                                                   dtype=query.dtype)\n \n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex a705aeffe..647eda892 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -95,7 +95,12 @@ class AutoWeightsLoader:\n         self,\n         weights: Iterable[Tuple[str, torch.Tensor]],\n     ) -> Iterable[Tuple[str, Iterable[Tuple[str, torch.Tensor]]]]:\n-        weights_by_parts = ((weight_name.split(\".\", 1), weight_data)\n+        # Faster single-split than str.split with maxsplit=1, avoids list allocs\n+        def _split_once(name: str):\n+            idx = name.find(\".\")\n+            return (name, \"\") if idx == -1 else (name[:idx], name[idx + 1:])\n+\n+        weights_by_parts = ((_split_once(weight_name), weight_data)\n                             for weight_name, weight_data in weights)\n \n         for prefix, group in itertools.groupby(weights_by_parts,\n@@ -600,9 +605,11 @@ def make_empty_intermediate_tensors_factory(keys: List[str], hidden_size: int):\n         dtype: torch.dtype,\n         device: torch.device,\n     ) -> IntermediateTensors:\n+        # Avoid zero-initialization cost; these buffers are immediately\n+        # written by downstream kernels. Using torch.empty saves time.\n+        shape = (batch_size, hidden_size)\n         return IntermediateTensors({\n-            key:\n-            torch.zeros((batch_size, hidden_size), dtype=dtype, device=device)\n+            key: torch.empty(shape, dtype=dtype, device=device)\n             for key in keys\n         })\n \n@@ -650,4 +657,32 @@ def cast_overflow_tensors(\n     if tensors.isinf().any() or tensors.isnan().any():\n         clamp_value = torch.finfo(tensors.dtype).max - offset\n         tensors = torch.clamp(tensors, min=-clamp_value, max=clamp_value)\n-    return tensors\n\\ No newline at end of file\n+    return tensors\n+\n+\n+def fast_topk(\n+    input: torch.Tensor,\n+    k: int,\n+    dim: int = -1,\n+    largest: bool = True,\n+    sorted: bool = True,\n+):\n+    \"\"\"\n+    A faster topk for common k==1 cases.\n+\n+    Falls back to torch.topk for general k. When k == 1, uses a\n+    reduction (max/min) which avoids the heavier topk machinery and\n+    can be substantially faster on typical MoE gating shapes.\n+\n+    Returns (values, indices) like torch.topk.\n+    \"\"\"\n+    if k == 1:\n+        if largest:\n+            vals, idxs = torch.max(input, dim=dim, keepdim=True)\n+        else:\n+            vals, idxs = torch.min(input, dim=dim, keepdim=True)\n+        # torch.topk returns indices with dtype long\n+        return vals, idxs.to(torch.long)\n+\n+    # General case: defer to torch.topk implementation\n+    return torch.topk(input, k, dim=dim, largest=largest, sorted=sorted)\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex f3fff585b..e43aba2c0 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -199,6 +199,8 @@ from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n \n import torch\n from compressed_tensors.quantization import QuantizationStrategy\n+import torch.nn.functional as F\n+\n \n from vllm import _custom_ops as ops\n from vllm import envs\n@@ -223,7 +225,6 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n from vllm.model_executor.layers.quantization.utils.quant_utils import (\n     scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n-from vllm.platforms import current_platform\n from vllm.utils import cdiv, round_down\n \n try:\n@@ -532,14 +533,10 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=F.pad(_chunk_cu_seq_lens, (1, 0, 0, 0), mode='constant', value=0),\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -628,14 +625,6 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n \n         self.rotary_emb = rotary_emb\n \n-        if current_platform.is_cuda():\n-            # Hack for V1 for now to avoid torch library overhead (since we are\n-            # already inside an attention custom op), pull out the forward\n-            # method from the rotary embedding and call it directly (and avoid\n-            # calling forward_native, when we can call forward_cuda)\n-            # TODO(lucas): we should probably find a cleaner way to do this\n-            self.rotary_emb = rotary_emb.forward_cuda\n-\n         self.q_proj = q_proj\n         self.kv_b_proj = kv_b_proj\n         self.o_proj = o_proj\n", "model_name_or_path": "gpt-5-2025-08-07"}
