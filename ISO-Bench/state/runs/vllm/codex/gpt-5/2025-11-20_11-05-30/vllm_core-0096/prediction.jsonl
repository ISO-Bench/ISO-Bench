{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-92b0ce2", "model_patch": "diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b0f52167..0df9d8f6b 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -471,13 +471,17 @@ class MLACommonMetadataBuilder(Generic[M]):\n               common_prefix_len: int) -> M:\n         assert self._num_decodes + self._num_prefills == num_reqs\n \n+        # Note: be careful with CPU <-> GPU transfers here to avoid\n+        # unnecessary synchronizations on the default stream.\n         device = self.runner.device\n-        query_start_loc = self.runner.query_start_loc_cpu[:num_reqs + 1].to(\n-            device, non_blocking=True)\n-        seq_lens = self.runner.seq_lens_cpu[:num_reqs].to(device,\n-                                                          non_blocking=True)\n+        # Fetch block table device tensor first to avoid accidental syncs later.\n         block_table = (\n             self.runner.input_batch.block_table.get_device_tensor()[:num_reqs])\n+        # Host tensors copied once; keep CPU views for small scalar reductions.\n+        query_start_loc = self.runner.query_start_loc_cpu[:num_reqs + 1].to(\n+            device, non_blocking=True)\n+        seq_lens_cpu = self.runner.seq_lens_cpu[:num_reqs]\n+        seq_lens = seq_lens_cpu.to(device, non_blocking=True)\n         slot_mapping = self.runner.slot_mapping_cpu[:num_actual_tokens].to(\n             device, non_blocking=True).long()\n         input_positions = self.runner.positions_cpu[:num_actual_tokens].to(\n@@ -490,16 +494,21 @@ class MLACommonMetadataBuilder(Generic[M]):\n \n             context_lens_cpu = self.runner.input_batch.\\\n                 num_computed_tokens_cpu_tensor[reqs_start:num_reqs]\n-            context_lens = context_lens_cpu.to(device, non_blocking=True)\n+\n+            # Avoid GPU syncs by checking context presence on CPU first.\n+            has_context = context_lens_cpu.max().item() > 0\n \n             chunked_context_metadata = None\n-            if self.chunked_prefill_enabled and self._num_prefills > 0 \\\n-                and context_lens.max() > 0:\n+            if self.chunked_prefill_enabled and self._num_prefills > 0 and \\\n+                    has_context:\n                 # NOTE: it is recommend you read the `Chunked Prefill` section\n                 # in the comment at the top of the file before trying to\n                 # understand the following code\n \n-                num_prefills_with_context = (context_lens > 0).sum().item()\n+                # Compute simple reductions on CPU to avoid device -> host\n+                # synchronizations.\n+                num_prefills_with_context = int(\n+                    (context_lens_cpu > 0).sum().item())\n \n                 # currently we allocate an equal amount of workspace for each\n                 # prefill in the batch, we could probably use a more advanced\n@@ -516,12 +525,14 @@ class MLACommonMetadataBuilder(Generic[M]):\n                                                self.page_size)\n \n                 assert max_context_chunk > 0\n-                num_chunks = cdiv(context_lens.max(), max_context_chunk)\n+                num_chunks = cdiv(context_lens_cpu.max().item(),\n+                                   max_context_chunk)\n \n                 # if `max_context_chunk = 256`, `num_chunks = 3`, and\n                 #   `num_prefills_with_context = 4`, create a tensor that looks\n                 # like\n                 #  [[0, 0, 0, 0], [256, 256, 256, 256], [512, 512, 512, 512]]\n+                context_lens = context_lens_cpu.to(device, non_blocking=True)\n                 chunk_starts = \\\n                     torch.arange(num_chunks, device=device, dtype=torch.int32) \\\n                     .unsqueeze(1).expand(-1, self._num_prefills) \\\n@@ -531,14 +542,17 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+                # Prepend an initial zero without allocating an entire zeros\n+                # tensor that we immediately concatenate.\n+                cu_seq_lens = torch.empty((num_chunks, self._num_prefills + 1),\n+                                          dtype=torch.int32,\n+                                          device=device)\n+                cu_seq_lens[:, 0] = 0\n+                cu_seq_lens[:, 1:] = _chunk_cu_seq_lens\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=cu_seq_lens,\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -553,7 +567,8 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 block_table=block_table[reqs_start:, ...],\n                 query_start_loc=query_start_loc[reqs_start:] -\n                 query_start_loc[reqs_start],\n-                max_query_len=seq_lens[reqs_start:].max().item(),\n+                # Avoid device sync by using CPU reduction for this scalar.\n+                max_query_len=seq_lens_cpu[reqs_start:].max().item(),\n                 chunked_context=chunked_context_metadata,\n             )\n \n@@ -863,6 +878,9 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         output = None\n         iters = len(prefill_metadata.chunked_context.seq_tot)\n         workspace = prefill_metadata.chunked_context.workspace\n+        # Reuse merge buffers to avoid per-iteration allocations.\n+        merge_output_tmp = None\n+        merge_output_lse_tmp = None\n \n         for i in range(iters):\n             toks = prefill_metadata.chunked_context.seq_tot[i]\n@@ -912,18 +930,20 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                if merge_output_tmp is None:\n+                    merge_output_tmp = torch.empty_like(output)\n+                    merge_output_lse_tmp = torch.empty_like(output_lse)\n                 merge_attn_states(\n-                    output=output_tmp,\n-                    output_lse=output_lse_tmp,\n+                    output=merge_output_tmp,\n+                    output_lse=merge_output_lse_tmp,\n                     prefix_output=output,\n                     prefix_lse=output_lse,\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                # Ping-pong buffers without new allocations.\n+                output, merge_output_tmp = merge_output_tmp, output\n+                output_lse, merge_output_lse_tmp = merge_output_lse_tmp, output_lse\n \n         return output, output_lse\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
