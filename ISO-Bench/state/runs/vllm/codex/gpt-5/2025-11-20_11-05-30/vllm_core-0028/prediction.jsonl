{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-526078a", "model_patch": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101a0..bfd5340ac 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -155,9 +155,10 @@ def create_and_prepopulate_kv_cache(\n         perm = torch.arange(\n             1, blocks_end)  # Sequential order starting from block 1\n \n-    inv_perm = torch.zeros(blocks_end, dtype=torch.long, device=device)\n-    inv_perm[1:] = torch.argsort(\n-        perm) + 1  # Add 1 to account for starting from block 1\n+    # Avoid zero-initialization; set first element explicitly\n+    inv_perm = torch.empty(blocks_end, dtype=torch.long, device=device)\n+    inv_perm[0] = 0\n+    inv_perm[1:] = torch.argsort(perm) + 1  # Account for starting from block 1\n     kv_cache[:, 1:blocks_end, ...] = kv_cache[:, perm, ...]\n \n     # Construct the right block table\n@@ -212,7 +213,7 @@ def run_attention_backend(backend: _Backend, kv_cache_spec: FullAttentionSpec,\n \n         from vllm.v1.attention.backends.flashinfer import PerLayerParameters\n \n-        def mock_get_per_layer_parameters(vllm_config):\n+        def mock_get_per_layer_parameters(vllm_config, impl_cls):\n             # Return mock parameters for a single layer\n             head_size = vllm_config.model_config.get_head_size()\n             return {\n@@ -297,7 +298,8 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n     5. Comparing the vLLM backend's output to the ground-truth SDPA output.\n     \"\"\"\n     batch_spec = BATCH_SPECS[batch_spec_name]\n-    vllm_config = create_vllm_config(model_name=model)\n+    vllm_config = create_vllm_config(model_name=model,\n+                                     max_model_len=max(batch_spec.seq_lens))\n     device = torch.device(\"cuda:0\")\n \n     kv_cache_spec = create_standard_kv_cache_spec(vllm_config)\n@@ -359,12 +361,15 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n         #  (context_len + i)\n         kv_len = s_len\n         offset = context_len\n-        attn_mask = torch.full((q_len, kv_len),\n-                               float('-inf'),\n-                               device=device,\n-                               dtype=dtype)\n-        for i in range(q_len):\n-            attn_mask[i, :offset + i + 1] = 0.0\n+        # Vectorized causal mask construction\n+        rows = torch.arange(q_len, device=device).unsqueeze(1)\n+        cols = torch.arange(kv_len, device=device).unsqueeze(0)\n+        allowed = cols <= (offset + rows)\n+        attn_mask = torch.where(\n+            allowed,\n+            torch.zeros((), device=device, dtype=dtype),\n+            torch.full((), float('-inf'), device=device, dtype=dtype),\n+        )\n \n         sdpa_out_i = torch.nn.functional.scaled_dot_product_attention(\n             q_sdpa_in,\ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdda5..2c0395ea2 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -43,12 +43,16 @@ def create_common_attn_metadata(\n         max_block_idx: int = 1000) -> CommonAttentionMetadata:\n     \"\"\"Create CommonAttentionMetadata from a BatchSpec and ModelParams.\"\"\"\n     # Create query start locations\n-    query_start_loc = torch.zeros(batch_spec.batch_size + 1,\n+    # Avoid zero-initialization and use cumsum with out-parameter\n+    query_start_loc = torch.empty(batch_spec.batch_size + 1,\n                                   dtype=torch.int32,\n                                   device=device)\n-    query_start_loc[1:] = torch.tensor(batch_spec.query_lens,\n-                                       dtype=torch.int32,\n-                                       device=device).cumsum(0)\n+    query_start_loc[0] = 0\n+    torch.cumsum(torch.tensor(batch_spec.query_lens,\n+                              dtype=torch.int32,\n+                              device=device),\n+                 dim=0,\n+                 out=query_start_loc[1:])\n     query_start_loc_cpu = query_start_loc.cpu()\n     num_tokens = batch_spec.compute_num_tokens()\n \ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26c8..31ea760b6 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -422,12 +422,15 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                 < block_table_bounds.unsqueeze(1))\n         paged_kv_indices = block_table_tensor[mask]\n \n-        paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n-                        dtype=block_table_bounds.dtype,\n-                        device=block_table_bounds.device),\n-            block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n-        ])\n+        # Build indptr without extra zero-initialization or temp tensors\n+        paged_kv_indptr = torch.empty(block_table_bounds.numel() + 1,\n+                                      dtype=torch.int32,\n+                                      device=block_table_bounds.device)\n+        paged_kv_indptr[0] = 0\n+        torch.cumsum(block_table_bounds,\n+                     dim=0,\n+                     dtype=torch.int32,\n+                     out=paged_kv_indptr[1:])\n \n         paged_kv_last_page_len = seq_lens % page_size\n         paged_kv_last_page_len = torch.where(paged_kv_last_page_len == 0,\n", "model_name_or_path": "gpt-5-2025-08-07"}
