diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py
index 74fe378fd..c44639509 100644
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -1,5 +1,6 @@
 import asyncio
 import atexit
+import gc
 import importlib
 import inspect
 import multiprocessing
@@ -99,6 +100,17 @@ async def lifespan(app: FastAPI):
             task.add_done_callback(_running_tasks.remove)
         else:
             task = None
+
+        # Mark the startup heap as static so that it's ignored by GC.
+        # This reduces GC pause times for longâ€‘lived servers.
+        try:
+            gc.collect()
+            # gc.freeze is available on Python 3.11+
+            if hasattr(gc, "freeze"):
+                gc.freeze()  # type: ignore[attr-defined]
+        except Exception:
+            # Be conservative: GC tuning is best-effort.
+            pass
         try:
             yield
         finally:
diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py
index 14e41346d..e908b2ec9 100644
--- a/vllm/entrypoints/openai/protocol.py
+++ b/vllm/entrypoints/openai/protocol.py
@@ -19,6 +19,9 @@ from vllm.utils import random_uuid, resolve_obj_by_qualname
 
 logger = init_logger(__name__)
 
+# Cache of model field names per class to avoid recomputing on each validation
+_FIELD_NAME_CACHE: Dict[type, set] = {}
+
 # torch is mocked during docs generation,
 # so we have to provide the values as literals
 _MOCK_LONG_INFO = Namespace(min=-9223372036854775808, max=9223372036854775807)
@@ -47,11 +50,15 @@ class OpenAIBaseModel(BaseModel):
     def __log_extra_fields__(cls, data):
         if isinstance(data, dict):
             # Get all class field names and their potential aliases
-            field_names = set()
-            for field_name, field in cls.model_fields.items():
-                field_names.add(field_name)
-                if hasattr(field, 'alias') and field.alias:
-                    field_names.add(field.alias)
+            field_names = _FIELD_NAME_CACHE.get(cls)
+            if field_names is None:
+                field_names = set()
+                for field_name, field in cls.model_fields.items():
+                    field_names.add(field_name)
+                    alias = getattr(field, 'alias', None)
+                    if alias:
+                        field_names.add(alias)
+                _FIELD_NAME_CACHE[cls] = field_names
 
             # Compare against both field names and aliases
             extra_fields = data.keys() - field_names
@@ -170,10 +177,15 @@ def get_logits_processors(processors: Optional[LogitsProcessors],
                           pattern: Optional[str]) -> Optional[List[Any]]:
     if processors and pattern:
         logits_processors = []
+        # Compile regex once per call
+        try:
+            compiled = re.compile(pattern)
+        except re.error as e:
+            raise ValueError(f"Invalid logits processor pattern: {e}")
         for processor in processors:
             qualname = processor if isinstance(processor,
                                                str) else processor.qualname
-            if not re.match(pattern, qualname):
+            if not compiled.match(qualname):
                 raise ValueError(
                     f"Logits processor '{qualname}' is not allowed by this "
                     "server. See --logits-processor-pattern engine argument "
diff --git a/vllm/envs.py b/vllm/envs.py
index c4a568c68..9d864272a 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -93,6 +93,8 @@ def get_default_config_root():
 
 # begin-env-vars-definition
 
+# Lazily evaluated environment variables map. Use a small cache so hot paths
+# (e.g., API request handling) avoid repeated getenv and path operations.
 environment_variables: Dict[str, Callable[[], Any]] = {
 
     # ================== Installation Time Env Vars ==================
@@ -473,11 +475,31 @@ environment_variables: Dict[str, Callable[[], Any]] = {
 
 
 def __getattr__(name: str):
-    # lazy evaluation of environment variables
+    # lazy evaluation of environment variables with caching
     if name in environment_variables:
-        return environment_variables[name]()
+        cache = globals().setdefault("_ENV_CACHE", {})  # type: ignore[assignment]
+        if name in cache:
+            return cache[name]
+        value = environment_variables[name]()
+        cache[name] = value
+        return value
     raise AttributeError(f"module {__name__!r} has no attribute {name!r}")
 
 
 def __dir__():
     return list(environment_variables.keys())
+
+
+def _invalidate_env_cache(keys: Optional[List[str]] = None) -> None:
+    """Internal: clear cached env values to reflect changed os.environ.
+
+    Useful for tests or when environment variables are mutated at runtime.
+    """
+    cache = globals().get("_ENV_CACHE")
+    if not cache:
+        return
+    if keys is None:
+        cache.clear()  # type: ignore[call-arg]
+    else:
+        for k in keys:
+            cache.pop(k, None)  # type: ignore[union-attr]
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index 3f097ca7f..2e7c87bcd 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -248,7 +248,8 @@ class AsyncLLM(EngineClient):
             while True:
                 # Note: drain queue without await if possible (avoids
                 # task switching under load which helps performance).
-                out = q.get_nowait() if q.qsize() > 0 else await q.get()
+                get_nowait = q.get_nowait
+                out = get_nowait() if q.qsize() > 0 else await q.get()
 
                 # Note: both Detokenizer and EngineCore handle their
                 # own request cleanup based on finished.
@@ -268,15 +269,16 @@ class AsyncLLM(EngineClient):
 
     def _process_request_outputs(self, request_outputs: List[RequestOutput]):
         """Process outputs by putting them into per-request queues."""
-
+        rid_to_queue = self.rid_to_queue
         for request_output in request_outputs:
             request_id = request_output.request_id
 
             # Note: it is possible a request was aborted and removed from
             # the state due to client cancellations, so if we encounter a
             # request id not in the state, we skip.
-            if request_id in self.rid_to_queue:
-                self.rid_to_queue[request_id].put_nowait(request_output)
+            q = rid_to_queue.get(request_id)
+            if q is not None:
+                q.put_nowait(request_output)
 
     async def _run_output_handler(self):
         """Background loop: pulls from EngineCore and pushes to AsyncStreams."""
diff --git a/vllm/v1/request.py b/vllm/v1/request.py
index f4783ae36..2fda804ed 100644
--- a/vllm/v1/request.py
+++ b/vllm/v1/request.py
@@ -14,6 +14,27 @@ if TYPE_CHECKING:
 
 
 class Request:
+    __slots__ = (
+        "request_id",
+        "inputs",
+        "sampling_params",
+        "eos_token_id",
+        "metrics",
+        "lora_request",
+        "status",
+        "stop_reason",
+        "max_tokens",
+        "prompt",
+        "prompt_token_ids",
+        "num_prompt_tokens",
+        "_output_token_ids",
+        "_all_token_ids",
+        "num_computed_tokens",
+        "mm_positions",
+        "mm_inputs",
+        "mm_hashes",
+        "_kv_block_hashes",
+    )
 
     def __init__(
         self,
diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py
index e79145300..423f70ae9 100644
--- a/vllm/v1/worker/gpu_input_batch.py
+++ b/vllm/v1/worker/gpu_input_batch.py
@@ -59,23 +59,24 @@ class InputBatch:
         # Find a way to reduce the CPU memory usage.
         # This buffer is not directly transferred to the GPU, so it does not
         # need to be pinned.
-        self.token_ids_cpu_tensor = torch.zeros(
+        self.token_ids_cpu_tensor = torch.empty(
             (max_num_reqs, max_model_len),
             device="cpu",
             dtype=torch.int32,
             pin_memory=False,
         )
         self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()
+        self.num_tokens = np.zeros(max_num_reqs, dtype=np.int32)
         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)
         self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)
 
         # Attention-related.
-        self.block_table = torch.zeros(
+        self.block_table = torch.empty(
             (max_num_reqs, max_num_blocks_per_req),
             device=self.device,
             dtype=torch.int32,
         )
-        self.block_table_cpu_tensor = torch.zeros(
+        self.block_table_cpu_tensor = torch.empty(
             (max_num_reqs, max_num_blocks_per_req),
             device="cpu",
             dtype=torch.int32,
@@ -189,6 +190,7 @@ class InputBatch:
         end_idx = start_idx + len(request.output_token_ids)
         self.token_ids_cpu[req_index,
                            start_idx:end_idx] = request.output_token_ids
+        self.num_tokens[req_index] = request.num_tokens
 
         self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens
         num_blocks = len(request.block_ids)
@@ -290,14 +292,17 @@ class InputBatch:
             self.req_ids[last_req_index] = None
             self.req_id_to_index[req_id] = empty_index
 
-            # TODO(woosuk): Optimize the copy of token_ids_cpu and
-            # block_table_cpu.
-            self.token_ids_cpu[empty_index] = self.token_ids_cpu[
-                last_req_index]
+            # Optimize: Copy only the valid token range.
+            num_tokens = self.num_tokens[last_req_index]
+            if num_tokens > 0:
+                self.token_ids_cpu[empty_index, :num_tokens] = (
+                    self.token_ids_cpu[last_req_index, :num_tokens])
+            self.num_tokens[empty_index] = num_tokens
             self.num_prompt_tokens[empty_index] = \
                 self.num_prompt_tokens[last_req_index]
             self.num_computed_tokens_cpu[
                 empty_index] = self.num_computed_tokens_cpu[last_req_index]
+            # NOTE(woosuk): block_table_cpu copy may be heavy; copying full row.
             self.block_table_cpu[empty_index] = self.block_table_cpu[
                 last_req_index]
             self.temperature_cpu[empty_index] = self.temperature_cpu[
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 995de54e8..a1b8a683f 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -143,27 +143,27 @@ class GPUModelRunner:
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                              dtype=torch.int32,
                                              device="cpu",
                                              pin_memory=self.pin_memory)
@@ -644,6 +644,7 @@ class GPUModelRunner:
                 # Append the sampled token to the output token ids.
                 token_id = sampled_token_ids[i]
                 self.input_batch.token_ids_cpu[i, seq_len] = token_id
+                self.input_batch.num_tokens[i] = seq_len + 1
                 req_state.output_token_ids.append(token_id)
             else:
                 # Ignore the sampled token from the partial request.
