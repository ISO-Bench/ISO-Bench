{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0e74d79", "model_patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..ed4a7b2e0 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -455,7 +455,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                       is not None)\n         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n-        self.decode_only = True\n+        # decode_only is a per-batch property and should be reset in prepare()\n \n         # Attention metadata inputs.\n         if self.attn_backend is not None:\n@@ -477,6 +477,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 finished_requests_ids: Optional[List[str]] = None) -> None:\n         self.finished_requests_ids = finished_requests_ids\n \n+        # if the current batch is decode-only.\n+        # will be set to False if there is any non-decode request.\n+        self.decode_only = True\n+\n         # Intermediate data (data in CPU before going to GPU) for\n         # the current sequence group.\n         self.inter_data_list: List[\n@@ -1437,15 +1441,19 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n \n         # Prepare dummy inputs. These will be reused for all batch sizes.\n         max_batch_size = self.max_batchsize_to_capture\n-        input_tokens = torch.zeros(max_batch_size,\n+        input_tokens = torch.empty(max_batch_size,\n                                    dtype=torch.long,\n                                    device=self.device)\n-        input_positions = torch.zeros(max_batch_size,\n-                                      dtype=torch.long,\n-                                      device=self.device)\n+        # Allocate positions with final shape directly to avoid zero-fill and\n+        # tensor tiling overhead during CUDA graph capture.\n         if self.model_config.uses_mrope:\n-            input_positions = torch.tile(input_positions,\n-                                         (3, 1)).cuda(device=self.device)\n+            input_positions = torch.empty((3, max_batch_size),\n+                                          dtype=torch.long,\n+                                          device=self.device)\n+        else:\n+            input_positions = torch.empty(max_batch_size,\n+                                          dtype=torch.long,\n+                                          device=self.device)\n         # Prepare dummy previous_hidden_states only if needed by the model.\n         # This is used by draft models such as EAGLE.\n         previous_hidden_states = None\n", "model_name_or_path": "gpt-5-2025-08-07"}
