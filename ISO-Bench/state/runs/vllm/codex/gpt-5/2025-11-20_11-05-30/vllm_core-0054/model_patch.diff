diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index e0169f1a4..792f9231a 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -275,9 +275,13 @@ class AsyncLLM(EngineClient):
                 out = q.get_nowait() if not q.empty() else await q.get()
 
                 # Coalesce any additional queued outputs
-                while not q.empty():
-                    next_out = q.get_nowait()
-                    if sampling_params.output_kind == RequestOutputKind.DELTA:
+                is_delta = sampling_params.output_kind == RequestOutputKind.DELTA
+                while True:
+                    try:
+                        next_out = q.get_nowait()
+                    except asyncio.QueueEmpty:
+                        break
+                    if is_delta:
                         out.add(next_out)
                     else:
                         out = next_out
@@ -315,6 +319,7 @@ class AsyncLLM(EngineClient):
                     slices = np.array_split(
                         outputs.outputs,
                         cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))
+                num_slices = len(slices)
 
                 for i, outputs_slice in enumerate(slices):
                     # 2) Process EngineCoreOutputs.
@@ -324,7 +329,7 @@ class AsyncLLM(EngineClient):
                     assert not processed_outputs.request_outputs
 
                     # Allow other asyncio tasks to run between chunks
-                    if i + 1 < len(slices):
+                    if i + 1 < num_slices:
                         await asyncio.sleep(0)
 
                     # 3) Abort any reqs that finished due to stop strings.
diff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py
index 12df34177..e2c57dfcd 100644
--- a/vllm/v1/engine/output_processor.py
+++ b/vllm/v1/engine/output_processor.py
@@ -24,6 +24,35 @@ class OutputProcessorOutput:
     reqs_to_abort: list[str]
 
 
+
+
+class RequestOutputCollector:
+
+    def __init__(self) -> None:
+        # Map a queue to a per-request aggregation map
+        self._queues: dict[asyncio.Queue, dict[str, RequestOutput]] = {}
+
+    def add(self, queue: asyncio.Queue, ro: RequestOutput,
+            output_kind: RequestOutputKind) -> None:
+        qmap = self._queues.setdefault(queue, {})
+        prev = qmap.get(ro.request_id)
+        if prev is None:
+            qmap[ro.request_id] = ro
+            return
+        # Merge behavior depends on output kind
+        if output_kind == RequestOutputKind.DELTA:
+            prev.add(ro)
+        else:
+            # Replace with the most recent output for non-delta kinds
+            qmap[ro.request_id] = ro
+
+    def flush(self) -> None:
+        # Push aggregated outputs to their respective queues
+        for queue, qmap in self._queues.items():
+            for ro in qmap.values():
+                queue.put_nowait(ro)
+        self._queues.clear()
+
 class RequestState:
 
     def __init__(
@@ -56,6 +85,10 @@ class RequestState:
         self.is_prefilling = True
         self.queue = queue
 
+        # Cached flags to avoid repeated enum comparisons in hot paths
+        self._final_only = (output_kind == RequestOutputKind.FINAL_ONLY)
+        self._delta = (output_kind == RequestOutputKind.DELTA)
+
         self.stats = RequestStateStats(
             arrival_time=arrival_time) if log_stats else None
 
@@ -103,7 +136,7 @@ class RequestState:
     ) -> Optional[RequestOutput]:
 
         finished = finish_reason is not None
-        final_only = self.output_kind == RequestOutputKind.FINAL_ONLY
+        final_only = self._final_only
 
         if not finished and final_only:
             # Only the final output is required in FINAL_ONLY mode.
@@ -153,10 +186,13 @@ class RequestState:
     ) -> CompletionOutput:
 
         finished = finish_reason is not None
-        delta = self.output_kind == RequestOutputKind.DELTA
+        delta = self._delta
 
         # Prepare text and token_ids, based on delta mode
-        text = self.detokenizer.get_next_output_text(finished, delta)
+        if self.detokenizer.tokenizer is None:
+            text = ""
+        else:
+            text = self.detokenizer.get_next_output_text(finished, delta)
         if not delta:
             token_ids = self.detokenizer.output_token_ids
 
@@ -267,6 +303,7 @@ class OutputProcessor:
 
         request_outputs: list[RequestOutput] = []
         reqs_to_abort: list[str] = []
+        collector = RequestOutputCollector()
         for engine_core_output in engine_core_outputs:
             req_id = engine_core_output.request_id
             req_state = self.request_states.get(req_id)
@@ -286,21 +323,32 @@ class OutputProcessor:
             req_state.is_prefilling = False
 
             # 2) Detokenize the token ids into text and perform stop checks.
-            stop_string = req_state.detokenizer.update(
-                new_token_ids, finish_reason == FinishReason.STOP)
+            detok = req_state.detokenizer
+            if detok.tokenizer is None:
+                # Fast path: skip detokenization when disabled
+                detok.token_ids.extend(new_token_ids)
+                stop_string = None
+            else:
+                stop_string = detok.update(new_token_ids,
+                                           finish_reason == FinishReason.STOP)
             if stop_string and finish_reason != FinishReason.STOP:
                 finish_reason = FinishReason.STOP
                 stop_reason = stop_string
 
             # 3) Compute sample and prompt logprobs for request, if required.
-            req_state.logprobs_processor.update_from_output(engine_core_output)
+            if (engine_core_output.new_logprobs is not None
+                    or engine_core_output.new_prompt_logprobs_tensors
+                    is not None):
+                req_state.logprobs_processor.update_from_output(
+                    engine_core_output)
 
             # 4) Create and handle RequestOutput objects.
             if request_output := req_state.make_request_output(
                     new_token_ids, finish_reason, stop_reason):
                 if req_state.queue is not None:
-                    # AsyncLLM: put into queue for handling by generate().
-                    req_state.queue.put_nowait(request_output)
+                    # AsyncLLM: aggregate per-request outputs for this iteration
+                    collector.add(req_state.queue, request_output,
+                                   req_state.output_kind)
                 else:
                     # LLMEngine: return list of RequestOutputs.
                     request_outputs.append(request_output)
@@ -322,6 +370,9 @@ class OutputProcessor:
                                                  iteration_stats)
 
         self.lora_states.update_iteration_stats(iteration_stats)
+        # Flush aggregated outputs to queues (AsyncLLM)
+        collector.flush()
+
 
         return OutputProcessorOutput(
             request_outputs=request_outputs,
diff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py
index 6284ae4b4..589c7deb5 100644
--- a/vllm/v1/sample/rejection_sampler.py
+++ b/vllm/v1/sample/rejection_sampler.py
@@ -149,12 +149,13 @@ def rejection_sample(
     assert target_probs.shape == (num_tokens, vocab_size)
 
     # Create output buffer.
-    output_token_ids = torch.empty(
+    # Initialize with placeholder in a single op to avoid extra fill.
+    output_token_ids = torch.full(
         (batch_size, max_spec_len + 1),
+        PLACEHOLDER_TOKEN_ID,
         dtype=torch.int32,  # Consistent with SamplerOutput.sampled_token_ids.
         device=device,
     )
-    output_token_ids.fill_(PLACEHOLDER_TOKEN_ID)
 
     if sampling_metadata.all_greedy:
         is_greedy = None
diff --git a/vllm/v1/spec_decode/metadata.py b/vllm/v1/spec_decode/metadata.py
index 1cf650d5f..ef834e756 100644
--- a/vllm/v1/spec_decode/metadata.py
+++ b/vllm/v1/spec_decode/metadata.py
@@ -42,13 +42,14 @@ class SpecDecodeMetadata:
         cu_num_draft_tokens_tensor = torch.from_numpy(cu_num_draft_tokens).to(
             device)
 
-        target_logits_indices = torch.zeros(num_tokens,
+        # These index buffers are written by later kernels; avoid zero-init.
+        target_logits_indices = torch.empty(num_tokens,
                                             dtype=torch.int32,
                                             device=device)
-        bonus_logits_indices = torch.zeros(batch_size,
+        bonus_logits_indices = torch.empty(batch_size,
                                            dtype=torch.int32,
                                            device=device)
-        logits_indices = torch.zeros(num_tokens + batch_size,
+        logits_indices = torch.empty(num_tokens + batch_size,
                                      dtype=torch.int32,
                                      device=device)
         return cls(
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index c6741fdc5..fac9860d0 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -193,10 +193,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         self.num_sms = self.device_properties.multi_processor_count
 
         # Persistent buffers for CUDA graphs.
-        self.input_ids = torch.zeros(self.max_num_tokens,
+        # Persistent buffers; values are always overwritten before use.
+        self.input_ids = torch.empty(self.max_num_tokens,
                                      dtype=torch.int32,
                                      device=self.device)
-        self.positions = torch.zeros(self.max_num_tokens,
+        self.positions = torch.empty(self.max_num_tokens,
                                      dtype=torch.int64,
                                      device=self.device)
         # None in the first PP rank. The rest are set after load_model.
@@ -214,16 +215,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):
             # identical position IDs, making M-RoPE functionally equivalent to
             # 1D-RoPE.
             # See page 5 of https://arxiv.org/abs/2409.12191
-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),
+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),
                                                dtype=torch.int64,
                                                device=self.device)
-            self.mrope_positions_cpu = torch.zeros(
+            self.mrope_positions_cpu = torch.empty(
                 (3, self.max_num_tokens + 1),
                 dtype=torch.int64,
                 device="cpu",
                 pin_memory=self.pin_memory)
 
-        self.inputs_embeds = torch.zeros(
+        self.inputs_embeds = torch.empty(
             (self.max_num_tokens, self.hidden_size),
             dtype=self.dtype,
             device=self.device)
@@ -236,27 +237,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.input_ids_np = self.input_ids_cpu.numpy()
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,
                                             dtype=torch.int32,
                                             device="cpu",
                                             pin_memory=self.pin_memory)
         self.slot_mapping_np = self.slot_mapping_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
