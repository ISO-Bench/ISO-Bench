{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3a1e648", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex e0169f1a4..792f9231a 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -275,9 +275,13 @@ class AsyncLLM(EngineClient):\n                 out = q.get_nowait() if not q.empty() else await q.get()\n \n                 # Coalesce any additional queued outputs\n-                while not q.empty():\n-                    next_out = q.get_nowait()\n-                    if sampling_params.output_kind == RequestOutputKind.DELTA:\n+                is_delta = sampling_params.output_kind == RequestOutputKind.DELTA\n+                while True:\n+                    try:\n+                        next_out = q.get_nowait()\n+                    except asyncio.QueueEmpty:\n+                        break\n+                    if is_delta:\n                         out.add(next_out)\n                     else:\n                         out = next_out\n@@ -315,6 +319,7 @@ class AsyncLLM(EngineClient):\n                     slices = np.array_split(\n                         outputs.outputs,\n                         cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n+                num_slices = len(slices)\n \n                 for i, outputs_slice in enumerate(slices):\n                     # 2) Process EngineCoreOutputs.\n@@ -324,7 +329,7 @@ class AsyncLLM(EngineClient):\n                     assert not processed_outputs.request_outputs\n \n                     # Allow other asyncio tasks to run between chunks\n-                    if i + 1 < len(slices):\n+                    if i + 1 < num_slices:\n                         await asyncio.sleep(0)\n \n                     # 3) Abort any reqs that finished due to stop strings.\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 12df34177..e2c57dfcd 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -24,6 +24,35 @@ class OutputProcessorOutput:\n     reqs_to_abort: list[str]\n \n \n+\n+\n+class RequestOutputCollector:\n+\n+    def __init__(self) -> None:\n+        # Map a queue to a per-request aggregation map\n+        self._queues: dict[asyncio.Queue, dict[str, RequestOutput]] = {}\n+\n+    def add(self, queue: asyncio.Queue, ro: RequestOutput,\n+            output_kind: RequestOutputKind) -> None:\n+        qmap = self._queues.setdefault(queue, {})\n+        prev = qmap.get(ro.request_id)\n+        if prev is None:\n+            qmap[ro.request_id] = ro\n+            return\n+        # Merge behavior depends on output kind\n+        if output_kind == RequestOutputKind.DELTA:\n+            prev.add(ro)\n+        else:\n+            # Replace with the most recent output for non-delta kinds\n+            qmap[ro.request_id] = ro\n+\n+    def flush(self) -> None:\n+        # Push aggregated outputs to their respective queues\n+        for queue, qmap in self._queues.items():\n+            for ro in qmap.values():\n+                queue.put_nowait(ro)\n+        self._queues.clear()\n+\n class RequestState:\n \n     def __init__(\n@@ -56,6 +85,10 @@ class RequestState:\n         self.is_prefilling = True\n         self.queue = queue\n \n+        # Cached flags to avoid repeated enum comparisons in hot paths\n+        self._final_only = (output_kind == RequestOutputKind.FINAL_ONLY)\n+        self._delta = (output_kind == RequestOutputKind.DELTA)\n+\n         self.stats = RequestStateStats(\n             arrival_time=arrival_time) if log_stats else None\n \n@@ -103,7 +136,7 @@ class RequestState:\n     ) -> Optional[RequestOutput]:\n \n         finished = finish_reason is not None\n-        final_only = self.output_kind == RequestOutputKind.FINAL_ONLY\n+        final_only = self._final_only\n \n         if not finished and final_only:\n             # Only the final output is required in FINAL_ONLY mode.\n@@ -153,10 +186,13 @@ class RequestState:\n     ) -> CompletionOutput:\n \n         finished = finish_reason is not None\n-        delta = self.output_kind == RequestOutputKind.DELTA\n+        delta = self._delta\n \n         # Prepare text and token_ids, based on delta mode\n-        text = self.detokenizer.get_next_output_text(finished, delta)\n+        if self.detokenizer.tokenizer is None:\n+            text = \"\"\n+        else:\n+            text = self.detokenizer.get_next_output_text(finished, delta)\n         if not delta:\n             token_ids = self.detokenizer.output_token_ids\n \n@@ -267,6 +303,7 @@ class OutputProcessor:\n \n         request_outputs: list[RequestOutput] = []\n         reqs_to_abort: list[str] = []\n+        collector = RequestOutputCollector()\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n             req_state = self.request_states.get(req_id)\n@@ -286,21 +323,32 @@ class OutputProcessor:\n             req_state.is_prefilling = False\n \n             # 2) Detokenize the token ids into text and perform stop checks.\n-            stop_string = req_state.detokenizer.update(\n-                new_token_ids, finish_reason == FinishReason.STOP)\n+            detok = req_state.detokenizer\n+            if detok.tokenizer is None:\n+                # Fast path: skip detokenization when disabled\n+                detok.token_ids.extend(new_token_ids)\n+                stop_string = None\n+            else:\n+                stop_string = detok.update(new_token_ids,\n+                                           finish_reason == FinishReason.STOP)\n             if stop_string and finish_reason != FinishReason.STOP:\n                 finish_reason = FinishReason.STOP\n                 stop_reason = stop_string\n \n             # 3) Compute sample and prompt logprobs for request, if required.\n-            req_state.logprobs_processor.update_from_output(engine_core_output)\n+            if (engine_core_output.new_logprobs is not None\n+                    or engine_core_output.new_prompt_logprobs_tensors\n+                    is not None):\n+                req_state.logprobs_processor.update_from_output(\n+                    engine_core_output)\n \n             # 4) Create and handle RequestOutput objects.\n             if request_output := req_state.make_request_output(\n                     new_token_ids, finish_reason, stop_reason):\n                 if req_state.queue is not None:\n-                    # AsyncLLM: put into queue for handling by generate().\n-                    req_state.queue.put_nowait(request_output)\n+                    # AsyncLLM: aggregate per-request outputs for this iteration\n+                    collector.add(req_state.queue, request_output,\n+                                   req_state.output_kind)\n                 else:\n                     # LLMEngine: return list of RequestOutputs.\n                     request_outputs.append(request_output)\n@@ -322,6 +370,9 @@ class OutputProcessor:\n                                                  iteration_stats)\n \n         self.lora_states.update_iteration_stats(iteration_stats)\n+        # Flush aggregated outputs to queues (AsyncLLM)\n+        collector.flush()\n+\n \n         return OutputProcessorOutput(\n             request_outputs=request_outputs,\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 6284ae4b4..589c7deb5 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -149,12 +149,13 @@ def rejection_sample(\n     assert target_probs.shape == (num_tokens, vocab_size)\n \n     # Create output buffer.\n-    output_token_ids = torch.empty(\n+    # Initialize with placeholder in a single op to avoid extra fill.\n+    output_token_ids = torch.full(\n         (batch_size, max_spec_len + 1),\n+        PLACEHOLDER_TOKEN_ID,\n         dtype=torch.int32,  # Consistent with SamplerOutput.sampled_token_ids.\n         device=device,\n     )\n-    output_token_ids.fill_(PLACEHOLDER_TOKEN_ID)\n \n     if sampling_metadata.all_greedy:\n         is_greedy = None\ndiff --git a/vllm/v1/spec_decode/metadata.py b/vllm/v1/spec_decode/metadata.py\nindex 1cf650d5f..ef834e756 100644\n--- a/vllm/v1/spec_decode/metadata.py\n+++ b/vllm/v1/spec_decode/metadata.py\n@@ -42,13 +42,14 @@ class SpecDecodeMetadata:\n         cu_num_draft_tokens_tensor = torch.from_numpy(cu_num_draft_tokens).to(\n             device)\n \n-        target_logits_indices = torch.zeros(num_tokens,\n+        # These index buffers are written by later kernels; avoid zero-init.\n+        target_logits_indices = torch.empty(num_tokens,\n                                             dtype=torch.int32,\n                                             device=device)\n-        bonus_logits_indices = torch.zeros(batch_size,\n+        bonus_logits_indices = torch.empty(batch_size,\n                                            dtype=torch.int32,\n                                            device=device)\n-        logits_indices = torch.zeros(num_tokens + batch_size,\n+        logits_indices = torch.empty(num_tokens + batch_size,\n                                      dtype=torch.int32,\n                                      device=device)\n         return cls(\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex c6741fdc5..fac9860d0 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -193,10 +193,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        # Persistent buffers; values are always overwritten before use.\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n         # None in the first PP rank. The rest are set after load_model.\n@@ -214,16 +215,16 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -236,27 +237,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
