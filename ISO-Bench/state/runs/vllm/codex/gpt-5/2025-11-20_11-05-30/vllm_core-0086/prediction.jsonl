{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-f508e03", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex a9f036fa2..35afbc8ae 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -91,7 +91,8 @@ def _get_logits(hidden_states: torch.Tensor, embedding: torch.Tensor,\n     # Get the logits for the next tokens.\n     logits = torch.matmul(hidden_states, embedding.t())\n     if embedding_bias is not None:\n-        logits += embedding_bias\n+        # In-place add to avoid creating a new tensor.\n+        logits.add_(embedding_bias)\n     logits = tensor_model_parallel_all_gather(logits)\n     # Remove paddings in vocab (if any).\n     logits = logits[:, :vocab_size]\n@@ -165,23 +166,32 @@ def _apply_penalties(\n         # Return early if all sequences have zero penalties.\n         return logits\n \n-    max_output_len = max(len(tokens) for tokens in output_tokens)\n-    padded_output_tokens = [\n-        tokens + [vocab_size] * (max_output_len - len(tokens))\n-        for tokens in output_tokens\n-    ]\n-    output_tokens_tensor = torch.tensor(padded_output_tokens,\n-                                        dtype=torch.long,\n-                                        device=logits.device)\n-\n-    # Compute the bin counts for the output tokens.\n-    # vocab_size + 1 for padding.\n-    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n-                             dtype=torch.long,\n-                             device=logits.device)\n-    bin_counts.scatter_add_(1, output_tokens_tensor,\n-                            torch.ones_like(output_tokens_tensor))\n-    bin_counts = bin_counts[:, :vocab_size]  # Remove the padding bin.\n+    # Compute the bin counts for output tokens without padding or large zero\n+    # buffers. Flatten all tokens and use a single bincount over global\n+    # indices: seq_id * vocab_size + token_id.\n+    flat_tokens: List[int] = []\n+    flat_seq_ids: List[int] = []\n+    for i, tokens in enumerate(output_tokens):\n+        if not tokens:\n+            continue\n+        flat_tokens.extend(tokens)\n+        flat_seq_ids.extend([i] * len(tokens))\n+\n+    if flat_tokens:\n+        token_tensor = torch.tensor(flat_tokens,\n+                                    dtype=torch.long,\n+                                    device=logits.device)\n+        seq_tensor = torch.tensor(flat_seq_ids,\n+                                  dtype=torch.long,\n+                                  device=logits.device)\n+        global_idx = seq_tensor * vocab_size + token_tensor\n+        counts_1d = torch.bincount(global_idx,\n+                                   minlength=num_seqs * vocab_size)\n+        bin_counts = counts_1d.view(num_seqs, vocab_size)\n+    else:\n+        bin_counts = torch.zeros((num_seqs, vocab_size),\n+                                 dtype=torch.long,\n+                                 device=logits.device)\n \n     frequency_penalties = torch.tensor(frequency_penalties,\n                                        dtype=logits.dtype,\n@@ -235,22 +245,22 @@ def _apply_top_p_top_k(\n     top_ps: List[float],\n     top_ks: List[int],\n ) -> torch.Tensor:\n-    p = torch.tensor(top_ps, dtype=logits.dtype, device=logits.device)\n-    k = torch.tensor(top_ks, dtype=torch.int, device=logits.device)\n+    # Prefer as_tensor to avoid unnecessary copies.\n+    p = torch.as_tensor(top_ps, dtype=logits.dtype, device=logits.device)\n+    k = torch.as_tensor(top_ks, dtype=torch.int, device=logits.device)\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=True)\n \n     # Apply top-p.\n     probs_sort = logits_sort.softmax(dim=-1)\n     probs_sum = probs_sort.cumsum(dim=-1)\n     top_p_mask = (probs_sum - probs_sort) > p.unsqueeze(dim=1)\n-    logits_sort[top_p_mask] = -float(\"inf\")\n+    logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n     # Apply top-k.\n     # Create a mask for the top-k elements.\n-    top_k_mask = torch.arange(logits_idx.shape[-1], device=logits_idx.device)\n-    top_k_mask = top_k_mask.expand(logits_idx.shape[0], -1)\n-    top_k_mask = top_k_mask >= k.unsqueeze(dim=1)\n-    logits_sort[top_k_mask] = -float(\"inf\")\n+    col_idx = torch.arange(logits_idx.shape[-1], device=logits_idx.device)\n+    top_k_mask = col_idx.unsqueeze(0) >= k.unsqueeze(1)\n+    logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n \n     # Re-sort the probabilities.\n     logits = torch.gather(logits_sort,\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..3a43a77b5 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -81,20 +81,21 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n+    cts = tokenizer.convert_tokens_to_string\n+    added_vocab = tokenizer.get_added_vocab()\n+    all_special_tokens = set(tokenizer.all_special_tokens) if skip_special_tokens else None\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if skip_special_tokens and token in all_special_tokens:  # type: ignore[arg-type]\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in added_vocab:\n             if current_sub_text:\n-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-                sub_texts.append(sub_text)\n+                sub_texts.append(cts(current_sub_text))\n                 current_sub_text = []\n             sub_texts.append(token)\n         else:\n             current_sub_text.append(token)\n     if current_sub_text:\n-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n-        sub_texts.append(sub_text)\n+        sub_texts.append(cts(current_sub_text))\n     return \" \".join(sub_texts)\n \n \n@@ -129,16 +130,23 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n-        prefix_text = tokenizer.convert_tokens_to_string(\n-            output_tokens[prefix_offset:read_offset])\n+    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n+        prefix_slice = output_tokens[prefix_offset:read_offset]\n+        if prefix_slice:\n+            prefix_text = tokenizer.convert_tokens_to_string(prefix_slice)\n+        else:\n+            prefix_text = \"\"\n         new_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:])\n     else:\n-        prefix_text = _convert_tokens_to_string_with_added_encoders(\n-            tokenizer,\n-            output_tokens[prefix_offset:read_offset],\n-            skip_special_tokens=skip_special_tokens)\n+        prefix_slice = output_tokens[prefix_offset:read_offset]\n+        if prefix_slice:\n+            prefix_text = _convert_tokens_to_string_with_added_encoders(\n+                tokenizer,\n+                prefix_slice,\n+                skip_special_tokens=skip_special_tokens)\n+        else:\n+            prefix_text = \"\"\n         new_text = _convert_tokens_to_string_with_added_encoders(\n             tokenizer,\n             output_tokens[prefix_offset:],\ndiff --git a/vllm/worker/worker.py b/vllm/worker/worker.py\nindex 5b0a60dbf..ce26c673e 100644\n--- a/vllm/worker/worker.py\n+++ b/vllm/worker/worker.py\n@@ -355,8 +355,10 @@ def _init_distributed_environment(\n             init_method=distributed_init_method,\n         )\n \n-    # A small all_reduce for warmup.\n-    torch.distributed.all_reduce(torch.zeros(1).cuda())\n+    # A small all_reduce for warmup. Avoid zero-initialization overhead\n+    # since the value is not used.\n+    _dummy = torch.empty(1, device=\"cuda\")\n+    torch.distributed.all_reduce(_dummy)\n     initialize_model_parallel(parallel_config.tensor_parallel_size,\n                               parallel_config.pipeline_parallel_size)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
