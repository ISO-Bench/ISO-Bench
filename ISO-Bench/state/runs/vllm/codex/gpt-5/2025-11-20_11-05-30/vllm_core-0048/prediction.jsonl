{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5fc5ce0", "model_patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex a0c25df6b..20517c8a4 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -13,6 +13,18 @@ cmake_minimum_required(VERSION 3.26)\n # cmake --install . --component _C\n project(vllm_extensions LANGUAGES CXX)\n \n+# Default to an optimized build when unspecified\n+if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)\n+  set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n+endif()\n+\n+# Enable interprocedural optimization (LTO) for Release if supported\n+include(CheckIPOSupported)\n+check_ipo_supported(RESULT _ipo_supported OUTPUT _ipo_error)\n+if(_ipo_supported)\n+  set(CMAKE_INTERPROCEDURAL_OPTIMIZATION_RELEASE ON CACHE BOOL \"Enable IPO for Release\" FORCE)\n+endif()\n+\n # CUDA by default, can be overridden by using -DVLLM_TARGET_DEVICE=... (used by setup.py)\n set(VLLM_TARGET_DEVICE \"cuda\" CACHE STRING \"Target device backend for vLLM\")\n \n@@ -156,9 +168,21 @@ endif()\n #\n get_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG})\n \n+# Drop device line information to reduce runtime overhead\n+list(REMOVE_ITEM VLLM_GPU_FLAGS \"-lineinfo\")\n+\n #\n # Set nvcc parallelism.\n #\n+# Auto-set nvcc parallelism to CPU count if not provided\n+if(VLLM_GPU_LANG STREQUAL \"CUDA\" AND NOT DEFINED NVCC_THREADS)\n+  include(ProcessorCount)\n+  ProcessorCount(_NPROC)\n+  if(_NPROC GREATER 0)\n+    set(NVCC_THREADS ${_NPROC} CACHE STRING \"NVCC parallel threads\" FORCE)\n+  endif()\n+endif()\n+\n if(NVCC_THREADS AND VLLM_GPU_LANG STREQUAL \"CUDA\")\n   list(APPEND VLLM_GPU_FLAGS \"--threads=${NVCC_THREADS}\")\n endif()\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0e70d77e1..ac9437b24 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -233,7 +233,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # Only relevant for models using ALiBi (e.g, MPT)\n         self.use_alibi = check_use_alibi(model_config)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -246,27 +246,27 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -484,14 +484,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.input_batch.block_table.commit(num_reqs)\n \n         # Get the number of scheduled tokens for each request.\n-        # TODO: The Python loop can be slow. Optimize.\n-        num_scheduled_tokens = np.empty(num_reqs, dtype=np.int32)\n-        max_num_scheduled_tokens = 0\n-        for i, req_id in enumerate(self.input_batch.req_ids):\n-            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n-            num_scheduled_tokens[i] = num_tokens\n-            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n-                                           num_tokens)\n+        req_ids = self.input_batch.req_ids\n+        tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]\n+        num_scheduled_tokens = np.array(tokens, dtype=np.int32)\n+        max_num_scheduled_tokens = int(max(tokens))\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n@@ -1663,7 +1659,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                         num_blocks, kv_cache_spec.block_size,\n                         kv_cache_spec.num_kv_heads, kv_cache_spec.head_size)\n                     dtype = kv_cache_spec.dtype\n-                    kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n+                    kv_caches[layer_name] = torch.empty(kv_cache_shape,\n                                                         dtype=dtype,\n                                                         device=self.device)\n                 else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
