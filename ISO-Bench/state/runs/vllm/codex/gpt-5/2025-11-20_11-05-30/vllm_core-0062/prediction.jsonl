{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3d92516", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex af4671ec2..1bb0e8633 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -139,11 +139,22 @@ class SchedulerOutputs:\n         # Swap in and swap out should never happen at the same time.\n         assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n \n-        self.num_loras: int = len(self.lora_requests)\n+        # Single pass over scheduled groups to compute counts for LoRA and\n+        # prompt-adapter requests, avoiding repeated set construction.\n+        lora_reqs: Set[LoRARequest] = set()\n+        prompt_adapter_reqs: Set[PromptAdapterRequest] = set()\n+        for g in self.scheduled_seq_groups:\n+            sg = g.seq_group\n+            if sg.lora_request is not None:\n+                lora_reqs.add(sg.lora_request)\n+            if sg.prompt_adapter_request is not None:\n+                prompt_adapter_reqs.add(sg.prompt_adapter_request)\n+\n+        self.num_loras = len(lora_reqs)\n         if self.num_loras > 0:\n             self._sort_by_lora_ids()\n \n-        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n+        self.num_prompt_adapters = len(prompt_adapter_reqs)\n \n     def is_empty(self) -> bool:\n         # NOTE: We do not consider the ignored sequence groups.\n@@ -1255,7 +1266,8 @@ class Scheduler:\n                 encoder_seq_data = None\n                 cross_block_table = None\n \n-            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            running_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n+            for seq in running_seqs:\n                 seq_id = seq.seq_id\n                 seq_data[seq_id] = seq.data\n                 block_tables[seq_id] = self.block_manager.get_block_table(seq)\n@@ -1264,7 +1276,7 @@ class Scheduler:\n             if self.cache_config.enable_prefix_caching:\n                 common_computed_block_nums = (\n                     self.block_manager.get_common_computed_block_ids(\n-                        seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n+                        running_seqs))\n \n             do_sample = True\n             is_prompt = seq_group.is_prefill()\n@@ -1544,16 +1556,16 @@ class Scheduler:\n             self.last_prompt_latency = now - self.prev_time\n         self.prev_time, self.prev_prompt = now, False\n         # Delay scheduling prompts to let waiting queue fill up\n-        if self.scheduler_config.delay_factor > 0 and self.waiting:\n-            earliest_arrival_time = min(\n-                [e.metrics.arrival_time for e in self.waiting])\n-            passed_delay = (\n-                (now - earliest_arrival_time) >\n-                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n-                or not self.running)\n-        else:\n-            passed_delay = True\n-        return passed_delay\n+        delay_factor = self.scheduler_config.delay_factor\n+        if delay_factor <= 0 or not self.waiting:\n+            return True\n+        # If there is no running request, don't wait.\n+        if not self.running:\n+            return True\n+        earliest_arrival_time = min(e.metrics.arrival_time\n+                                    for e in self.waiting)\n+        return (now - earliest_arrival_time) > (delay_factor *\n+                                                self.last_prompt_latency)\n \n     def _get_num_lookahead_slots(self, is_prefill: bool,\n                                  enable_chunking: bool) -> int:\n@@ -1597,10 +1609,9 @@ class Scheduler:\n \n         Returns 0 if the new token cannot be computed due to token budget.\n         \"\"\"\n-        num_new_tokens = 0\n         seqs = seq_group.get_seqs(status=status)\n-        for seq in seqs:\n-            num_new_tokens += seq.get_num_new_tokens()\n+        # Use built-in sum to reduce Python-loop overhead.\n+        num_new_tokens = sum(seq.get_num_new_tokens() for seq in seqs)\n         assert num_new_tokens > 0\n         # Chunk if a running request cannot fit in the given budget.\n         # If number of seq > 1, it means it is doing beam search\ndiff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..f30cdd3b0 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,8 +90,8 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n-        x = self.act_fn(gate_up)\n+        x, _ = self.gate_up_proj(x)\n+        x = self.act_fn(x)\n         x, _ = self.down_proj(x)\n         return x\n \n@@ -134,6 +134,7 @@ class LlamaAttention(nn.Module):\n         self.q_size = self.num_heads * self.head_dim\n         self.kv_size = self.num_kv_heads * self.head_dim\n         self.scaling = self.head_dim**-0.5\n+        self._split_sizes = [self.q_size, self.kv_size, self.kv_size]\n         self.rope_theta = rope_theta\n         self.max_position_embeddings = max_position_embeddings\n \n@@ -184,11 +185,11 @@ class LlamaAttention(nn.Module):\n         attn_metadata: AttentionMetadata,\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n+        q, k, v = qkv.split(self._split_sizes, dim=-1)\n         q, k = self.rotary_emb(positions, q, k)\n         attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n-        output, _ = self.o_proj(attn_output)\n-        return output\n+        attn_output, _ = self.o_proj(attn_output)\n+        return attn_output\n \n \n class LlamaDecoderLayer(nn.Module):\n", "model_name_or_path": "gpt-5-2025-08-07"}
