{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3014c92", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex 49e63c231..24d00cd3b 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -337,10 +337,19 @@ class BlockTable:\n         This is required for the scheduler to determine whether a sequence can\n         continue generation, or if it must be preempted.\n         \"\"\"\n-\n-        all_token_ids = token_ids + [-1] * num_lookahead_slots\n-        token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n-        return len(token_blocks)\n+        # Number of slots to account for (new tokens + lookahead slots)\n+        total_new_slots = len(token_ids) + num_lookahead_slots\n+\n+        # The first touched block is always the current (possibly partially\n+        # filled) block. Even when total_new_slots is 0, we still count touching\n+        # the current block for consistency with previous behavior.\n+        first_chunk_capacity = self._block_size - (self._num_full_slots %\n+                                                   self._block_size)\n+        remaining = total_new_slots - first_chunk_capacity\n+        if remaining <= 0:\n+            return 1\n+        # Additional blocks needed beyond the first touched block.\n+        return 1 + cdiv(remaining, self._block_size)\n \n     def _chunk_token_blocks_for_append(\n             self, token_ids: List[int]) -> List[List[int]]:\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex f272e23ee..2987dda4e 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -501,7 +501,24 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        \"\"\"Mark blocks as computed, used in prefix caching.\n+\n+        This operation is designed to be lightweight and incremental. It only\n+        updates internal tracking state for active blocks; blocks that have\n+        already been moved to the evictor are considered computed implicitly.\n+        \"\"\"\n+        if not block_ids:\n+            return\n+        for block_id in block_ids:\n+            if self._block_tracker[block_id].active:\n+                self._block_tracker[block_id].computed = True\n+            elif block_id in self.evictor:\n+                # Already tracked by evictor as a computed, cached block\n+                continue\n+            else:\n+                # Invalid block_id for this allocator\n+                raise ValueError(\n+                    \"Mark block as computed which is not belonged to GPU\")\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 3f9573f55..83d1d0a9f 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -82,6 +82,10 @@ class RotaryEmbedding(CustomOp):\n         if not self.use_native2:\n             cache = cache.to(dtype)\n             self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n+            # Cached converted copy to avoid repeated .to() each forward\n+            self._cos_sin_cache_converted: Optional[torch.Tensor] = None\n+            self._cos_sin_cache_device: Optional[torch.device] = None\n+            self._cos_sin_cache_dtype: Optional[torch.dtype] = None\n         else:\n             cos, sin = cache.chunk(2, dim=-1)\n             freqs_cis = cos + 1j * sin\n@@ -107,12 +111,28 @@ class RotaryEmbedding(CustomOp):\n         inv_freq = self._compute_inv_freq(self.base)\n         t = torch.arange(self.max_position_embeddings, dtype=torch.float)\n \n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        # Use outer product instead of einsum for efficiency\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos()\n         sin = freqs.sin()\n         cache = torch.cat((cos, sin), dim=-1)\n         return cache\n \n+    def _get_converted_cos_sin_cache(self, device: torch.device,\n+                                      dtype: torch.dtype) -> torch.Tensor:\n+        \"\"\"Return cos/sin cache on the requested device/dtype, caching it.\"\"\"\n+        cached = getattr(self, \"_cos_sin_cache_converted\", None)\n+        if (cached is not None and self._cos_sin_cache_device == device\n+                and self._cos_sin_cache_dtype == dtype\n+                and cached.device == device and cached.dtype == dtype\n+                and cached.shape == self.cos_sin_cache.shape):\n+            return cached\n+        converted = self.cos_sin_cache.to(device=device, dtype=dtype)\n+        self._cos_sin_cache_converted = converted\n+        self._cos_sin_cache_device = device\n+        self._cos_sin_cache_dtype = dtype\n+        return converted\n+\n     def forward_native(\n         self,\n         positions: torch.Tensor,\n@@ -120,46 +140,38 @@ class RotaryEmbedding(CustomOp):\n         key: torch.Tensor,\n         offsets: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"A PyTorch-native implementation equivalent to forward().\n-\n-        This method mimics the implementation of the custom CUDA kernel\n-        used in `forward_cuda()`.\n-        \"\"\"\n-        query = query.view(*query.shape[:-1], -1, self.head_size)\n-        key = key.view(*key.shape[:-1], -1, self.head_size)\n+        \"\"\"PyTorch-native implementation using complex multiply fast path.\"\"\"\n+        if positions.dim() == 1:\n+            batch_size = 1\n+            seq_len = positions.shape[0]\n+        else:\n+            batch_size, seq_len = positions.shape\n \n-        query_rot = query[..., :self.rotary_dim]\n-        key_rot = key[..., :self.rotary_dim]\n-        if self.rotary_dim < self.head_size:\n-            query_pass = query[..., self.rotary_dim:]\n-            key_pass = key[..., self.rotary_dim:]\n+        if offsets is not None:\n+            positions = positions + offsets\n \n-        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(\n-            positions.device, dtype=query.dtype)\n-        cos_sin = self.cos_sin_cache[torch.add(positions, offsets)\n-                                     if offsets is not None else positions]\n+        # Prepare complex rotation factors from cached cos/sin\n+        cos_sin_cache = self._get_converted_cos_sin_cache(positions.device,\n+                                                          query.dtype)\n+        cos_sin = cos_sin_cache.index_select(0, positions.flatten())\n         cos, sin = cos_sin.chunk(2, dim=-1)\n-        if self.is_neox_style:\n-            # NOTE(woosuk): Here we assume that the positions tensor has the\n-            # shape [batch_size, seq_len].\n-            cos = cos.repeat(1, 1, 2).unsqueeze(-2)\n-            sin = sin.repeat(1, 1, 2).unsqueeze(-2)\n-        else:\n-            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)\n-            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)\n+        freqs_cis = (cos + 1j * sin).view(batch_size, 1, seq_len, -1)\n \n-        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj\n-        query_rot = query_rot * cos + rotate_fn(query_rot) * sin\n-        key_rot = key_rot * cos + rotate_fn(key_rot) * sin\n+        # Apply rotation to rotary dims only, keep the pass-through dims\n+        query_shape = query.shape\n+        query = query.view(batch_size, seq_len, -1, self.head_size)\n+        query_rot = query[..., :self.rotary_dim]\n+        query_pass = query[..., self.rotary_dim:]\n+        query_rot = _apply_rotary_emb(query_rot, freqs_cis)\n+        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n+\n+        key_shape = key.shape\n+        key = key.view(batch_size, seq_len, -1, self.head_size)\n+        key_rot = key[..., :self.rotary_dim]\n+        key_pass = key[..., self.rotary_dim:]\n+        key_rot = _apply_rotary_emb(key_rot, freqs_cis)\n+        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n \n-        if self.rotary_dim < self.head_size:\n-            query = torch.cat((query_rot, query_pass), dim=-1)\n-            key = torch.cat((key_rot, key_pass), dim=-1)\n-        else:\n-            query = query_rot\n-            key = key_rot\n-        query = query.flatten(-2)\n-        key = key.flatten(-2)\n         return query, key\n \n     def forward_native2(\n@@ -207,18 +219,18 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm import _custom_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n-                                                   dtype=query.dtype)\n+        cos_sin_cache = self._get_converted_cos_sin_cache(positions.device,\n+                                                          query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n             ops.batched_rotary_embedding(positions, query, key, self.head_size,\n-                                         self.cos_sin_cache,\n+                                         cos_sin_cache,\n                                          self.is_neox_style, self.rotary_dim,\n                                          offsets)\n         else:\n             ops.rotary_embedding(positions, query, key, self.head_size,\n-                                 self.cos_sin_cache, self.is_neox_style)\n+                                 cos_sin_cache, self.is_neox_style)\n         return query, key\n \n     def forward_xpu(\n@@ -230,18 +242,18 @@ class RotaryEmbedding(CustomOp):\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         from vllm._ipex_ops import ipex_ops as ops\n \n-        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n-                                                   dtype=query.dtype)\n+        cos_sin_cache = self._get_converted_cos_sin_cache(positions.device,\n+                                                          query.dtype)\n         # ops.rotary_embedding()/batched_rotary_embedding()\n         # are in-place operations that update the query and key tensors.\n         if offsets is not None:\n             ops.batched_rotary_embedding(positions, query, key, self.head_size,\n-                                         self.cos_sin_cache,\n+                                         cos_sin_cache,\n                                          self.is_neox_style, self.rotary_dim,\n                                          offsets)\n         else:\n             ops.rotary_embedding(positions, query, key, self.head_size,\n-                                 self.cos_sin_cache, self.is_neox_style)\n+                                 cos_sin_cache, self.is_neox_style)\n         return query, key\n \n     def forward_tpu(\n@@ -324,7 +336,7 @@ class LinearScalingRotaryEmbedding(RotaryEmbedding):\n             t = torch.arange(max_len, dtype=torch.float)\n             t = t / scaling_factor\n \n-            freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+            freqs = torch.outer(t, inv_freq)\n             cos = freqs.cos()\n             sin = freqs.sin()\n             cache = torch.cat((cos, sin), dim=-1)\n@@ -381,7 +393,7 @@ class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):\n         inv_freq = self._compute_inv_freq(base)\n         t = torch.arange(max_len, dtype=torch.float)\n \n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos()\n         sin = freqs.sin()\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -483,7 +495,7 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):\n         inv_freq = self._compute_inv_freq(self.scaling_factor)\n         t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                          dtype=torch.float32)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = (freqs.cos() * self.mscale)\n         sin = (freqs.sin() * self.mscale)\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -574,7 +586,7 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n     ) -> torch.Tensor:\n         inv_freq = self._compute_inv_freq(rescale_factors)\n         t = torch.arange(max_position_embeddings, dtype=torch.float)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos() * mscale * self.scaling_factor\n         sin = freqs.sin() * mscale * self.scaling_factor\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -591,14 +603,19 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n         key = key.view(*key.shape[:-1], -1, self.head_size)\n \n         k = self.original_max_position_embeddings\n-        long_prompt_offset = (torch.any(positions > k).float() *\n-                              torch.full_like(positions, k)).long()\n-        idx = (torch.add(positions, long_prompt_offset)\n-               if long_prompt_offset is not None else positions)\n-        self.long_short_cos_sin_cache: torch.Tensor = (\n-            self.long_short_cos_sin_cache.to(idx.device))\n+        # Compute offset as a scalar (0 or k) to avoid materializing full tensor\n+        long_prompt_offset = ((positions > k).any().to(positions.dtype) * k)\n+        idx = positions + long_prompt_offset\n+        # Cache device-converted long/short cache to avoid repeated .to()\n+        if not hasattr(self, \"_long_short_cache_device\"):\n+            self._long_short_cache_device = None  # type: ignore[attr-defined]\n+            self._long_short_cache_converted = None  # type: ignore[attr-defined]\n+        if self._long_short_cache_device is not idx.device:  # type: ignore[attr-defined]\n+            self._long_short_cache_converted = self.long_short_cos_sin_cache.to(idx.device)  # type: ignore[attr-defined]\n+            self._long_short_cache_device = idx.device  # type: ignore[attr-defined]\n+        cache = self._long_short_cache_converted  # type: ignore[attr-defined]\n         idx = torch.add(idx, offsets) if offsets is not None else idx\n-        cos_sin = torch.index_select(self.long_short_cos_sin_cache, 0, idx)\n+        cos_sin = torch.index_select(cache, 0, idx)\n \n         cos, sin = cos_sin.chunk(2, dim=-1)\n         cos = cos.repeat(1, 2).unsqueeze(-2)\n@@ -675,7 +692,7 @@ class DeepseekScalingRotaryEmbedding(RotaryEmbedding):\n         t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                          device=\"cuda\",\n                          dtype=torch.float32)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = (freqs.cos() * self.mscale)\n         sin = (freqs.sin() * self.mscale)\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -696,10 +713,10 @@ class DeepseekScalingRotaryEmbedding(RotaryEmbedding):\n             query_pass = query[..., self.rotary_dim:]\n             key_pass = key[..., self.rotary_dim:]\n \n-        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(\n-            positions.device)\n-        cos_sin = self.cos_sin_cache[torch.add(positions, offsets)\n-                                     if offsets is not None else positions]\n+        cos_sin_cache = self._get_converted_cos_sin_cache(positions.device,\n+                                                          query.dtype)\n+        cos_sin = cos_sin_cache[torch.add(positions, offsets)\n+                                if offsets is not None else positions]\n         cos, sin = cos_sin.chunk(2, dim=-1)\n         if self.is_neox_style:\n             # NOTE(woosuk): Here we assume that the positions tensor has the\ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 87508a116..6be55de19 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -7,6 +7,10 @@ from vllm.logger import init_logger\n from vllm.utils import is_hip\n \n logger = init_logger(__name__)\n+# Cache for imported modules and model classes to avoid repeated imports\n+_IMPORTED_MODULES: Dict[str, object] = {}\n+_MODEL_CLASS_CACHE: Dict[str, Optional[Type[nn.Module]]] = {}\n+\n \n # Architecture -> (module, class).\n _GENERATION_MODELS = {\n@@ -114,10 +118,20 @@ class ModelRegistry:\n                     \"Model architecture %s is partially supported by ROCm: %s\",\n                     model_arch, _ROCM_PARTIALLY_SUPPORTED_MODELS[model_arch])\n \n+        # Fast path: return from cache if available\n+        if model_arch in _MODEL_CLASS_CACHE:\n+            return _MODEL_CLASS_CACHE[model_arch]\n+\n         module_name, model_cls_name = _MODELS[model_arch]\n-        module = importlib.import_module(\n-            f\"vllm.model_executor.models.{module_name}\")\n-        return getattr(module, model_cls_name, None)\n+        # Import module with caching\n+        module = _IMPORTED_MODULES.get(module_name)\n+        if module is None:\n+            module = importlib.import_module(\n+                f\"vllm.model_executor.models.{module_name}\")\n+            _IMPORTED_MODULES[module_name] = module\n+        model_cls = getattr(module, model_cls_name, None)\n+        _MODEL_CLASS_CACHE[model_arch] = model_cls\n+        return model_cls\n \n     @staticmethod\n     def get_supported_archs() -> List[str]:\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex f3025a68d..4ecc861ae 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -520,7 +520,7 @@ def create_kv_caches_with_random(\n     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n \n     scale = head_size**-0.5\n-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n+    x = 16 // get_dtype_size(torch_dtype)\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n     key_caches: List[torch.Tensor] = []\n     for _ in range(num_layers):\n", "model_name_or_path": "gpt-5-2025-08-07"}
