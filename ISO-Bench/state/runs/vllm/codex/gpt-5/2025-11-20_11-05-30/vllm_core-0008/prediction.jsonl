{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d263bd9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex fec6d6112..a891335ef 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -470,10 +470,10 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-            if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+            if has_initial_states is not None and torch.any(has_initial_states):\n+                zero_init_indices = mamba_cache_params.state_indices_tensor[\n+                    ~has_initial_states]\n+                mamba_cache_params.ssm_state[zero_init_indices] = 0\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -499,8 +499,8 @@ class MambaMixer2(CustomOp):\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            mamba_cache_params.ssm_state[\n+                mamba_cache_params.state_indices_tensor] = varlen_state\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex b186300a0..7ddef9dc7 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -217,7 +217,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+        self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n@@ -236,32 +236,39 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n         self.seq_lens_np = self.seq_lens_cpu.numpy()\n \n+        # Preallocate a reusable CPU tensor for token indices to avoid\n+        # constructing a new tensor wrapper every step.\n+        self._token_indices_cpu = torch.empty(self.max_num_tokens,\n+                                              dtype=torch.int64,\n+                                              device=\"cpu\",\n+                                              pin_memory=self.pin_memory)\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         \"\"\"Update the cached states and the persistent batch with the scheduler\n         output.\n@@ -521,9 +528,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): We use torch.index_select instead of np.take here\n         # because torch.index_select is much faster than np.take for large\n         # tensors.\n+        # Reuse a persistent CPU tensor to hold token indices.\n+        self._token_indices_cpu[:total_num_scheduled_tokens].copy_(\n+            torch.from_numpy(token_indices))\n         torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                            0,\n-                           torch.from_numpy(token_indices),\n+                           self._token_indices_cpu[:total_num_scheduled_tokens],\n                            out=self.input_ids_cpu[:total_num_scheduled_tokens])\n \n         # Calculate the slot mapping.\n@@ -1373,7 +1383,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # NOTE(woosuk): Here, we should use int32 because the sampler uses\n             # int32 for bonus_token_ids. If the dtype mismatches, re-compilation\n             # will occur at runtime.\n-            bonus_token_ids = torch.zeros(num_reqs,\n+            bonus_token_ids = torch.empty(num_reqs,\n                                           device=self.device,\n                                           dtype=torch.int32)\n             self.rejection_sampler(\n@@ -1526,7 +1536,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,\n                     layer_spec.head_size)\n                 dtype = layer_spec.dtype\n-                kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n+                kv_caches[layer_name] = torch.empty(kv_cache_shape,\n                                                     dtype=dtype,\n                                                     device=self.device)\n             else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
