{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-6a11fdf", "model_patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex e29eba375..de06a19b2 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -700,5 +700,6 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         if self.enable_caching:\n-            for seq in seq_group.seqs_dict.values():\n+            # Iterate via get_seqs to leverage optimized path\n+            for seq in seq_group.get_seqs():\n                 self.compute_full_blocks_in_seq(seq)\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ab50cfdfd..1f0db8433 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -444,7 +444,10 @@ class SequenceGroup:\n         prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n     ) -> None:\n         self.request_id = request_id\n-        self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n+        # Maintain both a list and a dict for fast access patterns.\n+        # The list avoids repeated allocations when enumerating sequences.\n+        self.seqs = list(seqs)\n+        self.seqs_dict = {seq.seq_id: seq for seq in self.seqs}\n         self.sampling_params = sampling_params\n         self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                       last_token_time=arrival_time,\n@@ -458,7 +461,8 @@ class SequenceGroup:\n         self.prompt_adapter_request = prompt_adapter_request\n         self.encoder_seq = encoder_seq\n         self.trace_headers = trace_headers\n-        self._first_seq = next(iter(self.seqs_dict.values()))\n+        # Cache an arbitrary sequence for shared attributes.\n+        self._first_seq = self.seqs[0]\n \n     @property\n     def prompt(self) -> Optional[str]:\n@@ -548,9 +552,9 @@ class SequenceGroup:\n         self,\n         status: Optional[SequenceStatus] = None,\n     ) -> List[Sequence]:\n-        return list(self.seqs_dict.values()) if status is None else [\n-            seq for seq in self.seqs_dict.values() if seq.status == status\n-        ]\n+        if status is None:\n+            return self.seqs\n+        return [seq for seq in self.seqs if seq.status == status]\n \n     def is_encoder_decoder(self) -> bool:\n         return self.encoder_seq is not None\n@@ -559,16 +563,14 @@ class SequenceGroup:\n         return self.encoder_seq\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n-        return [\n-            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n-        ]\n+        return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n-        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n+        return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n-        for seq in self.seqs_dict.values():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n@@ -583,7 +585,7 @@ class SequenceGroup:\n         # Optimization. We don't need to call get_seqs if we don't need to\n         # filter by states.\n         if status is None:\n-            return len(self.seqs_dict)\n+            return len(self.seqs)\n \n         return len(self.get_seqs(status))\n \n@@ -601,19 +603,27 @@ class SequenceGroup:\n     def add(self, seq: Sequence) -> None:\n         if seq.seq_id in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n+        self.seqs.append(seq)\n         self.seqs_dict[seq.seq_id] = seq\n \n     def remove(self, seq_id: int) -> None:\n         if seq_id not in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq_id} not found.\")\n+        seq = self.seqs_dict[seq_id]\n+        # Remove by identity to avoid O(n) searches by id on every element.\n+        # This preserves order of remaining sequences.\n+        for i, s in enumerate(self.seqs):\n+            if s is seq:\n+                del self.seqs[i]\n+                break\n         del self.seqs_dict[seq_id]\n \n     def is_finished(self) -> bool:\n-        return all(seq.is_finished() for seq in self.get_seqs())\n+        return all(seq.is_finished() for seq in self.seqs)\n \n     def is_prefill(self) -> bool:\n         # Every sequence should be in the same stage.\n-        return self.get_seqs()[0].is_prefill()\n+        return self.seqs[0].is_prefill()\n \n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nindex 76f418674..001af67f3 100644\n--- a/vllm/transformers_utils/detokenizer.py\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -40,7 +40,7 @@ class Detokenizer:\n         assert prms is not None\n \n         # We can pick any sequence for the prompt.\n-        seq = next(iter(seq_group.seqs_dict.values()))\n+        seq = seq_group.get_seqs()[0]\n         # Only prompt, without the generated token.\n         all_token_ids = seq.get_token_ids()\n         prompt_token_ids = all_token_ids[:-1]\n", "model_name_or_path": "gpt-5-2025-08-07"}
