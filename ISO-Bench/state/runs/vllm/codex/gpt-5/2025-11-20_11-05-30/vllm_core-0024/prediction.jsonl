{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0df4d9b", "model_patch": "diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex d073dd6d2..7a096c857 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -22,8 +22,12 @@ __global__ void rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  // Precompute the base offset for the current token to reduce\n+  // expensive integer multiplications inside the loops.\n+  const int base = blockIdx.x * hidden_size;\n+\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[base + idx];\n     variance += x * x;\n   }\n \n@@ -32,14 +36,16 @@ __global__ void rms_norm_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        ((scalar_t)(x * s_variance)) * weight[idx];\n+    const float x = (float)input[base + idx];\n+    // Compute in float and cast once to avoid extra casts and improve ILP.\n+    const float w = (float)weight[idx];\n+    out[base + idx] = (scalar_t)(x * s_variance * w);\n   }\n }\n \n@@ -71,8 +77,10 @@ fused_add_rms_norm_kernel(\n   auto* __restrict__ weight_v =\n       reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n \n+  const int base = blockIdx.x * vec_hidden_size;\n+\n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n-    int id = blockIdx.x * vec_hidden_size + idx;\n+    int id = base + idx;\n     _f16Vec<scalar_t, width> temp = input_v[id];\n     temp += residual_v[id];\n     variance += temp.sum_squares();\n@@ -84,12 +92,13 @@ fused_add_rms_norm_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n-    int id = blockIdx.x * vec_hidden_size + idx;\n+    int id = base + idx;\n     _f16Vec<scalar_t, width> temp = residual_v[id];\n     temp *= s_variance;\n     temp *= weight_v[idx];\n@@ -110,12 +119,13 @@ fused_add_rms_norm_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  const int base = blockIdx.x * hidden_size;\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n-    z += residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[base + idx];\n+    z += residual[base + idx];\n     float x = (float)z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = z;\n+    residual[base + idx] = z;\n   }\n \n   using BlockReduce = cub::BlockReduce<float, 1024>;\n@@ -123,14 +133,15 @@ fused_add_rms_norm_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)residual[blockIdx.x * hidden_size + idx];\n-    input[blockIdx.x * hidden_size + idx] =\n-        ((scalar_t)(x * s_variance)) * weight[idx];\n+    const float x = (float)residual[base + idx];\n+    const float w = (float)weight[idx];\n+    input[base + idx] = (scalar_t)(x * s_variance * w);\n   }\n }\n \n@@ -148,7 +159,10 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  // Use a smaller block size when we have many tokens to improve latency\n+  // hiding and resident blocks per SM. This mirrors the fused kernel below.\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\ndiff --git a/csrc/layernorm_quant_kernels.cu b/csrc/layernorm_quant_kernels.cu\nindex d595b9e88..731900004 100644\n--- a/csrc/layernorm_quant_kernels.cu\n+++ b/csrc/layernorm_quant_kernels.cu\n@@ -31,8 +31,9 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  const int base = blockIdx.x * hidden_size;\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[base + idx];\n     variance += x * x;\n   }\n \n@@ -41,7 +42,8 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n@@ -49,10 +51,11 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   float const scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n-    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n+    const float x = (float)input[base + idx];\n+    const float w = (float)weight[idx];\n+    const float out_norm = x * s_variance * w;\n+    out[base + idx] = scaled_fp8_conversion<true, fp8_type>(out_norm,\n+                                                            scale_inv);\n   }\n }\n \n@@ -86,8 +89,9 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   auto* __restrict__ weight_v =\n       reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n \n+  const int base = blockIdx.x * vec_hidden_size;\n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n-    int id = blockIdx.x * vec_hidden_size + idx;\n+    int id = base + idx;\n     _f16Vec<scalar_t, width> temp = input_v[id];\n     temp += residual_v[id];\n     variance += temp.sum_squares();\n@@ -99,7 +103,8 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n@@ -107,7 +112,7 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   float const scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n-    int id = blockIdx.x * vec_hidden_size + idx;\n+    int id = base + idx;\n     _f16Vec<scalar_t, width> temp = residual_v[id];\n     temp *= s_variance;\n     temp *= weight_v[idx];\n@@ -134,12 +139,13 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n+  const int base = blockIdx.x * hidden_size;\n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n-    z += residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[base + idx];\n+    z += residual[base + idx];\n     float x = (float)z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = z;\n+    residual[base + idx] = z;\n   }\n \n   using BlockReduce = cub::BlockReduce<float, 1024>;\n@@ -147,7 +153,8 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   variance = BlockReduce(reduceStore).Reduce(variance, cub::Sum{}, blockDim.x);\n \n   if (threadIdx.x == 0) {\n-    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+    const float inv_hidden_size = 1.0f / static_cast<float>(hidden_size);\n+    s_variance = rsqrtf(variance * inv_hidden_size + epsilon);\n   }\n   __syncthreads();\n \n@@ -155,10 +162,11 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   float const scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)residual[blockIdx.x * hidden_size + idx];\n-    float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n-    out[blockIdx.x * hidden_size + idx] =\n-        scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n+    const float x = (float)residual[base + idx];\n+    const float w = (float)weight[idx];\n+    const float out_norm = x * s_variance * w;\n+    out[base + idx] = scaled_fp8_conversion<true, fp8_type>(out_norm,\n+                                                            scale_inv);\n   }\n }\n \n@@ -173,7 +181,9 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  // Align block size heuristic with unfused kernel for better occupancy.\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(\ndiff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py\nindex 5106b9914..ba4b11c70 100644\n--- a/vllm/model_executor/models/deepseek_v2.py\n+++ b/vllm/model_executor/models/deepseek_v2.py\n@@ -822,11 +822,11 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP, MixtureOfExperts):\n             device: torch.device) -> IntermediateTensors:\n         return IntermediateTensors({\n             \"hidden_states\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n             \"residual\":\n-            torch.zeros((batch_size, self.config.hidden_size),\n+            torch.empty((batch_size, self.config.hidden_size),\n                         dtype=dtype,\n                         device=device),\n         })\n", "model_name_or_path": "gpt-5-2025-08-07"}
