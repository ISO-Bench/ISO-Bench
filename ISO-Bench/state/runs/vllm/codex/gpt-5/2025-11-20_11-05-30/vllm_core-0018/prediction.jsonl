{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-5460070", "model_patch": "diff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 2df7d74e4..b049276f5 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -72,6 +72,10 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             allocator=self,\n         )\n \n+        # Stats for cache efficiency (lightweight; helps benchmarking)\n+        self._cache_hits: int = 0\n+        self._cache_lookups: int = 0\n+\n     # Implements Block.Factory.\n     def _create_block(\n         self,\n@@ -111,26 +115,65 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert device is None\n         assert_prefix_caching_block_or_none(prev_block)\n \n-        block = self._create_block(\n-            prev_block=prev_block,\n-            token_ids=token_ids,\n-            block_size=self._block_size,\n-            allocator=self,\n-        )\n-        assert block.content_hash is not None\n+        # Fast-path: if we can compute a content hash for this block without\n+        # building a temporary Block, check the cache first to avoid extra\n+        # allocations. We can compute the hash only when this block is full and\n+        # either it's the first block or the previous block already has a hash.\n+        cached_block_id: Optional[int] = None\n+        content_hash: Optional[int] = None\n+        if len(token_ids) == self._block_size:\n+            is_first_block = prev_block is None\n+            prev_block_hash = (None if is_first_block else\n+                               prev_block.content_hash  # type: ignore\n+                               )\n+            if is_first_block or prev_block_hash is not None:\n+                content_hash = PrefixCachingBlock.hash_block_tokens(\n+                    is_first_block,\n+                    prev_block_hash,\n+                    cur_block_token_ids=token_ids,\n+                )\n+                self._cache_lookups += 1\n+                cached_block_id = self._cached_blocks.get(content_hash)\n \n-        cached_block_id = self._cached_blocks.get(block.content_hash, None)\n         if cached_block_id is not None:\n+            # Cache hit: create the logical block once, bind to cached id, and\n+            # bump refcount. Avoids a miss-then-recreate path.\n+            block = self._create_block(\n+                prev_block=prev_block,\n+                token_ids=token_ids,\n+                block_size=self._block_size,\n+                allocator=self,\n+            )\n             block.block_id = cached_block_id\n+            # Cache the hash locally to avoid recomputing later.\n+            if content_hash is not None:\n+                assert isinstance(block, PrefixCachingBlock)\n+                block._cached_content_hash = content_hash  # type: ignore[attr-defined]\n+            self._cache_hits += 1\n             self._incr_refcount_cached_block(block, block.block_id)\n             return block\n \n+        # Cache miss or hash unavailable: allocate a mutable block once and\n+        # append token ids.\n         block = self.allocate_mutable(prev_block)\n         block.append_token_ids(token_ids)\n         assert block.content_hash is not None\n \n         return block\n \n+    def get_prefix_cache_hit_rate(self) -> float:\n+        lookups = self._cache_lookups\n+        if lookups == 0:\n+            return 0.0\n+        return self._cache_hits / lookups\n+\n+    # Backwards-compatible helper for older user code/tests\n+    def allocate_immutable_block(self,\n+                                 prev_block: Optional[Block],\n+                                 token_ids: List[int],\n+                                 device: Optional[Device] = None) -> Block:\n+        return self.allocate_immutable(prev_block, token_ids, device)\n+\n     def allocate_mutable(self,\n                          prev_block: Optional[Block],\n                          device: Optional[Device] = None) -> Block:\n@@ -384,7 +427,8 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         If the block is added into evictor, we need to update corresponding\n         info in evictor's metadata.\n         \"\"\"\n-\n+        if not block_ids:\n+            return\n         for block_id in block_ids:\n             if block_id in self._blocks:\n                 self._blocks[block_id].last_accessed = now\n@@ -396,7 +440,8 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n         \"\"\"Mark blocks as computed, used in prefix caching.\"\"\"\n-\n+        if not block_ids:\n+            return\n         for block_id in block_ids:\n             if block_id in self._blocks:\n                 # only those full block is valid for prefix caching\n@@ -416,24 +461,43 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             self, seq_block_ids: List[List[int]]) -> List[int]:\n         \"\"\"Return the block ids that are common for a given sequence group.\n \n-        Only those blocks that are immutable and already be marked\n-        compyted would be taken consideration.\n+        Only those blocks that are immutable and already marked computed are\n+        considered. The last block for each sequence is excluded to avoid the\n+        case where the entire prompt is cached (which would be incorrect in the\n+        model runner).\n         \"\"\"\n-\n-        # NOTE We exclude the last block to avoid the case where the entire\n-        # prompt is cached. This would cause erroneous behavior in model\n-        # runner.\n-\n-        ids_list = [\n-            list(\n-                takewhile(lambda block_id: self.block_is_computed(block_id),\n-                          seq[:-1])) for seq in seq_block_ids\n-        ]\n-        # It returns a list of int although type annotation says list of string.\n-        return commonprefix([\n-            ids for ids in ids_list  # type: ignore\n-            if ids != []\n-        ])\n+        if not seq_block_ids:\n+            return []\n+\n+        # Build per-sequence prefix of contiguous computed blocks (excluding\n+        # the last block of each sequence).\n+        computed_prefixes: List[List[int]] = []\n+        for seq in seq_block_ids:\n+            if not seq:\n+                continue\n+            upto = max(len(seq) - 1, 0)\n+            prefix: List[int] = []\n+            for bid in seq[:upto]:\n+                if self.block_is_computed(bid):\n+                    prefix.append(bid)\n+                else:\n+                    break\n+            if prefix:\n+                computed_prefixes.append(prefix)\n+\n+        if not computed_prefixes:\n+            return []\n+\n+        # Compute common prefix across all sequences' computed prefixes.\n+        first = computed_prefixes[0]\n+        max_len = min(len(p) for p in computed_prefixes)\n+        i = 0\n+        while i < max_len:\n+            token = first[i]\n+            if any(p[i] != token for p in computed_prefixes[1:]):\n+                break\n+            i += 1\n+        return first[:i]\n \n     def get_num_blocks_touched(self,\n                                blocks: List[Block],\n", "model_name_or_path": "gpt-5-2025-08-07"}
