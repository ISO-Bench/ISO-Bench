{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-acaea3b", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/v1/spec_decode/test_eagle.py b/tests/v1/spec_decode/test_eagle.py\nindex e000d955c..352a39edc 100644\n--- a/tests/v1/spec_decode/test_eagle.py\n+++ b/tests/v1/spec_decode/test_eagle.py\n@@ -100,8 +100,11 @@ def test_prepare_inputs():\n         dtype=torch.int32,\n         device=device)\n \n+    # n1 + n2 + n3 - a - b -c\n+    num_tokens = cu_target_query_lens[-1].item() - num_rejected_tokens.sum().item()\n+\n     cu_num_tokens, token_indices = EagleProposer.prepare_inputs(\n-        cu_target_query_lens, num_rejected_tokens)\n+        cu_target_query_lens, num_rejected_tokens, num_tokens)\n \n     assert torch.equal(cu_num_tokens, expected_cu_num_tokens)\n     assert token_indices.shape[0] == expected_cu_num_tokens[-1].item()\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex f94ab75f9..169dbc3db 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -508,8 +508,6 @@ class MambaMixer2(CustomOp):\n                 query_start_loc=query_start_loc_p).transpose(\n                     0, 1)[:num_prefill_tokens]\n \n-            # TODO: Why is this needed?\n-            hidden_states_B_C_p = hidden_states_B_C_p.contiguous()\n             hidden_states_p, B_p, C_p = split_hidden_states_B_C_fn(\n                 hidden_states_B_C_p)\n \n@@ -517,10 +515,15 @@ class MambaMixer2(CustomOp):\n             initial_states = None\n             if (mamba2_metadata.has_initial_states is not None\n                     and mamba2_metadata.prep_initial_states):\n-                # making a copy of the states\n-                initial_states = torch.where(\n-                    mamba2_metadata.has_initial_states[:, None, None, None],\n-                    mamba_cache_params.ssm_state[state_indices_tensor_p], 0)\n+                # Instead of allocating a large zero tensor via torch.where,\n+                # zero-initialize only the missing states in-place and gather.\n+                has_init = mamba2_metadata.has_initial_states\n+                missing = ~has_init\n+                if any(missing):\n+                    for idx in state_indices_tensor_p[missing]:\n+                        mamba_cache_params.ssm_state[idx].zero_()\n+                initial_states = mamba_cache_params.ssm_state[\n+                    state_indices_tensor_p]\n \n             scan_output, varlen_state = mamba_chunk_scan_combined(\n                 hidden_states_p.view(1, num_prefill_tokens,\n@@ -602,7 +605,7 @@ class MambaMixer2(CustomOp):\n                                      self.head_dim))\n \n         # Merge prefill and decode outputs before passing to gated MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n+        hidden_states = torch.cat(ssd_output_list, dim=0)\n \n         # 4. gated MLP\n         # GatedRMSNorm internally applying SiLU to the gate\ndiff --git a/vllm/v1/spec_decode/eagle.py b/vllm/v1/spec_decode/eagle.py\nindex 3926a86ee..5147c204a 100644\n--- a/vllm/v1/spec_decode/eagle.py\n+++ b/vllm/v1/spec_decode/eagle.py\n@@ -271,6 +271,7 @@ class EagleProposer:\n         cu_target_query_lens: torch.Tensor,\n         # [batch_size]\n         num_rejected_tokens: torch.Tensor,\n+        num_tokens: int,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         # cu_target_query_lens: [0, a, a + b, a + b + c]\n         # num_rejected_tokens: [n1, n2, n3]\n@@ -292,8 +293,6 @@ class EagleProposer:\n         torch.cumsum(num_tokens_per_req, dim=0, out=cu_num_tokens[1:])\n         cu_num_tokens[0] = 0\n \n-        # FIXME(woosuk): Avoid synchronization.\n-        num_tokens = cu_num_tokens[-1].item()\n         token_indices = torch.empty(\n             num_tokens,\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 42847e2f8..6e4aa56ae 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -201,19 +201,19 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.num_sms = self.device_properties.multi_processor_count\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n-        self.query_start_loc = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc = torch.empty(self.max_num_reqs + 1,\n                                            dtype=torch.int32,\n                                            device=self.device)\n-        self.seq_lens = torch.zeros(self.max_num_reqs,\n+        self.seq_lens = torch.empty(self.max_num_reqs,\n                                     dtype=torch.int32,\n                                     device=self.device)\n-        self.slot_mapping = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping = torch.empty(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=self.device)\n \n@@ -232,10 +232,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n@@ -244,7 +244,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # Only relevant for models using ALiBi (e.g, MPT)\n         self.use_alibi = check_use_alibi(model_config)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -258,21 +258,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n@@ -1386,18 +1386,22 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             else:\n                 # TODO(woosuk): Refactor this.\n                 num_draft_tokens = spec_decode_metadata.num_draft_tokens\n-                num_rejected_tokens = [\n+                num_rejected_tokens_list = [\n                     n + 1 - len(valid_sampled_token_ids[i]) if n > 0 else 0\n                     for i, n in enumerate(num_draft_tokens)\n                 ]\n+                # Precompute totals on CPU to avoid GPU sync\n+                rejected_sum = sum(num_rejected_tokens_list)\n+                num_tokens = num_scheduled_tokens - rejected_sum\n                 num_rejected_tokens = torch.tensor(\n-                    num_rejected_tokens,\n+                    num_rejected_tokens_list,\n                     dtype=torch.int32,\n                     device=self.device,\n                 )\n                 cu_num_tokens, token_indices = self.drafter.prepare_inputs(\n                     eagle_attn_metadata.query_start_loc,\n                     num_rejected_tokens,\n+                    num_tokens,\n                 )\n                 target_token_ids = self.input_ids[token_indices]\n                 target_positions = positions[token_indices]\n", "model_name_or_path": "gpt-5-2025-08-07"}
