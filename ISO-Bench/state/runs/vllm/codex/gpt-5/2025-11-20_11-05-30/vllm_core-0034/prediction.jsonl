{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-95a178f", "model_patch": "diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex d054ca341..0cbe8371e 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -103,13 +103,13 @@ async def test_multi_step(\n         model,\n         server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n     test_completions = await completions_with_server_args(\n         prompts,\n         model,\n         ms_server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n \n     # Assert multi-step scheduling produces identical tokens\n     # to single-step scheduling.\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 1eab83f3b..d8fecffc6 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1305,14 +1305,16 @@ class LLMEngine:\n         else:\n             outputs_by_sequence_group = outputs\n \n-        finished_before: List[int] = []\n+        # Track indices of sequence groups that were already finished before\n+        # processing to avoid double work. Use a set for O(1) membership.\n+        finished_before: Set[int] = set()\n         for i, seq_group_meta in enumerate(seq_group_metadata_list):\n             scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n             seq_group = scheduled_seq_group.seq_group\n \n             if seq_group.is_finished():\n-                finished_before.append(i)\n+                finished_before.add(i)\n                 continue\n \n             if len(outputs) > 1:\n@@ -1649,14 +1651,14 @@ class LLMEngine:\n                 or not seq_group_metadata_list):\n             return False\n \n-        # TODO(will) this is a sanity check for nowto make sure that all the\n+        # TODO(will) this is a sanity check for now to make sure that all the\n         # seqs are on the same steps. Eventually we will want to do some sort of\n         # dynamic scheduling when doing multi-step decoding.\n         ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps\n-        if any([\n-                seq_group.state.remaining_steps != ref_remaining_steps\n-                for seq_group in seq_group_metadata_list[1:]\n-        ]):\n+        if any(\n+            seq_group.state.remaining_steps != ref_remaining_steps\n+            for seq_group in seq_group_metadata_list[1:]\n+        ):\n             raise AssertionError((\"All running sequence groups should \"\n                                   \"have the same remaining steps.\"))\n \ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex be0c75bc0..290fb4685 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -298,7 +298,9 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         # if CPU is ahead.\n         if self.is_driver_worker and get_pp_group().is_last_rank:\n             if self.pinned_sampled_token_ids is None:\n-                self.pinned_sampled_token_ids = torch.zeros(\n+                # Use uninitialized pinned CPU memory; contents will be\n+                # immediately overwritten by GPU->CPU copy.\n+                self.pinned_sampled_token_ids = torch.empty(\n                     (self.scheduler_config.max_num_seqs, 1),\n                     dtype=torch.long,\n                     device=\"cpu\",\n", "model_name_or_path": "gpt-5-2025-08-07"}
