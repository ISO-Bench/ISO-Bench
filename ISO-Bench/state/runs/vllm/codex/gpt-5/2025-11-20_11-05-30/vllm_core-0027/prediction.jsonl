{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-c9e2d64", "model_patch": "diff --git a/tests/weight_loading/test_weight_loading.py b/tests/weight_loading/test_weight_loading.py\nindex e456bfab8..fd6539d06 100644\n--- a/tests/weight_loading/test_weight_loading.py\n+++ b/tests/weight_loading/test_weight_loading.py\n@@ -12,7 +12,9 @@ MODEL_NAME = os.environ.get(\"MODEL_NAME\",\n                             \"robertgshaw2/zephyr-7b-beta-channelwise-gptq\")\n REVISION = os.environ.get(\"REVISION\", \"main\")\n QUANTIZATION = os.environ.get(\"QUANTIZATION\", \"gptq_marlin\")\n-MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"89\")\n+# Lower the default minimum capability to broaden GPU support and avoid\n+# unnecessarily skipping tests on capable devices.\n+MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"80\")\n \n \n @pytest.mark.skipif(\ndiff --git a/vllm/model_executor/layers/quantization/awq_marlin.py b/vllm/model_executor/layers/quantization/awq_marlin.py\nindex a43b2e597..34892cd1a 100644\n--- a/vllm/model_executor/layers/quantization/awq_marlin.py\n+++ b/vllm/model_executor/layers/quantization/awq_marlin.py\n@@ -134,6 +134,29 @@ class AWQMarlinConfig(QuantizationConfig):\n                     self.full_config).get_quant_method(layer, prefix)\n             return AWQMarlinLinearMethod(self)\n         elif isinstance(layer, FusedMoE):\n+            # Prefer the WNA16 MoE kernel by default for large expert counts\n+            # when compatible, which can reduce overhead and improve\n+            # performance on many-expert MoEs.\n+            try:\n+                num_experts = getattr(layer, \"num_experts\", None)\n+                if num_experts is not None and num_experts >= 64:\n+                    # Local import to avoid circular import on module load.\n+                    from vllm.model_executor.layers.quantization.moe_wna16 import (  # noqa: E501\n+                        MoeWNA16Config)\n+                    cfg = {\n+                        \"quant_method\": \"awq\",\n+                        \"bits\": self.weight_bits,\n+                        \"group_size\": self.group_size,\n+                        \"zero_point\": self.zero_point,\n+                        \"lm_head\": self.lm_head_quantized,\n+                        \"modules_to_not_convert\": self.modules_to_not_convert,\n+                    }\n+                    if MoeWNA16Config.is_moe_wna16_compatible(cfg):\n+                        return MoeWNA16Config.from_config(cfg).get_quant_method(\n+                            layer, prefix)\n+            except Exception:\n+                # Fall back to Marlin MoE if any issue occurs.\n+                pass\n             return AWQMoEMethod(self)\n         return None\n \ndiff --git a/vllm/model_executor/layers/quantization/gptq_marlin.py b/vllm/model_executor/layers/quantization/gptq_marlin.py\nindex 0a9d86b00..035f0a621 100644\n--- a/vllm/model_executor/layers/quantization/gptq_marlin.py\n+++ b/vllm/model_executor/layers/quantization/gptq_marlin.py\n@@ -160,6 +160,28 @@ class GPTQMarlinConfig(QuantizationConfig):\n     ) -> Optional[Union[\"GPTQMarlinLinearMethod\", \"GPTQMarlinMoEMethod\",\n                         UnquantizedLinearMethod, UnquantizedEmbeddingMethod]]:\n         if isinstance(layer, FusedMoE):\n+            # For many-expert MoEs, prefer the WNA16 MoE kernel where\n+            # compatible as it can reduce routing/memory overhead.\n+            try:\n+                num_experts = getattr(layer, \"num_experts\", None)\n+                if num_experts is not None and num_experts >= 64:\n+                    # Local import to avoid circular import on module load.\n+                    from vllm.model_executor.layers.quantization.moe_wna16 import (  # noqa: E501\n+                        MoeWNA16Config)\n+                    cfg = {\n+                        \"quant_method\": \"gptq\",\n+                        \"bits\": self.weight_bits,\n+                        \"group_size\": self.group_size,\n+                        \"desc_act\": self.desc_act,\n+                        \"sym\": self.is_sym,\n+                        \"lm_head\": self.lm_head_quantized,\n+                    }\n+                    if MoeWNA16Config.is_moe_wna16_compatible(cfg):\n+                        return MoeWNA16Config.from_config(cfg).get_quant_method(\n+                            layer, prefix)\n+            except Exception:\n+                # If any issue arises, fall back to Marlin MoE.\n+                pass\n             return GPTQMarlinMoEMethod(self)\n         return get_linear_quant_method(self, layer, prefix,\n                                        GPTQMarlinLinearMethod)\ndiff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py\nindex b9460e7d7..eca5cec3b 100644\n--- a/vllm/model_executor/layers/quantization/moe_wna16.py\n+++ b/vllm/model_executor/layers/quantization/moe_wna16.py\n@@ -217,7 +217,9 @@ class MoeWNA16Method(FusedMoEMethodBase):\n         layer.register_parameter(\"w2_qweight\", w2_qweight)\n         set_weight_attrs(w2_qweight, extra_weight_attrs)\n \n-        w13_scales = torch.nn.Parameter(torch.zeros(\n+        # Scales and zero-points are immediately loaded from checkpoints; avoid\n+        # the extra memset by using empty() instead of zeros().\n+        w13_scales = torch.nn.Parameter(torch.empty(\n             num_experts,\n             2 * intermediate_size_per_partition,\n             hidden_size // group_size,\n@@ -226,7 +228,7 @@ class MoeWNA16Method(FusedMoEMethodBase):\n         layer.register_parameter(\"w13_scales\", w13_scales)\n         set_weight_attrs(w13_scales, extra_weight_attrs)\n \n-        w2_scales = torch.nn.Parameter(torch.zeros(\n+        w2_scales = torch.nn.Parameter(torch.empty(\n             num_experts,\n             hidden_size,\n             intermediate_size_per_partition // group_size,\n@@ -236,7 +238,7 @@ class MoeWNA16Method(FusedMoEMethodBase):\n         set_weight_attrs(w2_scales, extra_weight_attrs)\n \n         if self.quant_config.has_zp:\n-            w13_qzeros = torch.nn.Parameter(torch.zeros(\n+            w13_qzeros = torch.nn.Parameter(torch.empty(\n                 num_experts,\n                 2 * intermediate_size_per_partition // bit8_pack_factor,\n                 hidden_size // group_size,\n@@ -245,7 +247,7 @@ class MoeWNA16Method(FusedMoEMethodBase):\n             layer.register_parameter(\"w13_qzeros\", w13_qzeros)\n             set_weight_attrs(w13_qzeros, extra_weight_attrs)\n \n-            w2_qzeros = torch.nn.Parameter(torch.zeros(\n+            w2_qzeros = torch.nn.Parameter(torch.empty(\n                 num_experts,\n                 hidden_size // bit8_pack_factor,\n                 intermediate_size_per_partition // group_size,\n", "model_name_or_path": "gpt-5-2025-08-07"}
