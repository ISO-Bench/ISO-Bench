diff --git a/Dockerfile b/Dockerfile
index f41753aeb..0549c0ee3 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -41,13 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads
 
 RUN python3 setup.py build_ext --inplace
 
-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.
-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78
-RUN apt-get install -y git && \
-    git clone https://github.com/stanford-futuredata/megablocks.git && \
-    cd megablocks && \
-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \
-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel
 
 # image to run unit testing suite
 FROM dev AS test
@@ -87,10 +80,5 @@ RUN --mount=type=cache,target=/root/.cache/pip \
 
 COPY vllm vllm
 COPY --from=build /workspace/vllm/*.so /workspace/vllm/
-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/
-RUN --mount=type=cache,target=/root/.cache/pip \
-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \
-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl
-
 ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
 
diff --git a/README.md b/README.md
index 84cadee48..e4b3b5026 100644
--- a/README.md
+++ b/README.md
@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get
 ```bash
 pip install vllm
 ```
-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):
-```bash
-pip install megablocks
-```
 
 ## Getting Started
 
diff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst
index e21cdd65d..f9663fa07 100644
--- a/docs/source/models/supported_models.rst
+++ b/docs/source/models/supported_models.rst
@@ -76,7 +76,7 @@ Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-pr
 .. note::
     Currently, the ROCm version of vLLM does not support Mixtral.
     Additionally, it only supports Mistral for context lengths up to 4096.
-
+    Mixtral models require additional optional dependencies (megablocks and stanford-stk) to be installed; see the README for details.
 .. tip::
     The easiest way to check if your model is supported is to run the program below:
 
diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/config.py b/vllm/config.py
index 6bafa73c7..9e88f0d19 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -12,6 +12,9 @@ logger = init_logger(__name__)
 
 _GB = 1 << 30
 
+_SUPPORTED_LOAD_FORMATS = ("auto", "pt", "safetensors", "npcache", "dummy")
+_ROCM_NOT_SUPPORTED_LOAD_FORMATS = ("safetensors",)
+
 
 class ModelConfig:
     """Configuration for the model.
@@ -98,22 +101,20 @@ class ModelConfig:
 
     def _verify_load_format(self) -> None:
         load_format = self.load_format.lower()
-        supported_load_format = [
-            "auto", "pt", "safetensors", "npcache", "dummy"
-        ]
-        rocm_not_supported_load_format = ["safetensors"]
+        supported_load_format = _SUPPORTED_LOAD_FORMATS
+        rocm_not_supported_load_format = _ROCM_NOT_SUPPORTED_LOAD_FORMATS
         if load_format not in supported_load_format:
             raise ValueError(
                 f"Unknown load format: {self.load_format}. Must be one of "
                 "'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.")
         if is_hip():
-            if load_format in ["safetensors"]:
+            if load_format in rocm_not_supported_load_format:
                 rocm_supported_load_format = [
                     f for f in supported_load_format
                     if (f not in rocm_not_supported_load_format)
                 ]
                 raise ValueError(
-                    f"load format \'{load_format}\' is not supported in ROCm. "
+                    f"load format '{load_format}' is not supported in ROCm. "
                     f"Supported load format are "
                     f"{rocm_supported_load_format}")
             # Force ROCm to load from pt weights if nothing specific is set
diff --git a/vllm/model_executor/layers/fused_moe.py b/vllm/model_executor/layers/fused_moe.py
new file mode 100644
index 000000000..a63d6a5c6
--- /dev/null
+++ b/vllm/model_executor/layers/fused_moe.py
@@ -0,0 +1,67 @@
+import torch
+from typing import Tuple
+
+
+def _align_to_block_size(counts: torch.Tensor, block_size: int) -> torch.Tensor:
+    """Align per-expert token counts up to a multiple of block_size.
+
+    Args:
+        counts: 1D tensor of length num_experts holding counts per expert.
+        block_size: block size to align to.
+
+    Returns:
+        1D tensor of aligned counts per expert (same dtype/device as counts).
+    """
+    # Simple integer ceil division then multiply by block size.
+    return ((counts + (block_size - 1)) // block_size) * block_size
+
+
+def moe_align_block_size(
+    topk_ids: torch.Tensor,
+    num_experts: int,
+    block_size: int,
+    topk: int,
+) -> Tuple[torch.Tensor, torch.Tensor, int]:
+    """Align MoE expert assignments to a block size efficiently.
+
+    This optimized implementation keeps computations on the input device
+    (CPU or CUDA), avoids unnecessary host-device transfers, and uses
+    allocation-efficient tensor ops.
+
+    Args:
+        topk_ids: Flattened tensor of expert IDs of shape (num_tokens * topk,)
+            with dtype torch.int32.
+        num_experts: Total number of experts.
+        block_size: Block size to align to.
+        topk: Number of experts per token (top-k routing). Unused here but part
+            of the public API.
+
+    Returns:
+        sorted_ids: Indices that sort the tokens by expert (int32 tensor).
+        expert_ids: Expanded expert IDs after padding to block size (int32 tensor).
+        num_tokens_post_pad: Total number of (padded) token assignments (int).
+    """
+    assert topk_ids.dtype == torch.int32, "topk_ids must be int32"
+
+    device = topk_ids.device
+
+    # Group tokens by expert using argsort on-device.
+    # Convert to int64 for argsort stability if needed; int32 works fine.
+    sorted_ids = torch.argsort(topk_ids, stable=True) if hasattr(torch, 'argsort') else torch.sort(topk_ids)[1]
+    # Ensure int32 dtype for output indices
+    if sorted_ids.dtype != torch.int32:
+        sorted_ids = sorted_ids.to(torch.int32)
+
+    # Count tokens per expert on device with bincount (requires int64 input).
+    counts = torch.bincount(topk_ids.to(torch.int64), minlength=num_experts)
+    # Align counts to block size.
+    padded_counts = _align_to_block_size(counts, block_size)
+
+    # Total padded tokens (int on host). This sync is cheap due to tiny size.
+    num_tokens_post_pad = int(padded_counts.sum().item())
+
+    # Build expanded expert id list after padding using repeat_interleave.
+    expert_range = torch.arange(num_experts, device=device, dtype=torch.int32)
+    expert_ids = expert_range.repeat_interleave(padded_counts.to(torch.int64))
+
+    return sorted_ids, expert_ids, num_tokens_post_pad
diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py
index 5596884f3..9e0e4adfa 100644
--- a/vllm/model_executor/models/__init__.py
+++ b/vllm/model_executor/models/__init__.py
@@ -1,4 +1,5 @@
 import importlib
+
 from typing import List, Optional, Type
 
 import torch.nn as nn
@@ -39,6 +40,8 @@ _MODELS = {
 }
 
 # Models not supported by ROCm.
+_SUPPORTED_ARCHS = list(_MODELS.keys())
+
 _ROCM_UNSUPPORTED_MODELS = ["MixtralForCausalLM"]
 
 # Models partially supported by ROCm.
@@ -72,7 +75,7 @@ class ModelRegistry:
 
     @staticmethod
     def get_supported_archs() -> List[str]:
-        return list(_MODELS.keys())
+        return _SUPPORTED_ARCHS
 
 
 __all__ = [
diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py
index 8e0a094c7..2879c77f0 100644
--- a/vllm/model_executor/models/mixtral.py
+++ b/vllm/model_executor/models/mixtral.py
@@ -340,9 +340,10 @@ class BlockSparseMoE(nn.Module):
         all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)
         # weights, selected_experts: (sequence_length, top-k)
         weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)
-        weights /= weights.sum(dim=-1, keepdim=True)
-        weights = weights.flatten().to(x.dtype)
-        selected_experts = selected_experts.flatten()
+        denom = weights.sum(dim=-1, keepdim=True)
+        weights.mul_(denom.reciprocal())
+        weights = weights.to(x.dtype).reshape(-1)
+        selected_experts = selected_experts.reshape(-1)
 
         (indices, bin_ids, bins, padded_bins,
          _) = self.indices_and_padded_bins(selected_experts)
@@ -477,9 +478,8 @@ class MixtralForCausalLM(nn.Module):
         hidden_states = self.tok_embeddings(input_ids)
 
         # forward
-        for i in range(len(self.layers)):
+        for i, layer in enumerate(self.layers):
             cache_event = None if cache_events is None else cache_events[i]
-            layer = self.layers[i]
             hidden_states = layer(
                 positions,
                 hidden_states,
