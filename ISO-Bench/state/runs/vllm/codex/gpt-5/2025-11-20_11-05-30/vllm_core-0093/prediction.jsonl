{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-1da8f0e", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex 28d8e3905..104242d8c 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -62,8 +62,8 @@ class InputBatch:\n         # TODO(woosuk): This buffer could be too large if max_model_len is big.\n         # Find a way to reduce the CPU memory usage.\n         # This buffer is not directly transferred to the GPU, so it does not\n-        # need to be pinned.\n-        self.token_ids_cpu_tensor = torch.zeros(\n+        # need to be pinned. Use empty() to avoid unnecessary zero-init.\n+        self.token_ids_cpu_tensor = torch.empty(\n             (max_num_reqs, max_model_len),\n             device=\"cpu\",\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex a00c00c30..82b9a807f 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -135,11 +135,12 @@ class GPUModelRunner:\n         self.device_properties = torch.cuda.get_device_properties(self.device)\n         self.num_sms = self.device_properties.multi_processor_count\n \n-        # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        # Persistent buffers for CUDA graphs. Use empty() to avoid costly\n+        # zero-initialization; these buffers are fully written before use.\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n \n@@ -155,16 +156,16 @@ class GPUModelRunner:\n             # identical position IDs, making M-RoPE functionally equivalent to\n             # 1D-RoPE.\n             # See page 5 of https://arxiv.org/abs/2409.12191\n-            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n+            self.mrope_positions = torch.empty((3, self.max_num_tokens + 1),\n                                                dtype=torch.int64,\n                                                device=self.device)\n-            self.mrope_positions_cpu = torch.zeros(\n+            self.mrope_positions_cpu = torch.empty(\n                 (3, self.max_num_tokens + 1),\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n \n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n@@ -177,27 +178,27 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
