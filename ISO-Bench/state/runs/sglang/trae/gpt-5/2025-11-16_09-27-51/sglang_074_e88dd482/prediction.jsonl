{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-7360067", "model_patch": "diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 9b2722126..e7fa0b63c 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -37,6 +37,7 @@ from transformers import (\n     PreTrainedTokenizerBase,\n     PreTrainedTokenizerFast,\n )\n+from functools import lru_cache\n \n AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)\n ASSISTANT_SUFFIX = \"Assistant:\"\n@@ -74,13 +75,20 @@ class RequestFuncOutput:\n \n \n def remove_prefix(text: str, prefix: str) -> str:\n-    return text[len(prefix) :] if text.startswith(prefix) else text\n+    try:\n+        return text.removeprefix(prefix)\n+    except AttributeError:\n+        return text[len(prefix) :] if text.startswith(prefix) else text\n \n \n def remove_suffix(text: str, suffix: str) -> str:\n-    return text[: -len(suffix)] if text.endswith(suffix) else text\n+    try:\n+        return text.removesuffix(suffix)\n+    except AttributeError:\n+        return text[: -len(suffix)] if text.endswith(suffix) else text\n \n \n+@lru_cache(maxsize=1)\n def get_auth_headers() -> Dict[str, str]:\n     api_key = os.environ.get(\"OPENAI_API_KEY\")\n     if api_key:\n@@ -811,15 +819,19 @@ def sample_generated_shared_prefix_requests(\n \n     # Generate system prompts for each group\n     system_prompts = []\n+    system_prompt_token_lens = []\n     for _ in range(num_groups):\n         system_prompt = gen_prompt(tokenizer, system_prompt_len)\n         system_prompts.append(system_prompt)\n+        system_prompt_token_lens.append(len(tokenizer.encode(system_prompt)))\n \n     # Generate questions\n     questions = []\n+    question_token_lens = []\n     for _ in range(num_groups * prompts_per_group):\n         question = gen_prompt(tokenizer, question_len)\n         questions.append(question)\n+        question_token_lens.append(len(tokenizer.encode(question)))\n \n     # Combine system prompts with questions\n     input_requests = []\n@@ -850,10 +862,10 @@ def sample_generated_shared_prefix_requests(\n     print(f\"Total input tokens: {total_input_tokens}\")\n     print(f\"Total output tokens: {total_output_tokens}\")\n     print(\n-        f\"Average system prompt length: {sum(len(tokenizer.encode(sp)) for sp in system_prompts) / len(system_prompts):.1f} tokens\"\n+        f\"Average system prompt length: {sum(system_prompt_token_lens) / len(system_prompt_token_lens):.1f} tokens\"\n     )\n     print(\n-        f\"Average question length: {sum(len(tokenizer.encode(q)) for q in questions) / len(questions):.1f} tokens\\n\"\n+        f\"Average question length: {sum(question_token_lens) / len(question_token_lens):.1f} tokens\\n\"\n     )\n \n     # Save to cache\n@@ -890,6 +902,7 @@ def calculate_metrics(\n     tokenizer: PreTrainedTokenizerBase,\n     backend: str,\n ) -> Tuple[BenchmarkMetrics, List[int]]:\n+    disable_retokenize = _get_bool_env_var(\"SGLANG_BENCH_DISABLE_RETOKENIZE\", \"false\")\n     output_lens: List[int] = []\n     retokenized_output_lens: List[int] = []\n     total_input = 0\n@@ -902,9 +915,12 @@ def calculate_metrics(\n         if outputs[i].success:\n             output_len = outputs[i].output_len\n             output_lens.append(output_len)\n-            retokenized_output_len = len(\n-                tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)\n-            )\n+            if disable_retokenize:\n+                retokenized_output_len = output_len\n+            else:\n+                retokenized_output_len = len(\n+                    tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)\n+                )\n             retokenized_output_lens.append(retokenized_output_len)\n             total_input += input_requests[i][1]\n             if output_len > 1:\n", "model_name_or_path": "gpt-5-2025-08-07"}
