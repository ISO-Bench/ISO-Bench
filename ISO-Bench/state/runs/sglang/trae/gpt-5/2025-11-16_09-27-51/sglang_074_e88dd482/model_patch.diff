diff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py
index 9b2722126..e7fa0b63c 100644
--- a/python/sglang/bench_serving.py
+++ b/python/sglang/bench_serving.py
@@ -37,6 +37,7 @@ from transformers import (
     PreTrainedTokenizerBase,
     PreTrainedTokenizerFast,
 )
+from functools import lru_cache
 
 AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)
 ASSISTANT_SUFFIX = "Assistant:"
@@ -74,13 +75,20 @@ class RequestFuncOutput:
 
 
 def remove_prefix(text: str, prefix: str) -> str:
-    return text[len(prefix) :] if text.startswith(prefix) else text
+    try:
+        return text.removeprefix(prefix)
+    except AttributeError:
+        return text[len(prefix) :] if text.startswith(prefix) else text
 
 
 def remove_suffix(text: str, suffix: str) -> str:
-    return text[: -len(suffix)] if text.endswith(suffix) else text
+    try:
+        return text.removesuffix(suffix)
+    except AttributeError:
+        return text[: -len(suffix)] if text.endswith(suffix) else text
 
 
+@lru_cache(maxsize=1)
 def get_auth_headers() -> Dict[str, str]:
     api_key = os.environ.get("OPENAI_API_KEY")
     if api_key:
@@ -811,15 +819,19 @@ def sample_generated_shared_prefix_requests(
 
     # Generate system prompts for each group
     system_prompts = []
+    system_prompt_token_lens = []
     for _ in range(num_groups):
         system_prompt = gen_prompt(tokenizer, system_prompt_len)
         system_prompts.append(system_prompt)
+        system_prompt_token_lens.append(len(tokenizer.encode(system_prompt)))
 
     # Generate questions
     questions = []
+    question_token_lens = []
     for _ in range(num_groups * prompts_per_group):
         question = gen_prompt(tokenizer, question_len)
         questions.append(question)
+        question_token_lens.append(len(tokenizer.encode(question)))
 
     # Combine system prompts with questions
     input_requests = []
@@ -850,10 +862,10 @@ def sample_generated_shared_prefix_requests(
     print(f"Total input tokens: {total_input_tokens}")
     print(f"Total output tokens: {total_output_tokens}")
     print(
-        f"Average system prompt length: {sum(len(tokenizer.encode(sp)) for sp in system_prompts) / len(system_prompts):.1f} tokens"
+        f"Average system prompt length: {sum(system_prompt_token_lens) / len(system_prompt_token_lens):.1f} tokens"
     )
     print(
-        f"Average question length: {sum(len(tokenizer.encode(q)) for q in questions) / len(questions):.1f} tokens\n"
+        f"Average question length: {sum(question_token_lens) / len(question_token_lens):.1f} tokens\n"
     )
 
     # Save to cache
@@ -890,6 +902,7 @@ def calculate_metrics(
     tokenizer: PreTrainedTokenizerBase,
     backend: str,
 ) -> Tuple[BenchmarkMetrics, List[int]]:
+    disable_retokenize = _get_bool_env_var("SGLANG_BENCH_DISABLE_RETOKENIZE", "false")
     output_lens: List[int] = []
     retokenized_output_lens: List[int] = []
     total_input = 0
@@ -902,9 +915,12 @@ def calculate_metrics(
         if outputs[i].success:
             output_len = outputs[i].output_len
             output_lens.append(output_len)
-            retokenized_output_len = len(
-                tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)
-            )
+            if disable_retokenize:
+                retokenized_output_len = output_len
+            else:
+                retokenized_output_len = len(
+                    tokenizer.encode(outputs[i].generated_text, add_special_tokens=False)
+                )
             retokenized_output_lens.append(retokenized_output_len)
             total_input += input_requests[i][1]
             if output_len > 1:
