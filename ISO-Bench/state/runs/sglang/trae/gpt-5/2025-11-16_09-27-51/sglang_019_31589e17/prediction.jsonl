{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ae6a5b2", "model_patch": "diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 29f18f0ef..b4fc4d7a7 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -454,6 +454,7 @@ class DeepseekV2MoE(nn.Module):\n                 num_expert_group=self.num_expert_group,\n                 correction_bias=self.correction_bias,\n                 routed_scaling_factor=self.routed_scaling_factor,\n+                num_token_non_padded=state.forward_batch.num_token_non_padded,\n                 expert_location_dispatch_info=ExpertLocationDispatchInfo.init_new(\n                     layer_id=self.layer_id,\n                 ),\ndiff --git a/python/sglang/srt/two_batch_overlap.py b/python/sglang/srt/two_batch_overlap.py\nindex 6b0241f40..7cdc88837 100644\n--- a/python/sglang/srt/two_batch_overlap.py\n+++ b/python/sglang/srt/two_batch_overlap.py\n@@ -110,7 +110,8 @@ def compute_split_indices_for_cuda_graph_replay(\n \n class TboCudaGraphRunnerPlugin:\n     def __init__(self):\n-        pass  # TODO add logic here\n+        # Buffer to hold non-padded token counts for TBO children [A, B]\n+        self._tbo_children_num_token_non_padded = torch.zeros((2,), dtype=torch.int32)\n \n     def capture_one_batch_size(self, batch: ForwardBatch, num_tokens: int):\n         if not global_server_args_dict[\"enable_two_batch_overlap\"]:\n@@ -126,13 +127,48 @@ class TboCudaGraphRunnerPlugin:\n \n         TboForwardBatchPreparer.prepare(batch)\n \n+        # Initialize children's num_token_non_padded for graph capture path\n+        # Use token split computed from sequence split for decode; extend uses token computation from lens.\n+        tbo_split_token_index = compute_split_token_index(\n+            split_seq_index=batch.tbo_split_seq_index,\n+            forward_mode=batch.forward_mode,\n+            extend_seq_lens=None,\n+        )\n+        a_tokens = int(tbo_split_token_index)\n+        b_tokens = int(num_tokens - a_tokens)\n+        # Keep buffer device consistent with parent's num_token_non_padded when available\n+        parent_device = (\n+            batch.num_token_non_padded.device\n+            if isinstance(batch.num_token_non_padded, torch.Tensor)\n+            else self._tbo_children_num_token_non_padded.device\n+        )\n+        if self._tbo_children_num_token_non_padded.device != parent_device:\n+            self._tbo_children_num_token_non_padded = self._tbo_children_num_token_non_padded.to(parent_device)\n+        self._tbo_children_num_token_non_padded[0] = a_tokens\n+        self._tbo_children_num_token_non_padded[1] = b_tokens\n+        # Assign scalars to children for masking padded regions in grouped_topk\n+        if batch.tbo_children is not None:\n+            batch.tbo_children[0].num_token_non_padded = self._tbo_children_num_token_non_padded[0]\n+            batch.tbo_children[1].num_token_non_padded = self._tbo_children_num_token_non_padded[1]\n+\n     def replay_prepare(\n         self, forward_mode: ForwardMode, bs: int, num_token_non_padded: int\n     ):\n         if not global_server_args_dict[\"enable_two_batch_overlap\"]:\n             return\n \n-        pass  # TODO add logic here\n+        # Compute split indices for the current bs used by CUDA Graph replay\n+        tbo_split_seq_index, tbo_split_token_index = compute_split_indices_for_cuda_graph_replay(\n+            forward_mode=forward_mode, cuda_graph_num_tokens=bs\n+        )\n+        # Compute children's non-padded token counts for this replay\n+        a_tokens = min(int(num_token_non_padded), int(tbo_split_token_index))\n+        b_tokens = max(int(num_token_non_padded) - int(tbo_split_token_index), 0)\n+        # Ensure the buffer is on default (CPU) device for CUDA graph runner usage\n+        if self._tbo_children_num_token_non_padded.device != torch.tensor(0).device:\n+            self._tbo_children_num_token_non_padded = self._tbo_children_num_token_non_padded.cpu()\n+        self._tbo_children_num_token_non_padded[0] = a_tokens\n+        self._tbo_children_num_token_non_padded[1] = b_tokens\n \n \n class TboDPAttentionPreparer:\n@@ -324,7 +360,7 @@ class TboForwardBatchPreparer:\n         # TODO improve, e.g. unify w/ `init_raw`\n         if global_server_args_dict[\"moe_dense_tp_size\"] == 1:\n             sum_len = end_token_index - start_token_index\n-            gathered_buffer = torch.zeros(\n+            gathered_buffer = torch.empty(\n                 (sum_len, batch.gathered_buffer.shape[1]),\n                 dtype=batch.gathered_buffer.dtype,\n                 device=batch.gathered_buffer.device,\n@@ -357,7 +393,11 @@ class TboForwardBatchPreparer:\n                 top_p_normalized_logprobs=False,\n                 top_p=None,\n                 mm_inputs=None,\n-                num_token_non_padded=None,\n+                num_token_non_padded=torch.tensor(\n+                    end_token_index - start_token_index,\n+                    dtype=torch.int32,\n+                    device=output_dict[\"input_ids\"].device,\n+                ),\n             )\n         )\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
