diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
new file mode 100644
index 000000000..a01e1413c
--- /dev/null
+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py
@@ -0,0 +1,81 @@
+import argparse
+import itertools
+import time
+
+import torch
+import triton
+
+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel
+
+
+def benchmark_pre_reorder(batch_size, topk, hidden_size, num_experts, block_size, iters=100):
+    device = torch.device("cuda")
+
+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=device)
+    # Use empty to avoid redundant memory zeroing
+    gateup_input_ptr = torch.empty(
+        batch_size * topk, hidden_size, dtype=torch.float16, device=device
+    )
+    src2dst_ptr = torch.arange(batch_size * topk, dtype=torch.int32, device=device).view(
+        batch_size, topk
+    )
+    topk_ids_ptr = torch.randint(0, num_experts, (batch_size, topk), dtype=torch.int32, device=device)
+
+    start_expert_id = 0
+    end_expert_id = num_experts - 1
+
+    grid = (batch_size,)
+
+    # Warmup (JIT compile)
+    pre_reorder_triton_kernel[grid](
+        input_ptr,
+        gateup_input_ptr,
+        src2dst_ptr,
+        topk_ids_ptr,
+        None,
+        start_expert_id,
+        end_expert_id,
+        topk,
+        hidden_size,
+        BLOCK_SIZE=block_size,
+    )
+    torch.cuda.synchronize()
+
+    start = time.time()
+    for _ in range(iters):
+        pre_reorder_triton_kernel[grid](
+            input_ptr,
+            gateup_input_ptr,
+            src2dst_ptr,
+            topk_ids_ptr,
+            None,
+            start_expert_id,
+            end_expert_id,
+            topk,
+            hidden_size,
+            BLOCK_SIZE=block_size,
+        )
+    torch.cuda.synchronize()
+    return (time.time() - start) / iters
+
+
+def main():
+    parser = argparse.ArgumentParser()
+    parser.add_argument("--batch", type=int, default=4096)
+    parser.add_argument("--topk", type=int, default=2)
+    parser.add_argument("--hidden", type=int, default=4096)
+    parser.add_argument("--experts", type=int, default=64)
+    parser.add_argument("--block", type=int, default=128)
+    parser.add_argument("--iters", type=int, default=50)
+    args = parser.parse_args()
+
+    ms = benchmark_pre_reorder(
+        args.batch, args.topk, args.hidden, args.experts, args.block, args.iters
+    )
+    print(
+        f"avg per-iter latency: {ms*1000:.3f} ms | batch={args.batch} hidden={args.hidden} topk={args.topk} experts={args.experts} block={args.block}"
+    )
+
+
+if __name__ == "__main__":
+    main()
diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 8c005527a..1f70b3ada 100644
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -151,12 +151,14 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):
 
 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)
 
-    compute_seg_indptr_triton_kernel[(num_experts,)](
-        reorder_topk_ids, seg_indptr, topk_ids.numel()
+    # Compute segment indptr via searchsorted to avoid a per-expert kernel
+    expert_ids = torch.arange(
+        num_experts + 1, device=topk_ids.device, dtype=reorder_topk_ids.dtype
     )
+    torch.searchsorted(reorder_topk_ids, expert_ids, right=True, out=seg_indptr)
 
     BLOCK_SIZE = 512
     grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)
@@ -679,7 +681,8 @@ def grouped_gemm_triton(
         "BLOCK_SIZE_K": 128,
     }
 
-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0  # initialize prefix sum base; kernel populates the rest
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )
