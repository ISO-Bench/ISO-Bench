diff --git a/benchmark/lora/launch_server.py b/benchmark/lora/launch_server.py
index 1fa4d7135..b9159509b 100644
--- a/benchmark/lora/launch_server.py
+++ b/benchmark/lora/launch_server.py
@@ -1,7 +1,7 @@
 import argparse
 import os
 
-NUM_LORAS = 128
+NUM_LORAS = 8
 LORA_PATH = {
     "base": "mistralai/Mistral-7B-Instruct-v0.3",
     "lora": "/home/ying/test_lora",
@@ -11,13 +11,13 @@ LORA_PATH = {
 def launch_server(args):
     base_path = LORA_PATH["base"]
     lora_path = LORA_PATH["lora"]
-    max_loras_per_batch = 4
 
     if args.base_only:
-        cmd = f"python -m sglang.launch_server --model {base_path} "
+        cmd = f"python3 -m sglang.launch_server --model {base_path} "
     else:
-        cmd = f"python -m sglang.launch_server --model {base_path} --lora-paths "
-        for i in range(NUM_LORAS):
+        cmd = f"python3 -m sglang.launch_server --model {base_path} --lora-paths "
+        num = getattr(args, "num_loras", NUM_LORAS)
+        for i in range(num):
             lora_name = f"lora{i}"
             cmd += f"{lora_name}={lora_path} "
     cmd += f"--disable-radix --disable-cuda-graph "
diff --git a/python/sglang/srt/lora/lora.py b/python/sglang/srt/lora/lora.py
index 379b233bd..783c38b73 100644
--- a/python/sglang/srt/lora/lora.py
+++ b/python/sglang/srt/lora/lora.py
@@ -120,12 +120,11 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
         )
         # FIXME
         assert lora_a_output.shape[-1] == self.lora_rank * 2
-        lora_output = torch.empty_like(base_output)
-        output_dim = lora_output.shape[-1] // 2
+        output_dim = base_output.shape[-1] // 2
         for i in range(2):
             left = output_dim * i
             right = left + output_dim
-            lora_output[:, left:right] = self.segment_gemm.run(
+            seg = self.segment_gemm.run(
                 x=lora_a_output[
                     :, self.lora_rank * i : self.lora_rank * (i + 1)
                 ].contiguous(),
@@ -135,7 +134,8 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
                 seg_lens=self.seq_lens,
                 weight_indices=self.weight_indices,
             )
-        return base_output + lora_output * self.scaling
+            base_output[:, left:right].add_(seg, alpha=self.scaling)
+        return base_output
 
 
 class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
@@ -165,10 +165,9 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
             weight_indices=self.weight_indices,
         )
         # FIXME parallelize qkv
-        lora_output = torch.empty_like(base_output)
         # q
         output_dim_q = self.B_buffer_q.shape[-2]
-        lora_output[:, :output_dim_q] = self.segment_gemm.run(
+        q_seg = self.segment_gemm.run(
             x=lora_a_output[:, : self.lora_rank].contiguous(),
             weights=self.B_buffer_q,
             batch_size=self.bs,
@@ -176,24 +175,24 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):
             seg_lens=self.seq_lens,
             weight_indices=self.weight_indices,
         )
+        base_output[:, :output_dim_q].add_(q_seg, alpha=self.scaling)
         # kv
         output_dim_kv = self.B_buffer_kv.shape[-2] // 2
         for i in range(2):
             left = output_dim_kv * i
             right = left + output_dim_kv
-            lora_output[:, output_dim_q + left : output_dim_q + right] = (
-                self.segment_gemm.run(
-                    x=lora_a_output[
-                        :, self.lora_rank * (i + 1) : self.lora_rank * (i + 2)
-                    ].contiguous(),
-                    weights=self.B_buffer_kv[:, left:right, :].contiguous(),
-                    batch_size=self.bs,
-                    weight_column_major=True,
-                    seg_lens=self.seq_lens,
-                    weight_indices=self.weight_indices,
-                )
+            kv_seg = self.segment_gemm.run(
+                x=lora_a_output[
+                    :, self.lora_rank * (i + 1) : self.lora_rank * (i + 2)
+                ].contiguous(),
+                weights=self.B_buffer_kv[:, left:right, :].contiguous(),
+                batch_size=self.bs,
+                weight_column_major=True,
+                seg_lens=self.seq_lens,
+                weight_indices=self.weight_indices,
             )
-        return base_output + lora_output * self.scaling
+            base_output[:, output_dim_q + left : output_dim_q + right].add_(kv_seg, alpha=self.scaling)
+        return base_output
 
 
 class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
@@ -227,7 +226,7 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):
             seg_lens=self.seq_lens,
             weight_indices=self.weight_indices,
         )
-        return base_output + lora_output * self.scaling
+        return base_output.add_(lora_output, alpha=self.scaling)
 
     def forward(self, input_):
         # duplicate the logic in RowParallelLinear
@@ -299,7 +298,7 @@ class LoRALayer(nn.Module):
 
     def load_to_gpu(self):
         for name, weight in self.weights.items():
-            self.weight_gpu[name] = weight.to(torch.float16).to("cuda")
+            self.weight_gpu[name] = weight.to(device="cuda", dtype=torch.float16, non_blocking=True)
 
     def offload_from_gpu(self):
         for name, weight in self.weights.items():
@@ -336,7 +335,7 @@ class LoRAAdapter(nn.Module):
 
     def load_to_gpu(self):
         for name, weight in self.weights.items():
-            self.weights_gpu[name] = weight.to(torch.float16).to("cuda")
+            self.weights_gpu[name] = weight.to(device="cuda", dtype=torch.float16, non_blocking=True)
         for layer in self.layers:
             layer.load_to_gpu()
 
diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py
index 59cd7e157..3c07879a7 100644
--- a/python/sglang/srt/lora/lora_manager.py
+++ b/python/sglang/srt/lora/lora_manager.py
@@ -138,9 +138,7 @@ class LoRAManager:
         self.origin_target_modules = set()
         for name, path in self.lora_paths.items():
             self.configs[name] = LoRAConfig(path)
-            self.origin_target_modules = set(self.origin_target_modules) | set(
-                self.configs[name].target_modules
-            )
+            self.origin_target_modules |= set(self.configs[name].target_modules)
         if hasattr(self.base_model, "get_module_name"):
             self.target_modules = {
                 self.base_model.get_module_name(module)
@@ -254,7 +252,7 @@ class LoRAManager:
         if uid is None:
             for i in range(num_layer):
                 for k in self.A_buffer.keys():
-                    self.A_buffer[k][i][buffer_id] *= 0
+                    self.A_buffer[k][i][buffer_id].zero_()
             return
 
         for i in range(num_layer):
@@ -297,9 +295,11 @@ class LoRAManager:
             if forward_batch.forward_mode.is_extend()
             else torch.ones(bs)
         )
-        weight_indices = torch.empty((bs,), dtype=torch.int64, device="cuda")
-        for i, lora_path in enumerate(forward_batch.lora_paths):
-            weight_indices[i] = self.buffer_id[lora_path]
+        weight_indices = torch.tensor(
+            [self.buffer_id[p] for p in forward_batch.lora_paths],
+            dtype=torch.int64,
+            device="cuda",
+        )
 
         for module_name, module in self.lora_modules:
             layer_id = get_layer_id(module_name)
