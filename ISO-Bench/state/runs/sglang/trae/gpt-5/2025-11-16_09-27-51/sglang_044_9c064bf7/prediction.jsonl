{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-58d1082", "model_patch": "diff --git a/benchmark/lora/launch_server.py b/benchmark/lora/launch_server.py\nindex 1fa4d7135..b9159509b 100644\n--- a/benchmark/lora/launch_server.py\n+++ b/benchmark/lora/launch_server.py\n@@ -1,7 +1,7 @@\n import argparse\n import os\n \n-NUM_LORAS = 128\n+NUM_LORAS = 8\n LORA_PATH = {\n     \"base\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n     \"lora\": \"/home/ying/test_lora\",\n@@ -11,13 +11,13 @@ LORA_PATH = {\n def launch_server(args):\n     base_path = LORA_PATH[\"base\"]\n     lora_path = LORA_PATH[\"lora\"]\n-    max_loras_per_batch = 4\n \n     if args.base_only:\n-        cmd = f\"python -m sglang.launch_server --model {base_path} \"\n+        cmd = f\"python3 -m sglang.launch_server --model {base_path} \"\n     else:\n-        cmd = f\"python -m sglang.launch_server --model {base_path} --lora-paths \"\n-        for i in range(NUM_LORAS):\n+        cmd = f\"python3 -m sglang.launch_server --model {base_path} --lora-paths \"\n+        num = getattr(args, \"num_loras\", NUM_LORAS)\n+        for i in range(num):\n             lora_name = f\"lora{i}\"\n             cmd += f\"{lora_name}={lora_path} \"\n     cmd += f\"--disable-radix --disable-cuda-graph \"\ndiff --git a/python/sglang/srt/lora/lora.py b/python/sglang/srt/lora/lora.py\nindex 379b233bd..783c38b73 100644\n--- a/python/sglang/srt/lora/lora.py\n+++ b/python/sglang/srt/lora/lora.py\n@@ -120,12 +120,11 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         )\n         # FIXME\n         assert lora_a_output.shape[-1] == self.lora_rank * 2\n-        lora_output = torch.empty_like(base_output)\n-        output_dim = lora_output.shape[-1] // 2\n+        output_dim = base_output.shape[-1] // 2\n         for i in range(2):\n             left = output_dim * i\n             right = left + output_dim\n-            lora_output[:, left:right] = self.segment_gemm.run(\n+            seg = self.segment_gemm.run(\n                 x=lora_a_output[\n                     :, self.lora_rank * i : self.lora_rank * (i + 1)\n                 ].contiguous(),\n@@ -135,7 +134,8 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n                 seg_lens=self.seq_lens,\n                 weight_indices=self.weight_indices,\n             )\n-        return base_output + lora_output * self.scaling\n+            base_output[:, left:right].add_(seg, alpha=self.scaling)\n+        return base_output\n \n \n class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n@@ -165,10 +165,9 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             weight_indices=self.weight_indices,\n         )\n         # FIXME parallelize qkv\n-        lora_output = torch.empty_like(base_output)\n         # q\n         output_dim_q = self.B_buffer_q.shape[-2]\n-        lora_output[:, :output_dim_q] = self.segment_gemm.run(\n+        q_seg = self.segment_gemm.run(\n             x=lora_a_output[:, : self.lora_rank].contiguous(),\n             weights=self.B_buffer_q,\n             batch_size=self.bs,\n@@ -176,24 +175,24 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             seg_lens=self.seq_lens,\n             weight_indices=self.weight_indices,\n         )\n+        base_output[:, :output_dim_q].add_(q_seg, alpha=self.scaling)\n         # kv\n         output_dim_kv = self.B_buffer_kv.shape[-2] // 2\n         for i in range(2):\n             left = output_dim_kv * i\n             right = left + output_dim_kv\n-            lora_output[:, output_dim_q + left : output_dim_q + right] = (\n-                self.segment_gemm.run(\n-                    x=lora_a_output[\n-                        :, self.lora_rank * (i + 1) : self.lora_rank * (i + 2)\n-                    ].contiguous(),\n-                    weights=self.B_buffer_kv[:, left:right, :].contiguous(),\n-                    batch_size=self.bs,\n-                    weight_column_major=True,\n-                    seg_lens=self.seq_lens,\n-                    weight_indices=self.weight_indices,\n-                )\n+            kv_seg = self.segment_gemm.run(\n+                x=lora_a_output[\n+                    :, self.lora_rank * (i + 1) : self.lora_rank * (i + 2)\n+                ].contiguous(),\n+                weights=self.B_buffer_kv[:, left:right, :].contiguous(),\n+                batch_size=self.bs,\n+                weight_column_major=True,\n+                seg_lens=self.seq_lens,\n+                weight_indices=self.weight_indices,\n             )\n-        return base_output + lora_output * self.scaling\n+            base_output[:, output_dim_q + left : output_dim_q + right].add_(kv_seg, alpha=self.scaling)\n+        return base_output\n \n \n class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n@@ -227,7 +226,7 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n             seg_lens=self.seq_lens,\n             weight_indices=self.weight_indices,\n         )\n-        return base_output + lora_output * self.scaling\n+        return base_output.add_(lora_output, alpha=self.scaling)\n \n     def forward(self, input_):\n         # duplicate the logic in RowParallelLinear\n@@ -299,7 +298,7 @@ class LoRALayer(nn.Module):\n \n     def load_to_gpu(self):\n         for name, weight in self.weights.items():\n-            self.weight_gpu[name] = weight.to(torch.float16).to(\"cuda\")\n+            self.weight_gpu[name] = weight.to(device=\"cuda\", dtype=torch.float16, non_blocking=True)\n \n     def offload_from_gpu(self):\n         for name, weight in self.weights.items():\n@@ -336,7 +335,7 @@ class LoRAAdapter(nn.Module):\n \n     def load_to_gpu(self):\n         for name, weight in self.weights.items():\n-            self.weights_gpu[name] = weight.to(torch.float16).to(\"cuda\")\n+            self.weights_gpu[name] = weight.to(device=\"cuda\", dtype=torch.float16, non_blocking=True)\n         for layer in self.layers:\n             layer.load_to_gpu()\n \ndiff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 59cd7e157..3c07879a7 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -138,9 +138,7 @@ class LoRAManager:\n         self.origin_target_modules = set()\n         for name, path in self.lora_paths.items():\n             self.configs[name] = LoRAConfig(path)\n-            self.origin_target_modules = set(self.origin_target_modules) | set(\n-                self.configs[name].target_modules\n-            )\n+            self.origin_target_modules |= set(self.configs[name].target_modules)\n         if hasattr(self.base_model, \"get_module_name\"):\n             self.target_modules = {\n                 self.base_model.get_module_name(module)\n@@ -254,7 +252,7 @@ class LoRAManager:\n         if uid is None:\n             for i in range(num_layer):\n                 for k in self.A_buffer.keys():\n-                    self.A_buffer[k][i][buffer_id] *= 0\n+                    self.A_buffer[k][i][buffer_id].zero_()\n             return\n \n         for i in range(num_layer):\n@@ -297,9 +295,11 @@ class LoRAManager:\n             if forward_batch.forward_mode.is_extend()\n             else torch.ones(bs)\n         )\n-        weight_indices = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n-        for i, lora_path in enumerate(forward_batch.lora_paths):\n-            weight_indices[i] = self.buffer_id[lora_path]\n+        weight_indices = torch.tensor(\n+            [self.buffer_id[p] for p in forward_batch.lora_paths],\n+            dtype=torch.int64,\n+            device=\"cuda\",\n+        )\n \n         for module_name, module in self.lora_modules:\n             layer_id = get_layer_id(module_name)\n", "model_name_or_path": "gpt-5-2025-08-07"}
