{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9282306", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py\nindex 4e4ba9a1e..8015c18a0 100644\n--- a/python/sglang/srt/models/llama4.py\n+++ b/python/sglang/srt/models/llama4.py\n@@ -48,7 +48,7 @@ from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch\n from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP\n-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers\n+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers\n \n logger = logging.getLogger(__name__)\n \n@@ -63,7 +63,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)\n+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)\n         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(\n             hidden_states.dtype\n         )\ndiff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py\nindex 19fa1807c..d88fd6d1c 100644\n--- a/python/sglang/srt/speculative/eagle_utils.py\n+++ b/python/sglang/srt/speculative/eagle_utils.py\n@@ -117,10 +117,12 @@ class EagleDraftInput:\n     ):\n         bs = self.accept_length.numel()\n \n-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        qo_indptr[0] = 0\n         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)\n \n-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        cum_kv_seq_len[0] = 0\n         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n \n         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.\n@@ -278,9 +280,10 @@ class EagleVerifyInput:\n             dtype=torch.int32,\n             device=\"cuda\",\n         )\n-        cum_kv_seq_len = torch.zeros(\n+        cum_kv_seq_len = torch.empty(\n             (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n+        cum_kv_seq_len[0] = 0\n \n         paged_kernel_lens = paged_kernel_lens + self.draft_token_num\n         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n@@ -381,7 +384,7 @@ class EagleVerifyInput:\n             )\n             target_probs = target_probs.reshape(bs, self.draft_token_num, -1)\n \n-            draft_probs = torch.zeros(\n+            draft_probs = torch.empty(\n                 target_probs.shape, dtype=torch.float32, device=\"cuda\"\n             )\n             coins = torch.rand_like(candidates, dtype=torch.float32, device=\"cuda\")\n@@ -772,16 +775,6 @@ def select_top_k_tokens(\n     return input_ids, hidden_states, scores, tree_info\n \n \n-def fast_topk(values, topk, dim):\n-    if topk == 1:\n-        # Use max along the specified dimension to get both value and index\n-        max_value, max_index = torch.max(values, dim=dim)\n-        return max_value.unsqueeze(1), max_index.unsqueeze(1)\n-    else:\n-        # Use topk for efficiency with larger k values\n-        return torch.topk(values, topk, dim=dim)\n-\n-\n def _generate_simulated_accept_index(\n     accept_index,\n     predict,\ndiff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py\nindex 514603424..27d28da8f 100644\n--- a/python/sglang/srt/speculative/eagle_worker.py\n+++ b/python/sglang/srt/speculative/eagle_worker.py\n@@ -31,11 +31,10 @@ from sglang.srt.speculative.eagle_utils import (\n     EagleVerifyInput,\n     EagleVerifyOutput,\n     assign_draft_cache_locs,\n-    fast_topk,\n     select_top_k_tokens,\n )\n from sglang.srt.speculative.spec_info import SpeculativeAlgorithm\n-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available\n+from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available, fast_topk\n \n if is_cuda_available():\n     from sgl_kernel import segment_packbits\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex b43fe4273..8d43c90e8 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -1713,6 +1713,38 @@ def next_power_of_2(n: int):\n setattr(triton, \"next_power_of_2\", next_power_of_2)\n \n \n+\n+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):\n+    \"\"\"\n+    Faster top-k for common small-k cases.\n+    - For k==1, use max which is typically faster than topk.\n+    - For k==2 on floating dtypes, compute two passes of max with a scatter mask.\n+    - Otherwise, fallback to torch.topk.\n+    Returns (values, indices) with the same shapes as torch.topk(..., k=topk, dim=dim).\n+    \"\"\"\n+    if not isinstance(topk, int) or topk <= 0:\n+        raise ValueError(\"topk must be a positive integer\")\n+\n+    if topk == 1:\n+        v, idx = torch.max(values, dim=dim)\n+        return v.unsqueeze(dim), idx.unsqueeze(dim)\n+\n+    if topk == 2 and values.dtype.is_floating_point:\n+        # First max\n+        v1, idx1 = torch.max(values, dim=dim)\n+        # Mask out the top1 positions and get the second max\n+        neg_inf = torch.finfo(values.dtype).min\n+        masked = values.clone()\n+        masked.scatter_(dim, idx1.unsqueeze(dim), neg_inf)\n+        v2, idx2 = torch.max(masked, dim=dim)\n+        vals = torch.stack((v1, v2), dim=dim)\n+        idxs = torch.stack((idx1, idx2), dim=dim)\n+        return vals, idxs\n+\n+    # General fallback\n+    return torch.topk(values, topk, dim=dim)\n+\n+\n @contextmanager\n def empty_context(*args, **kwargs):\n     try:\n", "model_name_or_path": "gpt-5-2025-08-07"}
