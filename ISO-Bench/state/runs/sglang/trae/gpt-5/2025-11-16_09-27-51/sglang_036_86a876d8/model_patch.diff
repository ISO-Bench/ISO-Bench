diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/models/llama4.py b/python/sglang/srt/models/llama4.py
index 4e4ba9a1e..8015c18a0 100644
--- a/python/sglang/srt/models/llama4.py
+++ b/python/sglang/srt/models/llama4.py
@@ -48,7 +48,7 @@ from sglang.srt.layers.vocab_parallel_embedding import VocabParallelEmbedding
 from sglang.srt.managers.schedule_batch import global_server_args_dict
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch
 from sglang.srt.models.llama import LlamaForCausalLM, LlamaMLP
-from sglang.srt.utils import add_prefix, get_compiler_backend, make_layers
+from sglang.srt.utils import add_prefix, fast_topk, get_compiler_backend, make_layers
 
 logger = logging.getLogger(__name__)
 
@@ -63,7 +63,7 @@ class Llama4MoE(nn.Module):
         topk: int,
         renormalize: bool,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        router_scores_aK, router_indices_aK = torch.topk(gating_output, topk, dim=-1)
+        router_scores_aK, router_indices_aK = fast_topk(gating_output, topk, dim=-1)
         router_scores_aK = torch.sigmoid(router_scores_aK.float()).to(
             hidden_states.dtype
         )
diff --git a/python/sglang/srt/speculative/eagle_utils.py b/python/sglang/srt/speculative/eagle_utils.py
index 19fa1807c..d88fd6d1c 100644
--- a/python/sglang/srt/speculative/eagle_utils.py
+++ b/python/sglang/srt/speculative/eagle_utils.py
@@ -117,10 +117,12 @@ class EagleDraftInput:
     ):
         bs = self.accept_length.numel()
 
-        qo_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        qo_indptr[0] = 0
         qo_indptr[1:] = torch.cumsum(self.accept_length, dim=0)
 
-        cum_kv_seq_len = torch.zeros((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len = torch.empty((bs + 1,), dtype=torch.int32, device="cuda")
+        cum_kv_seq_len[0] = 0
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)
 
         # TODO: replace cum_kv_seq_len[-1] with paged_kernel_lens_sum to avoid the device sync.
@@ -278,9 +280,10 @@ class EagleVerifyInput:
             dtype=torch.int32,
             device="cuda",
         )
-        cum_kv_seq_len = torch.zeros(
+        cum_kv_seq_len = torch.empty(
             (batch_size + 1,), dtype=torch.int32, device="cuda"
         )
+        cum_kv_seq_len[0] = 0
 
         paged_kernel_lens = paged_kernel_lens + self.draft_token_num
         cum_kv_seq_len[1:] = torch.cumsum(paged_kernel_lens, dim=0)
@@ -381,7 +384,7 @@ class EagleVerifyInput:
             )
             target_probs = target_probs.reshape(bs, self.draft_token_num, -1)
 
-            draft_probs = torch.zeros(
+            draft_probs = torch.empty(
                 target_probs.shape, dtype=torch.float32, device="cuda"
             )
             coins = torch.rand_like(candidates, dtype=torch.float32, device="cuda")
@@ -772,16 +775,6 @@ def select_top_k_tokens(
     return input_ids, hidden_states, scores, tree_info
 
 
-def fast_topk(values, topk, dim):
-    if topk == 1:
-        # Use max along the specified dimension to get both value and index
-        max_value, max_index = torch.max(values, dim=dim)
-        return max_value.unsqueeze(1), max_index.unsqueeze(1)
-    else:
-        # Use topk for efficiency with larger k values
-        return torch.topk(values, topk, dim=dim)
-
-
 def _generate_simulated_accept_index(
     accept_index,
     predict,
diff --git a/python/sglang/srt/speculative/eagle_worker.py b/python/sglang/srt/speculative/eagle_worker.py
index 514603424..27d28da8f 100644
--- a/python/sglang/srt/speculative/eagle_worker.py
+++ b/python/sglang/srt/speculative/eagle_worker.py
@@ -31,11 +31,10 @@ from sglang.srt.speculative.eagle_utils import (
     EagleVerifyInput,
     EagleVerifyOutput,
     assign_draft_cache_locs,
-    fast_topk,
     select_top_k_tokens,
 )
 from sglang.srt.speculative.spec_info import SpeculativeAlgorithm
-from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available
+from sglang.srt.utils import empty_context, get_available_gpu_memory, is_cuda_available, fast_topk
 
 if is_cuda_available():
     from sgl_kernel import segment_packbits
diff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py
index b43fe4273..8d43c90e8 100644
--- a/python/sglang/srt/utils.py
+++ b/python/sglang/srt/utils.py
@@ -1713,6 +1713,38 @@ def next_power_of_2(n: int):
 setattr(triton, "next_power_of_2", next_power_of_2)
 
 
+
+def fast_topk(values: torch.Tensor, topk: int, dim: int = -1):
+    """
+    Faster top-k for common small-k cases.
+    - For k==1, use max which is typically faster than topk.
+    - For k==2 on floating dtypes, compute two passes of max with a scatter mask.
+    - Otherwise, fallback to torch.topk.
+    Returns (values, indices) with the same shapes as torch.topk(..., k=topk, dim=dim).
+    """
+    if not isinstance(topk, int) or topk <= 0:
+        raise ValueError("topk must be a positive integer")
+
+    if topk == 1:
+        v, idx = torch.max(values, dim=dim)
+        return v.unsqueeze(dim), idx.unsqueeze(dim)
+
+    if topk == 2 and values.dtype.is_floating_point:
+        # First max
+        v1, idx1 = torch.max(values, dim=dim)
+        # Mask out the top1 positions and get the second max
+        neg_inf = torch.finfo(values.dtype).min
+        masked = values.clone()
+        masked.scatter_(dim, idx1.unsqueeze(dim), neg_inf)
+        v2, idx2 = torch.max(masked, dim=dim)
+        vals = torch.stack((v1, v2), dim=dim)
+        idxs = torch.stack((idx1, idx2), dim=dim)
+        return vals, idxs
+
+    # General fallback
+    return torch.topk(values, topk, dim=dim)
+
+
 @contextmanager
 def empty_context(*args, **kwargs):
     try:
