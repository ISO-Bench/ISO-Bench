diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py
index 62604fe56..949881728 100644
--- a/python/sglang/srt/layers/attention/flashattention_backend.py
+++ b/python/sglang/srt/layers/attention/flashattention_backend.py
@@ -720,13 +720,13 @@ class FlashAttentionBackend(AttentionBackend):
         """
         self.decode_cuda_graph_metadata = {
             # Page table for token mapping (batch_size, max_context_len)
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "page_table_draft_decode": torch.zeros(
+            "page_table_draft_decode": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
@@ -735,27 +735,27 @@ class FlashAttentionBackend(AttentionBackend):
             "strided_indices": torch.arange(
                 0, self.max_context_len, self.page_size, device=self.device
             ),
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
             "cu_seqlens_q": torch.arange(
                 0, max_bs + 128, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
+            "cu_seqlens_k": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
         }
 
         self.target_verify_metadata = {
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
-            "cu_seqlens_q": torch.zeros(
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
+            "cu_seqlens_q": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
-            "cu_seqlens_k": torch.zeros(
+            "cu_seqlens_k": torch.empty(
                 max_bs + 128, dtype=torch.int32, device=self.device
             ),
             "max_seqlen_q": 0,
