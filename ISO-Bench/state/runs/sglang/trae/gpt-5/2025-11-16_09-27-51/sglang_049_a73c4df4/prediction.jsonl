{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-89a5541", "model_patch": "diff --git a/sgl-kernel/csrc/cpu/activation.cpp b/sgl-kernel/csrc/cpu/activation.cpp\nnew file mode 100644\nindex 000000000..d5c2b7c25\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/activation.cpp\n@@ -0,0 +1,103 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t, typename func_t, typename vec_func_t>\n+void act_and_mul_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    const scalar_t* __restrict__ input,\n+    int64_t num_tokens,\n+    int64_t dim,\n+    const func_t& f,\n+    const vec_func_t& vf) {\n+  using Vec = at::vec::Vectorized<scalar_t>;\n+\n+  constexpr int64_t kVecSize = Vec::size();\n+  at::parallel_for(0, num_tokens, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t i = begin; i < end; ++i) {\n+      const scalar_t* input_ptr = input + i * 2 * dim;\n+      const scalar_t* input_other_ptr = input_ptr + dim;\n+      scalar_t* output_ptr = output + i * dim;\n+\n+      int64_t d = 0;\n+      for (; d + kVecSize <= dim; d += kVecSize) {\n+        Vec x = Vec::loadu(input_ptr + d);\n+        Vec y = Vec::loadu(input_other_ptr + d);\n+        Vec act = vf(x);\n+        (act * y).store(output_ptr + d);\n+      }\n+      for (; d < dim; ++d) {\n+        output_ptr[d] = f(input_ptr[d]) * input_other_ptr[d];\n+      }\n+    }\n+  });\n+}\n+\n+// Example activation functions\n+struct SiluFn {\n+  inline float operator()(float x) const {\n+    return x / (1.0f + std::exp(-x));\n+  }\n+};\n+\n+struct GeluTanhFn {\n+  inline float operator()(float x) const {\n+    const float k0 = 0.79788456f;  // sqrt(2/pi)\n+    const float k1 = 0.044715f;\n+    float u = k0 * (x + k1 * x * x * x);\n+    return 0.5f * x * (1.0f + std::tanh(u));\n+  }\n+};\n+\n+}  // namespace\n+\n+// Public entry points (not registered). Prefer uninitialized output to avoid\n+// touching memory unnecessarily.\n+\n+void silu_and_mul_cpu_out(\n+    at::Tensor& out,\n+    const at::Tensor& input) {\n+  TORCH_CHECK(input.dim() == 3, \"input must be [num_tokens, 2, dim]\");\n+  int64_t num_tokens = input.size(0);\n+  int64_t dim = input.size(2);\n+  out.resize_({num_tokens, dim});\n+\n+  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"silu_and_mul_cpu_out\", [&] {\n+    act_and_mul_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        input.data_ptr<scalar_t>(),\n+        num_tokens,\n+        dim,\n+        SiluFn{},\n+        [](const at::vec::Vectorized<scalar_t>& x) {\n+          // sigmoid(x) = 1 / (1 + exp(-x)), so x * sigmoid(x) = x / (1 + exp(-x))\n+          return x / (1 + (-x).exp());\n+        });\n+  });\n+}\n+\n+void gelu_tanh_and_mul_cpu_out(\n+    at::Tensor& out,\n+    const at::Tensor& input) {\n+  TORCH_CHECK(input.dim() == 3, \"input must be [num_tokens, 2, dim]\");\n+  int64_t num_tokens = input.size(0);\n+  int64_t dim = input.size(2);\n+  out.resize_({num_tokens, dim});\n+\n+  AT_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"gelu_tanh_and_mul_cpu_out\", [&] {\n+    act_and_mul_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        input.data_ptr<scalar_t>(),\n+        num_tokens,\n+        dim,\n+        GeluTanhFn{},\n+        [](const at::vec::Vectorized<scalar_t>& x) {\n+          using Vec = at::vec::Vectorized<scalar_t>;\n+          const Vec k0 = Vec((scalar_t)0.79788456f);\n+          const Vec k1 = Vec((scalar_t)0.044715f);\n+          Vec u = k0 * (x + k1 * x * x * x);\n+          return (Vec((scalar_t)0.5f) * x) * (Vec((scalar_t)1.0f) + u.tanh());\n+        });\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/bmm.cpp b/sgl-kernel/csrc/cpu/bmm.cpp\nnew file mode 100644\nindex 000000000..b9a5547ac\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/bmm.cpp\n@@ -0,0 +1,54 @@\n+#include \"common.h\"\n+\n+// Simple CPU BMM kernel using naive loops with cache-friendly blocking.\n+// Uses uninitialized output and accumulates into registers to reduce memory traffic.\n+void bmm_cpu_naive(\n+    const at::Tensor& A, // [B, M, K]\n+    const at::Tensor& B, // [B, K, N]\n+    at::Tensor& C       // [B, M, N]\n+) {\n+  TORCH_CHECK(A.dim() == 3 && B.dim() == 3, \"A and B must be 3D\");\n+  int64_t Bz = A.size(0);\n+  int64_t M = A.size(1);\n+  int64_t K = A.size(2);\n+  TORCH_CHECK(B.size(0) == Bz && B.size(1) == K, \"Shape mismatch\");\n+  int64_t N = B.size(2);\n+\n+  C.resize_({Bz, M, N});\n+\n+  AT_DISPATCH_FLOATING_TYPES(A.scalar_type(), \"bmm_cpu_naive\", [&] {\n+    const scalar_t* a_ptr = A.data_ptr<scalar_t>();\n+    const scalar_t* b_ptr = B.data_ptr<scalar_t>();\n+    scalar_t* c_ptr = C.data_ptr<scalar_t>();\n+\n+    constexpr int64_t BM = 64;\n+    constexpr int64_t BN = 64;\n+    constexpr int64_t BK = 64;\n+\n+    for (int64_t b = 0; b < Bz; ++b) {\n+      const scalar_t* Ab = a_ptr + b * M * K;\n+      const scalar_t* Bb = b_ptr + b * K * N;\n+      scalar_t* Cb = c_ptr + b * M * N;\n+\n+      for (int64_t mm = 0; mm < M; mm += BM) {\n+        int64_t mmax = std::min(mm + BM, M);\n+        for (int64_t nn = 0; nn < N; nn += BN) {\n+          int64_t nmax = std::min(nn + BN, N);\n+          // Local tile initialized lazily in registers\n+          for (int64_t kk = 0; kk < K; kk += BK) {\n+            int64_t kmax = std::min(kk + BK, K);\n+            for (int64_t m = mm; m < mmax; ++m) {\n+              for (int64_t n = nn; n < nmax; ++n) {\n+                scalar_t acc = (kk == 0) ? (scalar_t)0 : Cb[m * N + n];\n+                for (int64_t k = kk; k < kmax; ++k) {\n+                  acc += Ab[m * K + k] * Bb[k * N + n];\n+                }\n+                Cb[m * N + n] = acc;\n+              }\n+            }\n+          }\n+        }\n+      }\n+    }\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/common.h b/sgl-kernel/csrc/cpu/common.h\nnew file mode 100644\nindex 000000000..77c41420d\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/common.h\n@@ -0,0 +1,48 @@\n+/* Copyright 2025 SGLang Team. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/cpu/vec/vec.h>\n+#include <c10/util/irange.h>\n+\n+// Common helpers for CPU kernels. Keep lightweight to avoid build-time impact.\n+\n+#ifndef SGLANG_RESTRICT\n+#  if defined(__CUDACC__) || defined(__HIPCC__)\n+#    define SGLANG_RESTRICT __restrict__\n+#  elif defined(_MSC_VER)\n+#    define SGLANG_RESTRICT __restrict\n+#  else\n+#    define SGLANG_RESTRICT __restrict__\n+#  endif\n+#endif\n+\n+#ifndef CEILDIV\n+#  define CEILDIV(x, y) (((x) + (y) - 1) / (y))\n+#endif\n+\n+// Round up to a multiple of n (n power-of-two not required)\n+static inline int64_t round_up(int64_t x, int64_t n) {\n+  return CEILDIV(x, n) * n;\n+}\n+\n+// Fast zero for POD types when needed, otherwise prefer uninitialized alloc\n+// to avoid touching memory.\n+template <typename T>\n+inline void maybe_memset_zero(T* ptr, int64_t num) {\n+  std::memset(ptr, 0, sizeof(T) * static_cast<size_t>(num));\n+}\ndiff --git a/sgl-kernel/csrc/cpu/vec.h b/sgl-kernel/csrc/cpu/vec.h\nnew file mode 100644\nindex 000000000..36a530ee0\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/vec.h\n@@ -0,0 +1,35 @@\n+/* Copyright 2025 SGLang Team. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+#include <ATen/cpu/vec/vec.h>\n+\n+namespace sglang_cpu_vec {\n+\n+// Minimal helpers mirroring ATen vec interfaces we actually use here.\n+using at::vec::Vectorized;\n+\n+template <typename T>\n+inline Vectorized<T> loadu(const T* ptr) {\n+  return Vectorized<T>::loadu(ptr);\n+}\n+\n+template <typename T>\n+inline void store(T* ptr, const Vectorized<T>& v) {\n+  v.store(ptr);\n+}\n+\n+}  // namespace sglang_cpu_vec\n", "model_name_or_path": "gpt-5-2025-08-07"}
