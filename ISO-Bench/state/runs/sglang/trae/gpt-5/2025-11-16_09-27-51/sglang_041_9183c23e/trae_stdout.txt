Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e
Initialising MCP tools...
╭───────────────────────────────────────────────────────────────── Task Details ─────────────────────────────────────────────────────────────────╮
│ Task: I've uploaded a python code repository in the directory                                                                                  │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e.                                                 │
│ Consider the following test script showing an example usage of the repository:                                                                 │
│                                                                                                                                                │
│ <test_script>                                                                                                                                  │
│ # This is a performance optimization task                                                                                                      │
│ # The specific operations to optimize are in the files listed below                                                                            │
│ # Focus on performance improvements in the target functions                                                                                    │
│                                                                                                                                                │
│ </test_script>                                                                                                                                 │
│                                                                                                                                                │
│ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                       │
│                                                                                                                                                │
│ Basic guidelines:                                                                                                                              │
│ 1. Your task is to make changes to non-test files in the                                                                                       │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e directory to improve the performance of the      │
│ <test_script>.                                                                                                                                 │
│ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                      │
│ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.      │
│ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with     │
│ running them.                                                                                                                                  │
│                                                                                                                                                │
│ Follow these steps to improve performance:                                                                                                     │
│ 1. As a first step, explore the repository structure.                                                                                          │
│ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch    │
│ (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch/test_opt.py) to reproduce  │
│ and time the example, then execute it with python <filename.py> from the repo root.                                                            │
│ 3. Edit the source code of the repository to improve performance.                                                                              │
│ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                     │
│                                                                                                                                                │
│ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                  │
│                                                                                                                                                │
│ <example_optimization_diff>                                                                                                                    │
│ diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py                                                 │
│ index 13eb233bd..1aae28b00 100644                                                                                                              │
│ --- a/python/sglang/srt/managers/io_struct.py                                                                                                  │
│ +++ b/python/sglang/srt/managers/io_struct.py                                                                                                  │
│ @@ -426,8 +426,7 @@ class UpdateWeightsFromDistributedReqOutput:                                                                               │
│                                                                                                                                                │
│  @dataclass                                                                                                                                    │
│  class UpdateWeightsFromTensorReqInput:                                                                                                        │
│ -    name: str                                                                                                                                 │
│ -    tensor: torch.Tensor                                                                                                                      │
│ +    serialized_named_tensors: bytes  # indeed Dict                                                                                            │
│                                                                                                                                                │
│                                                                                                                                                │
│  @dataclass                                                                                                                                    │
│ diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py                                                 │
│ index c8e14a746..6168441d1 100644                                                                                                              │
│ --- a/python/sglang/srt/managers/tp_worker.py                                                                                                  │
│ +++ b/python/sglang/srt/managers/tp_worker.py                                                                                                  │
│ @@ -30,7 +30,7 @@ from sglang.srt.managers.schedule_batch import ModelWorkerBatch, global_server_a                                             │
│  from sglang.srt.model_executor.forward_batch_info import ForwardBatch                                                                         │
│  from sglang.srt.model_executor.model_runner import ModelRunner                                                                                │
│  from sglang.srt.server_args import ServerArgs                                                                                                 │
│ -from sglang.srt.utils import broadcast_pyobj, set_random_seed                                                                                 │
│ +from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed                                                      │
│                                                                                                                                                │
│  logger = logging.getLogger(__name__)                                                                                                          │
│                                                                                                                                                │
│ @@ -197,7 +197,7 @@ class TpModelWorker:                                                                                                       │
│                                                                                                                                                │
│      def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):                                                          │
│ </example_optimization_diff>                                                                                                                   │
│                                                                                                                                                │
│ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                          │
│ These changes have NOT been applied to your codebase yet.                                                                                      │
│ Your task is to:                                                                                                                               │
│ 1. Understand the optimization pattern shown (e.g., torch.zeros → torch.empty)                                                                 │
│ 2. Look at the CURRENT code in the target files                                                                                                │
│ 3. Find places where you can apply SIMILAR optimizations                                                                                       │
│ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                          │
│                                                                                                                                                │
│ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                             │
│ You need to IMPLEMENT similar optimizations yourself.                                                                                          │
│                                                                                                                                                │
│ HERE'S WHAT YOU NEED TO DO:                                                                                                                    │
│ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                      │
│ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                             │
│ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                 │
│ 4. These are NEW changes you're making - not already in the code                                                                               │
│                                                                                                                                                │
│ START WITH THIS COMMAND to see what needs changing:                                                                                            │
│ ```bash                                                                                                                                        │
│ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                           │
│ - Analyze the target files for performance bottlenecks                                                                                         │
│ - Look for unnecessary memory allocations or initializations                                                                                   │
│ - Consider more efficient algorithms or data structures                                                                                        │
│                                                                                                                                                │
│ Target files to optimize:                                                                                                                      │
│ - python/sglang/srt/managers/io_struct.py                                                                                                      │
│ - python/sglang/srt/managers/tp_worker.py                                                                                                      │
│ - python/sglang/srt/model_executor/model_runner.py                                                                                             │
│                                                                                                                                                │
│ IMPORTANT: You MUST make actual code changes to at least one file.                                                                             │
│ The task will fail if no files are modified.                                                                                                   │
│                                                                                                                                                │
│ ## Constraints                                                                                                                                 │
│ - No public API breakage                                                                                                                       │
│ - All TestPack checks must pass                                                                                                                │
│                                                                                                                                                │
│ ## Target Files (ONLY modify these)                                                                                                            │
│ - `python/sglang/srt/managers/io_struct.py`                                                                                                    │
│ - `python/sglang/srt/managers/tp_worker.py`                                                                                                    │
│ - `python/sglang/srt/model_executor/model_runner.py`                                                                                           │
│ - `python/sglang/srt/server.py`                                                                                                                │
│ - `python/sglang/srt/utils.py`                                                                                                                 │
│ - `test/srt/test_update_weights_from_tensor.py`                                                                                                │
│                                                                                                                                                │
│ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                              │
│ Based on the human commit analysis, focus on these areas:                                                                                      │
│ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                      │
│ - Tensor initialization strategies                                                                                                             │
│ - Kernel parameter optimization                                                                                                                │
│ - Buffer reuse and caching                                                                                                                     │
│                                                                                                                                                │
│ ### Human Developer's Approach:                                                                                                                │
│ ```                                                                                                                                            │
│ Speed up `update_weights_from_tensor` (#2695)                                                                                                  │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ ### Files Modified (statistics):                                                                                                               │
│ ```                                                                                                                                            │
│ python/sglang/srt/managers/io_struct.py          |  3 +--                                                                                      │
│  python/sglang/srt/managers/tp_worker.py          |  4 ++--                                                                                    │
│  python/sglang/srt/model_executor/model_runner.py |  8 +++----                                                                                 │
│  python/sglang/srt/server.py                      | 11 ++++++---                                                                               │
│  python/sglang/srt/utils.py                       | 17 ++++++++++++--                                                                          │
│  test/srt/test_update_weights_from_tensor.py      | 30 ++++++++++++++----------                                                                │
│  6 files changed, 48 insertions(+), 25 deletions(-)                                                                                            │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                              │
│ 1. Start editing files by iteration 3                                                                                                          │
│ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch/test_opt.py     │
│ before and after edits (do not create timing scripts outside                                                                                   │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch)                                  │
│ 3. Make at least 3 concrete optimizations                                                                                                      │
│ 4. Commit changes by iteration 8                                                                                                               │
│ 5. Use finish command by iteration 10                                                                                                          │
│                                                                                                                                                │
│ ## TASK COMPLETION COMMAND:                                                                                                                    │
│ When you have made optimizations:                                                                                                              │
│ ```bash                                                                                                                                        │
│ git add -A                                                                                                                                     │
│ git reset -q .bench_scratch || true                                                                                                            │
│ git commit -m 'Optimize MoE align sum kernels performance'                                                                                     │
│ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                          │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/model_patch.diff || true                         │
│ finish                                                                                                                                         │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                              │
│                                                                                                                                                │
│ Model Provider: openai                                                                                                                         │
│ Model: gpt-5-2025-08-07                                                                                                                        │
│ Max Steps: 120                                                                                                                                 │
│ Trajectory File: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_041_9183c23e/trajectory.json             │
│ Tools: bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                        │
│ Project_path: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e                                    │
│ Issue: I've uploaded a python code repository in the directory                                                                                 │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e.                                                 │
│ Consider the following test script showing an example usage of the repository:                                                                 │
│                                                                                                                                                │
│ <test_script>                                                                                                                                  │
│ # This is a performance optimization task                                                                                                      │
│ # The specific operations to optimize are in the files listed below                                                                            │
│ # Focus on performance improvements in the target functions                                                                                    │
│                                                                                                                                                │
│ </test_script>                                                                                                                                 │
│                                                                                                                                                │
│ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                       │
│                                                                                                                                                │
│ Basic guidelines:                                                                                                                              │
│ 1. Your task is to make changes to non-test files in the                                                                                       │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e directory to improve the performance of the      │
│ <test_script>.                                                                                                                                 │
│ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                      │
│ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.      │
│ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with     │
│ running them.                                                                                                                                  │
│                                                                                                                                                │
│ Follow these steps to improve performance:                                                                                                     │
│ 1. As a first step, explore the repository structure.                                                                                          │
│ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch    │
│ (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch/test_opt.py) to reproduce  │
│ and time the example, then execute it with python <filename.py> from the repo root.                                                            │
│ 3. Edit the source code of the repository to improve performance.                                                                              │
│ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                     │
│                                                                                                                                                │
│ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                  │
│                                                                                                                                                │
│ <example_optimization_diff>                                                                                                                    │
│ diff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py                                                 │
│ index 13eb233bd..1aae28b00 100644                                                                                                              │
│ --- a/python/sglang/srt/managers/io_struct.py                                                                                                  │
│ +++ b/python/sglang/srt/managers/io_struct.py                                                                                                  │
│ @@ -426,8 +426,7 @@ class UpdateWeightsFromDistributedReqOutput:                                                                               │
│                                                                                                                                                │
│  @dataclass                                                                                                                                    │
│  class UpdateWeightsFromTensorReqInput:                                                                                                        │
│ -    name: str                                                                                                                                 │
│ -    tensor: torch.Tensor                                                                                                                      │
│ +    serialized_named_tensors: bytes  # indeed Dict                                                                                            │
│                                                                                                                                                │
│                                                                                                                                                │
│  @dataclass                                                                                                                                    │
│ diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py                                                 │
│ index c8e14a746..6168441d1 100644                                                                                                              │
│ --- a/python/sglang/srt/managers/tp_worker.py                                                                                                  │
│ +++ b/python/sglang/srt/managers/tp_worker.py                                                                                                  │
│ @@ -30,7 +30,7 @@ from sglang.srt.managers.schedule_batch import ModelWorkerBatch, global_server_a                                             │
│  from sglang.srt.model_executor.forward_batch_info import ForwardBatch                                                                         │
│  from sglang.srt.model_executor.model_runner import ModelRunner                                                                                │
│  from sglang.srt.server_args import ServerArgs                                                                                                 │
│ -from sglang.srt.utils import broadcast_pyobj, set_random_seed                                                                                 │
│ +from sglang.srt.utils import MultiprocessingSerializer, broadcast_pyobj, set_random_seed                                                      │
│                                                                                                                                                │
│  logger = logging.getLogger(__name__)                                                                                                          │
│                                                                                                                                                │
│ @@ -197,7 +197,7 @@ class TpModelWorker:                                                                                                       │
│                                                                                                                                                │
│      def update_weights_from_tensor(self, recv_req: UpdateWeightsFromTensorReqInput):                                                          │
│ </example_optimization_diff>                                                                                                                   │
│                                                                                                                                                │
│ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                          │
│ These changes have NOT been applied to your codebase yet.                                                                                      │
│ Your task is to:                                                                                                                               │
│ 1. Understand the optimization pattern shown (e.g., torch.zeros → torch.empty)                                                                 │
│ 2. Look at the CURRENT code in the target files                                                                                                │
│ 3. Find places where you can apply SIMILAR optimizations                                                                                       │
│ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                          │
│                                                                                                                                                │
│ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                             │
│ You need to IMPLEMENT similar optimizations yourself.                                                                                          │
│                                                                                                                                                │
│ HERE'S WHAT YOU NEED TO DO:                                                                                                                    │
│ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                      │
│ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                             │
│ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                 │
│ 4. These are NEW changes you're making - not already in the code                                                                               │
│                                                                                                                                                │
│ START WITH THIS COMMAND to see what needs changing:                                                                                            │
│ ```bash                                                                                                                                        │
│ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                           │
│ - Analyze the target files for performance bottlenecks                                                                                         │
│ - Look for unnecessary memory allocations or initializations                                                                                   │
│ - Consider more efficient algorithms or data structures                                                                                        │
│                                                                                                                                                │
│ Target files to optimize:                                                                                                                      │
│ - python/sglang/srt/managers/io_struct.py                                                                                                      │
│ - python/sglang/srt/managers/tp_worker.py                                                                                                      │
│ - python/sglang/srt/model_executor/model_runner.py                                                                                             │
│                                                                                                                                                │
│ IMPORTANT: You MUST make actual code changes to at least one file.                                                                             │
│ The task will fail if no files are modified.                                                                                                   │
│                                                                                                                                                │
│ ## Constraints                                                                                                                                 │
│ - No public API breakage                                                                                                                       │
│ - All TestPack checks must pass                                                                                                                │
│                                                                                                                                                │
│ ## Target Files (ONLY modify these)                                                                                                            │
│ - `python/sglang/srt/managers/io_struct.py`                                                                                                    │
│ - `python/sglang/srt/managers/tp_worker.py`                                                                                                    │
│ - `python/sglang/srt/model_executor/model_runner.py`                                                                                           │
│ - `python/sglang/srt/server.py`                                                                                                                │
│ - `python/sglang/srt/utils.py`                                                                                                                 │
│ - `test/srt/test_update_weights_from_tensor.py`                                                                                                │
│                                                                                                                                                │
│ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                              │
│ Based on the human commit analysis, focus on these areas:                                                                                      │
│ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                      │
│ - Tensor initialization strategies                                                                                                             │
│ - Kernel parameter optimization                                                                                                                │
│ - Buffer reuse and caching                                                                                                                     │
│                                                                                                                                                │
│ ### Human Developer's Approach:                                                                                                                │
│ ```                                                                                                                                            │
│ Speed up `update_weights_from_tensor` (#2695)                                                                                                  │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ ### Files Modified (statistics):                                                                                                               │
│ ```                                                                                                                                            │
│ python/sglang/srt/managers/io_struct.py          |  3 +--                                                                                      │
│  python/sglang/srt/managers/tp_worker.py          |  4 ++--                                                                                    │
│  python/sglang/srt/model_executor/model_runner.py |  8 +++----                                                                                 │
│  python/sglang/srt/server.py                      | 11 ++++++---                                                                               │
│  python/sglang/srt/utils.py                       | 17 ++++++++++++--                                                                          │
│  test/srt/test_update_weights_from_tensor.py      | 30 ++++++++++++++----------                                                                │
│  6 files changed, 48 insertions(+), 25 deletions(-)                                                                                            │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                              │
│ 1. Start editing files by iteration 3                                                                                                          │
│ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch/test_opt.py     │
│ before and after edits (do not create timing scripts outside                                                                                   │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/.bench_scratch)                                  │
│ 3. Make at least 3 concrete optimizations                                                                                                      │
│ 4. Commit changes by iteration 8                                                                                                               │
│ 5. Use finish command by iteration 10                                                                                                          │
│                                                                                                                                                │
│ ## TASK COMPLETION COMMAND:                                                                                                                    │
│ When you have made optimizations:                                                                                                              │
│ ```bash                                                                                                                                        │
│ git add -A                                                                                                                                     │
│ git reset -q .bench_scratch || true                                                                                                            │
│ git commit -m 'Optimize MoE align sum kernels performance'                                                                                     │
│ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                          │
│ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_041_9183c23e/model_patch.diff || true                         │
│ finish                                                                                                                                         │
│ ```                                                                                                                                            │
│                                                                                                                                                │
│ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                              │
│                                                                                                                                                │
│ Must_patch: true                                                                                                                               │
│ Patch_path: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_041_9183c23e/model_patch.diff                 │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 1: Completed                                                                                   │
│ LLM         │ 💬 I'll start by exploring the repository structure and searching for target optimization patterns.    │
│ Response    │ Then I will create a timing script in .bench_scratch to benchmark before and after changes, and        │
│             │ proceed to implement performance optimizations in the specified files.                                 │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "ls -la                            │ total 104                                       │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ drwxrwxr-x 14 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │ | sed -n '1,200p'", 'restart': False}          │ .                                               │   │
│             │ │                                                │ drwxrwxr-x 82 ubuntu ubuntu  4096 Nov 15 08:02  │   │
│             │ │                                                │ ..                                              │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu   341 Nov 15 06:01  │   │
│             │ │                                                │ .editorconfig                                   │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu   113 Nov 15 06:01  │   │
│             │ │                                                │ .git                                            │   │
│             │ │                                                │ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ .github                                         │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu  3629 Nov 15 06:01  │   │
│             │ │                                                │ .gitignore                                      │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu   123 Nov 15 06:01  │   │
│             │ │                                                │ .gitmodules                                     │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu    50 Nov 15 06:01  │   │
│             │ │                                                │ .isort.cfg                                      │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu   974 Nov 15 06:01  │   │
│             │ │                                                │ .pre-commit-config.yaml                         │   │
│             │ │                                                │ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ 3rdparty                                        │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu 11346 Nov 15 06:01  │   │
│             │ │                                                │ LICENSE                                         │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu  1720 Nov 15 06:01  │   │
│             │ │                                                │ Makefile                                        │   │
│             │ │                                                │ -rw-rw-r--  1 ubuntu ubuntu  5283 Nov 15 06:01  │   │
│             │ │                                                │ README.md                                       │   │
│             │ │                                                │ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ assets                                          │   │
│             │ │                                                │ drwxrwxr-x 28 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ benchmark                                       │   │
│             │ │                                                │ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ docker                                          │   │
│             │ │                                                │ drwxrwxr-x  9 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ docs                                            │   │
│             │ │                                                │ drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ examples                                        │   │
│             │ │                                                │ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ python                                          │   │
│             │ │                                                │ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ scripts                                         │   │
│             │ │                                                │ drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ sgl-kernel                                      │   │
│             │ │                                                │ drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ sgl-router                                      │   │
│             │ │                                                │ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 06:01  │   │
│             │ │                                                │ test                                            │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 8271 Output: 635                                                                                │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 2: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """                                     │   │
│             │ │                                                │     15  The definition of objects transfered    │   │
│             │ │                                                │ between different                               │   │
│             │ │                                                │     16  processes (TokenizerManager,            │   │
│             │ │                                                │ DetokenizerManager, Controller).                │   │
│             │ │                                                │     17  """                                     │   │
│             │ │                                                │     18                                          │   │
│             │ │                                                │     19  import uuid                             │   │
│             │ │                                                │     20  from dataclasses import dataclass       │   │
│             │ │                                                │     21  from enum import Enum                   │   │
│             │ │                                                │     22  from typing import Dict, List,          │   │
│             │ │                                                │ Optional, Tuple, Union                          │   │
│             │ │                                                │     23                                          │   │
│             │ │                                                │     24  import torch                            │   │
│             │ │                                                │     25                                          │   │
│             │ │                                                │     26  from sglang.srt.managers.schedule_batch │   │
│             │ │                                                │ import BaseFinishReason                         │   │
│             │ │                                                │     27  from                                    │   │
│             │ │                                                │ sglang.srt.sampling.sampling_params import      │   │
│             │ │                                                │ SamplingParams                                  │   │
│             │ │                                                │     28                                          │   │
│             │ │                                                │     29                                          │   │
│             │ │                                                │     30  @dataclass                              │   │
│             │ │                                                │     31  class SessionParams:                    │   │
│             │ │                                                │     32      id: Optional = None                 │   │
│             │ │                                                │     33      rid: Optional = None                │   │
│             │ │                                                │     34      offset: Optional = None             │   │
│             │ │                                                │     35      replace: Optional = None            │   │
│             │ │                                                │     36                                          │   │
│             │ │                                                │     37                                          │   │
│             │ │                                                │     38  @dataclass                              │   │
│             │ │                                                │     39  class GenerateReqInput:                 │   │
│             │ │                                                │     40      # The input prompt. It can be a     │   │
│             │ │                                                │ single prompt or a batch of prompts.            │   │
│             │ │                                                │     41      text: Optional[Union[List, str]] =  │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     42      # The token ids for text; one can   │   │
│             │ │                                                │ specify either text or input_ids                │   │
│             │ │                                                │     43      input_ids:                          │   │
│             │ │                                                │ Optional[Union[List[List], List]] = None        │   │
│             │ │                                                │     44      # The embeddings for input_ids; one │   │
│             │ │                                                │ can specify either text or input_ids or         │   │
│             │ │                                                │ input_embeds.                                   │   │
│             │ │                                                │     45      input_embeds:                       │   │
│             │ │                                                │ Optional[Union[List[List[List]], List[List]]] = │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     46      # The image input. It can be a file │   │
│             │ │                                                │ name, a url, or base64 encoded string.          │   │
│             │ │                                                │     47      # See also                          │   │
│             │ │                                                │ python/sglang/srt/utils.py:load_image.          │   │
│             │ │                                                │     48      image_data: Optional[Union[List,    │   │
│             │ │                                                │ str]] = None                                    │   │
│             │ │                                                │     49      # The sampling_params. See          │   │
│             │ │                                                │ descriptions below.                             │   │
│             │ │                                                │     50      sampling_params:                    │   │
│             │ │                                                │ Optional[Union[List[Dict], Dict]] = None        │   │
│             │ │                                                │     51      # The request id.                   │   │
│             │ │                                                │     52      rid: Optional[Union[List, str]] =   │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     53      # Whether to return logprobs.       │   │
│             │ │                                                │     54      return_logprob:                     │   │
│             │ │                                                │ Optional[Union[List, bool]] = None              │   │
│             │ │                                                │     55      # If return logprobs, the start     │   │
│             │ │                                                │ location in the prompt for returning logprobs.  │   │
│             │ │                                                │     56      # By default, this value is "-1",   │   │
│             │ │                                                │ which means it will only return logprobs for    │   │
│             │ │                                                │ output tokens.                                  │   │
│             │ │                                                │     57      logprob_start_len:                  │   │
│             │ │                                                │ Optional[Union[List, int]] = None               │   │
│             │ │                                                │     58      # If return logprobs, the number of │   │
│             │ │                                                │ top logprobs to return at each position.        │   │
│             │ │                                                │     59      top_logprobs_num:                   │   │
│             │ │                                                │ Optional[Union[List, int]] = None               │   │
│             │ │                                                │     60      # Whether to detokenize tokens in   │   │
│             │ │                                                │ text in the returned logprobs.                  │   │
│             │ │                                                │     61      return_text_in_logprobs: bool =     │   │
│             │ │                                                │ False                                           │   │
│             │ │                                                │     62      # Whether to stream output.         │   │
│             │ │                                                │     63      stream: bool = False                │   │
│             │ │                                                │     64      # The modalities of the image data  │   │
│             │ │                                                │     65      modalities: Optional[List] = None   │   │
│             │ │                                                │     66      # LoRA related                      │   │
│             │ │                                                │     67      lora_path:                          │   │
│             │ │                                                │ Optional[Union[List[Optional], Optional]] =     │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     68                                          │   │
│             │ │                                                │     69      # Session info for continual        │   │
│             │ │                                                │ prompting                                       │   │
│             │ │                                                │     70      session_params:                     │   │
│             │ │                                                │ Optional[Union[List[Dict], Dict]] = None        │   │
│             │ │                                                │     71                                          │   │
│             │ │                                                │     72      def                                 │   │
│             │ │                                                │ normalize_batch_and_arguments(self):            │   │
│             │ │                                                │     73          if (                            │   │
│             │ │                                                │     74              self.text is None and       │   │
│             │ │                                                │ self.input_ids is None and self.input_embeds is │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     75          ) or (                          │   │
│             │ │                                                │     76              self.text is not None       │   │
│             │ │                                                │     77              and self.input_ids is not   │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     78              and self.input_embeds is    │   │
│             │ │                                                │ not None                                        │   │
│             │ │                                                │     79          ):                              │   │
│             │ │                                                │     80              raise ValueError(           │   │
│             │ │                                                │     81                  "Either text, input_ids │   │
│             │ │                                                │ or input_embeds should be provided."            │   │
│             │ │                                                │     82              )                           │   │
│             │ │                                                │     83                                          │   │
│             │ │                                                │     84          # Derive the batch size         │   │
│             │ │                                                │     85          if self.text is not None:       │   │
│             │ │                                                │     86              if isinstance(self.text,    │   │
│             │ │                                                │ str):                                           │   │
│             │ │                                                │     87                  self.is_single = True   │   │
│             │ │                                                │     88                  self.batch_size = 1     │   │
│             │ │                                                │     89              else:                       │   │
│             │ │                                                │     90                  self.is_single = False  │   │
│             │ │                                                │     91                  self.batch_size =       │   │
│             │ │                                                │ len(self.text)                                  │   │
│             │ │                                                │     92              self.input_embeds = None    │   │
│             │ │                                                │     93          elif self.input_ids is not      │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │     94              if                          │   │
│             │ │                                                │ isinstance(self.input_ids[0], int):             │   │
│             │ │                                                │     95                  self.is_single = True   │   │
│             │ │                                                │     96                  self.batch_size = 1     │   │
│             │ │                                                │     97              else:                       │   │
│             │ │                                                │     98                  self.is_single = False  │   │
│             │ │                                                │     99                  self.batch_size =       │   │
│             │ │                                                │ len(self.input_ids)                             │   │
│             │ │                                                │    100              self.input_embeds = None    │   │
│             │ │                                                │    101          else:                           │   │
│             │ │                                                │    102              if                          │   │
│             │ │                                                │ isinstance(self.input_embeds[0][0], float):     │   │
│             │ │                                                │    103                  self.is_single = True   │   │
│             │ │                                                │    104                  self.batch_size = 1     │   │
│             │ │                                                │    105              else:                       │   │
│             │ │                                                │    106                  self.batch_size =       │   │
│             │ │                                                │ len(self.input_embeds)                          │   │
│             │ │                                                │    107                                          │   │
│             │ │                                                │    108          # Handle parallel sampling      │   │
│             │ │                                                │    109          # When parallel sampling is     │   │
│             │ │                                                │ used, we always treat the input as a batch.     │   │
│             │ │                                                │    110          if self.sampling_params is      │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    111              self.parallel_sample_num =  │   │
│             │ │                                                │ 1                                               │   │
│             │ │                                                │    112          elif                            │   │
│             │ │                                                │ isinstance(self.sampling_params, dict):         │   │
│             │ │                                                │    113              self.parallel_sample_num =  │   │
│             │ │                                                │ self.sampling_params.get("n", 1)                │   │
│             │ │                                                │    114          else:  #                        │   │
│             │ │                                                │ isinstance(self.sampling_params, list):         │   │
│             │ │                                                │    115              self.parallel_sample_num =  │   │
│             │ │                                                │ self.sampling_params[0].get("n", 1)             │   │
│             │ │                                                │    116              assert all(                 │   │
│             │ │                                                │    117                                          │   │
│             │ │                                                │ self.parallel_sample_num ==                     │   │
│             │ │                                                │ sampling_params.get("n", 1)                     │   │
│             │ │                                                │    118                  for sampling_params in  │   │
│             │ │                                                │ self.sampling_params                            │   │
│             │ │                                                │    119              ), "The parallel_sample_num │   │
│             │ │                                                │ should be the same for all samples in sample    │   │
│             │ │                                                │ params."                                        │   │
│             │ │                                                │    120                                          │   │
│             │ │                                                │    121          if self.parallel_sample_num > 1 │   │
│             │ │                                                │ and self.is_single:                             │   │
│             │ │                                                │    122              self.is_single = False      │   │
│             │ │                                                │    123              if self.text is not None:   │   │
│             │ │                                                │    124                  self.text =             │   │
│             │ │                                                │    125              if self.input_ids is not    │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    126                  self.input_ids =        │   │
│             │ │                                                │    127                                          │   │
│             │ │                                                │    128          # Fill in default arguments     │   │
│             │ │                                                │    129          if self.is_single:              │   │
│             │ │                                                │    130              if self.sampling_params is  │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    131                  self.sampling_params =  │   │
│             │ │                                                │ {}                                              │   │
│             │ │                                                │    132              if self.rid is None:        │   │
│             │ │                                                │    133                  self.rid =              │   │
│             │ │                                                │ uuid.uuid4().hex                                │   │
│             │ │                                                │    134              if self.return_logprob is   │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    135                  self.return_logprob =   │   │
│             │ │                                                │ False                                           │   │
│             │ │                                                │    136              if self.logprob_start_len   │   │
│             │ │                                                │ is None:                                        │   │
│             │ │                                                │    137                  self.logprob_start_len  │   │
│             │ │                                                │ = -1                                            │   │
│             │ │                                                │    138              if self.top_logprobs_num is │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    139                  self.top_logprobs_num = │   │
│             │ │                                                │ 0                                               │   │
│             │ │                                                │    140          else:                           │   │
│             │ │                                                │    141              if self.parallel_sample_num │   │
│             │ │                                                │ == 1:                                           │   │
│             │ │                                                │    142                  num = self.batch_size   │   │
│             │ │                                                │    143              else:                       │   │
│             │ │                                                │    144                  # Expand                │   │
│             │ │                                                │ parallel_sample_num                             │   │
│             │ │                                                │    145                  num = self.batch_size * │   │
│             │ │                                                │ self.parallel_sample_num                        │   │
│             │ │                                                │    146                                          │   │
│             │ │                                                │    147              if self.image_data is None: │   │
│             │ │                                                │    148                  self.image_data =       │   │
│             │ │                                                │ [None] * num                                    │   │
│             │ │                                                │    149              elif not                    │   │
│             │ │                                                │ isinstance(self.image_data, list):              │   │
│             │ │                                                │    150                  self.image_data =  *    │   │
│             │ │                                                │ num                                             │   │
│             │ │                                                │    151              elif                        │   │
│             │ │                                                │ isinstance(self.image_data, list):              │   │
│             │ │                                                │    152                  pass                    │   │
│             │ │                                                │    153                                          │   │
│             │ │                                                │    154              if self.sampling_params is  │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    155                  self.sampling_params =  │   │
│             │ │                                                │ [{}] * num                                      │   │
│             │ │                                                │    156              elif not                    │   │
│             │ │                                                │ isinstance(self.sampling_params, list):         │   │
│             │ │                                                │    157                  self.sampling_params =  │   │
│             │ │                                                │ * num                                           │   │
│             │ │                                                │    158                                          │   │
│             │ │                                                │    159              if self.rid is None:        │   │
│             │ │                                                │    160                  self.rid =              │   │
│             │ │                                                │    161              else:                       │   │
│             │ │                                                │    162                  assert                  │   │
│             │ │                                                │ isinstance(self.rid, list), "The rid should be  │   │
│             │ │                                                │ a list."                                        │   │
│             │ │                                                │    163                                          │   │
│             │ │                                                │    164              if self.return_logprob is   │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    165                  self.return_logprob =   │   │
│             │ │                                                │ [False] * num                                   │   │
│             │ │                                                │    166              elif not                    │   │
│             │ │                                                │ isinstance(self.return_logprob, list):          │   │
│             │ │                                                │    167                  self.return_logprob =   │   │
│             │ │                                                │ * num                                           │   │
│             │ │                                                │    168              else:                       │   │
│             │ │                                                │    169                  assert                  │   │
│             │ │                                                │ self.parallel_sample_num == 1                   │   │
│             │ │                                                │    170                                          │   │
│             │ │                                                │    171              if self.logprob_start_len   │   │
│             │ │                                                │ is None:                                        │   │
│             │ │                                                │    172                  self.logprob_start_len  │   │
│             │ │                                                │ = [-1] * num                                    │   │
│             │ │                                                │    173              elif not                    │   │
│             │ │                                                │ isinstance(self.logprob_start_len, list):       │   │
│             │ │                                                │    174                  self.logprob_start_len  │   │
│             │ │                                                │ =  * num                                        │   │
│             │ │                                                │    175              else:                       │   │
│             │ │                                                │    176                  assert                  │   │
│             │ │                                                │ self.parallel_sample_num == 1                   │   │
│             │ │                                                │    177                                          │   │
│             │ │                                                │    178              if self.top_logprobs_num is │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    179                  self.top_logprobs_num = │   │
│             │ │                                                │ [0] * num                                       │   │
│             │ │                                                │    180              elif not                    │   │
│             │ │                                                │ isinstance(self.top_logprobs_num, list):        │   │
│             │ │                                                │    181                  self.top_logprobs_num = │   │
│             │ │                                                │ * num                                           │   │
│             │ │                                                │    182              else:                       │   │
│             │ │                                                │    183                  assert                  │   │
│             │ │                                                │ self.parallel_sample_num == 1                   │   │
│             │ │                                                │    184                                          │   │
│             │ │                                                │    185      def regenerate_rid(self):           │   │
│             │ │                                                │    186          self.rid = uuid.uuid4().hex     │   │
│             │ │                                                │    187          return self.rid                 │   │
│             │ │                                                │    188                                          │   │
│             │ │                                                │    189      def __getitem__(self, i):           │   │
│             │ │                                                │    190          return GenerateReqInput(        │   │
│             │ │                                                │    191              text=self.text if self.text │   │
│             │ │                                                │ is not None else None,                          │   │
│             │ │                                                │    192              input_ids=self.input_ids if │   │
│             │ │                                                │ self.input_ids is not None else None,           │   │
│             │ │                                                │    193              image_data=self.image_data, │   │
│             │ │                                                │    194                                          │   │
│             │ │                                                │ sampling_params=self.sampling_params,           │   │
│             │ │                                                │    195              rid=self.rid,               │   │
│             │ │                                                │    196                                          │   │
│             │ │                                                │ return_logprob=self.return_logprob,             │   │
│             │ │                                                │    197                                          │   │
│             │ │                                                │ logprob_start_len=self.logprob_start_len,       │   │
│             │ │                                                │    198                                          │   │
│             │ │                                                │ top_logprobs_num=self.top_logprobs_num,         │   │
│             │ │                                                │    199                                          │   │
│             │ │                                                │ return_text_in_logprobs=self.return_text_in_lo… │   │
│             │ │                                                │    200              stream=self.stream,         │   │
│             │ │                                                │    201              modalities=self.modalities  │   │
│             │ │                                                │ if self.modalities else None,                   │   │
│             │ │                                                │    202              lora_path=self.lora_path if │   │
│             │ │                                                │ self.lora_path is not None else None,           │   │
│             │ │                                                │    203          )                               │   │
│             │ │                                                │    204                                          │   │
│             │ │                                                │    205                                          │   │
│             │ │                                                │    206  @dataclass                              │   │
│             │ │                                                │    207  class TokenizedGenerateReqInput:        │   │
│             │ │                                                │    208      # The request id                    │   │
│             │ │                                                │    209      rid: str                            │   │
│             │ │                                                │    210      # The input text                    │   │
│             │ │                                                │    211      input_text: str                     │   │
│             │ │                                                │    212      # The input token ids               │   │
│             │ │                                                │    213      input_ids: List                     │   │
│             │ │                                                │    214      # The image inputs                  │   │
│             │ │                                                │    215      image_inputs: dict                  │   │
│             │ │                                                │    216      # The sampling parameters           │   │
│             │ │                                                │    217      sampling_params: SamplingParams     │   │
│             │ │                                                │    218      # Whether to return the logprobs    │   │
│             │ │                                                │    219      return_logprob: bool                │   │
│             │ │                                                │    220      # If return logprobs, the start     │   │
│             │ │                                                │ location in the prompt for returning logprobs.  │   │
│             │ │                                                │    221      logprob_start_len: int              │   │
│             │ │                                                │    222      # If return logprobs, the number of │   │
│             │ │                                                │ top logprobs to return at each position.        │   │
│             │ │                                                │    223      top_logprobs_num: int               │   │
│             │ │                                                │    224      # Whether to stream output          │   │
│             │ │                                                │    225      stream: bool                        │   │
│             │ │                                                │    226                                          │   │
│             │ │                                                │    227      # LoRA related                      │   │
│             │ │                                                │    228      lora_path: Optional = None  # None  │   │
│             │ │                                                │ means just use the base model                   │   │
│             │ │                                                │    229      # The input embeds                  │   │
│             │ │                                                │    230      input_embeds:                       │   │
│             │ │                                                │ Optional[Union[List[List[List]], List[List]]] = │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    231                                          │   │
│             │ │                                                │    232      # Session info for continual        │   │
│             │ │                                                │ prompting                                       │   │
│             │ │                                                │    233      session_params:                     │   │
│             │ │                                                │ Optional[SessionParams] = None                  │   │
│             │ │                                                │    234                                          │   │
│             │ │                                                │    235                                          │   │
│             │ │                                                │    236  @dataclass                              │   │
│             │ │                                                │    237  class EmbeddingReqInput:                │   │
│             │ │                                                │    238      # The input prompt. It can be a     │   │
│             │ │                                                │ single prompt or a batch of prompts.            │   │
│             │ │                                                │    239      text: Optional[Union[List, str]] =  │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    240      # The token ids for text; one can   │   │
│             │ │                                                │ either specify text or input_ids.               │   │
│             │ │                                                │    241      input_ids:                          │   │
│             │ │                                                │ Optional[Union[List[List], List]] = None        │   │
│             │ │                                                │    242      # The request id.                   │   │
│             │ │                                                │    243      rid: Optional[Union[List, str]] =   │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    244      # Dummy sampling params for         │   │
│             │ │                                                │ compatibility                                   │   │
│             │ │                                                │    245      sampling_params: Union[List[Dict],  │   │
│             │ │                                                │ Dict] = None                                    │   │
│             │ │                                                │    246      # Dummy input embeds for            │   │
│             │ │                                                │ compatibility                                   │   │
│             │ │                                                │    247      input_embeds:                       │   │
│             │ │                                                │ Optional[Union[List[List[List]], List[List]]] = │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    248                                          │   │
│             │ │                                                │    249      def                                 │   │
│             │ │                                                │ normalize_batch_and_arguments(self):            │   │
│             │ │                                                │    250          if (self.text is None and       │   │
│             │ │                                                │ self.input_ids is None) or (                    │   │
│             │ │                                                │    251              self.text is not None and   │   │
│             │ │                                                │ self.input_ids is not None                      │   │
│             │ │                                                │    252          ):                              │   │
│             │ │                                                │    253              raise ValueError("Either    │   │
│             │ │                                                │ text or input_ids should be provided.")         │   │
│             │ │                                                │    254                                          │   │
│             │ │                                                │    255          # Derive the batch size         │   │
│             │ │                                                │    256          if self.text is not None:       │   │
│             │ │                                                │    257              if isinstance(self.text,    │   │
│             │ │                                                │ str):                                           │   │
│             │ │                                                │    258                  self.is_single = True   │   │
│             │ │                                                │    259                  self.batch_size = 1     │   │
│             │ │                                                │    260              else:                       │   │
│             │ │                                                │    261                  self.is_single = False  │   │
│             │ │                                                │    262                  self.batch_size =       │   │
│             │ │                                                │ len(self.text)                                  │   │
│             │ │                                                │    263          else:                           │   │
│             │ │                                                │    264              if                          │   │
│             │ │                                                │ isinstance(self.input_ids[0], int):             │   │
│             │ │                                                │    265                  self.is_single = True   │   │
│             │ │                                                │    266                  self.batch_size = 1     │   │
│             │ │                                                │    267              else:                       │   │
│             │ │                                                │    268                  self.is_single = False  │   │
│             │ │                                                │    269                  self.batch_size =       │   │
│             │ │                                                │ len(self.input_ids)                             │   │
│             │ │                                                │    270                                          │   │
│             │ │                                                │    271          # Fill in default arguments     │   │
│             │ │                                                │    272          if self.is_single:              │   │
│             │ │                                                │    273              if self.rid is None:        │   │
│             │ │                                                │    274                  self.rid =              │   │
│             │ │                                                │ uuid.uuid4().hex                                │   │
│             │ │                                                │    275              if self.sampling_params is  │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    276                  self.sampling_params =  │   │
│             │ │                                                │ {}                                              │   │
│             │ │                                                │    277                                          │   │
│             │ │                                                │ self.sampling_params["max_new_tokens"] = 0      │   │
│             │ │                                                │    278          else:                           │   │
│             │ │                                                │    279              if self.rid is None:        │   │
│             │ │                                                │    280                  self.rid =              │   │
│             │ │                                                │    281              else:                       │   │
│             │ │                                                │    282                  assert                  │   │
│             │ │                                                │ isinstance(self.rid, list), "The rid should be  │   │
│             │ │                                                │ a list."                                        │   │
│             │ │                                                │    283                                          │   │
│             │ │                                                │    284              if self.sampling_params is  │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    285                  self.sampling_params =  │   │
│             │ │                                                │ [{}] * self.batch_size                          │   │
│             │ │                                                │    286              for i in                    │   │
│             │ │                                                │ range(self.batch_size):                         │   │
│             │ │                                                │    287                                          │   │
│             │ │                                                │ self.sampling_params["max_new_tokens"] = 0      │   │
│             │ │                                                │    288                                          │   │
│             │ │                                                │    289      def regenerate_rid(self):           │   │
│             │ │                                                │    290          self.rid = uuid.uuid4().hex     │   │
│             │ │                                                │    291          return self.rid                 │   │
│             │ │                                                │    292                                          │   │
│             │ │                                                │    293      def __getitem__(self, i):           │   │
│             │ │                                                │    294          return EmbeddingReqInput(       │   │
│             │ │                                                │    295              text=self.text if self.text │   │
│             │ │                                                │ is not None else None,                          │   │
│             │ │                                                │    296              input_ids=self.input_ids if │   │
│             │ │                                                │ self.input_ids is not None else None,           │   │
│             │ │                                                │    297                                          │   │
│             │ │                                                │ sampling_params=self.sampling_params,           │   │
│             │ │                                                │    298              rid=self.rid,               │   │
│             │ │                                                │    299          )                               │   │
│             │ │                                                │    300                                          │   │
│             │ │                                                │    301                                          │   │
│             │ │                                                │    302  @dataclass                              │   │
│             │ │                                                │    303  class TokenizedEmbeddingReqInput:       │   │
│             │ │                                                │    304      # The request id                    │   │
│             │ │                                                │    305      rid: str                            │   │
│             │ │                                                │    306      # The input text                    │   │
│             │ │                                                │    307      input_text: str                     │   │
│             │ │                                                │    308      # The input token ids               │   │
│             │ │                                                │    309      input_ids: List                     │   │
│             │ │                                                │    310      # Dummy sampling params for         │   │
│             │ │                                                │ compatibility                                   │   │
│             │ │                                                │    311      sampling_params: SamplingParams     │   │
│             │ │                                                │    312                                          │   │
│             │ │                                                │    313                                          │   │
│             │ │                                                │    314  @dataclass                              │   │
│             │ │                                                │    315  class BatchTokenIDOut:                  │   │
│             │ │                                                │    316      # The request id                    │   │
│             │ │                                                │    317      rids: List                          │   │
│             │ │                                                │    318      # The finish reason                 │   │
│             │ │                                                │    319      finished_reasons:                   │   │
│             │ │                                                │ List[BaseFinishReason]                          │   │
│             │ │                                                │    320      # For incremental decoding          │   │
│             │ │                                                │    321      # The version id to sync decode     │   │
│             │ │                                                │ status with in detokenizer_manager              │   │
│             │ │                                                │    322      vids: List                          │   │
│             │ │                                                │    323      decoded_texts: List                 │   │
│             │ │                                                │    324      decode_ids: List                    │   │
│             │ │                                                │    325      read_offsets: List                  │   │
│             │ │                                                │    326      # Only used when                    │   │
│             │ │                                                │ --return-token-ids` is set                      │   │
│             │ │                                                │    327      origin_input_ids: Optional[List]    │   │
│             │ │                                                │    328      # Only used when                    │   │
│             │ │                                                │ `--skip-tokenizer-init` or `--return-token-ids` │   │
│             │ │                                                │ is set                                          │   │
│             │ │                                                │    329      output_ids: Optional[List]          │   │
│             │ │                                                │    330      # Detokenization configs            │   │
│             │ │                                                │    331      skip_special_tokens: List           │   │
│             │ │                                                │    332      spaces_between_special_tokens: List │   │
│             │ │                                                │    333      no_stop_trim: List                  │   │
│             │ │                                                │    334      # Token counts                      │   │
│             │ │                                                │    335      prompt_tokens: List                 │   │
│             │ │                                                │    336      completion_tokens: List             │   │
│             │ │                                                │    337      cached_tokens: List                 │   │
│             │ │                                                │    338      # Logprobs                          │   │
│             │ │                                                │    339      input_token_logprobs_val: List      │   │
│             │ │                                                │    340      input_token_logprobs_idx: List      │   │
│             │ │                                                │    341      output_token_logprobs_val: List     │   │
│             │ │                                                │    342      output_token_logprobs_idx: List     │   │
│             │ │                                                │    343      input_top_logprobs_val: List[List]  │   │
│             │ │                                                │    344      input_top_logprobs_idx: List[List]  │   │
│             │ │                                                │    345      output_top_logprobs_val: List[List] │   │
│             │ │                                                │    346      output_top_logprobs_idx: List[List] │   │
│             │ │                                                │    347      normalized_prompt_logprob: List     │   │
│             │ │                                                │    348                                          │   │
│             │ │                                                │    349                                          │   │
│             │ │                                                │    350  @dataclass                              │   │
│             │ │                                                │    351  class BatchStrOut:                      │   │
│             │ │                                                │    352      # The request id                    │   │
│             │ │                                                │    353      rids: List                          │   │
│             │ │                                                │    354      # The finish reason                 │   │
│             │ │                                                │    355      finished_reasons: List              │   │
│             │ │                                                │    356      # The output decoded strings        │   │
│             │ │                                                │    357      output_strs: List                   │   │
│             │ │                                                │    358                                          │   │
│             │ │                                                │    359      # The token ids                     │   │
│             │ │                                                │    360      origin_input_ids: Optional[List]    │   │
│             │ │                                                │    361      output_ids: Optional[List]          │   │
│             │ │                                                │    362                                          │   │
│             │ │                                                │    363      # Token counts                      │   │
│             │ │                                                │    364      # real input and output tokens can  │   │
│             │ │                                                │ be get from                                     │   │
│             │ │                                                │    365      # origin_input_ids and output_ids   │   │
│             │ │                                                │ by enabling --return_token_ids                  │   │
│             │ │                                                │    366      # TODO (Shuai): Rename this to      │   │
│             │ │                                                │ clarify the meaning.                            │   │
│             │ │                                                │    367      prompt_tokens: List                 │   │
│             │ │                                                │    368      completion_tokens: List             │   │
│             │ │                                                │    369      cached_tokens: List                 │   │
│             │ │                                                │    370                                          │   │
│             │ │                                                │    371      # Logprobs                          │   │
│             │ │                                                │    372      input_token_logprobs_val: List      │   │
│             │ │                                                │    373      input_token_logprobs_idx: List      │   │
│             │ │                                                │    374      output_token_logprobs_val: List     │   │
│             │ │                                                │    375      output_token_logprobs_idx: List     │   │
│             │ │                                                │    376      input_top_logprobs_val: List[List]  │   │
│             │ │                                                │    377      input_top_logprobs_idx: List[List]  │   │
│             │ │                                                │    378      output_top_logprobs_val: List[List] │   │
│             │ │                                                │    379      output_top_logprobs_idx: List[List] │   │
│             │ │                                                │    380      normalized_prompt_logprob: List     │   │
│             │ │                                                │    381                                          │   │
│             │ │                                                │    382                                          │   │
│             │ │                                                │    383  @dataclass                              │   │
│             │ │                                                │    384  class BatchEmbeddingOut:                │   │
│             │ │                                                │    385      # The request id                    │   │
│             │ │                                                │    386      rids: List                          │   │
│             │ │                                                │    387      # The finish reason                 │   │
│             │ │                                                │    388      finished_reasons:                   │   │
│             │ │                                                │ List[BaseFinishReason]                          │   │
│             │ │                                                │    389      # The output embedding              │   │
│             │ │                                                │    390      embeddings: List[List]              │   │
│             │ │                                                │    391      # Token counts                      │   │
│             │ │                                                │    392      prompt_tokens: List                 │   │
│             │ │                                                │    393                                          │   │
│             │ │                                                │    394                                          │   │
│             │ │                                                │    395  @dataclass                              │   │
│             │ │                                                │    396  class FlushCacheReq:                    │   │
│             │ │                                                │    397      pass                                │   │
│             │ │                                                │    398                                          │   │
│             │ │                                                │    399                                          │   │
│             │ │                                                │    400  @dataclass                              │   │
│             │ │                                                │    401  class UpdateWeightFromDiskReqInput:     │   │
│             │ │                                                │    402      # The model path with the new       │   │
│             │ │                                                │ weights                                         │   │
│             │ │                                                │    403      model_path: str                     │   │
│             │ │                                                │    404      # The format to load the weights    │   │
│             │ │                                                │    405      load_format: Optional = None        │   │
│             │ │                                                │    406                                          │   │
│             │ │                                                │    407                                          │   │
│             │ │                                                │    408  @dataclass                              │   │
│             │ │                                                │    409  class UpdateWeightFromDiskReqOutput:    │   │
│             │ │                                                │    410      success: bool                       │   │
│             │ │                                                │    411      message: str                        │   │
│             │ │                                                │    412                                          │   │
│             │ │                                                │    413                                          │   │
│             │ │                                                │    414  @dataclass                              │   │
│             │ │                                                │    415  class                                   │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput:           │   │
│             │ │                                                │    416      name: str                           │   │
│             │ │                                                │    417      dtype: str                          │   │
│             │ │                                                │    418      shape: List                         │   │
│             │ │                                                │    419                                          │   │
│             │ │                                                │    420                                          │   │
│             │ │                                                │    421  @dataclass                              │   │
│             │ │                                                │    422  class                                   │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqOutput:          │   │
│             │ │                                                │    423      success: bool                       │   │
│             │ │                                                │    424      message: str                        │   │
│             │ │                                                │    425                                          │   │
│             │ │                                                │    426                                          │   │
│             │ │                                                │    427  @dataclass                              │   │
│             │ │                                                │    428  class UpdateWeightsFromTensorReqInput:  │   │
│             │ │                                                │    429      name: str                           │   │
│             │ │                                                │    430      tensor: torch.Tensor                │   │
│             │ │                                                │    431                                          │   │
│             │ │                                                │    432                                          │   │
│             │ │                                                │    433  @dataclass                              │   │
│             │ │                                                │    434  class UpdateWeightsFromTensorReqOutput: │   │
│             │ │                                                │    435      success: bool                       │   │
│             │ │                                                │    436      message: str                        │   │
│             │ │                                                │    437                                          │   │
│             │ │                                                │    438                                          │   │
│             │ │                                                │    439  @dataclass                              │   │
│             │ │                                                │    440  class InitWeightsUpdateGroupReqInput:   │   │
│             │ │                                                │    441      # The master address                │   │
│             │ │                                                │    442      master_address: str                 │   │
│             │ │                                                │    443      # The master port                   │   │
│             │ │                                                │    444      master_port: int                    │   │
│             │ │                                                │    445      # The rank offset                   │   │
│             │ │                                                │    446      rank_offset: int                    │   │
│             │ │                                                │    447      # The world size                    │   │
│             │ │                                                │    448      world_size: int                     │   │
│             │ │                                                │    449      # The group name                    │   │
│             │ │                                                │    450      group_name: str =                   │   │
│             │ │                                                │ "weight_update_group"                           │   │
│             │ │                                                │    451      # The backend                       │   │
│             │ │                                                │    452      backend: str = "nccl"               │   │
│             │ │                                                │    453                                          │   │
│             │ │                                                │    454                                          │   │
│             │ │                                                │    455  @dataclass                              │   │
│             │ │                                                │    456  class InitWeightsUpdateGroupReqOutput:  │   │
│             │ │                                                │    457      success: bool                       │   │
│             │ │                                                │    458      message: str                        │   │
│             │ │                                                │    459                                          │   │
│             │ │                                                │    460                                          │   │
│             │ │                                                │    461  @dataclass                              │   │
│             │ │                                                │    462  class GetWeightsByNameReqInput:         │   │
│             │ │                                                │    463      name: str                           │   │
│             │ │                                                │    464      truncate_size: int = 100            │   │
│             │ │                                                │    465                                          │   │
│             │ │                                                │    466                                          │   │
│             │ │                                                │    467  @dataclass                              │   │
│             │ │                                                │    468  class GetWeightsByNameReqOutput:        │   │
│             │ │                                                │    469      parameter: list                     │   │
│             │ │                                                │    470                                          │   │
│             │ │                                                │    471                                          │   │
│             │ │                                                │    472  @dataclass                              │   │
│             │ │                                                │    473  class AbortReq:                         │   │
│             │ │                                                │    474      # The request id                    │   │
│             │ │                                                │    475      rid: str                            │   │
│             │ │                                                │    476                                          │   │
│             │ │                                                │    477                                          │   │
│             │ │                                                │    478  class ProfileReq(Enum):                 │   │
│             │ │                                                │    479      START_PROFILE = 1                   │   │
│             │ │                                                │    480      STOP_PROFILE = 2                    │   │
│             │ │                                                │    481                                          │   │
│             │ │                                                │    482                                          │   │
│             │ │                                                │    483  @dataclass                              │   │
│             │ │                                                │    484  class OpenSessionReqInput:              │   │
│             │ │                                                │    485      capacity_of_str_len: int            │   │
│             │ │                                                │    486      session_id: Optio<response          │   │
│             │ │                                                │ clipped><NOTE>To save on context only part of   │   │
│             │ │                                                │ this file has been shown to you. You should     │   │
│             │ │                                                │ retry this tool after you have searched inside  │   │
│             │ │                                                │ the file with `grep -n` in order to find the    │   │
│             │ │                                                │ line numbers of what you are looking            │   │
│             │ │                                                │ for.</NOTE>                                     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 17787 Output: 728                                                                               │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 3: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """A tensor parallel worker."""         │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import logging                          │   │
│             │ │                                                │     17  import threading                        │   │
│             │ │                                                │     18  from typing import Optional             │   │
│             │ │                                                │     19                                          │   │
│             │ │                                                │     20  from sglang.srt.configs.model_config    │   │
│             │ │                                                │ import ModelConfig                              │   │
│             │ │                                                │     21  from sglang.srt.hf_transformers_utils   │   │
│             │ │                                                │ import get_processor, get_tokenizer             │   │
│             │ │                                                │     22  from sglang.srt.managers.io_struct      │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     23      GetWeightsByNameReqInput,           │   │
│             │ │                                                │     24      InitWeightsUpdateGroupReqInput,     │   │
│             │ │                                                │     25      UpdateWeightFromDiskReqInput,       │   │
│             │ │                                                │     26                                          │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput,           │   │
│             │ │                                                │     27      UpdateWeightsFromTensorReqInput,    │   │
│             │ │                                                │     28  )                                       │   │
│             │ │                                                │     29  from sglang.srt.managers.schedule_batch │   │
│             │ │                                                │ import ModelWorkerBatch,                        │   │
│             │ │                                                │ global_server_args_dict                         │   │
│             │ │                                                │     30  from                                    │   │
│             │ │                                                │ sglang.srt.model_executor.forward_batch_info    │   │
│             │ │                                                │ import ForwardBatch                             │   │
│             │ │                                                │     31  from                                    │   │
│             │ │                                                │ sglang.srt.model_executor.model_runner import   │   │
│             │ │                                                │ ModelRunner                                     │   │
│             │ │                                                │     32  from sglang.srt.server_args import      │   │
│             │ │                                                │ ServerArgs                                      │   │
│             │ │                                                │     33  from sglang.srt.utils import            │   │
│             │ │                                                │ broadcast_pyobj, set_random_seed                │   │
│             │ │                                                │     34                                          │   │
│             │ │                                                │     35  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     36                                          │   │
│             │ │                                                │     37                                          │   │
│             │ │                                                │     38  class TpModelWorker:                    │   │
│             │ │                                                │     39      """A tensor parallel model          │   │
│             │ │                                                │ worker."""                                      │   │
│             │ │                                                │     40                                          │   │
│             │ │                                                │     41      def __init__(                       │   │
│             │ │                                                │     42          self,                           │   │
│             │ │                                                │     43          server_args: ServerArgs,        │   │
│             │ │                                                │     44          gpu_id: int,                    │   │
│             │ │                                                │     45          tp_rank: int,                   │   │
│             │ │                                                │     46          dp_rank: Optional,              │   │
│             │ │                                                │     47          nccl_port: int,                 │   │
│             │ │                                                │     48      ):                                  │   │
│             │ │                                                │     49          # Parse args                    │   │
│             │ │                                                │     50          self.tp_rank = tp_rank          │   │
│             │ │                                                │     51                                          │   │
│             │ │                                                │     52          # Init model and tokenizer      │   │
│             │ │                                                │     53          self.model_config =             │   │
│             │ │                                                │ ModelConfig(                                    │   │
│             │ │                                                │     54              server_args.model_path,     │   │
│             │ │                                                │     55                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │     56                                          │   │
│             │ │                                                │ revision=server_args.revision,                  │   │
│             │ │                                                │     57                                          │   │
│             │ │                                                │ context_length=server_args.context_length,      │   │
│             │ │                                                │     58                                          │   │
│             │ │                                                │ model_override_args=server_args.json_model_ove… │   │
│             │ │                                                │     59                                          │   │
│             │ │                                                │ is_embedding=server_args.is_embedding,          │   │
│             │ │                                                │     60              dtype=server_args.dtype,    │   │
│             │ │                                                │     61                                          │   │
│             │ │                                                │ quantization=server_args.quantization,          │   │
│             │ │                                                │     62          )                               │   │
│             │ │                                                │     63          self.model_runner =             │   │
│             │ │                                                │ ModelRunner(                                    │   │
│             │ │                                                │     64                                          │   │
│             │ │                                                │ model_config=self.model_config,                 │   │
│             │ │                                                │     65                                          │   │
│             │ │                                                │ mem_fraction_static=server_args.mem_fraction_s… │   │
│             │ │                                                │     66              gpu_id=gpu_id,              │   │
│             │ │                                                │     67              tp_rank=tp_rank,            │   │
│             │ │                                                │     68                                          │   │
│             │ │                                                │ tp_size=server_args.tp_size,                    │   │
│             │ │                                                │     69              nccl_port=nccl_port,        │   │
│             │ │                                                │     70              server_args=server_args,    │   │
│             │ │                                                │     71          )                               │   │
│             │ │                                                │     72          if                              │   │
│             │ │                                                │ server_args.skip_tokenizer_init:                │   │
│             │ │                                                │     73              self.tokenizer =            │   │
│             │ │                                                │ self.processor = None                           │   │
│             │ │                                                │     74          else:                           │   │
│             │ │                                                │     75              if                          │   │
│             │ │                                                │ self.model_config.is_multimodal:                │   │
│             │ │                                                │     76                  self.processor =        │   │
│             │ │                                                │ get_processor(                                  │   │
│             │ │                                                │     77                                          │   │
│             │ │                                                │ server_args.tokenizer_path,                     │   │
│             │ │                                                │     78                                          │   │
│             │ │                                                │ tokenizer_mode=server_args.tokenizer_mode,      │   │
│             │ │                                                │     79                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │     80                  )                       │   │
│             │ │                                                │     81                  self.tokenizer =        │   │
│             │ │                                                │ self.processor.tokenizer                        │   │
│             │ │                                                │     82              else:                       │   │
│             │ │                                                │     83                  self.tokenizer =        │   │
│             │ │                                                │ get_tokenizer(                                  │   │
│             │ │                                                │     84                                          │   │
│             │ │                                                │ server_args.tokenizer_path,                     │   │
│             │ │                                                │     85                                          │   │
│             │ │                                                │ tokenizer_mode=server_args.tokenizer_mode,      │   │
│             │ │                                                │     86                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │     87                  )                       │   │
│             │ │                                                │     88          self.device =                   │   │
│             │ │                                                │ self.model_runner.device                        │   │
│             │ │                                                │     89                                          │   │
│             │ │                                                │     90          # Profile number of tokens      │   │
│             │ │                                                │     91          self.max_total_num_tokens =     │   │
│             │ │                                                │ self.model_runner.max_total_num_tokens          │   │
│             │ │                                                │     92          self.max_prefill_tokens =       │   │
│             │ │                                                │ server_args.max_prefill_tokens                  │   │
│             │ │                                                │     93          self.max_running_requests =     │   │
│             │ │                                                │ min(                                            │   │
│             │ │                                                │     94              (                           │   │
│             │ │                                                │     95                                          │   │
│             │ │                                                │ self.max_total_num_tokens // 2                  │   │
│             │ │                                                │     96                  if                      │   │
│             │ │                                                │ server_args.max_running_requests is None        │   │
│             │ │                                                │     97                  else                    │   │
│             │ │                                                │ server_args.max_running_requests                │   │
│             │ │                                                │     98              ),                          │   │
│             │ │                                                │     99                                          │   │
│             │ │                                                │ self.model_runner.req_to_token_pool.size,       │   │
│             │ │                                                │    100          )                               │   │
│             │ │                                                │    101          self.max_req_len = min(         │   │
│             │ │                                                │    102                                          │   │
│             │ │                                                │ self.model_config.context_len - 1,              │   │
│             │ │                                                │    103              self.max_total_num_tokens - │   │
│             │ │                                                │ 1,                                              │   │
│             │ │                                                │    104          )                               │   │
│             │ │                                                │    105          self.max_req_input_len =        │   │
│             │ │                                                │ self.max_req_len - 5                            │   │
│             │ │                                                │    106          assert (                        │   │
│             │ │                                                │    107              self.max_req_len > 0 and    │   │
│             │ │                                                │ self.max_req_input_len > 0                      │   │
│             │ │                                                │    108          ), "Memory pool size is too     │   │
│             │ │                                                │ small"                                          │   │
│             │ │                                                │    109                                          │   │
│             │ │                                                │    110          # Sync random seed across TP    │   │
│             │ │                                                │ workers                                         │   │
│             │ │                                                │    111          self.random_seed =              │   │
│             │ │                                                │ broadcast_pyobj(                                │   │
│             │ │                                                │    112              ,                           │   │
│             │ │                                                │    113              self.tp_rank,               │   │
│             │ │                                                │    114                                          │   │
│             │ │                                                │ self.model_runner.tp_group.cpu_group,           │   │
│             │ │                                                │    115          )[0]                            │   │
│             │ │                                                │    116                                          │   │
│             │ │                                                │ set_random_seed(self.random_seed)               │   │
│             │ │                                                │    117                                          │   │
│             │ │                                                │    118      def get_worker_info(self):          │   │
│             │ │                                                │    119          return (                        │   │
│             │ │                                                │    120              self.max_total_num_tokens,  │   │
│             │ │                                                │    121              self.max_prefill_tokens,    │   │
│             │ │                                                │    122              self.max_running_requests,  │   │
│             │ │                                                │    123              self.max_req_len,           │   │
│             │ │                                                │    124              self.max_req_input_len,     │   │
│             │ │                                                │    125              self.random_seed,           │   │
│             │ │                                                │    126              self.device,                │   │
│             │ │                                                │    127              global_server_args_dict,    │   │
│             │ │                                                │    128                                          │   │
│             │ │                                                │ self.model_runner.req_to_token_pool.size,       │   │
│             │ │                                                │    129                                          │   │
│             │ │                                                │ self.model_runner.req_to_token_pool.max_contex… │   │
│             │ │                                                │    130                                          │   │
│             │ │                                                │ self.model_runner.token_to_kv_pool.size,        │   │
│             │ │                                                │    131          )                               │   │
│             │ │                                                │    132                                          │   │
│             │ │                                                │    133      def get_pad_input_ids_func(self):   │   │
│             │ │                                                │    134          return                          │   │
│             │ │                                                │ getattr(self.model_runner.model,                │   │
│             │ │                                                │ "pad_input_ids", None)                          │   │
│             │ │                                                │    135                                          │   │
│             │ │                                                │    136      def get_tp_cpu_group(self):         │   │
│             │ │                                                │    137          return                          │   │
│             │ │                                                │ self.model_runner.tp_group.cpu_group            │   │
│             │ │                                                │    138                                          │   │
│             │ │                                                │    139      def get_memory_pool(self):          │   │
│             │ │                                                │    140          return (                        │   │
│             │ │                                                │    141                                          │   │
│             │ │                                                │ self.model_runner.req_to_token_pool,            │   │
│             │ │                                                │    142                                          │   │
│             │ │                                                │ self.model_runner.token_to_kv_pool,             │   │
│             │ │                                                │    143          )                               │   │
│             │ │                                                │    144                                          │   │
│             │ │                                                │    145      def forward_batch_idle(self,        │   │
│             │ │                                                │ model_worker_batch: ModelWorkerBatch):          │   │
│             │ │                                                │    146          forward_batch =                 │   │
│             │ │                                                │ ForwardBatch.init_new(model_worker_batch,       │   │
│             │ │                                                │ self.model_runner)                              │   │
│             │ │                                                │    147                                          │   │
│             │ │                                                │ self.model_runner.forward(forward_batch)        │   │
│             │ │                                                │    148                                          │   │
│             │ │                                                │    149      def forward_batch_generation(       │   │
│             │ │                                                │    150          self,                           │   │
│             │ │                                                │    151          model_worker_batch:             │   │
│             │ │                                                │ ModelWorkerBatch,                               │   │
│             │ │                                                │    152          launch_done: Optional = None,   │   │
│             │ │                                                │    153          skip_sample: bool = False,      │   │
│             │ │                                                │    154      ):                                  │   │
│             │ │                                                │    155          forward_batch =                 │   │
│             │ │                                                │ ForwardBatch.init_new(model_worker_batch,       │   │
│             │ │                                                │ self.model_runner)                              │   │
│             │ │                                                │    156          logits_output =                 │   │
│             │ │                                                │ self.model_runner.forward(forward_batch)        │   │
│             │ │                                                │    157          if launch_done:                 │   │
│             │ │                                                │    158              launch_done.set()           │   │
│             │ │                                                │    159                                          │   │
│             │ │                                                │    160          if skip_sample:                 │   │
│             │ │                                                │    161              next_token_ids = None       │   │
│             │ │                                                │    162          else:                           │   │
│             │ │                                                │    163              next_token_ids =            │   │
│             │ │                                                │ self.model_runner.sample(logits_output,         │   │
│             │ │                                                │ model_worker_batch)                             │   │
│             │ │                                                │    164                                          │   │
│             │ │                                                │    165          return logits_output,           │   │
│             │ │                                                │ next_token_ids                                  │   │
│             │ │                                                │    166                                          │   │
│             │ │                                                │    167      def forward_batch_embedding(self,   │   │
│             │ │                                                │ model_worker_batch: ModelWorkerBatch):          │   │
│             │ │                                                │    168          forward_batch =                 │   │
│             │ │                                                │ ForwardBatch.init_new(model_worker_batch,       │   │
│             │ │                                                │ self.model_runner)                              │   │
│             │ │                                                │    169          logits_output =                 │   │
│             │ │                                                │ self.model_runner.forward(forward_batch)        │   │
│             │ │                                                │    170          embeddings =                    │   │
│             │ │                                                │ logits_output.embeddings                        │   │
│             │ │                                                │    171          return embeddings               │   │
│             │ │                                                │    172                                          │   │
│             │ │                                                │    173      def update_weights_from_disk(self,  │   │
│             │ │                                                │ recv_req: UpdateWeightFromDiskReqInput):        │   │
│             │ │                                                │    174          success, message =              │   │
│             │ │                                                │ self.model_runner.update_weights_from_disk(     │   │
│             │ │                                                │    175              recv_req.model_path,        │   │
│             │ │                                                │ recv_req.load_format                            │   │
│             │ │                                                │    176          )                               │   │
│             │ │                                                │    177          return success, message         │   │
│             │ │                                                │    178                                          │   │
│             │ │                                                │    179      def init_weights_update_group(self, │   │
│             │ │                                                │ recv_req: InitWeightsUpdateGroupReqInput):      │   │
│             │ │                                                │    180          success, message =              │   │
│             │ │                                                │ self.model_runner.init_weights_update_group(    │   │
│             │ │                                                │    181              recv_req.master_address,    │   │
│             │ │                                                │    182              recv_req.master_port,       │   │
│             │ │                                                │    183              recv_req.rank_offset,       │   │
│             │ │                                                │    184              recv_req.world_size,        │   │
│             │ │                                                │    185              recv_req.group_name,        │   │
│             │ │                                                │    186              recv_req.backend,           │   │
│             │ │                                                │    187          )                               │   │
│             │ │                                                │    188          return success, message         │   │
│             │ │                                                │    189                                          │   │
│             │ │                                                │    190      def                                 │   │
│             │ │                                                │ update_weights_from_distributed(                │   │
│             │ │                                                │    191          self, recv_req:                 │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput            │   │
│             │ │                                                │    192      ):                                  │   │
│             │ │                                                │    193          success, message =              │   │
│             │ │                                                │ self.model_runner.update_weights_from_distribu… │   │
│             │ │                                                │    194              recv_req.name,              │   │
│             │ │                                                │ recv_req.dtype, recv_req.shape                  │   │
│             │ │                                                │    195          )                               │   │
│             │ │                                                │    196          return success, message         │   │
│             │ │                                                │    197                                          │   │
│             │ │                                                │    198      def                                 │   │
│             │ │                                                │ update_weights_from_tensor(self, recv_req:      │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │    199          success, message =              │   │
│             │ │                                                │ self.model_runner.update_weights_from_tensor(   │   │
│             │ │                                                │    200              recv_req.name,              │   │
│             │ │                                                │ recv_req.tensor                                 │   │
│             │ │                                                │    201          )                               │   │
│             │ │                                                │    202          return success, message         │   │
│             │ │                                                │    203                                          │   │
│             │ │                                                │    204      def get_weights_by_name(self,       │   │
│             │ │                                                │ recv_req: GetWeightsByNameReqInput):            │   │
│             │ │                                                │    205          parameter =                     │   │
│             │ │                                                │ self.model_runner.get_weights_by_name(          │   │
│             │ │                                                │    206              recv_req.name,              │   │
│             │ │                                                │ recv_req.truncate_size                          │   │
│             │ │                                                │    207          )                               │   │
│             │ │                                                │    208          return parameter                │   │
│             │ │                                                │    209                                          │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 32835 Output: 886                                                                               │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 4: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """ModelRunner runs the forward passes  │   │
│             │ │                                                │ of the models."""                               │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import gc                               │   │
│             │ │                                                │     17  import json                             │   │
│             │ │                                                │     18  import logging                          │   │
│             │ │                                                │     19  import time                             │   │
│             │ │                                                │     20  from typing import Optional             │   │
│             │ │                                                │     21                                          │   │
│             │ │                                                │     22  import torch                            │   │
│             │ │                                                │     23  import torch.distributed as dist        │   │
│             │ │                                                │     24  from vllm.distributed import (          │   │
│             │ │                                                │     25      get_tp_group,                       │   │
│             │ │                                                │     26      init_distributed_environment,       │   │
│             │ │                                                │     27      initialize_model_parallel,          │   │
│             │ │                                                │     28      set_custom_all_reduce,              │   │
│             │ │                                                │     29  )                                       │   │
│             │ │                                                │     30                                          │   │
│             │ │                                                │     31  from sglang.srt.configs.device_config   │   │
│             │ │                                                │ import DeviceConfig                             │   │
│             │ │                                                │     32  from sglang.srt.configs.load_config     │   │
│             │ │                                                │ import LoadConfig                               │   │
│             │ │                                                │     33  from sglang.srt.configs.model_config    │   │
│             │ │                                                │ import AttentionArch, ModelConfig               │   │
│             │ │                                                │     34  from                                    │   │
│             │ │                                                │ sglang.srt.layers.attention.double_sparsity_ba… │   │
│             │ │                                                │ import DoubleSparseAttnBackend                  │   │
│             │ │                                                │     35  from                                    │   │
│             │ │                                                │ sglang.srt.layers.attention.flashinfer_backend  │   │
│             │ │                                                │ import FlashInferAttnBackend                    │   │
│             │ │                                                │     36  from                                    │   │
│             │ │                                                │ sglang.srt.layers.attention.torch_native_backe… │   │
│             │ │                                                │ import TorchNativeAttnBackend                   │   │
│             │ │                                                │     37  from                                    │   │
│             │ │                                                │ sglang.srt.layers.attention.triton_backend      │   │
│             │ │                                                │ import TritonAttnBackend                        │   │
│             │ │                                                │     38  from sglang.srt.layers.logits_processor │   │
│             │ │                                                │ import LogitsProcessorOutput                    │   │
│             │ │                                                │     39  from sglang.srt.layers.sampler import   │   │
│             │ │                                                │ Sampler                                         │   │
│             │ │                                                │     40  from sglang.srt.layers.torchao_utils    │   │
│             │ │                                                │ import apply_torchao_config_to_model            │   │
│             │ │                                                │     41  from sglang.srt.lora.lora_manager       │   │
│             │ │                                                │ import LoRAManager                              │   │
│             │ │                                                │     42  from sglang.srt.managers.schedule_batch │   │
│             │ │                                                │ import global_server_args_dict                  │   │
│             │ │                                                │     43  from sglang.srt.mem_cache.memory_pool   │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     44      DoubleSparseTokenToKVPool,          │   │
│             │ │                                                │     45      MHATokenToKVPool,                   │   │
│             │ │                                                │     46      MLATokenToKVPool,                   │   │
│             │ │                                                │     47      ReqToTokenPool,                     │   │
│             │ │                                                │     48  )                                       │   │
│             │ │                                                │     49  from                                    │   │
│             │ │                                                │ sglang.srt.model_executor.forward_batch_info    │   │
│             │ │                                                │ import ForwardBatch                             │   │
│             │ │                                                │     50  from sglang.srt.model_loader import     │   │
│             │ │                                                │ get_model                                       │   │
│             │ │                                                │     51  from sglang.srt.server_args import      │   │
│             │ │                                                │ ServerArgs                                      │   │
│             │ │                                                │     52  from sglang.srt.utils import (          │   │
│             │ │                                                │     53      enable_show_time_cost,              │   │
│             │ │                                                │     54      get_available_gpu_memory,           │   │
│             │ │                                                │     55      init_custom_process_group,          │   │
│             │ │                                                │     56      is_hip,                             │   │
│             │ │                                                │     57      monkey_patch_vllm_gguf_config,      │   │
│             │ │                                                │     58      monkey_patch_vllm_p2p_access_check, │   │
│             │ │                                                │     59      set_cpu_offload_max_bytes,          │   │
│             │ │                                                │     60  )                                       │   │
│             │ │                                                │     61                                          │   │
│             │ │                                                │     62  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     63                                          │   │
│             │ │                                                │     64                                          │   │
│             │ │                                                │     65  class ModelRunner:                      │   │
│             │ │                                                │     66      """ModelRunner runs the forward     │   │
│             │ │                                                │ passes of the models."""                        │   │
│             │ │                                                │     67                                          │   │
│             │ │                                                │     68      def __init__(                       │   │
│             │ │                                                │     69          self,                           │   │
│             │ │                                                │     70          model_config: ModelConfig,      │   │
│             │ │                                                │     71          mem_fraction_static: float,     │   │
│             │ │                                                │     72          gpu_id: int,                    │   │
│             │ │                                                │     73          tp_rank: int,                   │   │
│             │ │                                                │     74          tp_size: int,                   │   │
│             │ │                                                │     75          nccl_port: int,                 │   │
│             │ │                                                │     76          server_args: ServerArgs,        │   │
│             │ │                                                │     77      ):                                  │   │
│             │ │                                                │     78          # Parse args                    │   │
│             │ │                                                │     79          self.model_config =             │   │
│             │ │                                                │ model_config                                    │   │
│             │ │                                                │     80          self.mem_fraction_static =      │   │
│             │ │                                                │ mem_fraction_static                             │   │
│             │ │                                                │     81          self.device =                   │   │
│             │ │                                                │ server_args.device                              │   │
│             │ │                                                │     82          self.gpu_id = gpu_id            │   │
│             │ │                                                │     83          self.tp_rank = tp_rank          │   │
│             │ │                                                │     84          self.tp_size = tp_size          │   │
│             │ │                                                │     85          self.dist_port = nccl_port      │   │
│             │ │                                                │     86          self.server_args = server_args  │   │
│             │ │                                                │     87          self.is_generation =            │   │
│             │ │                                                │ model_config.is_generation                      │   │
│             │ │                                                │     88          self.is_multimodal =            │   │
│             │ │                                                │ model_config.is_multimodal                      │   │
│             │ │                                                │     89                                          │   │
│             │ │                                                │     90          # Model-specific adjustment     │   │
│             │ │                                                │     91          if (                            │   │
│             │ │                                                │     92                                          │   │
│             │ │                                                │ self.model_config.attention_arch ==             │   │
│             │ │                                                │ AttentionArch.MLA                               │   │
│             │ │                                                │     93              and not                     │   │
│             │ │                                                │ self.server_args.disable_mla                    │   │
│             │ │                                                │     94          ):                              │   │
│             │ │                                                │     95              logger.info("MLA            │   │
│             │ │                                                │ optimization is turned on. Use triton           │   │
│             │ │                                                │ backend.")                                      │   │
│             │ │                                                │     96                                          │   │
│             │ │                                                │ self.server_args.attention_backend = "triton"   │   │
│             │ │                                                │     97                                          │   │
│             │ │                                                │     98          if                              │   │
│             │ │                                                │ self.server_args.enable_double_sparsity:        │   │
│             │ │                                                │     99              logger.info(                │   │
│             │ │                                                │    100                  "Double sparsity        │   │
│             │ │                                                │ optimization is turned on. Use triton backend   │   │
│             │ │                                                │ without CUDA graph."                            │   │
│             │ │                                                │    101              )                           │   │
│             │ │                                                │    102                                          │   │
│             │ │                                                │ self.server_args.attention_backend = "triton"   │   │
│             │ │                                                │    103                                          │   │
│             │ │                                                │ self.server_args.disable_cuda_graph = True      │   │
│             │ │                                                │    104              if                          │   │
│             │ │                                                │ self.server_args.ds_heavy_channel_type is None: │   │
│             │ │                                                │    105                  raise ValueError(       │   │
│             │ │                                                │    106                      "Please specify the │   │
│             │ │                                                │ heavy channel type for double sparsity          │   │
│             │ │                                                │ optimization."                                  │   │
│             │ │                                                │    107                  )                       │   │
│             │ │                                                │    108                                          │   │
│             │ │                                                │ self.init_double_sparsity_channel_config(       │   │
│             │ │                                                │    109                                          │   │
│             │ │                                                │ self.server_args.ds_heavy_channel_type          │   │
│             │ │                                                │    110              )                           │   │
│             │ │                                                │    111                                          │   │
│             │ │                                                │    112          if self.is_multimodal:          │   │
│             │ │                                                │    113              self.mem_fraction_static *= │   │
│             │ │                                                │ 0.95                                            │   │
│             │ │                                                │    114              if                          │   │
│             │ │                                                │ self.model_config.hf_config.architectures == [  │   │
│             │ │                                                │    115                                          │   │
│             │ │                                                │ "MllamaForConditionalGeneration"                │   │
│             │ │                                                │    116              ]:                          │   │
│             │ │                                                │    117                                          │   │
│             │ │                                                │ logger.info("Automatically turn off             │   │
│             │ │                                                │ --chunked-prefill-size for mllama.")            │   │
│             │ │                                                │    118                                          │   │
│             │ │                                                │ server_args.chunked_prefill_size = -1           │   │
│             │ │                                                │    119              # TODO: qwen2-vl does not   │   │
│             │ │                                                │ support radix cache now, set                    │   │
│             │ │                                                │ disable_radix_cache=True automatically          │   │
│             │ │                                                │    120              if                          │   │
│             │ │                                                │ self.model_config.hf_config.architectures == [  │   │
│             │ │                                                │    121                                          │   │
│             │ │                                                │ "Qwen2VLForConditionalGeneration"               │   │
│             │ │                                                │    122              ]:                          │   │
│             │ │                                                │    123                  logger.info(            │   │
│             │ │                                                │    124                      "Automatically turn │   │
│             │ │                                                │ off --chunked-prefill-size and disable radix    │   │
│             │ │                                                │ cache for qwen2-vl."                            │   │
│             │ │                                                │    125                  )                       │   │
│             │ │                                                │    126                                          │   │
│             │ │                                                │ server_args.chunked_prefill_size = -1           │   │
│             │ │                                                │    127                                          │   │
│             │ │                                                │ server_args.disable_radix_cache = True          │   │
│             │ │                                                │    128                                          │   │
│             │ │                                                │    129          # Global vars                   │   │
│             │ │                                                │    130          if server_args.show_time_cost:  │   │
│             │ │                                                │    131              enable_show_time_cost()     │   │
│             │ │                                                │    132          if                              │   │
│             │ │                                                │ server_args.disable_outlines_disk_cache:        │   │
│             │ │                                                │    133              from outlines.caching       │   │
│             │ │                                                │ import disable_cache                            │   │
│             │ │                                                │    134                                          │   │
│             │ │                                                │    135              disable_cache()             │   │
│             │ │                                                │    136                                          │   │
│             │ │                                                │    137          global_server_args_dict.update( │   │
│             │ │                                                │    138              {                           │   │
│             │ │                                                │    139                  "attention_backend":    │   │
│             │ │                                                │ server_args.attention_backend,                  │   │
│             │ │                                                │    140                  "sampling_backend":     │   │
│             │ │                                                │ server_args.sampling_backend,                   │   │
│             │ │                                                │    141                                          │   │
│             │ │                                                │ "triton_attention_reduce_in_fp32":              │   │
│             │ │                                                │ server_args.triton_attention_reduce_in_fp32,    │   │
│             │ │                                                │    142                  "disable_mla":          │   │
│             │ │                                                │ server_args.disable_mla,                        │   │
│             │ │                                                │    143                  "torchao_config":       │   │
│             │ │                                                │ server_args.torchao_config,                     │   │
│             │ │                                                │    144                  "enable_nan_detection": │   │
│             │ │                                                │ server_args.enable_nan_detection,               │   │
│             │ │                                                │    145                  "enable_dp_attention":  │   │
│             │ │                                                │ server_args.enable_dp_attention,                │   │
│             │ │                                                │    146                  "enable_ep_moe":        │   │
│             │ │                                                │ server_args.enable_ep_moe,                      │   │
│             │ │                                                │    147              }                           │   │
│             │ │                                                │    148          )                               │   │
│             │ │                                                │    149                                          │   │
│             │ │                                                │    150                                          │   │
│             │ │                                                │ set_cpu_offload_max_bytes(int(server_args.cpu_… │   │
│             │ │                                                │ * 1024**3))                                     │   │
│             │ │                                                │    151                                          │   │
│             │ │                                                │    152          # Get memory before model       │   │
│             │ │                                                │ loading                                         │   │
│             │ │                                                │    153          min_per_gpu_memory =            │   │
│             │ │                                                │ self.init_torch_distributed()                   │   │
│             │ │                                                │    154                                          │   │
│             │ │                                                │    155          # Load the model                │   │
│             │ │                                                │    156          self.sampler = Sampler()        │   │
│             │ │                                                │    157          self.load_model()               │   │
│             │ │                                                │    158                                          │   │
│             │ │                                                │    159          # Apply torchao quantization    │   │
│             │ │                                                │    160          apply_torchao_config_to_model(  │   │
│             │ │                                                │    161              self.model,                 │   │
│             │ │                                                │ global_server_args_dict["torchao_config"]       │   │
│             │ │                                                │    162          )                               │   │
│             │ │                                                │    163                                          │   │
│             │ │                                                │    164          # Apply torch TP if the model   │   │
│             │ │                                                │ supports it                                     │   │
│             │ │                                                │    165          supports_torch_tp =             │   │
│             │ │                                                │ getattr(self.model, "supports_torch_tp", False) │   │
│             │ │                                                │    166          if self.tp_size > 1 and         │   │
│             │ │                                                │ supports_torch_tp:                              │   │
│             │ │                                                │    167              self.apply_torch_tp()       │   │
│             │ │                                                │    168              self.torch_tp_applied =     │   │
│             │ │                                                │ True                                            │   │
│             │ │                                                │    169          else:                           │   │
│             │ │                                                │    170              self.torch_tp_applied =     │   │
│             │ │                                                │ False                                           │   │
│             │ │                                                │    171                                          │   │
│             │ │                                                │    172          # Init memory pool and          │   │
│             │ │                                                │ attention backends                              │   │
│             │ │                                                │    173          if server_args.lora_paths is    │   │
│             │ │                                                │ not None:                                       │   │
│             │ │                                                │    174              self.init_lora_manager()    │   │
│             │ │                                                │    175          self.init_memory_pool(          │   │
│             │ │                                                │    176              min_per_gpu_memory,         │   │
│             │ │                                                │    177                                          │   │
│             │ │                                                │ server_args.max_running_requests,               │   │
│             │ │                                                │    178                                          │   │
│             │ │                                                │ server_args.max_total_tokens,                   │   │
│             │ │                                                │    179          )                               │   │
│             │ │                                                │    180          if self.device == "cuda":       │   │
│             │ │                                                │    181              self.init_cublas()          │   │
│             │ │                                                │    182                                          │   │
│             │ │                                                │ self.init_attention_backend()                   │   │
│             │ │                                                │    183              self.init_cuda_graphs()     │   │
│             │ │                                                │    184          else:                           │   │
│             │ │                                                │    185              self.cuda_graph_runner =    │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    186                                          │   │
│             │ │                                                │ self.init_attention_backend()                   │   │
│             │ │                                                │    187                                          │   │
│             │ │                                                │    188      def init_torch_distributed(self):   │   │
│             │ │                                                │    189          logger.info("Init torch         │   │
│             │ │                                                │ distributed begin.")                            │   │
│             │ │                                                │    190          # Init torch distributed        │   │
│             │ │                                                │    191                                          │   │
│             │ │                                                │ torch.get_device_module(self.device).set_devic… │   │
│             │ │                                                │    192          if self.device == "cuda":       │   │
│             │ │                                                │    193              backend = "nccl"            │   │
│             │ │                                                │    194          elif self.device == "xpu":      │   │
│             │ │                                                │    195              # TODO(liangan1):Just use   │   │
│             │ │                                                │ gloo to bypass the initilization fail           │   │
│             │ │                                                │    196              # Need to use xccl for xpu  │   │
│             │ │                                                │ backend in the future                           │   │
│             │ │                                                │    197              backend = "gloo"            │   │
│             │ │                                                │    198          elif self.device == "hpu":      │   │
│             │ │                                                │    199              backend = "hccl"            │   │
│             │ │                                                │    200                                          │   │
│             │ │                                                │    201          if not                          │   │
│             │ │                                                │ self.server_args.enable_p2p_check:              │   │
│             │ │                                                │    202                                          │   │
│             │ │                                                │ monkey_patch_vllm_p2p_access_check(self.gpu_id) │   │
│             │ │                                                │    203          if                              │   │
│             │ │                                                │ self.server_args.dist_init_addr:                │   │
│             │ │                                                │    204              dist_init_method =          │   │
│             │ │                                                │ f"tcp://{self.server_args.dist_init_addr}"      │   │
│             │ │                                                │    205          else:                           │   │
│             │ │                                                │    206              dist_init_method =          │   │
│             │ │                                                │ f"tcp://127.0.0.1:{self.dist_port}"             │   │
│             │ │                                                │    207          set_custom_all_reduce(not       │   │
│             │ │                                                │ self.server_args.disable_custom_all_reduce)     │   │
│             │ │                                                │    208          init_distributed_environment(   │   │
│             │ │                                                │    209              backend=backend,            │   │
│             │ │                                                │    210              world_size=self.tp_size,    │   │
│             │ │                                                │    211              rank=self.tp_rank,          │   │
│             │ │                                                │    212              local_rank=self.gpu_id,     │   │
│             │ │                                                │    213                                          │   │
│             │ │                                                │ distributed_init_method=dist_init_method,       │   │
│             │ │                                                │    214          )                               │   │
│             │ │                                                │    215                                          │   │
│             │ │                                                │ initialize_model_parallel(tensor_model_paralle… │   │
│             │ │                                                │    216          min_per_gpu_memory =            │   │
│             │ │                                                │ get_available_gpu_memory(                       │   │
│             │ │                                                │    217              self.device, self.gpu_id,   │   │
│             │ │                                                │ distributed=self.tp_size > 1                    │   │
│             │ │                                                │    218          )                               │   │
│             │ │                                                │    219          self.tp_group = get_tp_group()  │   │
│             │ │                                                │    220                                          │   │
│             │ │                                                │    221          # Check memory for tensor       │   │
│             │ │                                                │ parallelism                                     │   │
│             │ │                                                │    222          if self.tp_size > 1:            │   │
│             │ │                                                │    223              local_gpu_memory =          │   │
│             │ │                                                │ get_available_gpu_memory(self.device,           │   │
│             │ │                                                │ self.gpu_id)                                    │   │
│             │ │                                                │    224              if min_per_gpu_memory <     │   │
│             │ │                                                │ local_gpu_memory * 0.9:                         │   │
│             │ │                                                │    225                  raise ValueError(       │   │
│             │ │                                                │    226                      "The memory         │   │
│             │ │                                                │ capacity is unbalanced. Some GPUs may be        │   │
│             │ │                                                │ occupied by other processes."                   │   │
│             │ │                                                │    227                  )                       │   │
│             │ │                                                │    228                                          │   │
│             │ │                                                │    229          return min_per_gpu_memory       │   │
│             │ │                                                │    230                                          │   │
│             │ │                                                │    231      def load_model(self):               │   │
│             │ │                                                │    232          logger.info(                    │   │
│             │ │                                                │    233              f"Load weight begin. avail  │   │
│             │ │                                                │ mem={get_available_gpu_memory(self.device,      │   │
│             │ │                                                │ self.gpu_id):.2f} GB"                           │   │
│             │ │                                                │    234          )                               │   │
│             │ │                                                │    235                                          │   │
│             │ │                                                │    236          # This can reduce thread        │   │
│             │ │                                                │ conflicts and speed up weight loading.          │   │
│             │ │                                                │    237          torch.set_num_threads(1)        │   │
│             │ │                                                │    238          if self.device == "cuda":       │   │
│             │ │                                                │    239              if                          │   │
│             │ │                                                │ torch.cuda.get_device_capability()[0] < 8:      │   │
│             │ │                                                │    240                  logger.info(            │   │
│             │ │                                                │    241                      "Compute capability │   │
│             │ │                                                │ below sm80. Use float16 due to lack of bfloat16 │   │
│             │ │                                                │ support."                                       │   │
│             │ │                                                │    242                  )                       │   │
│             │ │                                                │    243                  self.server_args.dtype  │   │
│             │ │                                                │ = "float16"                                     │   │
│             │ │                                                │    244                  self.model_config.dtype │   │
│             │ │                                                │ = torch.float16                                 │   │
│             │ │                                                │    245                  if                      │   │
│             │ │                                                │ torch.cuda.get_device_capability()[1] < 5:      │   │
│             │ │                                                │    246                      raise               │   │
│             │ │                                                │ RuntimeError("SGLang only supports sm75 and     │   │
│             │ │                                                │ above.")                                        │   │
│             │ │                                                │    247                                          │   │
│             │ │                                                │    248          # Prepare the model config      │   │
│             │ │                                                │    249          self.load_config = LoadConfig(  │   │
│             │ │                                                │    250                                          │   │
│             │ │                                                │ load_format=self.server_args.load_format,       │   │
│             │ │                                                │    251                                          │   │
│             │ │                                                │ download_dir=self.server_args.download_dir,     │   │
│             │ │                                                │    252          )                               │   │
│             │ │                                                │    253          if self.server_args.load_format │   │
│             │ │                                                │ == "gguf":                                      │   │
│             │ │                                                │    254                                          │   │
│             │ │                                                │ monkey_patch_vllm_gguf_config()                 │   │
│             │ │                                                │    255                                          │   │
│             │ │                                                │    256          # Load the model                │   │
│             │ │                                                │    257          self.model = get_model(         │   │
│             │ │                                                │    258                                          │   │
│             │ │                                                │ model_config=self.model_config,                 │   │
│             │ │                                                │    259                                          │   │
│             │ │                                                │ load_config=self.load_config,                   │   │
│             │ │                                                │    260                                          │   │
│             │ │                                                │ device_config=DeviceConfig(self.device),        │   │
│             │ │                                                │    261          )                               │   │
│             │ │                                                │    262                                          │   │
│             │ │                                                │    263          # Parse other args              │   │
│             │ │                                                │    264          self.sliding_window_size = (    │   │
│             │ │                                                │    265                                          │   │
│             │ │                                                │ self.model.get_attention_sliding_window_size()  │   │
│             │ │                                                │    266              if hasattr(self.model,      │   │
│             │ │                                                │ "get_attention_sliding_window_size")            │   │
│             │ │                                                │    267              else None                   │   │
│             │ │                                                │    268          )                               │   │
│             │ │                                                │    269          self.dtype =                    │   │
│             │ │                                                │ self.model_config.dtype                         │   │
│             │ │                                                │    270                                          │   │
│             │ │                                                │    271          logger.info(                    │   │
│             │ │                                                │    272              f"Load weight end. "        │   │
│             │ │                                                │    273                                          │   │
│             │ │                                                │ f"type={type(self.model).__name__}, "           │   │
│             │ │                                                │    274              f"dtype={self.dtype}, "     │   │
│             │ │                                                │    275              f"avail                     │   │
│             │ │                                                │ mem={get_available_gpu_memory(self.device,      │   │
│             │ │                                                │ self.gpu_id):.2f} GB"                           │   │
│             │ │                                                │    276          )                               │   │
│             │ │                                                │    277                                          │   │
│             │ │                                                │    278      def update_weights_from_disk(       │   │
│             │ │                                                │    279          self, model_path: str,          │   │
│             │ │                                                │ load_format: str                                │   │
│             │ │                                                │    280      ) -> tuple:                         │   │
│             │ │                                                │    281          """Update engine weights        │   │
│             │ │                                                │ in-place from the disk."""                      │   │
│             │ │                                                │    282          from                            │   │
│             │ │                                                │ sglang.srt.model_loader.loader import (         │   │
│             │ │                                                │    283              DefaultModelLoader,         │   │
│             │ │                                                │    284              device_loading_context,     │   │
│             │ │                                                │    285              get_model_loader,           │   │
│             │ │                                                │    286          )                               │   │
│             │ │                                                │    287          from                            │   │
│             │ │                                                │ sglang.srt.model_loader.utils import            │   │
│             │ │                                                │ set_default_torch_dtype                         │   │
│             │ │                                                │    288                                          │   │
│             │ │                                                │    289          logger.info(                    │   │
│             │ │                                                │    290              f"Update engine weights     │   │
│             │ │                                                │ online from disk begin. "                       │   │
│             │ │                                                │    291              f"avail                     │   │
│             │ │                                                │ mem={get_available_gpu_memory(self.device,      │   │
│             │ │                                                │ self.gpu_id):.2f} GB"                           │   │
│             │ │                                                │    292          )                               │   │
│             │ │                                                │    293                                          │   │
│             │ │                                                │    294          target_device =                 │   │
│             │ │                                                │ torch.device(self.device)                       │   │
│             │ │                                                │    295          self.model_config.model_path =  │   │
│             │ │                                                │ model_path                                      │   │
│             │ │                                                │    296          load_config =                   │   │
│             │ │                                                │ LoadConfig(load_format=load_format)             │   │
│             │ │                                                │    297                                          │   │
│             │ │                                                │    298          # Only support vllm             │   │
│             │ │                                                │ DefaultModelLoader for now                      │   │
│             │ │                                                │    299          loader =                        │   │
│             │ │                                                │ get_model_loader(load_config)                   │   │
│             │ │                                                │    300          if not isinstance(loader,       │   │
│             │ │                                                │ DefaultModelLoader):                            │   │
│             │ │                                                │    301              message = f"Failed to get   │   │
│             │ │                                                │ model loader: {loader}."                        │   │
│             │ │                                                │    302              return False, message       │   │
│             │ │                                                │    303                                          │   │
│             │ │                                                │    304          def get_weight_iter(config):    │   │
│             │ │                                                │    305              iter =                      │   │
│             │ │                                                │ loader._get_weights_iterator(                   │   │
│             │ │                                                │    306                                          │   │
│             │ │                                                │ DefaultModelLoader.Source(                      │   │
│             │ │                                                │    307                      config.model_path,  │   │
│             │ │                                                │    308                                          │   │
│             │ │                                                │ revision=config.revision,                       │   │
│             │ │                                                │    309                                          │   │
│             │ │                                                │ fall_back_to_pt=getattr(                        │   │
│             │ │                                                │    310                          self.model,     │   │
│             │ │                                                │ "fall_back_to_pt_during_load", True             │   │
│             │ │                                                │    311                      ),                  │   │
│             │ │                                                │    312                  )                       │   │
│             │ │                                                │    313              )                           │   │
│             │ │                                                │    314              return iter                 │   │
│             │ │                                                │    315                                          │   │
│             │ │                                                │    316          def model_load_weights(model,   │   │
│             │ │                                                │ iter):                                          │   │
│             │ │                                                │    317              model.load_weights(iter)    │   │
│             │ │                                                │    318              for _, module in            │   │
│             │ │                                                │ self.model.named_modules():                     │   │
│             │ │                                                │    319                  quant_method =          │   │
│             │ │                                                │ getattr(module, "quant_method", None)           │   │
│             │ │                                                │    320                  if quant_method is not  │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    321                      with                │   │
│             │ │                                                │ device_loading_context(module, target_device):  │   │
│             │ │                                                │    322                                          │   │
│             │ │                                                │ quant_method.process_weights_after_loading(mod… │   │
│             │ │                                                │    323              return model                │   │
│             │ │                                                │    324                                          │   │
│             │ │                                                │    325          with                            │   │
│             │ │                                                │ set_default_torch_dtype(self.model_config.dtyp… │   │
│             │ │                                                │    326              try:                        │   │
│             │ │                                                │    327                  iter =                  │   │
│             │ │                                                │ get_weight_iter(self.model_config)              │   │
│             │ │                                                │    328              except Exception as e:      │   │
│             │ │                                                │    329                  message = f"Failed to   │   │
│             │ │                                                │ get weights iterator: {e}."                     │   │
│             │ │                                                │    330                  return False, message   │   │
│             │ │                                                │    331              try:                        │   │
│             │ │                                                │    332                  model =                 │   │
│             │ │                                                │ model_load_weights(self.model, iter)            │   │
│             │ │                                                │    333              except Exception as e:      │   │
│             │ │                                                │    334                  message = (             │   │
│             │ │                                                │    335                      f"Failed to update  │   │
│             │ │                                                │ weights: {e}.\nRolling back to original         │   │
│             │ │                                                │ weights."                                       │   │
│             │ │                                                │    336                  )                       │   │
│             │ │                                                │    337                  del iter                │   │
│             │ │                                                │    338                  gc.collect()            │   │
│             │ │                                                │    339                  iter =                  │   │
│             │ │                                                │ get_weight_iter(self.model_config)              │   │
│             │ │                                                │    340                  self.model =            │   │
│             │ │                                                │ model_load_weights(self.model, iter)            │   │
│             │ │                                                │    341                  return False, message   │   │
│             │ │                                                │    342                                          │   │
│             │ │                                                │    343          self.model = model              │   │
│             │ │                                                │    344          self.server_args.model_path =   │   │
│             │ │                                                │ model_path                                      │   │
│             │ │                                                │    345          self.server_args.load_format =  │   │
│             │ │                                                │ load_format                                     │   │
│             │ │                                                │    346          self.load_config = load_config  │   │
│             │ │                                                │    347                                          │   │
│             │ │                                                │    348          logger.info("Update weights     │   │
│             │ │                                                │ end.")                                          │   │
│             │ │                                                │    349          return True, "Succeeded to      │   │
│             │ │                                                │ update model weights."                          │   │
│             │ │                                                │    350                                          │   │
│             │ │                                                │    351      def init_weights_update_group(      │   │
│             │ │                                                │    352          self,                           │   │
│             │ │                                                │    353          master_address,                 │   │
│             │ │                                                │    354          master_port,                    │   │
│             │ │                                                │    355          rank_offset,                    │   │
│             │ │                                                │    356          world_size,                     │   │
│             │ │                                                │    357          group_name,                     │   │
│             │ │                                                │    358          backend="nccl",                 │   │
│             │ │                                                │    359      ):                                  │   │
│             │ │                                                │    360          """Initialize the Torch process │   │
│             │ │                                                │ group for model parameter updates.              │   │
│             │ │                                                │    361                                          │   │
│             │ │                                                │    362          `_model_update_group` is used   │   │
│             │ │                                                │ in the RLHF workflow, where rank                │   │
│             │ │                                                │    363          0 is the actor model in the     │   │
│             │ │                                                │ training engine, and the other ranks are        │   │
│             │ │                                                │    364          the inference engine, which is  │   │
│             │ │                                                │ used for rollout.                               │   │
│             │ │                                                │    365                                          │   │
│             │ │                                                │    366          In the RLHF workflow, the       │   │
│             │ │                                                │ training engine updates the model               │   │
│             │ │                                                │    367          weights/parameters online, and  │   │
│             │ │                                                │ broadcasts them to the inference                │   │
│             │ │                                                │    368          engine through the              │   │
│             │ │                                                │ `_model_update_group` process group.            │   │
│             │ │                                                │    369          """                             │   │
│             │ │                                                │    370          assert (                        │   │
│             │ │                                                │    371                                          │   │
│             │ │                                                │ torch.distributed.is_initialized()              │   │
│             │ │                                                │    372          ), "Default torch process group │   │
│             │ │                                                │ must be initialized"                            │   │
│             │ │                                                │    373          assert group_name != "", "Group │   │
│             │ │                                                │ name cannot be empty"                           │   │
│             │ │                                                │    374                                          │   │
│             │ │                                                │    375          rank = rank_offset +            │   │
│             │ │                                                │ self.tp_rank                                    │   │
│             │ │                                                │    376                                          │   │
│             │ │                                                │    377          logger.info(                    │   │
│             │ │                                                │    378              f"init custom process       │   │
│             │ │                                                │ group: master_address={master_address},         │   │
│             │ │                                                │ master_port={master_port}, "                    │   │
│             │ │                                                │    379                                          │   │
│             │ │                                                │ f"rank_offset={rank_offset},                    │   │
│             │ │                                                │ world_size={world_size},                        │   │
│             │ │                                                │ group_name={group_name}, backend={backend}"     │   │
│             │ │                                                │    380          )                               │   │
│             │ │                                                │    381                                          │   │
│             │ │                                                │    382          try:                            │   │
│             │ │                                                │    383              self._model_update_group =  │   │
│             │ │                                                │ init_custom_process_group(                      │   │
│             │ │                                                │    384                  backend=backend,        │   │
│             │ │                                                │    385                                          │   │
│             │ │                                                │ init_method=f"tcp://{master_address}:{master_p… │   │
│             │ │                                                │    386                  world_size=world_size,  │   │
│             │ │                                                │    387                  rank=rank,              │   │
│             │ │                                                │    388                  group_name=group_name,  │   │
│             │ │                                                │    389              )                           │   │
│             │ │                                                │    390                                          │   │
│             │ │                                                │ dist.barrier(group=self._model_update_group,    │   │
│             │ │                                                │ device_ids=)                                    │   │
│             │ │                                                │    391              return True, "Succeeded to  │   │
│             │ │                                                │ initialize custom process group."               │   │
│             │ │                                                │    392          except Exception as e:          │   │
│             │ │                                                │    393              message = f"Failed to       │   │
│             │ │                                                │ initialize custom process group: {e}."          │   │
│             │ │                                                │    394              logger.error(message)       │   │
│             │ │                                                │    395              return False, message       │   │
│             │ │                                                │    396                                          │   │
│             │ │                                                │    397      def                                 │   │
│             │ │                                                │ update_weights_from_distributed(self, name,     │   │
│             │ │                                                │ dtype, shape):                                  │   │
│             │ │                                                │    398          """                             │   │
│             │ │                                                │    399          Update specific parameter in    │   │
│             │ │                                                │ the model weights online                        │   │
│             │ │                                                │    400          through `_model_update_group`   │   │
│             │ │                                                │ process group.                                  │   │
│             │ │                                                │    401                                          │   │
│             │ │                                                │    402          Args:                           │   │
│             │ │                                                │    403              name: the name of the       │   │
│             │ │                                                │ parameter to be updated.                        │   │
│             │ │                                                │    404              dtype: the data type of the │   │
│             │ │                                                │ parameter to be updated.                        │   │
│             │ │                                                │    405              shape: the shape of the     │   │
│             │ │                                                │ parameter to be updated.                        │   │
│             │ │                                                │    406          """                             │   │
│             │ │                                                │    407          target_dtype = (                │   │
│             │ │                                                │    408              dtype if isinstance(dtype,  │   │
│             │ │                                                │ torch.dtype) else getattr(torch, dtype)         │   │
│             │ │                                                │    409          )                               │   │
│             │ │                                                │    410          current_dtype = self.dtype if   │   │
│             │ │                                                │ isinstance(self.dtype, str) else self.dtype     │   │
│             │ │                                                │    411                                          │   │
│             │ │                                                │    412          assert (                        │   │
│             │ │                                                │    413              self._model_update_group is │   │
│             │ │                                                │ not None                                        │   │
│             │ │                                                │    414          ), "model update group must be  │   │
│             │ │                                                │ initialized"                                    │   │
│             │ │                                                │    415                                          │   │
│             │ │                                                │    416          try:                            │   │
│             │ │                                                │    417              weigh<response              │   │
│             │ │                                                │ clipped><NOTE>To save on context only part of   │   │
│             │ │                                                │ this file has been shown to you. You should     │   │
│             │ │                                                │ retry this tool after you have searched inside  │   │
│             │ │                                                │ the file with `grep -n` in order to find the    │   │
│             │ │                                                │ line numbers of what you are looking            │   │
│             │ │                                                │ for.</NOTE>                                     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 50377 Output: 979                                                                               │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 5: Completed                                                                                   │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │ 431:    def update_weights_from_tensor(self,    │   │
│             │ │ "update_weights_from_tensor\\|serialize\\|Mul… │ name, tensor: torch.Tensor):                    │   │
│             │ │ -n                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ 'restart': False}                              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 72759 Output: 1067                                                                              │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 6: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │    420              return True, f"Succeeded to │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ update parameter {name} online."                │   │
│             │ │ 'view_range': [420, 540]}                      │    421                                          │   │
│             │ │                                                │    422          except Exception as e:          │   │
│             │ │                                                │    423              error_msg = (               │   │
│             │ │                                                │    424                  f"Failed to update      │   │
│             │ │                                                │ parameter online: {e}. "                        │   │
│             │ │                                                │    425                  f"The full weights of   │   │
│             │ │                                                │ the ModelRunner are partially updated. "        │   │
│             │ │                                                │    426                  f"Please discard the    │   │
│             │ │                                                │ whole weights."                                 │   │
│             │ │                                                │    427              )                           │   │
│             │ │                                                │    428              logger.error(error_msg)     │   │
│             │ │                                                │    429              return False, error_msg     │   │
│             │ │                                                │    430                                          │   │
│             │ │                                                │    431      def                                 │   │
│             │ │                                                │ update_weights_from_tensor(self, name, tensor:  │   │
│             │ │                                                │ torch.Tensor):                                  │   │
│             │ │                                                │    432          self.model.load_weights([(name, │   │
│             │ │                                                │ tensor)])                                       │   │
│             │ │                                                │    433          return True, "Success"  # TODO  │   │
│             │ │                                                │ error handling                                  │   │
│             │ │                                                │    434                                          │   │
│             │ │                                                │    435      def get_weights_by_name(            │   │
│             │ │                                                │    436          self, name: str, truncate_size: │   │
│             │ │                                                │ int = 100                                       │   │
│             │ │                                                │    437      ) -> Optional:                      │   │
│             │ │                                                │    438          """Get the weights of the       │   │
│             │ │                                                │ parameter by its name. Similar to               │   │
│             │ │                                                │ `get_parameter` in Hugging Face.                │   │
│             │ │                                                │    439                                          │   │
│             │ │                                                │    440          Only used for unit test with an │   │
│             │ │                                                │ unoptimized performance.                        │   │
│             │ │                                                │    441          For optimized performance,      │   │
│             │ │                                                │ please use torch.save and torch.load.           │   │
│             │ │                                                │    442          """                             │   │
│             │ │                                                │    443          # TODO: (chenyang) Add support  │   │
│             │ │                                                │ for Qwen models.                                │   │
│             │ │                                                │    444          try:                            │   │
│             │ │                                                │    445              return                      │   │
│             │ │                                                │ self.model.get_weights_by_name(                 │   │
│             │ │                                                │    446                  name, truncate_size,    │   │
│             │ │                                                │ tp_size=self.tp_size                            │   │
│             │ │                                                │    447              )                           │   │
│             │ │                                                │    448          except Exception as e:          │   │
│             │ │                                                │    449              logger.error(f"Error when   │   │
│             │ │                                                │ getting parameter {name}: {e}")                 │   │
│             │ │                                                │    450              return None                 │   │
│             │ │                                                │    451                                          │   │
│             │ │                                                │    452      def init_lora_manager(self):        │   │
│             │ │                                                │    453          self.lora_manager =             │   │
│             │ │                                                │ LoRAManager(                                    │   │
│             │ │                                                │    454              base_model=self.model,      │   │
│             │ │                                                │    455                                          │   │
│             │ │                                                │ lora_paths=self.server_args.lora_paths,         │   │
│             │ │                                                │    456                                          │   │
│             │ │                                                │ base_hf_config=self.model_config.hf_config,     │   │
│             │ │                                                │    457                                          │   │
│             │ │                                                │ max_loras_per_batch=self.server_args.max_loras… │   │
│             │ │                                                │    458                                          │   │
│             │ │                                                │ load_config=self.load_config,                   │   │
│             │ │                                                │    459              dtype=self.dtype,           │   │
│             │ │                                                │    460          )                               │   │
│             │ │                                                │    461          logger.info("LoRA manager       │   │
│             │ │                                                │ ready.")                                        │   │
│             │ │                                                │    462                                          │   │
│             │ │                                                │    463      def profile_max_num_token(self,     │   │
│             │ │                                                │ total_gpu_memory: int):                         │   │
│             │ │                                                │    464          available_gpu_memory =          │   │
│             │ │                                                │ get_available_gpu_memory(                       │   │
│             │ │                                                │    465              self.device, self.gpu_id,   │   │
│             │ │                                                │ distributed=self.tp_size > 1                    │   │
│             │ │                                                │    466          )                               │   │
│             │ │                                                │    467          if (                            │   │
│             │ │                                                │    468                                          │   │
│             │ │                                                │ self.model_config.attention_arch ==             │   │
│             │ │                                                │ AttentionArch.MLA                               │   │
│             │ │                                                │    469              and not                     │   │
│             │ │                                                │ self.server_args.disable_mla                    │   │
│             │ │                                                │    470          ):                              │   │
│             │ │                                                │    471              cell_size = (               │   │
│             │ │                                                │    472                                          │   │
│             │ │                                                │ (self.model_config.kv_lora_rank +               │   │
│             │ │                                                │ self.model_config.qk_rope_head_dim)             │   │
│             │ │                                                │    473                  *                       │   │
│             │ │                                                │ self.model_config.num_hidden_layers             │   │
│             │ │                                                │    474                  *                       │   │
│             │ │                                                │ torch._utils._element_size(self.kv_cache_dtype) │   │
│             │ │                                                │    475              )                           │   │
│             │ │                                                │    476          else:                           │   │
│             │ │                                                │    477              cell_size = (               │   │
│             │ │                                                │    478                                          │   │
│             │ │                                                │ self.model_config.get_num_kv_heads(self.tp_siz… │   │
│             │ │                                                │    479                  *                       │   │
│             │ │                                                │ self.model_config.head_dim                      │   │
│             │ │                                                │    480                  *                       │   │
│             │ │                                                │ self.model_config.num_hidden_layers             │   │
│             │ │                                                │    481                  * 2                     │   │
│             │ │                                                │    482                  *                       │   │
│             │ │                                                │ torch._utils._element_size(self.kv_cache_dtype) │   │
│             │ │                                                │    483              )                           │   │
│             │ │                                                │    484          rest_memory =                   │   │
│             │ │                                                │ available_gpu_memory - total_gpu_memory * (     │   │
│             │ │                                                │    485              1 -                         │   │
│             │ │                                                │ self.mem_fraction_static                        │   │
│             │ │                                                │    486          )                               │   │
│             │ │                                                │    487          max_num_token = int(rest_memory │   │
│             │ │                                                │ * (1 << 30) // cell_size)                       │   │
│             │ │                                                │    488          return max_num_token            │   │
│             │ │                                                │    489                                          │   │
│             │ │                                                │    490      def init_memory_pool(               │   │
│             │ │                                                │    491          self,                           │   │
│             │ │                                                │    492          total_gpu_memory: int,          │   │
│             │ │                                                │    493          max_num_reqs: Optional = None,  │   │
│             │ │                                                │    494          max_total_tokens: Optional =    │   │
│             │ │                                                │ None,                                           │   │
│             │ │                                                │    495      ):                                  │   │
│             │ │                                                │    496          if                              │   │
│             │ │                                                │ self.server_args.kv_cache_dtype == "auto":      │   │
│             │ │                                                │    497              self.kv_cache_dtype =       │   │
│             │ │                                                │ self.dtype                                      │   │
│             │ │                                                │    498          elif                            │   │
│             │ │                                                │ self.server_args.kv_cache_dtype == "fp8_e5m2":  │   │
│             │ │                                                │    499              if is_hip():  # Using       │   │
│             │ │                                                │ natively supported format                       │   │
│             │ │                                                │    500                  self.kv_cache_dtype =   │   │
│             │ │                                                │ torch.float8_e5m2fnuz                           │   │
│             │ │                                                │    501              else:                       │   │
│             │ │                                                │    502                  self.kv_cache_dtype =   │   │
│             │ │                                                │ torch.float8_e5m2                               │   │
│             │ │                                                │    503          else:                           │   │
│             │ │                                                │    504              raise ValueError(           │   │
│             │ │                                                │    505                  f"Unsupported           │   │
│             │ │                                                │ kv_cache_dtype:                                 │   │
│             │ │                                                │ {self.server_args.kv_cache_dtype}."             │   │
│             │ │                                                │    506              )                           │   │
│             │ │                                                │    507                                          │   │
│             │ │                                                │    508          self.max_total_num_tokens =     │   │
│             │ │                                                │ self.profile_max_num_token(total_gpu_memory)    │   │
│             │ │                                                │    509          if max_total_tokens is not      │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    510              if max_total_tokens >       │   │
│             │ │                                                │ self.max_total_num_tokens:                      │   │
│             │ │                                                │    511                  logging.warning(        │   │
│             │ │                                                │    512                                          │   │
│             │ │                                                │ f"max_total_tokens={max_total_tokens} is larger │   │
│             │ │                                                │ than the profiled value "                       │   │
│             │ │                                                │    513                                          │   │
│             │ │                                                │ f"{self.max_total_num_tokens}. "                │   │
│             │ │                                                │    514                      f"Use the profiled  │   │
│             │ │                                                │ value instead."                                 │   │
│             │ │                                                │    515                  )                       │   │
│             │ │                                                │    516              self.max_total_num_tokens = │   │
│             │ │                                                │ min(self.max_total_num_tokens,                  │   │
│             │ │                                                │ max_total_tokens)                               │   │
│             │ │                                                │    517                                          │   │
│             │ │                                                │    518          if self.max_total_num_tokens <= │   │
│             │ │                                                │ 0:                                              │   │
│             │ │                                                │    519              raise RuntimeError(         │   │
│             │ │                                                │    520                  "Not enough memory.     │   │
│             │ │                                                │ Please try to increase --mem-fraction-static."  │   │
│             │ │                                                │    521              )                           │   │
│             │ │                                                │    522                                          │   │
│             │ │                                                │    523          if max_num_reqs is None:        │   │
│             │ │                                                │    524              max_num_reqs = min(         │   │
│             │ │                                                │    525                  max(                    │   │
│             │ │                                                │    526                      int(                │   │
│             │ │                                                │    527                                          │   │
│             │ │                                                │ self.max_total_num_tokens /                     │   │
│             │ │                                                │ self.model_config.context_len * 512             │   │
│             │ │                                                │    528                      ),                  │   │
│             │ │                                                │    529                      2048,               │   │
│             │ │                                                │    530                  ),                      │   │
│             │ │                                                │    531                  4096,                   │   │
│             │ │                                                │    532              )                           │   │
│             │ │                                                │    533                                          │   │
│             │ │                                                │    534          self.req_to_token_pool =        │   │
│             │ │                                                │ ReqToTokenPool(                                 │   │
│             │ │                                                │    535              size=max_num_reqs + 1,      │   │
│             │ │                                                │    536                                          │   │
│             │ │                                                │ max_context_len=self.model_config.context_len + │   │
│             │ │                                                │ 4,                                              │   │
│             │ │                                                │    537              device=self.device,         │   │
│             │ │                                                │    538              use_records=False,          │   │
│             │ │                                                │    539          )                               │   │
│             │ │                                                │    540          if (                            │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 95282 Output: 1158                                                                              │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 7: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """Common utilities."""                 │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import base64                           │   │
│             │ │                                                │     17  import dataclasses                      │   │
│             │ │                                                │     18  import ipaddress                        │   │
│             │ │                                                │     19  import itertools                        │   │
│             │ │                                                │     20  import json                             │   │
│             │ │                                                │     21  import logging                          │   │
│             │ │                                                │     22  import os                               │   │
│             │ │                                                │     23  import pickle                           │   │
│             │ │                                                │     24  import random                           │   │
│             │ │                                                │     25  import re                               │   │
│             │ │                                                │     26  import resource                         │   │
│             │ │                                                │     27  import shutil                           │   │
│             │ │                                                │     28  import signal                           │   │
│             │ │                                                │     29  import socket                           │   │
│             │ │                                                │     30  import subprocess                       │   │
│             │ │                                                │     31  import tempfile                         │   │
│             │ │                                                │     32  import time                             │   │
│             │ │                                                │     33  import warnings                         │   │
│             │ │                                                │     34  from functools import lru_cache         │   │
│             │ │                                                │     35  from importlib.metadata import          │   │
│             │ │                                                │ PackageNotFoundError, version                   │   │
│             │ │                                                │     36  from io import BytesIO                  │   │
│             │ │                                                │     37  from typing import Any, Callable, Dict, │   │
│             │ │                                                │ List, Optional, Protocol, Tuple, Union          │   │
│             │ │                                                │     38                                          │   │
│             │ │                                                │     39  import numpy as np                      │   │
│             │ │                                                │     40  import psutil                           │   │
│             │ │                                                │     41  import requests                         │   │
│             │ │                                                │     42  import torch                            │   │
│             │ │                                                │     43  import torch.distributed                │   │
│             │ │                                                │     44  import torch.distributed as dist        │   │
│             │ │                                                │     45  import triton                           │   │
│             │ │                                                │     46  import zmq                              │   │
│             │ │                                                │     47  from fastapi.responses import           │   │
│             │ │                                                │ ORJSONResponse                                  │   │
│             │ │                                                │     48  from packaging import version as        │   │
│             │ │                                                │ pkg_version                                     │   │
│             │ │                                                │     49  from starlette.routing import Mount     │   │
│             │ │                                                │     50  from torch import nn                    │   │
│             │ │                                                │     51  from torch.func import functional_call  │   │
│             │ │                                                │     52  from torch.library import Library       │   │
│             │ │                                                │     53  from torch.profiler import              │   │
│             │ │                                                │ ProfilerActivity, profile, record_function      │   │
│             │ │                                                │     54  from triton.runtime.cache import (      │   │
│             │ │                                                │     55      FileCacheManager,                   │   │
│             │ │                                                │     56      default_cache_dir,                  │   │
│             │ │                                                │     57      default_dump_dir,                   │   │
│             │ │                                                │     58      default_override_dir,               │   │
│             │ │                                                │     59  )                                       │   │
│             │ │                                                │     60                                          │   │
│             │ │                                                │     61  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     62                                          │   │
│             │ │                                                │     63                                          │   │
│             │ │                                                │     64  show_time_cost = False                  │   │
│             │ │                                                │     65  time_infos = {}                         │   │
│             │ │                                                │     66                                          │   │
│             │ │                                                │     67                                          │   │
│             │ │                                                │     68  def is_hip() -> bool:                   │   │
│             │ │                                                │     69      """Return whether it is HIP on the  │   │
│             │ │                                                │ AMD ROCm platform."""                           │   │
│             │ │                                                │     70      return torch.version.hip is not     │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │     71                                          │   │
│             │ │                                                │     72                                          │   │
│             │ │                                                │     73  def is_cuda():                          │   │
│             │ │                                                │     74      return hasattr(torch, "cuda") and   │   │
│             │ │                                                │ torch.cuda.is_available()                       │   │
│             │ │                                                │     75                                          │   │
│             │ │                                                │     76                                          │   │
│             │ │                                                │     77  def is_cuda_alike():                    │   │
│             │ │                                                │     78      return is_cuda() or is_hip()        │   │
│             │ │                                                │     79                                          │   │
│             │ │                                                │     80                                          │   │
│             │ │                                                │     81  def is_hpu() -> bool:                   │   │
│             │ │                                                │     82      return hasattr(torch, "hpu") and    │   │
│             │ │                                                │ torch.hpu.is_available()                        │   │
│             │ │                                                │     83                                          │   │
│             │ │                                                │     84                                          │   │
│             │ │                                                │     85  def is_xpu() -> bool:                   │   │
│             │ │                                                │     86      return hasattr(torch, "xpu") and    │   │
│             │ │                                                │ torch.xpu.is_available()                        │   │
│             │ │                                                │     87                                          │   │
│             │ │                                                │     88                                          │   │
│             │ │                                                │     89  def is_flashinfer_available():          │   │
│             │ │                                                │     90      """                                 │   │
│             │ │                                                │     91      Check whether flashinfer is         │   │
│             │ │                                                │ available.                                      │   │
│             │ │                                                │     92      As of Oct. 6, 2024, it is only      │   │
│             │ │                                                │ available on NVIDIA GPUs.                       │   │
│             │ │                                                │     93      """                                 │   │
│             │ │                                                │     94      if not                              │   │
│             │ │                                                │ get_bool_env_var("SGLANG_IS_FLASHINFER_AVAILAB… │   │
│             │ │                                                │ default="true"):                                │   │
│             │ │                                                │     95          return False                    │   │
│             │ │                                                │     96      return torch.cuda.is_available()    │   │
│             │ │                                                │ and torch.version.cuda                          │   │
│             │ │                                                │     97                                          │   │
│             │ │                                                │     98                                          │   │
│             │ │                                                │     99  def is_ipv6(address):                   │   │
│             │ │                                                │    100      try:                                │   │
│             │ │                                                │    101          ipaddress.IPv6Address(address)  │   │
│             │ │                                                │    102          return True                     │   │
│             │ │                                                │    103      except ipaddress.AddressValueError: │   │
│             │ │                                                │    104          return False                    │   │
│             │ │                                                │    105                                          │   │
│             │ │                                                │    106                                          │   │
│             │ │                                                │    107  def enable_show_time_cost():            │   │
│             │ │                                                │    108      global show_time_cost               │   │
│             │ │                                                │    109      show_time_cost = True               │   │
│             │ │                                                │    110                                          │   │
│             │ │                                                │    111                                          │   │
│             │ │                                                │    112  class TimeInfo:                         │   │
│             │ │                                                │    113      def __init__(self, name,            │   │
│             │ │                                                │ interval=0.1, color=0, indent=0):               │   │
│             │ │                                                │    114          self.name = name                │   │
│             │ │                                                │    115          self.interval = interval        │   │
│             │ │                                                │    116          self.color = color              │   │
│             │ │                                                │    117          self.indent = indent            │   │
│             │ │                                                │    118                                          │   │
│             │ │                                                │    119          self.acc_time = 0               │   │
│             │ │                                                │    120          self.last_acc_time = 0          │   │
│             │ │                                                │    121                                          │   │
│             │ │                                                │    122      def check(self):                    │   │
│             │ │                                                │    123          if self.acc_time -              │   │
│             │ │                                                │ self.last_acc_time > self.interval:             │   │
│             │ │                                                │    124              self.last_acc_time =        │   │
│             │ │                                                │ self.acc_time                                   │   │
│             │ │                                                │    125              return True                 │   │
│             │ │                                                │    126          return False                    │   │
│             │ │                                                │    127                                          │   │
│             │ │                                                │    128      def pretty_print(self):             │   │
│             │ │                                                │    129          print(f"\x1b[{self.color}m",    │   │
│             │ │                                                │ end="")                                         │   │
│             │ │                                                │    130          print("-" * self.indent * 2,    │   │
│             │ │                                                │ end="")                                         │   │
│             │ │                                                │    131          print(f"{self.name}:            │   │
│             │ │                                                │ {self.acc_time:.3f}s\x1b[0m")                   │   │
│             │ │                                                │    132                                          │   │
│             │ │                                                │    133                                          │   │
│             │ │                                                │    134  def mark_start(name, interval=0.1,      │   │
│             │ │                                                │ color=0, indent=0):                             │   │
│             │ │                                                │    135      global time_infos, show_time_cost   │   │
│             │ │                                                │    136      if not show_time_cost:              │   │
│             │ │                                                │    137          return                          │   │
│             │ │                                                │    138      torch.cuda.synchronize()            │   │
│             │ │                                                │    139      if time_infos.get(name, None) is    │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    140          time_infos = TimeInfo(name,     │   │
│             │ │                                                │ interval, color, indent)                        │   │
│             │ │                                                │    141      time_infos.acc_time -= time.time()  │   │
│             │ │                                                │    142                                          │   │
│             │ │                                                │    143                                          │   │
│             │ │                                                │    144  def mark_end(name):                     │   │
│             │ │                                                │    145      global time_infos, show_time_cost   │   │
│             │ │                                                │    146      if not show_time_cost:              │   │
│             │ │                                                │    147          return                          │   │
│             │ │                                                │    148      torch.cuda.synchronize()            │   │
│             │ │                                                │    149      time_infos.acc_time += time.time()  │   │
│             │ │                                                │    150      if time_infos.check():              │   │
│             │ │                                                │    151          time_infos.pretty_print()       │   │
│             │ │                                                │    152                                          │   │
│             │ │                                                │    153                                          │   │
│             │ │                                                │    154  def calculate_time(show=False,          │   │
│             │ │                                                │ min_cost_ms=0.0):                               │   │
│             │ │                                                │    155      def wrapper(func):                  │   │
│             │ │                                                │    156          def inner_func(*args,           │   │
│             │ │                                                │ **kwargs):                                      │   │
│             │ │                                                │    157              torch.cuda.synchronize()    │   │
│             │ │                                                │    158              if show:                    │   │
│             │ │                                                │    159                  start_time =            │   │
│             │ │                                                │ time.time()                                     │   │
│             │ │                                                │    160              result = func(*args,        │   │
│             │ │                                                │ **kwargs)                                       │   │
│             │ │                                                │    161              torch.cuda.synchronize()    │   │
│             │ │                                                │    162              if show:                    │   │
│             │ │                                                │    163                  cost_time =             │   │
│             │ │                                                │ (time.time() - start_time) * 1000               │   │
│             │ │                                                │    164                  if cost_time >          │   │
│             │ │                                                │ min_cost_ms:                                    │   │
│             │ │                                                │    165                      print(f"Function    │   │
│             │ │                                                │ {func.__name__} took {cost_time} ms to run.")   │   │
│             │ │                                                │    166              return result               │   │
│             │ │                                                │    167                                          │   │
│             │ │                                                │    168          return inner_func               │   │
│             │ │                                                │    169                                          │   │
│             │ │                                                │    170      return wrapper                      │   │
│             │ │                                                │    171                                          │   │
│             │ │                                                │    172                                          │   │
│             │ │                                                │    173  def get_available_gpu_memory(device,    │   │
│             │ │                                                │ gpu_id, distributed=False, empty_cache=True):   │   │
│             │ │                                                │    174      """                                 │   │
│             │ │                                                │    175      Get available memory for            │   │
│             │ │                                                │ cuda:gpu_id device.                             │   │
│             │ │                                                │    176      When distributed is True, the       │   │
│             │ │                                                │ available memory is the minimum available       │   │
│             │ │                                                │ memory of all GPUs.                             │   │
│             │ │                                                │    177      """                                 │   │
│             │ │                                                │    178      if device == "cuda":                │   │
│             │ │                                                │    179          num_gpus =                      │   │
│             │ │                                                │ torch.cuda.device_count()                       │   │
│             │ │                                                │    180          assert gpu_id < num_gpus        │   │
│             │ │                                                │    181                                          │   │
│             │ │                                                │    182          if torch.cuda.current_device()  │   │
│             │ │                                                │ != gpu_id:                                      │   │
│             │ │                                                │    183              print(                      │   │
│             │ │                                                │    184                  f"WARNING: current      │   │
│             │ │                                                │ device is not {gpu_id}, but                     │   │
│             │ │                                                │ {torch.cuda.current_device()}, ",               │   │
│             │ │                                                │    185                  "which may cause        │   │
│             │ │                                                │ useless memory allocation for torch CUDA        │   │
│             │ │                                                │ context.",                                      │   │
│             │ │                                                │    186              )                           │   │
│             │ │                                                │    187                                          │   │
│             │ │                                                │    188          if empty_cache:                 │   │
│             │ │                                                │    189              torch.cuda.empty_cache()    │   │
│             │ │                                                │    190          free_gpu_memory, _ =            │   │
│             │ │                                                │ torch.cuda.mem_get_info(gpu_id)                 │   │
│             │ │                                                │    191                                          │   │
│             │ │                                                │    192      elif device == "xpu":               │   │
│             │ │                                                │    193          num_gpus =                      │   │
│             │ │                                                │ torch.xpu.device_count()                        │   │
│             │ │                                                │    194          assert gpu_id < num_gpus        │   │
│             │ │                                                │    195                                          │   │
│             │ │                                                │    196          if torch.xpu.current_device()   │   │
│             │ │                                                │ != gpu_id:                                      │   │
│             │ │                                                │    197              print(                      │   │
│             │ │                                                │    198                  f"WARNING: current      │   │
│             │ │                                                │ device is not {gpu_id}, but                     │   │
│             │ │                                                │ {torch.xpu.current_device()}, ",                │   │
│             │ │                                                │    199                  "which may cause        │   │
│             │ │                                                │ useless memory allocation for torch XPU         │   │
│             │ │                                                │ context.",                                      │   │
│             │ │                                                │    200              )                           │   │
│             │ │                                                │    201                                          │   │
│             │ │                                                │    202          if empty_cache:                 │   │
│             │ │                                                │    203              torch.xpu.empty_cache()     │   │
│             │ │                                                │    204          used_memory =                   │   │
│             │ │                                                │ torch.xpu.memory_allocated()                    │   │
│             │ │                                                │    205          total_gpu_memory =              │   │
│             │ │                                                │ torch.xpu.get_device_properties(gpu_id).total_… │   │
│             │ │                                                │    206          free_gpu_memory =               │   │
│             │ │                                                │ total_gpu_memory - used_memory                  │   │
│             │ │                                                │    207                                          │   │
│             │ │                                                │    208      elif device == "hpu":               │   │
│             │ │                                                │    209          num_gpus =                      │   │
│             │ │                                                │ torch.hpu.device_count()                        │   │
│             │ │                                                │    210          assert gpu_id < num_gpus        │   │
│             │ │                                                │    211                                          │   │
│             │ │                                                │    212          if torch.hpu.current_device()   │   │
│             │ │                                                │ != gpu_id:                                      │   │
│             │ │                                                │    213              print(                      │   │
│             │ │                                                │    214                  f"WARNING: current      │   │
│             │ │                                                │ device is not {gpu_id}, but                     │   │
│             │ │                                                │ {torch.hpu.current_device()}, ",                │   │
│             │ │                                                │    215                  "which may cause        │   │
│             │ │                                                │ useless memory allocation for torch HPU         │   │
│             │ │                                                │ context.",                                      │   │
│             │ │                                                │    216              )                           │   │
│             │ │                                                │    217                                          │   │
│             │ │                                                │    218          free_gpu_memory,                │   │
│             │ │                                                │ total_gpu_memory = torch.hpu.mem_get_info()     │   │
│             │ │                                                │    219                                          │   │
│             │ │                                                │    220      if distributed:                     │   │
│             │ │                                                │    221          tensor =                        │   │
│             │ │                                                │ torch.tensor(free_gpu_memory,                   │   │
│             │ │                                                │ dtype=torch.float32).to(                        │   │
│             │ │                                                │    222              torch.device(device,        │   │
│             │ │                                                │ gpu_id)                                         │   │
│             │ │                                                │    223          )                               │   │
│             │ │                                                │    224                                          │   │
│             │ │                                                │ torch.distributed.all_reduce(tensor,            │   │
│             │ │                                                │ op=torch.distributed.ReduceOp.MIN)              │   │
│             │ │                                                │    225          free_gpu_memory = tensor.item() │   │
│             │ │                                                │    226                                          │   │
│             │ │                                                │    227      return free_gpu_memory / (1 << 30)  │   │
│             │ │                                                │    228                                          │   │
│             │ │                                                │    229                                          │   │
│             │ │                                                │    230  def is_pin_memory_available() -> bool:  │   │
│             │ │                                                │    231      return torch.cuda.is_available()    │   │
│             │ │                                                │    232                                          │   │
│             │ │                                                │    233                                          │   │
│             │ │                                                │    234  _CPU_OFFLOAD_BYTES = 0                  │   │
│             │ │                                                │    235  _CPU_OFFLOAD_MAX_BYTES = 0              │   │
│             │ │                                                │    236                                          │   │
│             │ │                                                │    237                                          │   │
│             │ │                                                │    238  def                                     │   │
│             │ │                                                │ set_cpu_offload_max_bytes(max_bytes: int) ->    │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    239      global _CPU_OFFLOAD_MAX_BYTES,      │   │
│             │ │                                                │ _CPU_OFFLOAD_BYTES                              │   │
│             │ │                                                │    240      _CPU_OFFLOAD_BYTES = 0              │   │
│             │ │                                                │    241      _CPU_OFFLOAD_MAX_BYTES = max_bytes  │   │
│             │ │                                                │    242                                          │   │
│             │ │                                                │    243                                          │   │
│             │ │                                                │    244  def maybe_offload_to_cpu(module:        │   │
│             │ │                                                │ torch.nn.Module) -> torch.nn.Module:            │   │
│             │ │                                                │    245      device =                            │   │
│             │ │                                                │ next(module.parameters()).device                │   │
│             │ │                                                │    246                                          │   │
│             │ │                                                │    247      if device == torch.device("cpu"):   │   │
│             │ │                                                │    248          return module                   │   │
│             │ │                                                │    249                                          │   │
│             │ │                                                │    250      global _CPU_OFFLOAD_MAX_BYTES,      │   │
│             │ │                                                │ _CPU_OFFLOAD_BYTES                              │   │
│             │ │                                                │    251      if _CPU_OFFLOAD_BYTES >=            │   │
│             │ │                                                │ _CPU_OFFLOAD_MAX_BYTES:                         │   │
│             │ │                                                │    252          return module                   │   │
│             │ │                                                │    253                                          │   │
│             │ │                                                │    254      pin_memory =                        │   │
│             │ │                                                │ is_pin_memory_available()                       │   │
│             │ │                                                │    255      # offload parameters to CPU         │   │
│             │ │                                                │    256      # use pin_memory if possible, which │   │
│             │ │                                                │ helps cudagraph capture speed                   │   │
│             │ │                                                │    257      offloaded_parameters = False        │   │
│             │ │                                                │    258      for p in module.parameters():       │   │
│             │ │                                                │    259          if _CPU_OFFLOAD_BYTES >=        │   │
│             │ │                                                │ _CPU_OFFLOAD_MAX_BYTES:                         │   │
│             │ │                                                │    260              # we use per-parameter      │   │
│             │ │                                                │ offloading                                      │   │
│             │ │                                                │    261              # one module might have     │   │
│             │ │                                                │ some parameters offloaded and some not          │   │
│             │ │                                                │    262              break                       │   │
│             │ │                                                │    263                                          │   │
│             │ │                                                │    264          # `torch.empty_like` does not   │   │
│             │ │                                                │ support `pin_memory` argument                   │   │
│             │ │                                                │    265          cpu_data = torch.empty_strided( │   │
│             │ │                                                │    266              size=p.data.size(),         │   │
│             │ │                                                │    267              stride=p.data.stride(),     │   │
│             │ │                                                │    268              dtype=p.data.dtype,         │   │
│             │ │                                                │    269              layout=p.data.layout,       │   │
│             │ │                                                │    270              device="cpu",               │   │
│             │ │                                                │    271              pin_memory=pin_memory,      │   │
│             │ │                                                │    272          )                               │   │
│             │ │                                                │    273          cpu_data.copy_(p.data)          │   │
│             │ │                                                │    274          p.data = cpu_data               │   │
│             │ │                                                │    275          _CPU_OFFLOAD_BYTES +=           │   │
│             │ │                                                │ p.data.numel() * p.data.element_size()          │   │
│             │ │                                                │    276          offloaded_parameters = True     │   │
│             │ │                                                │    277                                          │   │
│             │ │                                                │    278      if offloaded_parameters:            │   │
│             │ │                                                │    279          original_forward =              │   │
│             │ │                                                │ module.forward                                  │   │
│             │ │                                                │    280                                          │   │
│             │ │                                                │    281          def forward(*args, **kwargs):   │   │
│             │ │                                                │    282              module.forward =            │   │
│             │ │                                                │ original_forward                                │   │
│             │ │                                                │    283              device_state = {            │   │
│             │ │                                                │    284                  # here we blindly call  │   │
│             │ │                                                │ `to(device)`                                    │   │
│             │ │                                                │    285                  # if the parameter is   │   │
│             │ │                                                │ already on the device, it will be a no-op       │   │
│             │ │                                                │    286                  k: v.to(device,         │   │
│             │ │                                                │ non_blocking=True)                              │   │
│             │ │                                                │    287                  for k, v in             │   │
│             │ │                                                │ module.state_dict().items()                     │   │
│             │ │                                                │    288              }                           │   │
│             │ │                                                │    289              output =                    │   │
│             │ │                                                │ functional_call(module, device_state,           │   │
│             │ │                                                │ args=args, kwargs=kwargs)                       │   │
│             │ │                                                │    290              module.forward = forward    │   │
│             │ │                                                │    291              return output               │   │
│             │ │                                                │    292                                          │   │
│             │ │                                                │    293          module.forward = forward        │   │
│             │ │                                                │    294                                          │   │
│             │ │                                                │    295      return module                       │   │
│             │ │                                                │    296                                          │   │
│             │ │                                                │    297                                          │   │
│             │ │                                                │    298  class LayerFn(Protocol):                │   │
│             │ │                                                │    299                                          │   │
│             │ │                                                │    300      def __call__(self, layer_id: int,   │   │
│             │ │                                                │ prefix: str) -> torch.nn.Module: ...            │   │
│             │ │                                                │    301                                          │   │
│             │ │                                                │    302                                          │   │
│             │ │                                                │    303  def make_layers(                        │   │
│             │ │                                                │    304      num_hidden_layers: int,             │   │
│             │ │                                                │    305      layer_fn: LayerFn,                  │   │
│             │ │                                                │    306      prefix: str = "",                   │   │
│             │ │                                                │    307  ) -> Tuple:                             │   │
│             │ │                                                │    308      """Make a list of layers with the   │   │
│             │ │                                                │ given layer function"""                         │   │
│             │ │                                                │    309      modules = torch.nn.ModuleList(      │   │
│             │ │                                                │    310          [                               │   │
│             │ │                                                │    311                                          │   │
│             │ │                                                │ maybe_offload_to_cpu(layer_fn(idx=idx,          │   │
│             │ │                                                │ prefix=f"{prefix}.{idx}"))                      │   │
│             │ │                                                │    312              for idx in                  │   │
│             │ │                                                │ range(num_hidden_layers)                        │   │
│             │ │                                                │    313          ]                               │   │
│             │ │                                                │    314      )                                   │   │
│             │ │                                                │    315      return modules                      │   │
│             │ │                                                │    316                                          │   │
│             │ │                                                │    317                                          │   │
│             │ │                                                │    318  def set_random_seed(seed: int) -> None: │   │
│             │ │                                                │    319      """Set the random seed for all      │   │
│             │ │                                                │ libraries."""                                   │   │
│             │ │                                                │    320      random.seed(seed)                   │   │
│             │ │                                                │    321      np.random.seed(seed)                │   │
│             │ │                                                │    322      torch.manual_seed(seed)             │   │
│             │ │                                                │    323      if torch.cuda.is_available():       │   │
│             │ │                                                │    324                                          │   │
│             │ │                                                │ torch.cuda.manual_seed_all(seed)                │   │
│             │ │                                                │    325                                          │   │
│             │ │                                                │    326                                          │   │
│             │ │                                                │    327  def is_port_available(port):            │   │
│             │ │                                                │    328      """Return whether a port is         │   │
│             │ │                                                │ available."""                                   │   │
│             │ │                                                │    329      with socket.socket(socket.AF_INET,  │   │
│             │ │                                                │ socket.SOCK_STREAM) as s:                       │   │
│             │ │                                                │    330          try:                            │   │
│             │ │                                                │    331                                          │   │
│             │ │                                                │ s.setsockopt(socket.SOL_SOCKET,                 │   │
│             │ │                                                │ socket.SO_REUSEADDR, 1)                         │   │
│             │ │                                                │    332              s.bind(("", port))          │   │
│             │ │                                                │    333              s.listen(1)                 │   │
│             │ │                                                │    334              return True                 │   │
│             │ │                                                │    335          except socket.error:            │   │
│             │ │                                                │    336              return False                │   │
│             │ │                                                │    337                                          │   │
│             │ │                                                │    338                                          │   │
│             │ │                                                │    339  def decode_video_base64(video_base64):  │   │
│             │ │                                                │    340      from PIL import Image               │   │
│             │ │                                                │    341                                          │   │
│             │ │                                                │    342      # Decode the base64 string          │   │
│             │ │                                                │    343      video_bytes =                       │   │
│             │ │                                                │ base64.b64decode(video_base64)                  │   │
│             │ │                                                │    344                                          │   │
│             │ │                                                │    345      # Placeholder for the start indices │   │
│             │ │                                                │ of each PNG image                               │   │
│             │ │                                                │    346      img_starts = []                     │   │
│             │ │                                                │    347                                          │   │
│             │ │                                                │    348      frame_format = "PNG"  #             │   │
│             │ │                                                │ str(os.getenv('FRAME_FORMAT', "JPEG"))          │   │
│             │ │                                                │    349                                          │   │
│             │ │                                                │    350      assert frame_format in [            │   │
│             │ │                                                │    351          "PNG",                          │   │
│             │ │                                                │    352          "JPEG",                         │   │
│             │ │                                                │    353      ], "FRAME_FORMAT must be either     │   │
│             │ │                                                │ 'PNG' or 'JPEG'"                                │   │
│             │ │                                                │    354                                          │   │
│             │ │                                                │    355      if frame_format == "PNG":           │   │
│             │ │                                                │    356          # Find each PNG start signature │   │
│             │ │                                                │ to isolate images                               │   │
│             │ │                                                │    357          i = 0                           │   │
│             │ │                                                │    358          while i < len(video_bytes) - 7: │   │
│             │ │                                                │ # Adjusted for the length of the PNG signature  │   │
│             │ │                                                │    359              # Check if we found the     │   │
│             │ │                                                │ start of a PNG file                             │   │
│             │ │                                                │    360              if (                        │   │
│             │ │                                                │    361                  video_bytes == 0x89     │   │
│             │ │                                                │    362                  and video_bytes == 0x50 │   │
│             │ │                                                │    363                  and video_bytes == 0x4E │   │
│             │ │                                                │    364                  and video_bytes == 0x47 │   │
│             │ │                                                │    365                  and video_bytes == 0x0D │   │
│             │ │                                                │    366                  and video_bytes == 0x0A │   │
│             │ │                                                │    367                  and video_bytes == 0x1A │   │
│             │ │                                                │    368                  and video_bytes == 0x0A │   │
│             │ │                                                │    369              ):                          │   │
│             │ │                                                │    370                  img_starts.append(i)    │   │
│             │ │                                                │    371                  i += 8  # Skip the PNG  │   │
│             │ │                                                │ signature                                       │   │
│             │ │                                                │    372              else:                       │   │
│             │ │                                                │    373                  i += 1                  │   │
│             │ │                                                │    374      else:                               │   │
│             │ │                                                │    375          # Find each JPEG start (0xFFD8) │   │
│             │ │                                                │ to isolate images                               │   │
│             │ │                                                │    376          i = 0                           │   │
│             │ │                                                │    377          while (                         │   │
│             │ │                                                │    378              i < len(video_bytes) - 1    │   │
│             │ │                                                │    379          ):  # Adjusted for the length   │   │
│             │ │                                                │ of the JPEG SOI signature                       │   │
│             │ │                                                │    380              # Check if we found the     │   │
│             │ │                                                │ start of a JPEG file                            │   │
│             │ │                                                │    381              if video_bytes == 0xFF and  │   │
│             │ │                                                │ video_bytes == 0xD8:                            │   │
│             │ │                                                │    382                  img_starts.append(i)    │   │
│             │ │                                                │    383                  # Move to the next byte │   │
│             │ │                                                │ to continue searching for the next image start  │   │
│             │ │                                                │    384                  i += 2                  │   │
│             │ │                                                │    385              else:                       │   │
│             │ │                                                │    386                  i += 1                  │   │
│             │ │                                                │    387                                          │   │
│             │ │                                                │    388      frames = []                         │   │
│             │ │                                                │    389      for start_idx in img_starts:        │   │
│             │ │                                                │    390          # Assuming each image is        │   │
│             │ │                                                │ back-to-back, the end of one image is the start │   │
│             │ │                                                │ of another                                      │   │
│             │ │                                                │    391          # The last image goes until the │   │
│             │ │                                                │ end of the byte string                          │   │
│             │ │                                                │    392          end_idx = (                     │   │
│             │ │                                                │    393              img_starts                  │   │
│             │ │                                                │    394              if                          │   │
│             │ │                                                │ img_starts.index(start_idx) + 1 <               │   │
│             │ │                                                │ len(img_starts)                                 │   │
│             │ │                                                │    395              else len(video_bytes)       │   │
│             │ │                                                │    396          )                               │   │
│             │ │                                                │    397          img_bytes = video_bytes         │   │
│             │ │                                                │    398                                          │   │
│             │ │                                                │    399          # Convert bytes to a PIL Image  │   │
│             │ │                                                │    400          img =                           │   │
│             │ │                                                │ Image.open(BytesIO(img_bytes))                  │   │
│             │ │                                                │    401                                          │   │
│             │ │                                                │    402          # Convert PIL Image to a NumPy  │   │
│             │ │                                                │ array                                           │   │
│             │ │                                                │    403          frame = np.array(img)           │   │
│             │ │                                                │    404                                          │   │
│             │ │                                                │    405          # Append the frame to the list  │   │
│             │ │                                                │ of frames                                       │   │
│             │ │                                                │    406          frames.append(frame)            │   │
│             │ │                                                │    407                                          │   │
│             │ │                                                │    408      # Ensure there's at least one frame │   │
│             │ │                                                │ to avoid errors with np.stack                   │   │
│             │ │                                                │    409      if frames:                          │   │
│             │ │                                                │    410          return np.stack(frames,         │   │
│             │ │                                                │ axis=0), img.size                               │   │
│             │ │                                                │    411      else:                               │   │
│             │ │                                                │    412          return np.array([]), (          │   │
│             │ │                                                │    413              0,                          │   │
│             │ │                                                │    414              0,                          │   │
│             │ │                                                │    415          )  # Return an empty array and  │   │
│             │ │                                                │ size tuple if no frames were found              │   │
│             │ │                                                │    416                                          │   │
│             │ │                                                │    417                                          │   │
│             │ │                                                │    418  def load_image(image_file: Union):      │   │
│             │ │                                                │    419      from PIL import Image               │   │
│             │ │                                                │    420                                          │   │
│             │ │                                                │    421      image = image_size = None           │   │
│             │ │                                                │    422                                          │   │
│             │ │                                                │    423      if isinstance(image_file, bytes):   │   │
│             │ │                                                │    424          image =                         │   │
│             │ │                                                │ Image.open(BytesIO(image_file))                 │   │
│             │ │                                                │    425      elif                                │   │
│             │ │                                                │ image_file.startswith("http://") or             │   │
│             │ │                                                │ image_file.startswith("https://"):              │   │
│             │ │                                                │    426          timeout =                       │   │
│             │ │                                                │ int(os.getenv("REQUEST_TIMEOUT", "3"))          │   │
│             │ │                                                │    427          response =                      │   │
│             │ │                                                │ requests.get(image_file, timeout=timeout)       │   │
│             │ │                                                │    428          image =                         │   │
│             │ │                                                │ Image.open(BytesIO(response.content))           │   │
│             │ │                                                │    429      elif                                │   │
│             │ │                                                │ image_file.lower().endswith(("png", "jpg",      │   │
│             │ │                                                │ "jpeg", "webp", "gif")):                        │   │
│             │ │                                                │    430          image = Image.open(image_file)  │   │
│             │ │                                                │    431      elif                                │   │
│             │ │                                                │ image_file.startswith("data:"):                 │   │
│             │ │                                                │    432          image_file =                    │   │
│             │ │                                                │ image_file.split(",")[1]                        │   │
│             │ │                                                │    433          image =                         │   │
│             │ │                                                │ Image.open(BytesIO(base64.b64decode(image_file… │   │
│             │ │                                                │    434      elif                                │   │
│             │ │                                                │ image_file.startswith("video:"):                │   │
│             │ │                                                │    435          image_file =                    │   │
│             │ │                                                │ image_file.replace("video:", "")                │   │
│             │ │                                                │    436          image, image_size =             │   │
│             │ │                                                │ decode_video_base64(image_file)                 │   │
│             │ │                                                │    437      elif isinstance(image_file, str):   │   │
│             │ │                                                │    438          image =                         │   │
│             │ │                                                │ Image.open(BytesIO(base64.b64decode(image_file… │   │
│             │ │                                                │    439      else:                               │   │
│             │ │                                                │    440          raise ValueError(f"Invalid      │   │
│             │ │                                                │ image: {image}")                                │   │
│             │ │                                                │    441                                          │   │
│             │ │                                                │    442      return image, image_size            │   │
│             │ │                                                │    443                                          │   │
│             │ │                                                │    444                                          │   │
│             │ │                                                │    445  def suppress_other_loggers():           │   │
│             │ │                                                │    446      from vllm.logger import logger as   │   │
│             │ │                                                │ vllm_default_logger                             │   │
│             │ │                                                │    447                                          │   │
│             │ │                                                │    448                                          │   │
│             │ │                                                │ vllm_default_logger.setLevel(logging.WARN)      │   │
│             │ │                                                │    449                                          │   │
│             │ │                                                │ logging.getLogger("vllm.distributed.device_com… │   │
│             │ │                                                │    450          logging.WARN                    │   │
│             │ │                                                │    451      )                                   │   │
│             │ │                                                │    452                                          │   │
│             │ │                                                │ logging.getLogger("vllm.distributed.device_com… │   │
│             │ │                                                │    453          logging.WARN                    │   │
│             │ │                                                │    454      )                                   │   │
│             │ │                                                │    455                                          │   │
│             │ │                                                │    456      warnings.filterwarnings(            │   │
│             │ │                                                │    457          "ignore", category=UserWarning, │   │
│             │ │                                                │ message="The given NumPy array is not writable" │   │
│             │ │                                                │    458      )                                   │   │
│             │ │                                                │    459                                          │   │
│             │ │                                                │    460                                          │   │
│             │ │                                                │    461  def assert_pkg_version(pkg: str,        │   │
│             │ │                                                │ min_version: str, message: str):                │   │
│             │ │                                                │    462      try:                                │   │
│             │ │                                                │    463          installed_version =             │   │
│             │ │                                                │ version(pkg)                                    │   │
│             │ │                                                │    464          if                              │   │
│             │ │                                                │ pkg_version.parse(installed_version) <          │   │
│             │ │                                                │ pkg_version.parse(min_version):                 │   │
│             │ │                                                │    465              raise Exception(            │   │
│             │ │                                                │    466                  f"{pkg} is installed    │   │
│             │ │                                                │ with version {installed_version}, which "       │   │
│             │ │                                                │    467                  f"is less than the      │   │
│             │ │                                                │ minimum required version {min_version}. " +     │   │
│             │ │                                                │ message                                         │   │
│             │ │                                                │    468              )                           │   │
│             │ │                                                │    469      except PackageNotFoundError:        │   │
│             │ │                                                │    470          raise Exception(                │   │
│             │ │                                                │    471              f"{pkg} with minimum        │   │
│             │ │                                                │ required version {min_version} is not           │   │
│             │ │                                                │ installed. "                                    │   │
│             │ │                                                │    472              + message                   │   │
│             │ │                                                │    473          )                               │   │
│             │ │                                                │    474                                          │   │
│             │ │                                                │    475                                          │   │
│             │ │                                                │    476  def kill_process_tree(parent_pid,       │   │
│             │ │                                                │ include_parent: bool = True, skip_pid: int =    │   │
│             │ │                                                │ None):                                          │   │
│             │ │                                                │    477      """Kill the process and all its     │   │
│             │ │                                                │ child processes."""                             │   │
│             │ │                                                │    478      if parent_pid is None:              │   │
│             │ │                                                │    479          parent_pid = os.getpid()        │   │
│             │ │                                                │    480          include_parent = False          │   │
│             │ │                                                │    481                                          │   │
│             │ │                                                │    482      try:                                │   │
│             │ │                                                │    483          itself =                        │   │
│             │ │                                                │ psutil.Process(parent_pid)                      │   │
│             │ │                                                │    484      except psutil.NoSuchProcess:        │   │
│             │ │                                                │    485          return                          │   │
│             │ │                                                │    486                                          │   │
│             │ │                                                │    487      children =                          │   │
│             │ │                                                │ itself.children(recursive=True)                 │   │
│             │ │                                                │    488      for child in children:              │   │
│             │ │                                                │    489          if child.pid == skip_pid:       │   │
│             │ │                                                │    490              continue                    │   │
│             │ │                                                │    491          try:                            │   │
│             │ │                                                │    492              child.kill()                │   │
│             │ │                                                │    493          except psutil.NoSuchProcess:    │   │
│             │ │                                                │    494              pass                        │   │
│             │ │                                                │    495                                          │   │
│             │ │                                                │    496      if include_parent:                  │   │
│             │ │                                                │    497          try:                            │   │
│             │ │                                                │    498              itself.kill()               │   │
│             │ │                                                │    499                                          │   │
│             │ │                                                │    500              # Sometime processes cannot │   │
│             │ │                                                │ be killed with SIGKILL (e.g, PID=1 launched by  │   │
│             │ │                                                │ kubernetes),                                    │   │
│             │ │                                                │    501              # so we send an additional  │   │
│             │ │                                                │ signal to kill them.                            │   │
│             │ │                                                │    502                                          │   │
│             │ │                                                │ itself.send_signal(signal.SIGQUIT)              │   │
│             │ │                                                │    503          except psutil.NoSuchProcess:    │   │
│             │ │                                                │    504              pass                        │   │
│             │ │                                                │    505                                          │   │
│             │ │                                                │    506                                          │   │
│             │ │                                                │    507  def                                     │   │
│             │ │                                                │ monkey_patch_vllm_p2p_access_check(gpu_id:      │   │
│             │ │                                                │ int):                                           │   │
│             │ │                                                │    508      """                                 │   │
│             │ │                                                │    509      Monkey patch the slow p2p access    │   │
│             │ │                                                │ check in vllm.                                  │   │
│             │ │                                                │    510      NOTE: We assume the p2p access is   │   │
│             │ │                                                │ always allowed, which can be wrong for some     │   │
│             │ │                                                │ setups.                                         │   │
│             │ │                                                │    511      """                                 │   │
│             │ │                                                │    512                                          │   │
│             │ │                                                │    513      import                              │   │
│             │ │                                                │ vllm.distributed.device_communicators.custom_a… │   │
│             │ │                                                │ as tgt                                          │   │
│             │ │                                                │    514                                          │   │
│             │ │                                                │    515      setattr(tgt,                        │   │
│             │ │                                                │ "gpu_p2p_access_check", lambda *arg, **kwargs:  │   │
│             │ │                                                │ True)                                           │   │
│             │ │                                                │    516                                          │   │
│             │ │                                                │    517      # Suppress the warnings from this   │   │
│             │ │                                                │ delete function when using                      │   │
│             │ │                                                │ sglang.bench_one_batch                          │   │
│             │ │                                                │    518      fro<response clipped><NOTE>To save  │   │
│             │ │                                                │ on context only part of this file has been      │   │
│             │ │                                                │ shown to you. You should retry this tool after  │   │
│             │ │                                                │ you have searched inside the file with `grep    │   │
│             │ │                                                │ -n` in order to find the line numbers of what   │   │
│             │ │                                                │ you are looking for.</NOTE>                     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 119316 Output: 1312                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 8: Completed                                                                                   │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │ 707:def broadcast_pyobj(                        │   │
│             │ │ "broadcast_pyobj\\|MultiprocessingSerializer"  │                                                 │   │
│             │ │ -n                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ 'restart': False}                              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 149005 Output: 1393                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 9: Completed                                                                                   │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │    680      """Replace a submodule in a model   │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ with a new module."""                           │   │
│             │ │ 'view_range': [680, 900]}                      │    681      parent =                            │   │
│             │ │                                                │ model.get_submodule(".".join(module_name.split… │   │
│             │ │                                                │    682      target_name =                       │   │
│             │ │                                                │ module_name.split(".")[-1]                      │   │
│             │ │                                                │    683      setattr(parent, target_name,        │   │
│             │ │                                                │ new_module)                                     │   │
│             │ │                                                │    684      return new_module                   │   │
│             │ │                                                │    685                                          │   │
│             │ │                                                │    686                                          │   │
│             │ │                                                │    687  def set_weight_attrs(                   │   │
│             │ │                                                │    688      weight: torch.Tensor,               │   │
│             │ │                                                │    689      weight_attrs: Optional[Dict],       │   │
│             │ │                                                │    690  ):                                      │   │
│             │ │                                                │    691      """Set attributes on a weight       │   │
│             │ │                                                │ tensor.                                         │   │
│             │ │                                                │    692                                          │   │
│             │ │                                                │    693      This method is used to set          │   │
│             │ │                                                │ attributes on a weight tensor. This method      │   │
│             │ │                                                │    694      will not overwrite existing         │   │
│             │ │                                                │ attributes.                                     │   │
│             │ │                                                │    695                                          │   │
│             │ │                                                │    696      Args:                               │   │
│             │ │                                                │    697          weight: The weight tensor.      │   │
│             │ │                                                │    698          weight_attrs: A dictionary of   │   │
│             │ │                                                │ attributes to set on the weight tensor.         │   │
│             │ │                                                │    699      """                                 │   │
│             │ │                                                │    700      if weight_attrs is None:            │   │
│             │ │                                                │    701          return                          │   │
│             │ │                                                │    702      for key, value in                   │   │
│             │ │                                                │ weight_attrs.items():                           │   │
│             │ │                                                │    703          assert not hasattr(weight,      │   │
│             │ │                                                │ key), f"Overwriting existing tensor attribute:  │   │
│             │ │                                                │ {key}"                                          │   │
│             │ │                                                │    704          setattr(weight, key, value)     │   │
│             │ │                                                │    705                                          │   │
│             │ │                                                │    706                                          │   │
│             │ │                                                │    707  def broadcast_pyobj(                    │   │
│             │ │                                                │    708      data: List[Any],                    │   │
│             │ │                                                │    709      rank: int,                          │   │
│             │ │                                                │    710      dist_group: Optional = None,        │   │
│             │ │                                                │    711  ):                                      │   │
│             │ │                                                │    712      """Broadcast inputs from rank=0 to  │   │
│             │ │                                                │ all other ranks with torch.dist backend."""     │   │
│             │ │                                                │    713                                          │   │
│             │ │                                                │    714      if rank == 0:                       │   │
│             │ │                                                │    715          if len(data) == 0:              │   │
│             │ │                                                │    716              tensor_size =               │   │
│             │ │                                                │ torch.tensor([0], dtype=torch.long)             │   │
│             │ │                                                │    717              dist.broadcast(tensor_size, │   │
│             │ │                                                │ src=0, group=dist_group)                        │   │
│             │ │                                                │    718          else:                           │   │
│             │ │                                                │    719              serialized_data =           │   │
│             │ │                                                │ pickle.dumps(data)                              │   │
│             │ │                                                │    720              size = len(serialized_data) │   │
│             │ │                                                │    721              tensor_data =               │   │
│             │ │                                                │ torch.ByteTensor(                               │   │
│             │ │                                                │    722                                          │   │
│             │ │                                                │ np.frombuffer(serialized_data, dtype=np.uint8)  │   │
│             │ │                                                │    723              )                           │   │
│             │ │                                                │    724              tensor_size =               │   │
│             │ │                                                │ torch.tensor(, dtype=torch.long)                │   │
│             │ │                                                │    725                                          │   │
│             │ │                                                │    726              dist.broadcast(tensor_size, │   │
│             │ │                                                │ src=0, group=dist_group)                        │   │
│             │ │                                                │    727              dist.broadcast(tensor_data, │   │
│             │ │                                                │ src=0, group=dist_group)                        │   │
│             │ │                                                │    728          return data                     │   │
│             │ │                                                │    729      else:                               │   │
│             │ │                                                │    730          tensor_size = torch.tensor([0], │   │
│             │ │                                                │ dtype=torch.long)                               │   │
│             │ │                                                │    731          dist.broadcast(tensor_size,     │   │
│             │ │                                                │ src=0, group=dist_group)                        │   │
│             │ │                                                │    732          size = tensor_size.item()       │   │
│             │ │                                                │    733                                          │   │
│             │ │                                                │    734          if size == 0:                   │   │
│             │ │                                                │    735              return []                   │   │
│             │ │                                                │    736                                          │   │
│             │ │                                                │    737          tensor_data = torch.empty(size, │   │
│             │ │                                                │ dtype=torch.uint8)                              │   │
│             │ │                                                │    738          dist.broadcast(tensor_data,     │   │
│             │ │                                                │ src=0, group=dist_group)                        │   │
│             │ │                                                │    739                                          │   │
│             │ │                                                │    740          serialized_data =               │   │
│             │ │                                                │ bytes(tensor_data.cpu().numpy())                │   │
│             │ │                                                │    741          data =                          │   │
│             │ │                                                │ pickle.loads(serialized_data)                   │   │
│             │ │                                                │    742          return data                     │   │
│             │ │                                                │    743                                          │   │
│             │ │                                                │    744                                          │   │
│             │ │                                                │    745  step_counter = 0                        │   │
│             │ │                                                │    746                                          │   │
│             │ │                                                │    747                                          │   │
│             │ │                                                │    748  def pytorch_profile(name, func, *args,  │   │
│             │ │                                                │ data_size=-1):                                  │   │
│             │ │                                                │    749      """                                 │   │
│             │ │                                                │    750      Args:                               │   │
│             │ │                                                │    751          name (string): the name of      │   │
│             │ │                                                │ recorded function.                              │   │
│             │ │                                                │    752          func: the function to be        │   │
│             │ │                                                │ profiled.                                       │   │
│             │ │                                                │    753          args: the arguments of the      │   │
│             │ │                                                │ profiled function.                              │   │
│             │ │                                                │    754          data_size (int): some           │   │
│             │ │                                                │ measurement of the computation complexity.      │   │
│             │ │                                                │    755              Usually, it could be the    │   │
│             │ │                                                │ batch size.                                     │   │
│             │ │                                                │    756      """                                 │   │
│             │ │                                                │    757      global step_counter                 │   │
│             │ │                                                │    758      os.makedirs("trace", exist_ok=True) │   │
│             │ │                                                │    759      with profile(                       │   │
│             │ │                                                │    760                                          │   │
│             │ │                                                │ activities=[ProfilerActivity.CPU,               │   │
│             │ │                                                │ ProfilerActivity.CUDA],                         │   │
│             │ │                                                │    761          #                               │   │
│             │ │                                                │ schedule=torch.profiler.schedule(wait=1,        │   │
│             │ │                                                │ warmup=1, active=3, repeat=2),                  │   │
│             │ │                                                │    762          #                               │   │
│             │ │                                                │ on_trace_ready=tensorboard_trace_handler('./lo… │   │
│             │ │                                                │    763          record_shapes=True,             │   │
│             │ │                                                │    764          profile_memory=True,            │   │
│             │ │                                                │    765          with_stack=True,                │   │
│             │ │                                                │    766      ) as prof:                          │   │
│             │ │                                                │    767          with record_function(name):     │   │
│             │ │                                                │    768              with                        │   │
│             │ │                                                │ open(f"trace/size_{step_counter}.json", "w") as │   │
│             │ │                                                │ f:                                              │   │
│             │ │                                                │    769                  json.dump({"size":      │   │
│             │ │                                                │ data_size}, f)                                  │   │
│             │ │                                                │    770              result = func(*args)        │   │
│             │ │                                                │    771                                          │   │
│             │ │                                                │ prof.export_chrome_trace(f"trace/{name}_{step_… │   │
│             │ │                                                │    772      step_counter += 1                   │   │
│             │ │                                                │    773      return result                       │   │
│             │ │                                                │    774                                          │   │
│             │ │                                                │    775                                          │   │
│             │ │                                                │    776  def first_rank_print(*args, **kwargs):  │   │
│             │ │                                                │    777      if torch.cuda.current_device() ==   │   │
│             │ │                                                │ 0:                                              │   │
│             │ │                                                │    778          print(*args, **kwargs)          │   │
│             │ │                                                │    779      else:                               │   │
│             │ │                                                │    780          pass                            │   │
│             │ │                                                │    781                                          │   │
│             │ │                                                │    782                                          │   │
│             │ │                                                │    783  def get_zmq_socket(context:             │   │
│             │ │                                                │ zmq.Context, socket_type: zmq.SocketType,       │   │
│             │ │                                                │ endpoint: str):                                 │   │
│             │ │                                                │    784      mem = psutil.virtual_memory()       │   │
│             │ │                                                │    785      total_mem = mem.total / 1024**3     │   │
│             │ │                                                │    786      available_mem = mem.available /     │   │
│             │ │                                                │ 1024**3                                         │   │
│             │ │                                                │    787      if total_mem > 32 and available_mem │   │
│             │ │                                                │ > 16:                                           │   │
│             │ │                                                │    788          buf_size = int(0.5 * 1024**3)   │   │
│             │ │                                                │    789      else:                               │   │
│             │ │                                                │    790          buf_size = -1                   │   │
│             │ │                                                │    791                                          │   │
│             │ │                                                │    792      socket =                            │   │
│             │ │                                                │ context.socket(socket_type)                     │   │
│             │ │                                                │    793      if socket_type == zmq.PUSH:         │   │
│             │ │                                                │    794          socket.setsockopt(zmq.SNDHWM,   │   │
│             │ │                                                │ 0)                                              │   │
│             │ │                                                │    795          socket.setsockopt(zmq.SNDBUF,   │   │
│             │ │                                                │ buf_size)                                       │   │
│             │ │                                                │    796                                          │   │
│             │ │                                                │ socket.connect(f"ipc://{endpoint}")             │   │
│             │ │                                                │    797      elif socket_type == zmq.PULL:       │   │
│             │ │                                                │    798          socket.setsockopt(zmq.RCVHWM,   │   │
│             │ │                                                │ 0)                                              │   │
│             │ │                                                │    799          socket.setsockopt(zmq.RCVBUF,   │   │
│             │ │                                                │ buf_size)                                       │   │
│             │ │                                                │    800                                          │   │
│             │ │                                                │ socket.bind(f"ipc://{endpoint}")                │   │
│             │ │                                                │    801      else:                               │   │
│             │ │                                                │    802          raise ValueError(f"Unsupported  │   │
│             │ │                                                │ socket type: {socket_type}")                    │   │
│             │ │                                                │    803                                          │   │
│             │ │                                                │    804      return socket                       │   │
│             │ │                                                │    805                                          │   │
│             │ │                                                │    806                                          │   │
│             │ │                                                │    807  def dump_to_file(dirpath, name, value): │   │
│             │ │                                                │    808      from vllm.distributed import        │   │
│             │ │                                                │ get_tensor_model_parallel_rank                  │   │
│             │ │                                                │    809                                          │   │
│             │ │                                                │    810      if get_tensor_model_parallel_rank() │   │
│             │ │                                                │ != 0:                                           │   │
│             │ │                                                │    811          return                          │   │
│             │ │                                                │    812                                          │   │
│             │ │                                                │    813      os.makedirs(dirpath, exist_ok=True) │   │
│             │ │                                                │    814      if value.dtype is torch.bfloat16:   │   │
│             │ │                                                │    815          value = value.float()           │   │
│             │ │                                                │    816      value = value.cpu().numpy()         │   │
│             │ │                                                │    817      output_filename =                   │   │
│             │ │                                                │ os.path.join(dirpath,                           │   │
│             │ │                                                │ f"pytorch_dump_{name}.npy")                     │   │
│             │ │                                                │    818      logger.info(f"Dump a tensor to      │   │
│             │ │                                                │ {output_filename}. Shape = {value.shape}")      │   │
│             │ │                                                │    819      np.save(output_filename, value)     │   │
│             │ │                                                │    820                                          │   │
│             │ │                                                │    821                                          │   │
│             │ │                                                │    822  def is_triton_3():                      │   │
│             │ │                                                │    823      return                              │   │
│             │ │                                                │ triton.__version__.startswith("3.")             │   │
│             │ │                                                │    824                                          │   │
│             │ │                                                │    825                                          │   │
│             │ │                                                │    826  def maybe_torch_compile(*args,          │   │
│             │ │                                                │ **kwargs):                                      │   │
│             │ │                                                │    827      """                                 │   │
│             │ │                                                │    828      torch.compile does not work for     │   │
│             │ │                                                │ triton 2.2.0, which is needed in xlm1's jax.    │   │
│             │ │                                                │    829      Therefore, we disable it here.      │   │
│             │ │                                                │    830      """                                 │   │
│             │ │                                                │    831                                          │   │
│             │ │                                                │    832      def decorator(func):                │   │
│             │ │                                                │    833          if is_triton_3():               │   │
│             │ │                                                │    834              return torch.compile(*args, │   │
│             │ │                                                │ **kwargs)(func)                                 │   │
│             │ │                                                │    835          return func                     │   │
│             │ │                                                │    836                                          │   │
│             │ │                                                │    837      return decorator                    │   │
│             │ │                                                │    838                                          │   │
│             │ │                                                │    839                                          │   │
│             │ │                                                │    840  def delete_directory(dirpath):          │   │
│             │ │                                                │    841      try:                                │   │
│             │ │                                                │    842          # This will remove the          │   │
│             │ │                                                │ directory and all its contents                  │   │
│             │ │                                                │    843          shutil.rmtree(dirpath)          │   │
│             │ │                                                │    844      except OSError as e:                │   │
│             │ │                                                │    845          print(f"Warning: {dirpath} :    │   │
│             │ │                                                │ {e.strerror}")                                  │   │
│             │ │                                                │    846                                          │   │
│             │ │                                                │    847                                          │   │
│             │ │                                                │    848  # Temporary directory for prometheus    │   │
│             │ │                                                │ multiprocess mode                               │   │
│             │ │                                                │    849  # Cleaned up automatically when this    │   │
│             │ │                                                │ object is garbage collected                     │   │
│             │ │                                                │    850  prometheus_multiproc_dir:               │   │
│             │ │                                                │ tempfile.TemporaryDirectory                     │   │
│             │ │                                                │    851                                          │   │
│             │ │                                                │    852                                          │   │
│             │ │                                                │    853  def set_prometheus_multiproc_dir():     │   │
│             │ │                                                │    854      # Set prometheus multiprocess       │   │
│             │ │                                                │ directory                                       │   │
│             │ │                                                │    855      # sglang uses prometheus            │   │
│             │ │                                                │ multiprocess mode                               │   │
│             │ │                                                │    856      # we need to set this before        │   │
│             │ │                                                │ importing prometheus_client                     │   │
│             │ │                                                │    857      #                                   │   │
│             │ │                                                │ https://prometheus.github.io/client_python/mul… │   │
│             │ │                                                │    858      global prometheus_multiproc_dir     │   │
│             │ │                                                │    859                                          │   │
│             │ │                                                │    860      if "PROMETHEUS_MULTIPROC_DIR" in    │   │
│             │ │                                                │ os.environ:                                     │   │
│             │ │                                                │    861          logger.debug("User set          │   │
│             │ │                                                │ PROMETHEUS_MULTIPROC_DIR detected.")            │   │
│             │ │                                                │    862          prometheus_multiproc_dir =      │   │
│             │ │                                                │ tempfile.TemporaryDirectory(                    │   │
│             │ │                                                │    863                                          │   │
│             │ │                                                │ dir=os.environ["PROMETHEUS_MULTIPROC_DIR"]      │   │
│             │ │                                                │    864          )                               │   │
│             │ │                                                │    865      else:                               │   │
│             │ │                                                │    866          prometheus_multiproc_dir =      │   │
│             │ │                                                │ tempfile.TemporaryDirectory()                   │   │
│             │ │                                                │    867                                          │   │
│             │ │                                                │ os.environ["PROMETHEUS_MULTIPROC_DIR"] =        │   │
│             │ │                                                │ prometheus_multiproc_dir.name                   │   │
│             │ │                                                │    868                                          │   │
│             │ │                                                │ logger.debug(f"PROMETHEUS_MULTIPROC_DIR:        │   │
│             │ │                                                │ {os.environ['PROMETHEUS_MULTIPROC_DIR']}")      │   │
│             │ │                                                │    869                                          │   │
│             │ │                                                │    870                                          │   │
│             │ │                                                │    871  def add_prometheus_middleware(app):     │   │
│             │ │                                                │    872      # We need to import                 │   │
│             │ │                                                │ prometheus_client after setting the env         │   │
│             │ │                                                │ variable `PROMETHEUS_MULTIPROC_DIR`             │   │
│             │ │                                                │    873      from prometheus_client import       │   │
│             │ │                                                │ CollectorRegistry, make_asgi_app, multiprocess  │   │
│             │ │                                                │    874                                          │   │
│             │ │                                                │    875      registry = CollectorRegistry()      │   │
│             │ │                                                │    876                                          │   │
│             │ │                                                │ multiprocess.MultiProcessCollector(registry)    │   │
│             │ │                                                │    877      metrics_route = Mount("/metrics",   │   │
│             │ │                                                │ make_asgi_app(registry=registry))               │   │
│             │ │                                                │    878                                          │   │
│             │ │                                                │    879      # Workaround for 307 Redirect for   │   │
│             │ │                                                │ /metrics                                        │   │
│             │ │                                                │    880      metrics_route.path_regex =          │   │
│             │ │                                                │ re.compile("^/metrics(?P<path>.*)$")            │   │
│             │ │                                                │    881      app.routes.append(metrics_route)    │   │
│             │ │                                                │    882                                          │   │
│             │ │                                                │    883                                          │   │
│             │ │                                                │    884  def bind_port(port):                    │   │
│             │ │                                                │    885      """Bind to a specific port,         │   │
│             │ │                                                │ assuming it's available."""                     │   │
│             │ │                                                │    886      sock =                              │   │
│             │ │                                                │ socket.socket(socket.AF_INET,                   │   │
│             │ │                                                │ socket.SOCK_STREAM)                             │   │
│             │ │                                                │    887      sock.setsockopt(socket.SOL_SOCKET,  │   │
│             │ │                                                │ socket.SO_REUSEADDR, 1)  # Allows address reuse │   │
│             │ │                                                │    888      sock.bind(("", port))               │   │
│             │ │                                                │    889      sock.listen(1)                      │   │
│             │ │                                                │    890      return sock                         │   │
│             │ │                                                │    891                                          │   │
│             │ │                                                │    892                                          │   │
│             │ │                                                │    893  def get_amdgpu_memory_capacity():       │   │
│             │ │                                                │    894      try:                                │   │
│             │ │                                                │    895          # Run rocm-smi and capture the  │   │
│             │ │                                                │ output                                          │   │
│             │ │                                                │    896          result = subprocess.run(        │   │
│             │ │                                                │    897              [                           │   │
│             │ │                                                │    898                  "rocminfo | grep 'gfx'  │   │
│             │ │                                                │ -A 100 | grep 'Pool 1' -A 5 | grep 'Size:' |    │   │
│             │ │                                                │ awk '{print $2}'"                               │   │
│             │ │                                                │    899              ],                          │   │
│             │ │                                                │    900              stdout=subprocess.PIPE,     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 178822 Output: 1481                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 10: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """                                     │   │
│             │ │                                                │     15  The entry point of inference server.    │   │
│             │ │                                                │     16  SRT = SGLang Runtime.                   │   │
│             │ │                                                │     17  """                                     │   │
│             │ │                                                │     18                                          │   │
│             │ │                                                │     19  import asyncio                          │   │
│             │ │                                                │     20  import atexit                           │   │
│             │ │                                                │     21  import dataclasses                      │   │
│             │ │                                                │     22  import json                             │   │
│             │ │                                                │     23  import logging                          │   │
│             │ │                                                │     24  import multiprocessing as mp            │   │
│             │ │                                                │     25  import os                               │   │
│             │ │                                                │     26  import signal                           │   │
│             │ │                                                │     27  import threading                        │   │
│             │ │                                                │     28  import time                             │   │
│             │ │                                                │     29  from http import HTTPStatus             │   │
│             │ │                                                │     30  from typing import AsyncIterator, Dict, │   │
│             │ │                                                │ List, Optional, Union                           │   │
│             │ │                                                │     31                                          │   │
│             │ │                                                │     32  # Fix a bug of Python threading         │   │
│             │ │                                                │     33  setattr(threading, "_register_atexit",  │   │
│             │ │                                                │ lambda *args, **kwargs: None)                   │   │
│             │ │                                                │     34                                          │   │
│             │ │                                                │     35  import aiohttp                          │   │
│             │ │                                                │     36  import orjson                           │   │
│             │ │                                                │     37  import requests                         │   │
│             │ │                                                │     38  import uvicorn                          │   │
│             │ │                                                │     39  import uvloop                           │   │
│             │ │                                                │     40  from fastapi import FastAPI, File,      │   │
│             │ │                                                │ Form, Request, UploadFile                       │   │
│             │ │                                                │     41  from fastapi.middleware.cors import     │   │
│             │ │                                                │ CORSMiddleware                                  │   │
│             │ │                                                │     42  from fastapi.responses import           │   │
│             │ │                                                │ ORJSONResponse, Response, StreamingResponse     │   │
│             │ │                                                │     43  from uvicorn.config import              │   │
│             │ │                                                │ LOGGING_CONFIG                                  │   │
│             │ │                                                │     44                                          │   │
│             │ │                                                │     45  from                                    │   │
│             │ │                                                │ sglang.lang.backend.runtime_endpoint import     │   │
│             │ │                                                │ RuntimeEndpoint                                 │   │
│             │ │                                                │     46  from sglang.srt.hf_transformers_utils   │   │
│             │ │                                                │ import get_tokenizer                            │   │
│             │ │                                                │     47  from                                    │   │
│             │ │                                                │ sglang.srt.managers.data_parallel_controller    │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     48                                          │   │
│             │ │                                                │ run_data_parallel_controller_process,           │   │
│             │ │                                                │     49  )                                       │   │
│             │ │                                                │     50  from                                    │   │
│             │ │                                                │ sglang.srt.managers.detokenizer_manager import  │   │
│             │ │                                                │ run_detokenizer_process                         │   │
│             │ │                                                │     51  from sglang.srt.managers.io_struct      │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     52      CloseSessionReqInput,               │   │
│             │ │                                                │     53      EmbeddingReqInput,                  │   │
│             │ │                                                │     54      GenerateReqInput,                   │   │
│             │ │                                                │     55      GetWeightsByNameReqInput,           │   │
│             │ │                                                │     56      InitWeightsUpdateGroupReqInput,     │   │
│             │ │                                                │     57      OpenSessionReqInput,                │   │
│             │ │                                                │     58      UpdateWeightFromDiskReqInput,       │   │
│             │ │                                                │     59                                          │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput,           │   │
│             │ │                                                │     60      UpdateWeightsFromTensorReqInput,    │   │
│             │ │                                                │     61  )                                       │   │
│             │ │                                                │     62  from sglang.srt.managers.scheduler      │   │
│             │ │                                                │ import run_scheduler_process                    │   │
│             │ │                                                │     63  from                                    │   │
│             │ │                                                │ sglang.srt.managers.tokenizer_manager import    │   │
│             │ │                                                │ TokenizerManager                                │   │
│             │ │                                                │     64  from sglang.srt.metrics.func_timer      │   │
│             │ │                                                │ import enable_func_timer, time_func_latency     │   │
│             │ │                                                │     65  from sglang.srt.openai_api.adapter      │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     66      load_chat_template_for_openai_api,  │   │
│             │ │                                                │     67      v1_batches,                         │   │
│             │ │                                                │     68      v1_cancel_batch,                    │   │
│             │ │                                                │     69      v1_chat_completions,                │   │
│             │ │                                                │     70      v1_completions,                     │   │
│             │ │                                                │     71      v1_delete_file,                     │   │
│             │ │                                                │     72      v1_embeddings,                      │   │
│             │ │                                                │     73      v1_files_create,                    │   │
│             │ │                                                │     74      v1_retrieve_batch,                  │   │
│             │ │                                                │     75      v1_retrieve_file,                   │   │
│             │ │                                                │     76      v1_retrieve_file_content,           │   │
│             │ │                                                │     77  )                                       │   │
│             │ │                                                │     78  from sglang.srt.openai_api.protocol     │   │
│             │ │                                                │ import ModelCard, ModelList                     │   │
│             │ │                                                │     79  from sglang.srt.server_args import      │   │
│             │ │                                                │ PortArgs, ServerArgs                            │   │
│             │ │                                                │     80  from sglang.srt.utils import (          │   │
│             │ │                                                │     81      add_api_key_middleware,             │   │
│             │ │                                                │     82      add_prometheus_middleware,          │   │
│             │ │                                                │     83      assert_pkg_version,                 │   │
│             │ │                                                │     84      configure_logger,                   │   │
│             │ │                                                │     85      delete_directory,                   │   │
│             │ │                                                │     86      is_port_available,                  │   │
│             │ │                                                │     87      kill_process_tree,                  │   │
│             │ │                                                │     88      maybe_set_triton_cache_manager,     │   │
│             │ │                                                │     89      prepare_model_and_tokenizer,        │   │
│             │ │                                                │     90      set_prometheus_multiproc_dir,       │   │
│             │ │                                                │     91      set_ulimit,                         │   │
│             │ │                                                │     92  )                                       │   │
│             │ │                                                │     93  from sglang.utils import                │   │
│             │ │                                                │ get_exception_traceback                         │   │
│             │ │                                                │     94  from sglang.version import __version__  │   │
│             │ │                                                │     95                                          │   │
│             │ │                                                │     96  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     97                                          │   │
│             │ │                                                │     98                                          │   │
│             │ │                                                │ asyncio.set_event_loop_policy(uvloop.EventLoop… │   │
│             │ │                                                │     99                                          │   │
│             │ │                                                │    100  # Fast API                              │   │
│             │ │                                                │    101  app = FastAPI()                         │   │
│             │ │                                                │    102  app.add_middleware(                     │   │
│             │ │                                                │    103      CORSMiddleware,                     │   │
│             │ │                                                │    104      allow_origins=["*"],                │   │
│             │ │                                                │    105      allow_credentials=True,             │   │
│             │ │                                                │    106      allow_methods=["*"],                │   │
│             │ │                                                │    107      allow_headers=["*"],                │   │
│             │ │                                                │    108  )                                       │   │
│             │ │                                                │    109                                          │   │
│             │ │                                                │    110  tokenizer_manager: TokenizerManager =   │   │
│             │ │                                                │ None                                            │   │
│             │ │                                                │    111  scheduler_info: Dict = None             │   │
│             │ │                                                │    112                                          │   │
│             │ │                                                │    113                                          │   │
│             │ │                                                │    114  ##### Native API endpoints #####        │   │
│             │ │                                                │    115                                          │   │
│             │ │                                                │    116                                          │   │
│             │ │                                                │    117  @app.get("/health")                     │   │
│             │ │                                                │    118  async def health() -> Response:         │   │
│             │ │                                                │    119      """Check the health of the http     │   │
│             │ │                                                │ server."""                                      │   │
│             │ │                                                │    120      return Response(status_code=200)    │   │
│             │ │                                                │    121                                          │   │
│             │ │                                                │    122                                          │   │
│             │ │                                                │    123  @app.get("/health_generate")            │   │
│             │ │                                                │    124  async def health_generate(request:      │   │
│             │ │                                                │ Request) -> Response:                           │   │
│             │ │                                                │    125      """Check the health of the          │   │
│             │ │                                                │ inference server by generating one token."""    │   │
│             │ │                                                │    126                                          │   │
│             │ │                                                │    127      if tokenizer_manager.is_generation: │   │
│             │ │                                                │    128          gri = GenerateReqInput(         │   │
│             │ │                                                │    129              input_ids=[0],              │   │
│             │ │                                                │ sampling_params={"max_new_tokens": 1,           │   │
│             │ │                                                │ "temperature": 0.7}                             │   │
│             │ │                                                │    130          )                               │   │
│             │ │                                                │    131      else:                               │   │
│             │ │                                                │    132          gri = EmbeddingReqInput(        │   │
│             │ │                                                │    133              input_ids=[0],              │   │
│             │ │                                                │ sampling_params={"max_new_tokens": 1,           │   │
│             │ │                                                │ "temperature": 0.7}                             │   │
│             │ │                                                │    134          )                               │   │
│             │ │                                                │    135                                          │   │
│             │ │                                                │    136      try:                                │   │
│             │ │                                                │    137          async for _ in                  │   │
│             │ │                                                │ tokenizer_manager.generate_request(gri,         │   │
│             │ │                                                │ request):                                       │   │
│             │ │                                                │    138              break                       │   │
│             │ │                                                │    139          return                          │   │
│             │ │                                                │ Response(status_code=200)                       │   │
│             │ │                                                │    140      except Exception as e:              │   │
│             │ │                                                │    141          logger.exception(e)             │   │
│             │ │                                                │    142          return                          │   │
│             │ │                                                │ Response(status_code=503)                       │   │
│             │ │                                                │    143                                          │   │
│             │ │                                                │    144                                          │   │
│             │ │                                                │    145  @app.get("/get_model_info")             │   │
│             │ │                                                │    146  async def get_model_info():             │   │
│             │ │                                                │    147      """Get the model information."""    │   │
│             │ │                                                │    148      result = {                          │   │
│             │ │                                                │    149          "model_path":                   │   │
│             │ │                                                │ tokenizer_manager.model_path,                   │   │
│             │ │                                                │    150          "tokenizer_path":               │   │
│             │ │                                                │ tokenizer_manager.server_args.tokenizer_path,   │   │
│             │ │                                                │    151          "is_generation":                │   │
│             │ │                                                │ tokenizer_manager.is_generation,                │   │
│             │ │                                                │    152      }                                   │   │
│             │ │                                                │    153      return result                       │   │
│             │ │                                                │    154                                          │   │
│             │ │                                                │    155                                          │   │
│             │ │                                                │    156  @app.get("/get_server_info")            │   │
│             │ │                                                │    157  async def get_server_info():            │   │
│             │ │                                                │    158      return {                            │   │
│             │ │                                                │    159                                          │   │
│             │ │                                                │ **dataclasses.asdict(tokenizer_manager.server_… │   │
│             │ │                                                │ # server args                                   │   │
│             │ │                                                │    160          **scheduler_info,               │   │
│             │ │                                                │    161          "version": __version__,         │   │
│             │ │                                                │    162      }                                   │   │
│             │ │                                                │    163                                          │   │
│             │ │                                                │    164                                          │   │
│             │ │                                                │    165  @app.post("/flush_cache")               │   │
│             │ │                                                │    166  async def flush_cache():                │   │
│             │ │                                                │    167      """Flush the radix cache."""        │   │
│             │ │                                                │    168      tokenizer_manager.flush_cache()     │   │
│             │ │                                                │    169      return Response(                    │   │
│             │ │                                                │    170          content="Cache flushed.\nPlease │   │
│             │ │                                                │ check backend logs for more details. "          │   │
│             │ │                                                │    171          "(When there are running or     │   │
│             │ │                                                │ waiting requests, the operation will not be     │   │
│             │ │                                                │ performed.)\n",                                 │   │
│             │ │                                                │    172          status_code=200,                │   │
│             │ │                                                │    173      )                                   │   │
│             │ │                                                │    174                                          │   │
│             │ │                                                │    175                                          │   │
│             │ │                                                │    176  @app.get("/start_profile")              │   │
│             │ │                                                │    177  @app.post("/start_profile")             │   │
│             │ │                                                │    178  async def start_profile_async():        │   │
│             │ │                                                │    179      """Start profiling."""              │   │
│             │ │                                                │    180      tokenizer_manager.start_profile()   │   │
│             │ │                                                │    181      return Response(                    │   │
│             │ │                                                │    182          content="Start profiling.\n",   │   │
│             │ │                                                │    183          status_code=200,                │   │
│             │ │                                                │    184      )                                   │   │
│             │ │                                                │    185                                          │   │
│             │ │                                                │    186                                          │   │
│             │ │                                                │    187  @app.get("/stop_profile")               │   │
│             │ │                                                │    188  @app.post("/stop_profile")              │   │
│             │ │                                                │    189  async def stop_profile_async():         │   │
│             │ │                                                │    190      """Stop profiling."""               │   │
│             │ │                                                │    191      tokenizer_manager.stop_profile()    │   │
│             │ │                                                │    192      return Response(                    │   │
│             │ │                                                │    193          content="Stop profiling. This   │   │
│             │ │                                                │ will take some time.\n",                        │   │
│             │ │                                                │    194          status_code=200,                │   │
│             │ │                                                │    195      )                                   │   │
│             │ │                                                │    196                                          │   │
│             │ │                                                │    197                                          │   │
│             │ │                                                │    198  @app.post("/update_weights_from_disk")  │   │
│             │ │                                                │    199  @time_func_latency                      │   │
│             │ │                                                │    200  async def update_weights_from_disk(obj: │   │
│             │ │                                                │ UpdateWeightFromDiskReqInput, request:          │   │
│             │ │                                                │ Request):                                       │   │
│             │ │                                                │    201      """Update the weights from disk     │   │
│             │ │                                                │ in-place without re-launching the server."""    │   │
│             │ │                                                │    202      success, message = await            │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_disk(obj, │   │
│             │ │                                                │ request)                                        │   │
│             │ │                                                │    203      content = {"success": success,      │   │
│             │ │                                                │ "message": message}                             │   │
│             │ │                                                │    204      if success:                         │   │
│             │ │                                                │    205          return ORJSONResponse(          │   │
│             │ │                                                │    206              content,                    │   │
│             │ │                                                │    207              status_code=HTTPStatus.OK,  │   │
│             │ │                                                │    208          )                               │   │
│             │ │                                                │    209      else:                               │   │
│             │ │                                                │    210          return ORJSONResponse(          │   │
│             │ │                                                │    211              content,                    │   │
│             │ │                                                │    212                                          │   │
│             │ │                                                │ status_code=HTTPStatus.BAD_REQUEST,             │   │
│             │ │                                                │    213          )                               │   │
│             │ │                                                │    214                                          │   │
│             │ │                                                │    215                                          │   │
│             │ │                                                │    216  @app.post("/init_weights_update_group") │   │
│             │ │                                                │    217  async def init_weights_update_group(    │   │
│             │ │                                                │    218      obj:                                │   │
│             │ │                                                │ InitWeightsUpdateGroupReqInput, request:        │   │
│             │ │                                                │ Request                                         │   │
│             │ │                                                │    219  ):                                      │   │
│             │ │                                                │    220      """Initialize the parameter update  │   │
│             │ │                                                │ group."""                                       │   │
│             │ │                                                │    221      success, message = await            │   │
│             │ │                                                │ tokenizer_manager.init_weights_update_group(ob… │   │
│             │ │                                                │ request)                                        │   │
│             │ │                                                │    222      content = {"success": success,      │   │
│             │ │                                                │ "message": message}                             │   │
│             │ │                                                │    223      if success:                         │   │
│             │ │                                                │    224          return ORJSONResponse(content,  │   │
│             │ │                                                │ status_code=200)                                │   │
│             │ │                                                │    225      else:                               │   │
│             │ │                                                │    226          return ORJSONResponse(content,  │   │
│             │ │                                                │ status_code=HTTPStatus.BAD_REQUEST)             │   │
│             │ │                                                │    227                                          │   │
│             │ │                                                │    228                                          │   │
│             │ │                                                │    229                                          │   │
│             │ │                                                │ @app.post("/update_weights_from_distributed")   │   │
│             │ │                                                │    230  async def                               │   │
│             │ │                                                │ update_weights_from_distributed(                │   │
│             │ │                                                │    231      obj:                                │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput, request:  │   │
│             │ │                                                │ Request                                         │   │
│             │ │                                                │    232  ):                                      │   │
│             │ │                                                │    233      """Update model parameter from      │   │
│             │ │                                                │ distributed online."""                          │   │
│             │ │                                                │    234      success, message = await            │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_distribu… │   │
│             │ │                                                │    235          obj, request                    │   │
│             │ │                                                │    236      )                                   │   │
│             │ │                                                │    237      content = {"success": success,      │   │
│             │ │                                                │ "message": message}                             │   │
│             │ │                                                │    238      if success:                         │   │
│             │ │                                                │    239          return ORJSONResponse(content,  │   │
│             │ │                                                │ status_code=200)                                │   │
│             │ │                                                │    240      else:                               │   │
│             │ │                                                │    241          return ORJSONResponse(content,  │   │
│             │ │                                                │ status_code=HTTPStatus.BAD_REQUEST)             │   │
│             │ │                                                │    242                                          │   │
│             │ │                                                │    243                                          │   │
│             │ │                                                │    244  @app.api_route("/get_weights_by_name",  │   │
│             │ │                                                │ methods=["GET", "POST"])                        │   │
│             │ │                                                │    245  async def get_weights_by_name(obj:      │   │
│             │ │                                                │ GetWeightsByNameReqInput, request: Request):    │   │
│             │ │                                                │    246      """Get model parameter by name."""  │   │
│             │ │                                                │    247      try:                                │   │
│             │ │                                                │    248          ret = await                     │   │
│             │ │                                                │ tokenizer_manager.get_weights_by_name(obj,      │   │
│             │ │                                                │ request)                                        │   │
│             │ │                                                │    249          if ret is None:                 │   │
│             │ │                                                │    250              return                      │   │
│             │ │                                                │ _create_error_response("Get parameter by name   │   │
│             │ │                                                │ failed")                                        │   │
│             │ │                                                │    251          else:                           │   │
│             │ │                                                │    252              return ORJSONResponse(ret,  │   │
│             │ │                                                │ status_code=200)                                │   │
│             │ │                                                │    253      except Exception as e:              │   │
│             │ │                                                │    254          return                          │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    255                                          │   │
│             │ │                                                │    256                                          │   │
│             │ │                                                │    257  @app.api_route("/open_session",         │   │
│             │ │                                                │ methods=["GET", "POST"])                        │   │
│             │ │                                                │    258  async def open_session(obj:             │   │
│             │ │                                                │ OpenSessionReqInput, request: Request):         │   │
│             │ │                                                │    259      """Open a session, and return its   │   │
│             │ │                                                │ unique session id."""                           │   │
│             │ │                                                │    260      try:                                │   │
│             │ │                                                │    261          session_id = await              │   │
│             │ │                                                │ tokenizer_manager.open_session(obj, request)    │   │
│             │ │                                                │    262          if session_id is None:          │   │
│             │ │                                                │    263              raise Exception(            │   │
│             │ │                                                │    264                  "Failed to open the     │   │
│             │ │                                                │ session. Check if a session with the same id is │   │
│             │ │                                                │ still open."                                    │   │
│             │ │                                                │    265              )                           │   │
│             │ │                                                │    266          return session_id               │   │
│             │ │                                                │    267      except Exception as e:              │   │
│             │ │                                                │    268          return                          │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    269                                          │   │
│             │ │                                                │    270                                          │   │
│             │ │                                                │    271  @app.api_route("/close_session",        │   │
│             │ │                                                │ methods=["GET", "POST"])                        │   │
│             │ │                                                │    272  async def close_session(obj:            │   │
│             │ │                                                │ CloseSessionReqInput, request: Request):        │   │
│             │ │                                                │    273      """Close the session"""             │   │
│             │ │                                                │    274      try:                                │   │
│             │ │                                                │    275          await                           │   │
│             │ │                                                │ tokenizer_manager.close_session(obj, request)   │   │
│             │ │                                                │    276          return                          │   │
│             │ │                                                │ Response(status_code=200)                       │   │
│             │ │                                                │    277      except Exception as e:              │   │
│             │ │                                                │    278          return                          │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    279                                          │   │
│             │ │                                                │    280                                          │   │
│             │ │                                                │    281  # fastapi implicitly converts json in   │   │
│             │ │                                                │ the request to obj (dataclass)                  │   │
│             │ │                                                │    282  @app.api_route("/generate",             │   │
│             │ │                                                │ methods=["POST", "PUT"])                        │   │
│             │ │                                                │    283  @time_func_latency                      │   │
│             │ │                                                │    284  async def generate_request(obj:         │   │
│             │ │                                                │ GenerateReqInput, request: Request):            │   │
│             │ │                                                │    285      """Handle a generate request."""    │   │
│             │ │                                                │    286      if obj.stream:                      │   │
│             │ │                                                │    287                                          │   │
│             │ │                                                │    288          async def stream_results() ->   │   │
│             │ │                                                │ AsyncIterator:                                  │   │
│             │ │                                                │    289              try:                        │   │
│             │ │                                                │    290                  async for out in        │   │
│             │ │                                                │ tokenizer_manager.generate_request(obj,         │   │
│             │ │                                                │ request):                                       │   │
│             │ │                                                │    291                      yield b"data: " +   │   │
│             │ │                                                │ orjson.dumps(                                   │   │
│             │ │                                                │    292                          out,            │   │
│             │ │                                                │ option=orjson.OPT_NON_STR_KEYS                  │   │
│             │ │                                                │    293                      ) + b"\n\n"         │   │
│             │ │                                                │    294              except ValueError as e:     │   │
│             │ │                                                │    295                  out = {"error":         │   │
│             │ │                                                │ {"message": str(e)}}                            │   │
│             │ │                                                │    296                  yield b"data: " +       │   │
│             │ │                                                │ orjson.dumps(                                   │   │
│             │ │                                                │    297                      out,                │   │
│             │ │                                                │ option=orjson.OPT_NON_STR_KEYS                  │   │
│             │ │                                                │    298                  ) + b"\n\n"             │   │
│             │ │                                                │    299              yield b"data: [DONE]\n\n"   │   │
│             │ │                                                │    300                                          │   │
│             │ │                                                │    301          return StreamingResponse(       │   │
│             │ │                                                │    302              stream_results(),           │   │
│             │ │                                                │    303                                          │   │
│             │ │                                                │ media_type="text/event-stream",                 │   │
│             │ │                                                │    304                                          │   │
│             │ │                                                │ background=tokenizer_manager.create_abort_task… │   │
│             │ │                                                │    305          )                               │   │
│             │ │                                                │    306      else:                               │   │
│             │ │                                                │    307          try:                            │   │
│             │ │                                                │    308              ret = await                 │   │
│             │ │                                                │ tokenizer_manager.generate_request(obj,         │   │
│             │ │                                                │ request).__anext__()                            │   │
│             │ │                                                │    309              return ret                  │   │
│             │ │                                                │    310          except ValueError as e:         │   │
│             │ │                                                │    311              logger.error(f"Error: {e}") │   │
│             │ │                                                │    312              return                      │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    313                                          │   │
│             │ │                                                │    314                                          │   │
│             │ │                                                │    315  @app.api_route("/encode",               │   │
│             │ │                                                │ methods=["POST", "PUT"])                        │   │
│             │ │                                                │    316  @time_func_latency                      │   │
│             │ │                                                │    317  async def encode_request(obj:           │   │
│             │ │                                                │ EmbeddingReqInput, request: Request):           │   │
│             │ │                                                │    318      """Handle an embedding request."""  │   │
│             │ │                                                │    319      try:                                │   │
│             │ │                                                │    320          ret = await                     │   │
│             │ │                                                │ tokenizer_manager.generate_request(obj,         │   │
│             │ │                                                │ request).__anext__()                            │   │
│             │ │                                                │    321          return ret                      │   │
│             │ │                                                │    322      except ValueError as e:             │   │
│             │ │                                                │    323          return                          │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    324                                          │   │
│             │ │                                                │    325                                          │   │
│             │ │                                                │    326  @app.api_route("/classify",             │   │
│             │ │                                                │ methods=["POST", "PUT"])                        │   │
│             │ │                                                │    327  @time_func_latency                      │   │
│             │ │                                                │    328  async def classify_request(obj:         │   │
│             │ │                                                │ EmbeddingReqInput, request: Request):           │   │
│             │ │                                                │    329      """Handle a reward model request.   │   │
│             │ │                                                │ Now the arguments and return values are the     │   │
│             │ │                                                │ same as embedding models."""                    │   │
│             │ │                                                │    330      try:                                │   │
│             │ │                                                │    331          ret = await                     │   │
│             │ │                                                │ tokenizer_manager.generate_request(obj,         │   │
│             │ │                                                │ request).__anext__()                            │   │
│             │ │                                                │    332          return ret                      │   │
│             │ │                                                │    333      except ValueError as e:             │   │
│             │ │                                                │    334          return                          │   │
│             │ │                                                │ _create_error_response(e)                       │   │
│             │ │                                                │    335                                          │   │
│             │ │                                                │    336                                          │   │
│             │ │                                                │    337  ##### OpenAI-compatible API endpoints   │   │
│             │ │                                                │ #####                                           │   │
│             │ │                                                │    338                                          │   │
│             │ │                                                │    339                                          │   │
│             │ │                                                │    340  @app.post("/v1/completions")            │   │
│             │ │                                                │    341  @time_func_latency                      │   │
│             │ │                                                │    342  async def                               │   │
│             │ │                                                │ openai_v1_completions(raw_request: Request):    │   │
│             │ │                                                │    343      return await                        │   │
│             │ │                                                │ v1_completions(tokenizer_manager, raw_request)  │   │
│             │ │                                                │    344                                          │   │
│             │ │                                                │    345                                          │   │
│             │ │                                                │    346  @app.post("/v1/chat/completions")       │   │
│             │ │                                                │    347  @time_func_latency                      │   │
│             │ │                                                │    348  async def                               │   │
│             │ │                                                │ openai_v1_chat_completions(raw_request:         │   │
│             │ │                                                │ Request):                                       │   │
│             │ │                                                │    349      return await                        │   │
│             │ │                                                │ v1_chat_completions(tokenizer_manager,          │   │
│             │ │                                                │ raw_request)                                    │   │
│             │ │                                                │    350                                          │   │
│             │ │                                                │    351                                          │   │
│             │ │                                                │    352  @app.post("/v1/embeddings",             │   │
│             │ │                                                │ response_class=ORJSONResponse)                  │   │
│             │ │                                                │    353  @time_func_latency                      │   │
│             │ │                                                │    354  async def                               │   │
│             │ │                                                │ openai_v1_embeddings(raw_request: Request):     │   │
│             │ │                                                │    355      response = await                    │   │
│             │ │                                                │ v1_embeddings(tokenizer_manager, raw_request)   │   │
│             │ │                                                │    356      return response                     │   │
│             │ │                                                │    357                                          │   │
│             │ │                                                │    358                                          │   │
│             │ │                                                │    359  @app.get("/v1/models",                  │   │
│             │ │                                                │ response_class=ORJSONResponse)                  │   │
│             │ │                                                │    360  def available_models():                 │   │
│             │ │                                                │    361      """Show available models."""        │   │
│             │ │                                                │    362      served_model_names =                │   │
│             │ │                                                │    363      model_cards = []                    │   │
│             │ │                                                │    364      for served_model_name in            │   │
│             │ │                                                │ served_model_names:                             │   │
│             │ │                                                │    365                                          │   │
│             │ │                                                │ model_cards.append(ModelCard(id=served_model_n… │   │
│             │ │                                                │ root=served_model_name))                        │   │
│             │ │                                                │    366      return ModelList(data=model_cards)  │   │
│             │ │                                                │    367                                          │   │
│             │ │                                                │    368                                          │   │
│             │ │                                                │    369  @app.post("/v1/files")                  │   │
│             │ │                                                │    370  async def openai_v1_files(file:         │   │
│             │ │                                                │ UploadFile = File(...), purpose: str =          │   │
│             │ │                                                │ Form("batch")):                                 │   │
│             │ │                                                │    371      return await v1_files_create(       │   │
│             │ │                                                │    372          file, purpose,                  │   │
│             │ │                                                │ tokenizer_manager.server_args.file_storage_pth  │   │
│             │ │                                                │    373      )                                   │   │
│             │ │                                                │    374                                          │   │
│             │ │                                                │    375                                          │   │
│             │ │                                                │    376  @app.delete("/v1/files/{file_id}")      │   │
│             │ │                                                │    377  async def delete_file(file_id: str):    │   │
│             │ │                                                │    378      #                                   │   │
│             │ │                                                │ https://platform.openai.com/docs/api-reference… │   │
│             │ │                                                │    379      return await                        │   │
│             │ │                                                │ v1_delete_file(file_id)                         │   │
│             │ │                                                │    380                                          │   │
│             │ │                                                │    381                                          │   │
│             │ │                                                │    382  @app.post("/v1/batches")                │   │
│             │ │                                                │    383  async def                               │   │
│             │ │                                                │ openai_v1_batches(raw_request: Request):        │   │
│             │ │                                                │    384      return await                        │   │
│             │ │                                                │ v1_batches(tokenizer_manager, raw_request)      │   │
│             │ │                                                │    385                                          │   │
│             │ │                                                │    386                                          │   │
│             │ │                                                │    387                                          │   │
│             │ │                                                │ @app.post("/v1/batches/{batch_id}/cancel")      │   │
│             │ │                                                │    388  async def cancel_batches(batch_id:      │   │
│             │ │                                                │ str):                                           │   │
│             │ │                                                │    389      #                                   │   │
│             │ │                                                │ https://platform.openai.com/docs/api-reference… │   │
│             │ │                                                │    390      return await                        │   │
│             │ │                                                │ v1_cancel_batch(tokenizer_manager, batch_id)    │   │
│             │ │                                                │    391                                          │   │
│             │ │                                                │    392                                          │   │
│             │ │                                                │    393  @app.get("/v1/batches/{batch_id}")      │   │
│             │ │                                                │    394  async def retrieve_batch(batch_id:      │   │
│             │ │                                                │ str):                                           │   │
│             │ │                                                │    395      return await                        │   │
│             │ │                                                │ v1_retrieve_batch(batch_id)                     │   │
│             │ │                                                │    396                                          │   │
│             │ │                                                │    397                                          │   │
│             │ │                                                │    398  @app.get("/v1/files/{file_id}")         │   │
│             │ │                                                │    399  async def retrieve_file(file_id: str):  │   │
│             │ │                                                │    400      #                                   │   │
│             │ │                                                │ https://platform.openai.com/docs/api-reference… │   │
│             │ │                                                │    401      return await                        │   │
│             │ │                                                │ v1_retrieve_file(file_id)                       │   │
│             │ │                                                │    402                                          │   │
│             │ │                                                │    403                                          │   │
│             │ │                                                │    404  @app.get("/v1/files/{file_id}/content") │   │
│             │ │                                                │    405  async def                               │   │
│             │ │                                                │ retrieve_file_content(file_id: str):            │   │
│             │ │                                                │    406      #                                   │   │
│             │ │                                                │ https://platform.openai.com/docs/api-reference… │   │
│             │ │                                                │    407      return await                        │   │
│             │ │                                                │ v1_retrieve_file_content(file_id)               │   │
│             │ │                                                │    408                                          │   │
│             │ │                                                │    409                                          │   │
│             │ │                                                │    410  def _create_error_response(e):          │   │
│             │ │                                                │    411      return ORJSONResponse(              │   │
│             │ │                                                │    412          {"error": {"message": str(e)}}, │   │
│             │ │                                                │ status_code=HTTPStatus.BAD_REQUEST              │   │
│             │ │                                                │    413      )                                   │   │
│             │ │                                                │    414                                          │   │
│             │ │                                                │    415                                          │   │
│             │ │                                                │    416  def launch_engine(                      │   │
│             │ │                                                │    417      server_args: ServerArgs,            │   │
│             │ │                                                │    418  ):                                      │   │
│             │ │                                                │    419      """                                 │   │
│             │ │                                                │    420      Launch the TokenizerManager in the  │   │
│             │ │                                                │ main process, the Scheduler in a subprocess,    │   │
│             │ │                                                │ and the DetokenizerManager in another           │   │
│             │ │                                                │ subprocess.                                     │   │
│             │ │                                                │    421      """                                 │   │
│             │ │                                                │    422                                          │   │
│             │ │                                                │    423      global tokenizer_manager            │   │
│             │ │                                                │    424      global scheduler_info               │   │
│             │ │                                                │    425                                          │   │
│             │ │                                                │    426      # Configure global environment      │   │
│             │ │                                                │    427      configure_logger(server_args)       │   │
│             │ │                                                │    428      server_args.check_server_args()     │   │
│             │ │                                                │    429      _set_envs_and_config(server_args)   │   │
│             │ │                                                │    430                                          │   │
│             │ │                                                │    431      # Allocate ports for inter-process  │   │
│             │ │                                                │ communications                                  │   │
│             │ │                                                │    432      port_args =                         │   │
│             │ │                                                │ PortArgs.init_new(server_args)                  │   │
│             │ │                                                │    433      logger.info(f"{server_args=}")      │   │
│             │ │                                                │    434                                          │   │
│             │ │                                                │    435      # If using model from               │   │
│             │ │                                                │ www.modelscope.cn, first download the model.    │   │
│             │ │                                                │    436      server_args.model_path,             │   │
│             │ │                                                │ server_args.tokenizer_path =                    │   │
│             │ │                                                │ prepare_model_and_tokenizer(                    │   │
│             │ │                                                │    437          server_args.model_path,         │   │
│             │ │                                                │ server_args.tokenizer_path                      │   │
│             │ │                                                │    438      )                                   │   │
│             │ │                                                │    439                                          │   │
│             │ │                                                │    440      if server_args.dp_size == 1:        │   │
│             │ │                                                │    441          # Launch tensor parallel        │   │
│             │ │                                                │ scheduler processes                             │   │
│             │ │                                                │    442          scheduler_procs = []            │   │
│             │ │                                                │    443          scheduler_pipe_readers = []     │   │
│             │ │                                                │    444          tp_size_per_node =              │   │
│             │ │                                                │ server_args.tp_size // server_args.nnodes       │   │
│             │ │                                                │    445          tp_rank_range = range(          │   │
│             │ │                                                │    446              tp_size_per_node *          │   │
│             │ │                                                │ server_args.node_rank,                          │   │
│             │ │                                                │    447              tp_size_per_node *          │   │
│             │ │                                                │ (server_args.node_rank + 1),                    │   │
│             │ │                                                │    448          )                               │   │
│             │ │                                                │    449          for tp_rank in tp_rank_range:   │   │
│             │ │                                                │    450              reader, writer =            │   │
│             │ │                                                │ mp.Pipe(duplex=False)                           │   │
│             │ │                                                │    451              gpu_id =                    │   │
│             │ │                                                │ server_args.base_gpu_id + tp_rank %             │   │
│             │ │                                                │ tp_size_per_node                                │   │
│             │ │                                                │    452              proc = mp.Process(          │   │
│             │ │                                                │    453                                          │   │
│             │ │                                                │ target=run_scheduler_process,                   │   │
│             │ │                                                │    454                  args=(server_args,      │   │
│             │ │                                                │ port_args, gpu_id, tp_rank, None, writer),      │   │
│             │ │                                                │    455              )                           │   │
│             │ │                                                │    456              proc.start()                │   │
│             │ │                                                │    457                                          │   │
│             │ │                                                │ scheduler_procs.append(proc)                    │   │
│             │ │                                                │    458                                          │   │
│             │ │                                                │ scheduler_pipe_readers.append(reader)           │   │
│             │ │                                                │    459                                          │   │
│             │ │                                                │    460          if server_args.node_rank >= 1:  │   │
│             │ │                                                │    461              # For other nodes, they do  │   │
│             │ │                                                │ not need to run tokenizer or detokenizer,       │   │
│             │ │                                                │    462              # so they can just wait     │   │
│             │ │                                                │ here.                                           │   │
│             │ │                                                │    463              for proc in                 │   │
│             │ │                                                │ scheduler_procs:                                │   │
│             │ │                                                │    464                  proc.join()             │   │
│             │ │                                                │    465      else:                               │   │
│             │ │                                                │    466          # Launch the data parallel      │   │
│             │ │                                                │ controller                                      │   │
│             │ │                                                │    467          reader, writer =                │   │
│             │ │                                                │ mp.Pipe(duplex=False)                           │   │
│             │ │                                                │    468          scheduler_pipe_readers =        │   │
│             │ │                                                │    469          proc = mp.Process(              │   │
│             │ │                                                │    470                                          │   │
│             │ │                                                │ target=run_data_parallel_controller_process,    │   │
│             │ │                                                │    471              args=(server_args,          │   │
│             │ │                                                │ port_args, writer),                             │   │
│             │ │                                                │    472          )                               │   │
│             │ │                                                │    473          proc.start()                    │   │
│             │ │                                                │    474                                          │   │
│             │ │                                                │    475      # Launch detokenizer process        │   │
│             │ │                                                │    476      detoken_proc = mp.Process(          │   │
│             │ │                                                │    477          target=run_detokenizer_process, │   │
│             │ │                                                │    478          args=(                          │   │
│             │ │                                                │    479              server_args,                │   │
│             │ │                                                │    480              port_args,                  │   │
│             │ │                                                │    481          ),                              │   │
│             │ │                                                │    482      )                                   │   │
│             │ │                                                │    483      detoken_proc.start()                │   │
│             │ │                                                │    484                                          │   │
│             │ │                                                │    485      # Launch tokenizer process          │   │
│             │ │                                                │    486      tokenizer_manager =                 │   │
│             │ │                                                │ TokenizerManager(server_args, port_args)        │   │
│             │ │                                                │    487      if server_args.chat_template:       │   │
│             │ │                                                │    488                                          │   │
│             │ │                                                │ load_chat_template_for_openai_api(tokenizer_ma… │   │
│             │ │                                                │ server_args.chat_template)                      │   │
│             │ │                                                │    489                                          │   │
│             │ │                                                │    490      # Wait for model to finish loading  │   │
│             │ │                                                │    491      scheduler_infos = []                │   │
│             │ │                                                │    492      for i in                            │   │
│             │ │                                                │ range(len(scheduler_pipe_readers)):             │   │
│             │ │                                                │    493          try:                            │   │
│             │ │                                                │    494              data =                      │   │
│             │ │                                                │ scheduler_pipe_readers.recv()                   │   │
│             │ │                                                │    495          except EOFError as e:           │   │
│             │ │                                                │    496              logger.exception(e)         │   │
│             │ │                                                │    497              logger.error(               │   │
│             │ │                                                │    498                  f"Rank {i} scheduler is │   │
│             │ │                                                │ dead. Pleas<response clipped><NOTE>To save on   │   │
│             │ │                                                │ context only part of this file has been shown   │   │
│             │ │                                                │ to you. You should retry this tool after you    │   │
│             │ │                                                │ have searched inside the file with `grep -n` in │   │
│             │ │                                                │ order to find the line numbers of what you are  │   │
│             │ │                                                │ looking for.</NOTE>                             │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 211262 Output: 1571                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 11: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │ 877:    def update_weights_from_tensor(self,    │   │
│             │ │ "update_weights_from_tensor\\|weights.*tensor" │ name, tensor):                                  │   │
│             │ │ -n                                             │ 882:                                            │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ tokenizer_manager.update_weights_from_tensor(o… │   │
│             │ │ 'restart': False}                              │ None)                                           │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 249123 Output: 1781                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 12: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │    840          }                               │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │    841                                          │   │
│             │ │ 'view_range': [840, 940]}                      │    842      def init_weights_update_group(      │   │
│             │ │                                                │    843          self,                           │   │
│             │ │                                                │    844          master_address: str,            │   │
│             │ │                                                │    845          master_port: int,               │   │
│             │ │                                                │    846          rank_offset: int,               │   │
│             │ │                                                │    847          world_size: int,                │   │
│             │ │                                                │    848          group_name: str,                │   │
│             │ │                                                │    849          backend: str = "nccl",          │   │
│             │ │                                                │    850      ):                                  │   │
│             │ │                                                │    851          """Initialize parameter update  │   │
│             │ │                                                │ group."""                                       │   │
│             │ │                                                │    852          obj =                           │   │
│             │ │                                                │ InitWeightsUpdateGroupReqInput(                 │   │
│             │ │                                                │    853                                          │   │
│             │ │                                                │ master_address=master_address,                  │   │
│             │ │                                                │    854              master_port=master_port,    │   │
│             │ │                                                │    855              rank_offset=rank_offset,    │   │
│             │ │                                                │    856              world_size=world_size,      │   │
│             │ │                                                │    857              group_name=group_name,      │   │
│             │ │                                                │    858              backend=backend,            │   │
│             │ │                                                │    859          )                               │   │
│             │ │                                                │    860          loop = asyncio.get_event_loop() │   │
│             │ │                                                │    861          return loop.run_until_complete( │   │
│             │ │                                                │    862                                          │   │
│             │ │                                                │ tokenizer_manager.init_weights_update_group(ob… │   │
│             │ │                                                │ None)                                           │   │
│             │ │                                                │    863          )                               │   │
│             │ │                                                │    864                                          │   │
│             │ │                                                │    865      def                                 │   │
│             │ │                                                │ update_weights_from_distributed(self, name,     │   │
│             │ │                                                │ dtype, shape):                                  │   │
│             │ │                                                │    866          """Update weights from          │   │
│             │ │                                                │ distributed source."""                          │   │
│             │ │                                                │    867          obj =                           │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput(           │   │
│             │ │                                                │    868              name=name,                  │   │
│             │ │                                                │    869              dtype=dtype,                │   │
│             │ │                                                │    870              shape=shape,                │   │
│             │ │                                                │    871          )                               │   │
│             │ │                                                │    872          loop = asyncio.get_event_loop() │   │
│             │ │                                                │    873          return loop.run_until_complete( │   │
│             │ │                                                │    874                                          │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_distribu… │   │
│             │ │                                                │ None)                                           │   │
│             │ │                                                │    875          )                               │   │
│             │ │                                                │    876                                          │   │
│             │ │                                                │    877      def                                 │   │
│             │ │                                                │ update_weights_from_tensor(self, name, tensor): │   │
│             │ │                                                │    878          """Update weights from          │   │
│             │ │                                                │ distributed source."""                          │   │
│             │ │                                                │    879          obj =                           │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput(name=name,      │   │
│             │ │                                                │ tensor=tensor)                                  │   │
│             │ │                                                │    880          loop = asyncio.get_event_loop() │   │
│             │ │                                                │    881          return loop.run_until_complete( │   │
│             │ │                                                │    882                                          │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_tensor(o… │   │
│             │ │                                                │ None)                                           │   │
│             │ │                                                │    883          )                               │   │
│             │ │                                                │    884                                          │   │
│             │ │                                                │    885      def get_weights_by_name(self, name, │   │
│             │ │                                                │ truncate_size=100):                             │   │
│             │ │                                                │    886          """Get weights by parameter     │   │
│             │ │                                                │ name."""                                        │   │
│             │ │                                                │    887          obj =                           │   │
│             │ │                                                │ GetWeightsByNameReqInput(name=name,             │   │
│             │ │                                                │ truncate_size=truncate_size)                    │   │
│             │ │                                                │    888          loop = asyncio.get_event_loop() │   │
│             │ │                                                │    889          return                          │   │
│             │ │                                                │ loop.run_until_complete(tokenizer_manager.get_… │   │
│             │ │                                                │ None))                                          │   │
│             │ │                                                │    890                                          │   │
│             │ │                                                │    891                                          │   │
│             │ │                                                │    892  class Runtime:                          │   │
│             │ │                                                │    893      """                                 │   │
│             │ │                                                │    894      A wrapper for the HTTP server.      │   │
│             │ │                                                │    895      This is used for launching the      │   │
│             │ │                                                │ server in a python program without              │   │
│             │ │                                                │    896      using the commond line interface.   │   │
│             │ │                                                │    897                                          │   │
│             │ │                                                │    898      It is mainly used for the frontend  │   │
│             │ │                                                │ language.                                       │   │
│             │ │                                                │    899      You should use the Engine class     │   │
│             │ │                                                │ above if you want to do normal offline          │   │
│             │ │                                                │ processing.                                     │   │
│             │ │                                                │    900      """                                 │   │
│             │ │                                                │    901                                          │   │
│             │ │                                                │    902      def __init__(                       │   │
│             │ │                                                │    903          self,                           │   │
│             │ │                                                │    904          log_level: str = "error",       │   │
│             │ │                                                │    905          *args,                          │   │
│             │ │                                                │    906          **kwargs,                       │   │
│             │ │                                                │    907      ):                                  │   │
│             │ │                                                │    908          """See the arguments in         │   │
│             │ │                                                │ server_args.py::ServerArgs"""                   │   │
│             │ │                                                │    909          self.server_args =              │   │
│             │ │                                                │ ServerArgs(*args, log_level=log_level,          │   │
│             │ │                                                │ **kwargs)                                       │   │
│             │ │                                                │    910                                          │   │
│             │ │                                                │    911          # before python program         │   │
│             │ │                                                │ terminates, call shutdown implicitly.           │   │
│             │ │                                                │ Therefore, users don't have to explicitly call  │   │
│             │ │                                                │ .shutdown()                                     │   │
│             │ │                                                │    912          atexit.register(self.shutdown)  │   │
│             │ │                                                │    913                                          │   │
│             │ │                                                │    914          # Pre-allocate ports            │   │
│             │ │                                                │    915          for port in range(10000,        │   │
│             │ │                                                │ 40000):                                         │   │
│             │ │                                                │    916              if is_port_available(port): │   │
│             │ │                                                │    917                  break                   │   │
│             │ │                                                │    918              port += 1                   │   │
│             │ │                                                │    919          self.server_args.port = port    │   │
│             │ │                                                │    920                                          │   │
│             │ │                                                │    921          self.url =                      │   │
│             │ │                                                │ self.server_args.url()                          │   │
│             │ │                                                │    922          self.generate_url = self.url +  │   │
│             │ │                                                │ "/generate"                                     │   │
│             │ │                                                │    923                                          │   │
│             │ │                                                │    924          # NOTE: We store pid instead of │   │
│             │ │                                                │ proc to fix some issues during __delete__       │   │
│             │ │                                                │    925          self.pid = None                 │   │
│             │ │                                                │    926          pipe_reader, pipe_writer =      │   │
│             │ │                                                │ mp.Pipe(duplex=False)                           │   │
│             │ │                                                │    927                                          │   │
│             │ │                                                │    928          proc = mp.Process(              │   │
│             │ │                                                │    929              target=launch_server,       │   │
│             │ │                                                │    930              args=(self.server_args,     │   │
│             │ │                                                │ pipe_writer),                                   │   │
│             │ │                                                │    931          )                               │   │
│             │ │                                                │    932          proc.start()                    │   │
│             │ │                                                │    933          pipe_writer.close()             │   │
│             │ │                                                │    934          self.pid = proc.pid             │   │
│             │ │                                                │    935                                          │   │
│             │ │                                                │    936          try:                            │   │
│             │ │                                                │    937              init_state =                │   │
│             │ │                                                │ pipe_reader.recv()                              │   │
│             │ │                                                │    938          except EOFError:                │   │
│             │ │                                                │    939              init_state = ""             │   │
│             │ │                                                │    940                                          │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 287238 Output: 1869                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 13: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  import unittest                         │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2                                          │   │
│             │ │ 'view_range': [1, -1]}                         │      3  import torch                            │   │
│             │ │                                                │      4                                          │   │
│             │ │                                                │      5  import sglang as sgl                    │   │
│             │ │                                                │      6  from sglang.test.test_utils import      │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST               │   │
│             │ │                                                │      7                                          │   │
│             │ │                                                │      8                                          │   │
│             │ │                                                │      9  class                                   │   │
│             │ │                                                │ TestReleaseGPUOccupation(unittest.TestCase):    │   │
│             │ │                                                │     10      def                                 │   │
│             │ │                                                │ test_release_and_resume_occupation(self):       │   │
│             │ │                                                │     11          engine =                        │   │
│             │ │                                                │ sgl.Engine(model_path=DEFAULT_SMALL_MODEL_NAME… │   │
│             │ │                                                │     12                                          │   │
│             │ │                                                │     13          param_name =                    │   │
│             │ │                                                │ "model.layers.2.self_attn.k_proj.weight"        │   │
│             │ │                                                │     14                                          │   │
│             │ │                                                │     15          def                             │   │
│             │ │                                                │ _check_param(expect_values):                    │   │
│             │ │                                                │     16              actual_values =             │   │
│             │ │                                                │ torch.tensor(engine.get_weights_by_name(param_… │   │
│             │ │                                                │ :5]                                             │   │
│             │ │                                                │     17              assert torch.allclose(      │   │
│             │ │                                                │     18                  actual_values,          │   │
│             │ │                                                │ torch.tensor(expect_values), atol=0.001         │   │
│             │ │                                                │     19              ), f"{actual_values=}"      │   │
│             │ │                                                │     20                                          │   │
│             │ │                                                │     21          _check_param([0.0571, -0.0114,  │   │
│             │ │                                                │ 0.0444, 0.0215, -0.0149])                       │   │
│             │ │                                                │     22                                          │   │
│             │ │                                                │     23          new_tensor = torch.full((3072,  │   │
│             │ │                                                │ 2048), 1.5)                                     │   │
│             │ │                                                │     24                                          │   │
│             │ │                                                │ engine.update_weights_from_tensor(param_name,   │   │
│             │ │                                                │ new_tensor)                                     │   │
│             │ │                                                │     25                                          │   │
│             │ │                                                │     26          _check_param([1.5] * 5)         │   │
│             │ │                                                │     27                                          │   │
│             │ │                                                │     28          engine.shutdown()               │   │
│             │ │                                                │     29                                          │   │
│             │ │                                                │     30                                          │   │
│             │ │                                                │     31  if __name__ == "__main__":              │   │
│             │ │                                                │     32      unittest.main()                     │   │
│             │ │                                                │     33                                          │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 326493 Output: 2088                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 14: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │                                                 │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │                                                 │   │
│             │ │ '', 'path':                                    │                                                 │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │                                                 │   │
│             │ │ 'view_range': [1, -1]}                         │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 366386 Output: 2878                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 15: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -R                           │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ "DEFAULT_SMALL_MODEL_NAME_FOR_TEST" -n         │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST,              │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ | head -n 20', 'restart': False}               │ cls.model = DEFAULT_SMALL_MODEL_NAME_FOR_TEST   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ cls.tokenizer =                                 │   │
│             │ │                                                │ get_tokenizer(DEFAULT_SMALL_MODEL_NAME_FOR_TES… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sglang.test.test_utils import                   │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,   │   │
│             │ │                                                │ return_token_ids=True                           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ tokenizer =                                     │   │
│             │ │                                                │ AutoTokenizer.from_pretrained(DEFAULT_SMALL_MO… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST,              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ ("Engine", 1, 1,                                │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST),             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ ("Runtime", 1, 1,                               │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST),             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ test_suits.append(("Engine", 1, 2,              │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST))             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ ("Engine", 2, 2,                                │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST),             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST,              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path = DEFAULT_SMALL_MODEL_NAME_FOR_TEST  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path = DEFAULT_SMALL_MODEL_NAME_FOR_TEST  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path = DEFAULT_SMALL_MODEL_NAME_FOR_TEST  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path = DEFAULT_SMALL_MODEL_NAME_FOR_TEST  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sglang.test.test_utils import                   │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST               │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 407177 Output: 2954                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 16: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │                                                 │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │                                                 │   │
│             │ │ '', 'path':                                    │                                                 │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │                                                 │   │
│             │ │ 'view_range': [1, 80]}                         │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 449229 Output: 3042                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 17: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  import unittest                         │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2                                          │   │
│             │ │ 'view_range': [1, 46]}                         │      3  from transformers import AutoTokenizer  │   │
│             │ │                                                │      4                                          │   │
│             │ │                                                │      5  import sglang as sgl                    │   │
│             │ │                                                │      6  from sglang.test.test_utils import      │   │
│             │ │                                                │ DEFAULT_SMALL_MODEL_NAME_FOR_TEST               │   │
│             │ │                                                │      7                                          │   │
│             │ │                                                │      8                                          │   │
│             │ │                                                │      9  class                                   │   │
│             │ │                                                │ TestEngineTokenIds(unittest.TestCase):          │   │
│             │ │                                                │     10      def                                 │   │
│             │ │                                                │ test_token_ids_in_generate(self):               │   │
│             │ │                                                │     11          llm = sgl.Engine(               │   │
│             │ │                                                │     12                                          │   │
│             │ │                                                │ model_path=DEFAULT_SMALL_MODEL_NAME_FOR_TEST,   │   │
│             │ │                                                │ return_token_ids=True                           │   │
│             │ │                                                │     13          )                               │   │
│             │ │                                                │     14          tokenizer =                     │   │
│             │ │                                                │ AutoTokenizer.from_pretrained(DEFAULT_SMALL_MO… │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16          prompts = [                     │   │
│             │ │                                                │     17              "Hello, my name is",        │   │
│             │ │                                                │     18              "The president of the       │   │
│             │ │                                                │ United States is",                              │   │
│             │ │                                                │     19              "The capital of France is", │   │
│             │ │                                                │     20              "The future of AI is",      │   │
│             │ │                                                │     21          ]                               │   │
│             │ │                                                │     22                                          │   │
│             │ │                                                │     23          sampling_params =               │   │
│             │ │                                                │ {"temperature": 0, "top_p": 0.95}               │   │
│             │ │                                                │     24          outputs = llm.generate(prompts, │   │
│             │ │                                                │ sampling_params)                                │   │
│             │ │                                                │     25                                          │   │
│             │ │                                                │     26          for prompt, output in           │   │
│             │ │                                                │ zip(prompts, outputs):                          │   │
│             │ │                                                │     27              deocode_input =             │   │
│             │ │                                                │ tokenizer.decode(                               │   │
│             │ │                                                │     28                  output["input_ids"],    │   │
│             │ │                                                │ skip_special_tokens=True                        │   │
│             │ │                                                │     29              )                           │   │
│             │ │                                                │     30              assert (deocode_input in    │   │
│             │ │                                                │ prompt) or (                                    │   │
│             │ │                                                │     31                  prompt in deocode_input │   │
│             │ │                                                │     32              ), f"Decode input:          │   │
│             │ │                                                │ {deocode_input} mismatch for: {prompt}"         │   │
│             │ │                                                │     33                                          │   │
│             │ │                                                │     34              deocode_output =            │   │
│             │ │                                                │ tokenizer.decode(                               │   │
│             │ │                                                │     35                  output["output_ids"],   │   │
│             │ │                                                │ skip_special_tokens=True                        │   │
│             │ │                                                │     36              )                           │   │
│             │ │                                                │     37              assert (deocode_output in   │   │
│             │ │                                                │ output["text"]) or (                            │   │
│             │ │                                                │     38                  output["text"] in       │   │
│             │ │                                                │ deocode_output                                  │   │
│             │ │                                                │     39              ), f"Decode output:         │   │
│             │ │                                                │ {deocode_output} mismatch for:                  │   │
│             │ │                                                │ {output['text']}"                               │   │
│             │ │                                                │     40                                          │   │
│             │ │                                                │     41          llm.shutdown()                  │   │
│             │ │                                                │     42                                          │   │
│             │ │                                                │     43                                          │   │
│             │ │                                                │     44  if __name__ == "__main__":              │   │
│             │ │                                                │     45      unittest.main()                     │   │
│             │ │                                                │     46                                          │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 491417 Output: 3130                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 18: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "python - << 'PY'\nfrom importlib  │                                                 │   │
│             │ │ import import_module\nimport                   │                                                 │   │
│             │ │ inspect\nm=import_module('sglang.test.test_ut… │                                                 │   │
│             │ │ file:',m.__file__)\nprint('DEFAULT_SMALL_MODE… │                                                 │   │
│             │ │ 'restart': False}                              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 534224 Output: 3226                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 19: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "ls -R                             │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ few_shot_gsm8k.py                               │   │
│             │ │ | sed -n '1,200p'", 'restart': False}          │ few_shot_gsm8k_engine.py                        │   │
│             │ │                                                │ long_prompt.txt                                 │   │
│             │ │                                                │ run_eval.py                                     │   │
│             │ │                                                │ runners.py                                      │   │
│             │ │                                                │ simple_eval_common.py                           │   │
│             │ │                                                │ simple_eval_gpqa.py                             │   │
│             │ │                                                │ simple_eval_humaneval.py                        │   │
│             │ │                                                │ simple_eval_math.py                             │   │
│             │ │                                                │ simple_eval_mgsm.py                             │   │
│             │ │                                                │ simple_eval_mmlu.py                             │   │
│             │ │                                                │ srt                                             │   │
│             │ │                                                │ test_activation.py                              │   │
│             │ │                                                │ test_block_fp8.py                               │   │
│             │ │                                                │ test_layernorm.py                               │   │
│             │ │                                                │ test_programs.py                                │   │
│             │ │                                                │ test_utils.py                                   │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sampling                                        │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ penaltylib                                      │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ utils.py                                        │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 577460 Output: 3302                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 20: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  """Common utilities for testing and     │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ benchmarking"""                                 │   │
│             │ │ 'view_range': [1, -1]}                         │      2                                          │   │
│             │ │                                                │      3  import argparse                         │   │
│             │ │                                                │      4  import asyncio                          │   │
│             │ │                                                │      5  import copy                             │   │
│             │ │                                                │      6  import os                               │   │
│             │ │                                                │      7  import random                           │   │
│             │ │                                                │      8  import subprocess                       │   │
│             │ │                                                │      9  import threading                        │   │
│             │ │                                                │     10  import time                             │   │
│             │ │                                                │     11  from concurrent.futures import          │   │
│             │ │                                                │ ThreadPoolExecutor                              │   │
│             │ │                                                │     12  from functools import partial           │   │
│             │ │                                                │     13  from types import SimpleNamespace       │   │
│             │ │                                                │     14  from typing import Callable, List,      │   │
│             │ │                                                │ Optional                                        │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import numpy as np                      │   │
│             │ │                                                │     17  import requests                         │   │
│             │ │                                                │     18  import torch                            │   │
│             │ │                                                │     19  import torch.nn.functional as F         │   │
│             │ │                                                │     20                                          │   │
│             │ │                                                │     21  from sglang.bench_serving import        │   │
│             │ │                                                │ run_benchmark                                   │   │
│             │ │                                                │     22  from sglang.global_config import        │   │
│             │ │                                                │ global_config                                   │   │
│             │ │                                                │     23  from sglang.lang.backend.openai import  │   │
│             │ │                                                │ OpenAI                                          │   │
│             │ │                                                │     24  from                                    │   │
│             │ │                                                │ sglang.lang.backend.runtime_endpoint import     │   │
│             │ │                                                │ RuntimeEndpoint                                 │   │
│             │ │                                                │     25  from sglang.srt.utils import            │   │
│             │ │                                                │ get_bool_env_var, kill_process_tree             │   │
│             │ │                                                │     26  from sglang.test.run_eval import        │   │
│             │ │                                                │ run_eval                                        │   │
│             │ │                                                │     27  from sglang.utils import                │   │
│             │ │                                                │ get_exception_traceback                         │   │
│             │ │                                                │     28                                          │   │
│             │ │                                                │     29  DEFAULT_FP8_MODEL_NAME_FOR_TEST =       │   │
│             │ │                                                │ "neuralmagic/Meta-Llama-3.1-8B-FP8"             │   │
│             │ │                                                │     30  DEFAULT_MODEL_NAME_FOR_TEST =           │   │
│             │ │                                                │ "meta-llama/Llama-3.1-8B-Instruct"              │   │
│             │ │                                                │     31  DEFAULT_SMALL_MODEL_NAME_FOR_TEST =     │   │
│             │ │                                                │ "meta-llama/Llama-3.2-1B-Instruct"              │   │
│             │ │                                                │     32  DEFAULT_MOE_MODEL_NAME_FOR_TEST =       │   │
│             │ │                                                │ "mistralai/Mixtral-8x7B-Instruct-v0.1"          │   │
│             │ │                                                │     33  DEFAULT_SMALL_MOE_MODEL_NAME_FOR_TEST = │   │
│             │ │                                                │ "Qwen/Qwen1.5-MoE-A2.7B"                        │   │
│             │ │                                                │     34                                          │   │
│             │ │                                                │ DEFAULT_SMALL_EMBEDDING_MODEL_NAME_FOR_TEST =   │   │
│             │ │                                                │ "Alibaba-NLP/gte-Qwen2-1.5B-instruct"           │   │
│             │ │                                                │     35  DEFAULT_MLA_MODEL_NAME_FOR_TEST =       │   │
│             │ │                                                │ "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"   │   │
│             │ │                                                │     36  DEFAULT_MLA_FP8_MODEL_NAME_FOR_TEST =   │   │
│             │ │                                                │ "neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-F… │   │
│             │ │                                                │     37  DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH = 600 │   │
│             │ │                                                │     38  DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_TP1 │   │
│             │ │                                                │ =                                               │   │
│             │ │                                                │ "meta-llama/Llama-3.1-8B-Instruct,mistralai/Mi… │   │
│             │ │                                                │     39  DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_TP2 │   │
│             │ │                                                │ =                                               │   │
│             │ │                                                │ "meta-llama/Llama-3.1-70B-Instruct,mistralai/M… │   │
│             │ │                                                │     40                                          │   │
│             │ │                                                │ DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_FP8_TP1 =   │   │
│             │ │                                                │ "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8,ne… │   │
│             │ │                                                │     41                                          │   │
│             │ │                                                │ DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_FP8_TP2 =   │   │
│             │ │                                                │ "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8,n… │   │
│             │ │                                                │     42                                          │   │
│             │ │                                                │ DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_QUANT_TP1 = │   │
│             │ │                                                │ "hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ… │   │
│             │ │                                                │     43                                          │   │
│             │ │                                                │     44                                          │   │
│             │ │                                                │     45  def is_in_ci():                         │   │
│             │ │                                                │     46      """Return whether it is in CI       │   │
│             │ │                                                │ runner."""                                      │   │
│             │ │                                                │     47      return                              │   │
│             │ │                                                │ get_bool_env_var("SGLANG_IS_IN_CI")             │   │
│             │ │                                                │     48                                          │   │
│             │ │                                                │     49                                          │   │
│             │ │                                                │     50  if is_in_ci():                          │   │
│             │ │                                                │     51      DEFAULT_PORT_FOR_SRT_TEST_RUNNER =  │   │
│             │ │                                                │ 5157                                            │   │
│             │ │                                                │     52      DEFAULT_URL_FOR_TEST =              │   │
│             │ │                                                │ "http://127.0.0.1:6157"                         │   │
│             │ │                                                │     53  else:                                   │   │
│             │ │                                                │     54      DEFAULT_PORT_FOR_SRT_TEST_RUNNER =  │   │
│             │ │                                                │ 1157                                            │   │
│             │ │                                                │     55      DEFAULT_URL_FOR_TEST =              │   │
│             │ │                                                │ "http://127.0.0.1:2157"                         │   │
│             │ │                                                │     56                                          │   │
│             │ │                                                │     57                                          │   │
│             │ │                                                │     58  def call_generate_lightllm(prompt,      │   │
│             │ │                                                │ temperature, max_tokens, stop=None, url=None):  │   │
│             │ │                                                │     59      assert url is not None              │   │
│             │ │                                                │     60                                          │   │
│             │ │                                                │     61      data = {                            │   │
│             │ │                                                │     62          "inputs": prompt,               │   │
│             │ │                                                │     63          "parameters": {                 │   │
│             │ │                                                │     64              "temperature": temperature, │   │
│             │ │                                                │     65              "max_new_tokens":           │   │
│             │ │                                                │ max_tokens,                                     │   │
│             │ │                                                │     66              "stop_sequences": stop,     │   │
│             │ │                                                │     67          },                              │   │
│             │ │                                                │     68      }                                   │   │
│             │ │                                                │     69      res = requests.post(url, json=data) │   │
│             │ │                                                │     70      assert res.status_code == 200       │   │
│             │ │                                                │     71      pred =                              │   │
│             │ │                                                │ res.json()["generated_text"][0]                 │   │
│             │ │                                                │     72      return pred                         │   │
│             │ │                                                │     73                                          │   │
│             │ │                                                │     74                                          │   │
│             │ │                                                │     75  def call_generate_vllm(prompt,          │   │
│             │ │                                                │ temperature, max_tokens, stop=None, n=1,        │   │
│             │ │                                                │ url=None):                                      │   │
│             │ │                                                │     76      assert url is not None              │   │
│             │ │                                                │     77                                          │   │
│             │ │                                                │     78      data = {                            │   │
│             │ │                                                │     79          "prompt": prompt,               │   │
│             │ │                                                │     80          "temperature": temperature,     │   │
│             │ │                                                │     81          "max_tokens": max_tokens,       │   │
│             │ │                                                │     82          "stop": stop,                   │   │
│             │ │                                                │     83          "n": n,                         │   │
│             │ │                                                │     84      }                                   │   │
│             │ │                                                │     85      res = requests.post(url, json=data) │   │
│             │ │                                                │     86      assert res.status_code == 200       │   │
│             │ │                                                │     87      if n == 1:                          │   │
│             │ │                                                │     88          pred = res.json()["text"][0]    │   │
│             │ │                                                │     89      else:                               │   │
│             │ │                                                │     90          pred = [x for x in              │   │
│             │ │                                                │ res.json()["text"]]                             │   │
│             │ │                                                │     91      return pred                         │   │
│             │ │                                                │     92                                          │   │
│             │ │                                                │     93                                          │   │
│             │ │                                                │     94  def call_generate_outlines(             │   │
│             │ │                                                │     95      prompt, temperature, max_tokens,    │   │
│             │ │                                                │ stop=None, regex=None, n=1, url=None            │   │
│             │ │                                                │     96  ):                                      │   │
│             │ │                                                │     97      assert url is not None              │   │
│             │ │                                                │     98                                          │   │
│             │ │                                                │     99      data = {                            │   │
│             │ │                                                │    100          "prompt": prompt,               │   │
│             │ │                                                │    101          "temperature": temperature,     │   │
│             │ │                                                │    102          "max_tokens": max_tokens,       │   │
│             │ │                                                │    103          "stop": stop,                   │   │
│             │ │                                                │    104          "regex": regex,                 │   │
│             │ │                                                │    105          "n": n,                         │   │
│             │ │                                                │    106      }                                   │   │
│             │ │                                                │    107      res = requests.post(url, json=data) │   │
│             │ │                                                │    108      assert res.status_code == 200       │   │
│             │ │                                                │    109      if n == 1:                          │   │
│             │ │                                                │    110          pred = res.json()["text"][0]    │   │
│             │ │                                                │    111      else:                               │   │
│             │ │                                                │    112          pred = [x for x in              │   │
│             │ │                                                │ res.json()["text"]]                             │   │
│             │ │                                                │    113      return pred                         │   │
│             │ │                                                │    114                                          │   │
│             │ │                                                │    115                                          │   │
│             │ │                                                │    116  def call_generate_srt_raw(prompt,       │   │
│             │ │                                                │ temperature, max_tokens, stop=None, url=None):  │   │
│             │ │                                                │    117      assert url is not None              │   │
│             │ │                                                │    118                                          │   │
│             │ │                                                │    119      data = {                            │   │
│             │ │                                                │    120          "text": prompt,                 │   │
│             │ │                                                │    121          "sampling_params": {            │   │
│             │ │                                                │    122              "temperature": temperature, │   │
│             │ │                                                │    123              "max_new_tokens":           │   │
│             │ │                                                │ max_tokens,                                     │   │
│             │ │                                                │    124              "stop": stop,               │   │
│             │ │                                                │    125          },                              │   │
│             │ │                                                │    126      }                                   │   │
│             │ │                                                │    127      res = requests.post(url, json=data) │   │
│             │ │                                                │    128      assert res.status_code == 200       │   │
│             │ │                                                │    129      obj = res.json()                    │   │
│             │ │                                                │    130      pred = obj["text"]                  │   │
│             │ │                                                │    131      return pred                         │   │
│             │ │                                                │    132                                          │   │
│             │ │                                                │    133                                          │   │
│             │ │                                                │    134  def call_generate_gserver(prompt,       │   │
│             │ │                                                │ temperature, max_tokens, stop=None, url=None):  │   │
│             │ │                                                │    135      raise NotImplementedError()         │   │
│             │ │                                                │    136                                          │   │
│             │ │                                                │    137                                          │   │
│             │ │                                                │    138  def call_generate_guidance(             │   │
│             │ │                                                │    139      prompt, temperature, max_tokens,    │   │
│             │ │                                                │ stop=None, n=1, regex=None, model=None          │   │
│             │ │                                                │    140  ):                                      │   │
│             │ │                                                │    141      assert model is not None            │   │
│             │ │                                                │    142      from guidance import gen            │   │
│             │ │                                                │    143                                          │   │
│             │ │                                                │    144      rets = []                           │   │
│             │ │                                                │    145      for _ in range(n):                  │   │
│             │ │                                                │    146          out = (                         │   │
│             │ │                                                │    147              model                       │   │
│             │ │                                                │    148              + prompt                    │   │
│             │ │                                                │    149              + gen(                      │   │
│             │ │                                                │    150                  name="answer",          │   │
│             │ │                                                │    151                  max_tokens=max_tokens,  │   │
│             │ │                                                │    152                                          │   │
│             │ │                                                │ temperature=temperature,                        │   │
│             │ │                                                │    153                  stop=stop,              │   │
│             │ │                                                │    154                  regex=regex,            │   │
│             │ │                                                │    155              )                           │   │
│             │ │                                                │    156          )                               │   │
│             │ │                                                │    157          rets.append(out["answer"])      │   │
│             │ │                                                │    158      return rets if n > 1 else rets[0]   │   │
│             │ │                                                │    159                                          │   │
│             │ │                                                │    160                                          │   │
│             │ │                                                │    161  async def call_generate_lmql(           │   │
│             │ │                                                │    162      prompt, temperature, max_tokens,    │   │
│             │ │                                                │ stop=None, n=1, max_len=4096, model=None,       │   │
│             │ │                                                │ **kwargs                                        │   │
│             │ │                                                │    163  ):                                      │   │
│             │ │                                                │    164      assert model is not None            │   │
│             │ │                                                │    165      import lmql                         │   │
│             │ │                                                │    166                                          │   │
│             │ │                                                │    167      if stop != None:                    │   │
│             │ │                                                │    168                                          │   │
│             │ │                                                │    169          @lmql.query(model=model)        │   │
│             │ │                                                │    170          async def program(question,     │   │
│             │ │                                                │ max_tokens, stop):                              │   │
│             │ │                                                │    171              '''lmql                     │   │
│             │ │                                                │    172              """{question}[ANSWER]"""    │   │
│             │ │                                                │ where len(TOKENS(ANSWER)) < max_tokens and      │   │
│             │ │                                                │ STOPS_AT(ANSWER, stop)                          │   │
│             │ │                                                │    173              return ANSWER               │   │
│             │ │                                                │    174              '''                         │   │
│             │ │                                                │    175                                          │   │
│             │ │                                                │    176      else:                               │   │
│             │ │                                                │    177                                          │   │
│             │ │                                                │    178          @lmql.query(model=model)        │   │
│             │ │                                                │    179          async def program(question,     │   │
│             │ │                                                │ max_tokens):                                    │   │
│             │ │                                                │    180              '''lmql                     │   │
│             │ │                                                │    181              """{question}[ANSWER]"""    │   │
│             │ │                                                │ where len(TOKENS(ANSWER)) < max_tokens          │   │
│             │ │                                                │    182              return ANSWER               │   │
│             │ │                                                │    183              '''                         │   │
│             │ │                                                │    184                                          │   │
│             │ │                                                │    185      tasks = [                           │   │
│             │ │                                                │    186          program(                        │   │
│             │ │                                                │    187              question=prompt,            │   │
│             │ │                                                │    188              temperature=temperature,    │   │
│             │ │                                                │    189              max_tokens=max_tokens,      │   │
│             │ │                                                │    190              stop=stop,                  │   │
│             │ │                                                │    191              max_len=max_len,            │   │
│             │ │                                                │    192              **kwargs,                   │   │
│             │ │                                                │    193          )                               │   │
│             │ │                                                │    194          for _ in range(n)               │   │
│             │ │                                                │    195      ]                                   │   │
│             │ │                                                │    196      rets = await asyncio.gather(*tasks) │   │
│             │ │                                                │    197      return rets if n > 1 else rets[0]   │   │
│             │ │                                                │    198                                          │   │
│             │ │                                                │    199                                          │   │
│             │ │                                                │    200  def call_select_lightllm(context,       │   │
│             │ │                                                │ choices, url=None):                             │   │
│             │ │                                                │    201      assert url is not None              │   │
│             │ │                                                │    202                                          │   │
│             │ │                                                │    203      scores = []                         │   │
│             │ │                                                │    204      for i in range(len(choices)):       │   │
│             │ │                                                │    205          data = {                        │   │
│             │ │                                                │    206              "inputs": context +         │   │
│             │ │                                                │ choices,                                        │   │
│             │ │                                                │    207              "parameters": {             │   │
│             │ │                                                │    208                  "max_new_tokens": 1,    │   │
│             │ │                                                │    209              },                          │   │
│             │ │                                                │    210          }                               │   │
│             │ │                                                │    211          res = requests.post(url,        │   │
│             │ │                                                │ json=data)                                      │   │
│             │ │                                                │    212          assert res.status_code == 200   │   │
│             │ │                                                │    213          scores.append(0)                │   │
│             │ │                                                │    214      return np.argmax(scores)            │   │
│             │ │                                                │    215                                          │   │
│             │ │                                                │    216                                          │   │
│             │ │                                                │    217  def call_select_vllm(context, choices,  │   │
│             │ │                                                │ url=None):                                      │   │
│             │ │                                                │    218      assert url is not None              │   │
│             │ │                                                │    219                                          │   │
│             │ │                                                │    220      scores = []                         │   │
│             │ │                                                │    221      for i in range(len(choices)):       │   │
│             │ │                                                │    222          data = {                        │   │
│             │ │                                                │    223              "prompt": context +         │   │
│             │ │                                                │ choices,                                        │   │
│             │ │                                                │    224              "max_tokens": 1,            │   │
│             │ │                                                │    225              "prompt_logprobs": 1,       │   │
│             │ │                                                │    226          }                               │   │
│             │ │                                                │    227          res = requests.post(url,        │   │
│             │ │                                                │ json=data)                                      │   │
│             │ │                                                │    228          assert res.status_code == 200   │   │
│             │ │                                                │    229                                          │   │
│             │ │                                                │ scores.append(res.json().get("prompt_score",    │   │
│             │ │                                                │ 0))                                             │   │
│             │ │                                                │    230      return np.argmax(scores)            │   │
│             │ │                                                │    231                                          │   │
│             │ │                                                │    232      """                                 │   │
│             │ │                                                │    233      Modify                              │   │
│             │ │                                                │ vllm/entrypoints/api_server.py                  │   │
│             │ │                                                │    234                                          │   │
│             │ │                                                │    235      if final_output.prompt_logprobs is  │   │
│             │ │                                                │ not None:                                       │   │
│             │ │                                                │    236          score = np.mean([prob for t_id, │   │
│             │ │                                                │ prob in zip(final_output.prompt_token_ids[1:],  │   │
│             │ │                                                │ final_output.prompt_logprobs[1:])])             │   │
│             │ │                                                │    237          ret["prompt_score"] = score     │   │
│             │ │                                                │    238      """                                 │   │
│             │ │                                                │    239                                          │   │
│             │ │                                                │    240                                          │   │
│             │ │                                                │    241  def call_select_guidance(context,       │   │
│             │ │                                                │ choices, model=None):                           │   │
│             │ │                                                │    242      assert model is not None            │   │
│             │ │                                                │    243      from guidance import select         │   │
│             │ │                                                │    244                                          │   │
│             │ │                                                │    245      out = model + context +             │   │
│             │ │                                                │ select(choices, name="answer")                  │   │
│             │ │                                                │    246      return choices.index(out["answer"]) │   │
│             │ │                                                │    247                                          │   │
│             │ │                                                │    248                                          │   │
│             │ │                                                │    249  async def call_select_lmql(context,     │   │
│             │ │                                                │ choices, temperature=0, max_len=4096,           │   │
│             │ │                                                │ model=None):                                    │   │
│             │ │                                                │    250      assert model is not None            │   │
│             │ │                                                │    251      import lmql                         │   │
│             │ │                                                │    252                                          │   │
│             │ │                                                │    253      @lmql.query(model=model)            │   │
│             │ │                                                │    254      async def program(ctx, choices):    │   │
│             │ │                                                │    255          '''lmql                         │   │
│             │ │                                                │    256          """{ctx}[ANSWER]""" where       │   │
│             │ │                                                │ ANSWER in set(choices)                          │   │
│             │ │                                                │    257          return ANSWER                   │   │
│             │ │                                                │    258          '''                             │   │
│             │ │                                                │    259                                          │   │
│             │ │                                                │    260      answer = await program(             │   │
│             │ │                                                │    261          ctx=context, choices=choices,   │   │
│             │ │                                                │ temperature=temperature, max_len=max_len        │   │
│             │ │                                                │    262      )                                   │   │
│             │ │                                                │    263      return choices.index(answer)        │   │
│             │ │                                                │    264                                          │   │
│             │ │                                                │    265                                          │   │
│             │ │                                                │    266  def                                     │   │
│             │ │                                                │ add_common_other_args_and_parse(parser:         │   │
│             │ │                                                │ argparse.ArgumentParser):                       │   │
│             │ │                                                │    267      parser.add_argument("--parallel",   │   │
│             │ │                                                │ type=int, default=64)                           │   │
│             │ │                                                │    268      parser.add_argument("--host",       │   │
│             │ │                                                │ type=str, default="http://127.0.0.1")           │   │
│             │ │                                                │    269      parser.add_argument("--port",       │   │
│             │ │                                                │ type=int, default=None)                         │   │
│             │ │                                                │    270      parser.add_argument(                │   │
│             │ │                                                │    271          "--backend",                    │   │
│             │ │                                                │    272          type=str,                       │   │
│             │ │                                                │    273          required=True,                  │   │
│             │ │                                                │    274          choices=[                       │   │
│             │ │                                                │    275              "vllm",                     │   │
│             │ │                                                │    276              "outlines",                 │   │
│             │ │                                                │    277              "lightllm",                 │   │
│             │ │                                                │    278              "gserver",                  │   │
│             │ │                                                │    279              "guidance",                 │   │
│             │ │                                                │    280              "lmql",                     │   │
│             │ │                                                │    281              "srt-raw",                  │   │
│             │ │                                                │    282              "llama.cpp",                │   │
│             │ │                                                │    283          ],                              │   │
│             │ │                                                │    284      )                                   │   │
│             │ │                                                │    285      parser.add_argument("--n-ctx",      │   │
│             │ │                                                │ type=int, default=4096)                         │   │
│             │ │                                                │    286      parser.add_argument(                │   │
│             │ │                                                │    287          "--model-path", type=str,       │   │
│             │ │                                                │ default="meta-llama/Llama-2-7b-chat-hf"         │   │
│             │ │                                                │    288      )                                   │   │
│             │ │                                                │    289                                          │   │
│             │ │                                                │ parser.add_argument("--result-file", type=str,  │   │
│             │ │                                                │ default="result.jsonl")                         │   │
│             │ │                                                │    290      args = parser.parse_args()          │   │
│             │ │                                                │    291                                          │   │
│             │ │                                                │    292      if args.port is None:               │   │
│             │ │                                                │    293          default_port = {                │   │
│             │ │                                                │    294              "vllm": 21000,              │   │
│             │ │                                                │    295              "outlines": 21000,          │   │
│             │ │                                                │    296              "lightllm": 22000,          │   │
│             │ │                                                │    297              "lmql": 23000,              │   │
│             │ │                                                │    298              "srt-raw": 30000,           │   │
│             │ │                                                │    299              "gserver": 9988,            │   │
│             │ │                                                │    300          }                               │   │
│             │ │                                                │    301          args.port =                     │   │
│             │ │                                                │ default_port.get(args.backend, None)            │   │
│             │ │                                                │    302      return args                         │   │
│             │ │                                                │    303                                          │   │
│             │ │                                                │    304                                          │   │
│             │ │                                                │    305  def                                     │   │
│             │ │                                                │ add_common_sglang_args_and_parse(parser:        │   │
│             │ │                                                │ argparse.ArgumentParser):                       │   │
│             │ │                                                │    306      parser.add_argument("--parallel",   │   │
│             │ │                                                │ type=int, default=64)                           │   │
│             │ │                                                │    307      parser.add_argument("--host",       │   │
│             │ │                                                │ type=str, default="http://127.0.0.1")           │   │
│             │ │                                                │    308      parser.add_argument("--port",       │   │
│             │ │                                                │ type=int, default=30000)                        │   │
│             │ │                                                │    309      parser.add_argument("--backend",    │   │
│             │ │                                                │ type=str, default="srt")                        │   │
│             │ │                                                │    310                                          │   │
│             │ │                                                │ parser.add_argument("--result-file", type=str,  │   │
│             │ │                                                │ default="result.jsonl")                         │   │
│             │ │                                                │    311      args = parser.parse_args()          │   │
│             │ │                                                │    312      return args                         │   │
│             │ │                                                │    313                                          │   │
│             │ │                                                │    314                                          │   │
│             │ │                                                │    315  def select_sglang_backend(args:         │   │
│             │ │                                                │ argparse.Namespace):                            │   │
│             │ │                                                │    316      if args.backend.startswith("srt"):  │   │
│             │ │                                                │    317          if args.backend ==              │   │
│             │ │                                                │ "srt-no-parallel":                              │   │
│             │ │                                                │    318                                          │   │
│             │ │                                                │ global_config.enable_parallel_encoding = False  │   │
│             │ │                                                │    319          backend =                       │   │
│             │ │                                                │ RuntimeEndpoint(f"{args.host}:{args.port}")     │   │
│             │ │                                                │    320      elif                                │   │
│             │ │                                                │ args.backend.startswith("gpt-"):                │   │
│             │ │                                                │    321          backend = OpenAI(args.backend)  │   │
│             │ │                                                │    322      else:                               │   │
│             │ │                                                │    323          raise ValueError(f"Invalid      │   │
│             │ │                                                │ backend: {args.backend}")                       │   │
│             │ │                                                │    324      return backend                      │   │
│             │ │                                                │    325                                          │   │
│             │ │                                                │    326                                          │   │
│             │ │                                                │    327  def _get_call_generate(args:            │   │
│             │ │                                                │ argparse.Namespace):                            │   │
│             │ │                                                │    328      if args.backend == "lightllm":      │   │
│             │ │                                                │    329          return                          │   │
│             │ │                                                │ partial(call_generate_lightllm,                 │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    330      elif args.backend == "vllm":        │   │
│             │ │                                                │    331          return                          │   │
│             │ │                                                │ partial(call_generate_vllm,                     │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    332      elif args.backend == "srt-raw":     │   │
│             │ │                                                │    333          return                          │   │
│             │ │                                                │ partial(call_generate_srt_raw,                  │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    334      elif args.backend == "gserver":     │   │
│             │ │                                                │    335          return                          │   │
│             │ │                                                │ partial(call_generate_gserver,                  │   │
│             │ │                                                │ url=f"{args.host}:{args.port}")                 │   │
│             │ │                                                │    336      elif args.backend == "outlines":    │   │
│             │ │                                                │    337          return                          │   │
│             │ │                                                │ partial(call_generate_outlines,                 │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    338      elif args.backend == "guidance":    │   │
│             │ │                                                │    339          from guidance import models     │   │
│             │ │                                                │    340                                          │   │
│             │ │                                                │    341          model =                         │   │
│             │ │                                                │ models.LlamaCpp(args.model_path,                │   │
│             │ │                                                │ n_gpu_layers=-1, n_ctx=args.n_ctx)              │   │
│             │ │                                                │    342          call_generate =                 │   │
│             │ │                                                │ partial(call_generate_guidance, model=model)    │   │
│             │ │                                                │    343          call_generate("Hello,", 1.0, 8, │   │
│             │ │                                                │ ".")                                            │   │
│             │ │                                                │    344          return call_generate            │   │
│             │ │                                                │    345      elif args.backend == "lmql":        │   │
│             │ │                                                │    346          import lmql                     │   │
│             │ │                                                │    347                                          │   │
│             │ │                                                │    348          model =                         │   │
│             │ │                                                │ lmql.model(args.model_path,                     │   │
│             │ │                                                │ endpoint=f"{args.host}:{args.port}")            │   │
│             │ │                                                │    349          return                          │   │
│             │ │                                                │ partial(call_generate_lmql, model=model)        │   │
│             │ │                                                │    350      else:                               │   │
│             │ │                                                │    351          raise ValueError(f"Invalid      │   │
│             │ │                                                │ backend: {args.backend}")                       │   │
│             │ │                                                │    352                                          │   │
│             │ │                                                │    353                                          │   │
│             │ │                                                │    354  def _get_call_select(args:              │   │
│             │ │                                                │ argparse.Namespace):                            │   │
│             │ │                                                │    355      if args.backend == "lightllm":      │   │
│             │ │                                                │    356          return                          │   │
│             │ │                                                │ partial(call_select_lightllm,                   │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    357      elif args.backend == "vllm":        │   │
│             │ │                                                │    358          return                          │   │
│             │ │                                                │ partial(call_select_vllm,                       │   │
│             │ │                                                │ url=f"{args.host}:{args.port}/generate")        │   │
│             │ │                                                │    359      elif args.backend == "guidance":    │   │
│             │ │                                                │    360          from guidance import models     │   │
│             │ │                                                │    361                                          │   │
│             │ │                                                │    362          model =                         │   │
│             │ │                                                │ models.LlamaCpp(args.model_path,                │   │
│             │ │                                                │ n_gpu_layers=-1, n_ctx=args.n_ctx)              │   │
│             │ │                                                │    363          call_select =                   │   │
│             │ │                                                │ partial(call_select_guidance, model=model)      │   │
│             │ │                                                │    364                                          │   │
│             │ │                                                │    365          call_select("Hello,", ["world", │   │
│             │ │                                                │ "earth"])                                       │   │
│             │ │                                                │    366          return call_select              │   │
│             │ │                                                │    367                                          │   │
│             │ │                                                │    368      elif args.backend == "lmql":        │   │
│             │ │                                                │    369          import lmql                     │   │
│             │ │                                                │    370                                          │   │
│             │ │                                                │    371          model =                         │   │
│             │ │                                                │ lmql.model(args.model_path,                     │   │
│             │ │                                                │ endpoint=f"{args.host}:{args.port}")            │   │
│             │ │                                                │    372          return                          │   │
│             │ │                                                │ partial(call_select_lmql, model=model)          │   │
│             │ │                                                │    373      else:                               │   │
│             │ │                                                │    374          raise ValueError(f"Invalid      │   │
│             │ │                                                │ backend: {args.backend}")                       │   │
│             │ │                                                │    375                                          │   │
│             │ │                                                │    376                                          │   │
│             │ │                                                │    377  def get_call_generate(args:             │   │
│             │ │                                                │ argparse.Namespace):                            │   │
│             │ │                                                │    378      call_generate =                     │   │
│             │ │                                                │ _get_call_generate(args)                        │   │
│             │ │                                                │    379                                          │   │
│             │ │                                                │    380      def func(*args, **kwargs):          │   │
│             │ │                                                │    381          try:                            │   │
│             │ │                                                │    382              return call_generate(*args, │   │
│             │ │                                                │ **kwargs)                                       │   │
│             │ │                                                │    383          except Exception:               │   │
│             │ │                                                │    384              print("Exception in         │   │
│             │ │                                                │ call_generate:\n" + get_exception_traceback())  │   │
│             │ │                                                │    385              raise                       │   │
│             │ │                                                │    386                                          │   │
│             │ │                                                │    387      return func                         │   │
│             │ │                                                │    388                                          │   │
│             │ │                                                │    389                                          │   │
│             │ │                                                │    390  def get_call_select(args:               │   │
│             │ │                                                │ argparse.Namespace):                            │   │
│             │ │                                                │    391      call_select =                       │   │
│             │ │                                                │ _get_call_select(args)                          │   │
│             │ │                                                │    392                                          │   │
│             │ │                                                │    393      def func(*args, **kwargs):          │   │
│             │ │                                                │    394          try:                            │   │
│             │ │                                                │    395              return call_select(*args,   │   │
│             │ │                                                │ **kwargs)                                       │   │
│             │ │                                                │    396          except Exception:               │   │
│             │ │                                                │    397              print("Exception in         │   │
│             │ │                                                │ call_select:\n" + get_exception_traceback())    │   │
│             │ │                                                │    398              raise                       │   │
│             │ │                                                │    399                                          │   │
│             │ │                                                │    400      return func                         │   │
│             │ │                                                │    401                                          │   │
│             │ │                                                │    402                                          │   │
│             │ │                                                │    403  def popen_launch_server(                │   │
│             │ │                                                │    404      model: str,                         │   │
│             │ │                                                │    405      base_url: str,                      │   │
│             │ │                                                │    406      timeout: float,                     │   │
│             │ │                                                │    407      api_key: Optional = None,           │   │
│             │ │                                                │    408      other_args: tuple = (),             │   │
│             │ │                                                │    409      env: Optional = None,               │   │
│             │ │                                                │    410      return_stdout_stderr: Optional =    │   │
│             │ │                                                │ None,                                           │   │
│             │ │                                                │    411  ):                                      │   │
│             │ │                                                │    412      _, host, port = base_url.split(":") │   │
│             │ │                                                │    413      host = host[2:]                     │   │
│             │ │                                                │    414                                          │   │
│             │ │                                                │    415      command = [                         │   │
│             │ │                                                │    416          "python3",                      │   │
│             │ │                                                │    417          "-m",                           │   │
│             │ │                                                │    418          "sglang.launch_server",         │   │
│             │ │                                                │    419          "--model-path",                 │   │
│             │ │                                                │    420          model,                          │   │
│             │ │                                                │    421          "--host",                       │   │
│             │ │                                                │    422          host,                           │   │
│             │ │                                                │    423          "--port",                       │   │
│             │ │                                                │    424          port,                           │   │
│             │ │                                                │    425          *other_args,                    │   │
│             │ │                                                │    426      ]                                   │   │
│             │ │                                                │    427                                          │   │
│             │ │                                                │    428      if api_key:                         │   │
│             │ │                                                │    429          command += ["--api-key",        │   │
│             │ │                                                │ api_key]                                        │   │
│             │ │                                                │    430                                          │   │
│             │ │                                                │    431      if return_stdout_stderr:            │   │
│             │ │                                                │    432          process = subprocess.Popen(     │   │
│             │ │                                                │    433              command,                    │   │
│             │ │                                                │    434                                          │   │
│             │ │                                                │ stdout=return_stdout_stderr[0],                 │   │
│             │ │                                                │    435                                          │   │
│             │ │                                                │ stderr=return_stdout_stderr[1],                 │   │
│             │ │                                                │    436              env=env,                    │   │
│             │ │                                                │    437              text=True,                  │   │
│             │ │                                                │    438          )                               │   │
│             │ │                                                │    439      else:                               │   │
│             │ │                                                │    440          process =                       │   │
│             │ │                                                │ subprocess.Popen(command, stdout=None,          │   │
│             │ │                                                │ stderr=None, env=env)                           │   │
│             │ │                                                │    441                                          │   │
│             │ │                                                │    442      start_time = time.time()            │   │
│             │ │                                                │    443      with requests.Session() as session: │   │
│             │ │                                                │    444          while time.time() - start_time  │   │
│             │ │                                                │ < timeout:                                      │   │
│             │ │                                                │    445              try:                        │   │
│             │ │                                                │    446                  headers = {             │   │
│             │ │                                                │    447                      "Content-Type":     │   │
│             │ │                                                │ "application/json; charset=utf-8",              │   │
│             │ │                                                │    448                      "Authorization":    │   │
│             │ │                                                │ f"Bearer {api_key}",                            │   │
│             │ │                                                │    449                  }                       │   │
│             │ │                                                │    450                  response = session.get( │   │
│             │ │                                                │    451                                          │   │
│             │ │                                                │ f"{base_url}/health_generate",                  │   │
│             │ │                                                │    452                      headers=headers,    │   │
│             │ │                                                │    453                  )                       │   │
│             │ │                                                │    454                  if response.status_code │   │
│             │ │                                                │ == 200:                                         │   │
│             │ │                                                │    455                      return process      │   │
│             │ │                                                │    456              except                      │   │
│             │ │                                                │ requests.RequestException:                      │   │
│             │ │                                                │    457                  pass                    │   │
│             │ │                                                │    458              time.sleep(10)              │   │
│             │ │                                                │    459      raise TimeoutError("Server failed   │   │
│             │ │                                                │ to start within the timeout period.")           │   │
│             │ │                                                │    460                                          │   │
│             │ │                                                │    461                                          │   │
│             │ │                                                │    462  def run_with_timeout(                   │   │
│             │ │                                                │    463      func: Callable,                     │   │
│             │ │                                                │    464      args: tuple = (),                   │   │
│             │ │                                                │    465      kwargs: Optional = None,            │   │
│             │ │                                                │    466      timeout: float = None,              │   │
│             │ │                                                │    467  ):                                      │   │
│             │ │                                                │    468      """Run a function with timeout."""  │   │
│             │ │                                                │    469      ret_value = []                      │   │
│             │ │                                                │    470                                          │   │
│             │ │                                                │    471      def _target_func():                 │   │
│             │ │                                                │    472          ret_value.append(func(*args,    │   │
│             │ │                                                │ **(kwargs or {})))                              │   │
│             │ │                                                │    473                                          │   │
│             │ │                                                │    474      t =                                 │   │
│             │ │                                                │ threading.Thread(target=_target_func)           │   │
│             │ │                                                │    475      t.start()                           │   │
│             │ │                                                │    476      t.join(timeout=timeout)             │   │
│             │ │                                                │    477      if t.is_alive():                    │   │
│             │ │                                                │    478          raise TimeoutError()            │   │
│             │ │                                                │    479                                          │   │
│             │ │                                                │    480      if not ret_value:                   │   │
│             │ │                                                │    481          raise RuntimeError()            │   │
│             │ │                                                │    482                                          │   │
│             │ │                                                │    483      return ret_value[0]                 │   │
│             │ │                                                │    484                                          │   │
│             │ │                                                │    485                                          │   │
│             │ │                                                │    486  def run_unittest_files(files: List,     │   │
│             │ │                                                │ timeout_per_file: float):                       │   │
│             │ │                                                │    487      tic = time.time()                   │   │
│             │ │                                                │    488      success = True                      │   │
│             │ │                                                │    489                                          │   │
│             │ │                                                │    490      for filename in files:              │   │
│             │ │                                                │    491          global process                  │   │
│             │ │                                                │    492                                          │   │
│             │ │                                                │    493          def run_one_file(filename):     │   │
│             │ │                                                │    494              filename =                  │   │
│             │ │                                                │ os.path.join(os.getcwd(), filename)             │   │
│             │ │                                                │    495              print(f"\n\nRun:\npython3   │   │
│             │ │                                                │ {filename}\n\n", flush=True)                    │   │
│             │ │                                                │    496              process = subprocess.Popen( │   │
│             │ │                                                │    497                  ["python3", filename],  │   │
│             │ │                                                │ stdout=None, stderr=None, env=os.environ        │   │
│             │ │                                                │    498              )                           │   │
│             │ │                                                │    499              process.wait()              │   │
│             │ │                                                │    500              return process.returncode   │   │
│             │ │                                                │    501                                          │   │
│             │ │                                                │    502          try:                            │   │
│             │ │                                                │    503              ret_code =                  │   │
│             │ │                                                │ run_with_timeout(                               │   │
│             │ │                                                │    504                  run_one_file,           │   │
│             │ │                                                │ args=(filename,), timeout=timeout_per_file      │   │
│             │ │                                                │    505              )                           │   │
│             │ │                                                │    506              assert ret_code == 0        │   │
│             │ │                                                │    507          except TimeoutError:            │   │
│             │ │                                                │    508                                          │   │
│             │ │                                                │ kill_process_tree(process.pid)                  │   │
│             │ │                                                │    509              time.sleep(5)               │   │
│             │ │                                                │    510              print(                      │   │
│             │ │                                                │    511                  f"\nTimeout after       │   │
│             │ │                                                │ {timeout_per_file} seconds when running         │   │
│             │ │                                                │ {filename}\n",                                  │   │
│             │ │                                                │    512                  flush=True,             │   │
│             │ │                                                │    513              )                           │   │
│             │ │                                                │    514              success = False             │   │
│             │ │                                                │    515              break                       │   │
│             │ │                                                │    516                                          │   │
│             │ │                                                │    517      if success:                         │   │
│             │ │                                                │    518          print(f"Success. Time elapsed:  │   │
│             │ │                                                │ {time.time() - tic:.2f}s", flush=True)          │   │
│             │ │                                                │    519   <response clipped><NOTE>To save on     │   │
│             │ │                                                │ context only part of this file has been shown   │   │
│             │ │                                                │ to you. You should retry this tool after you    │   │
│             │ │                                                │ have searched inside the file with `grep -n` in │   │
│             │ │                                                │ order to find the line numbers of what you are  │   │
│             │ │                                                │ looking for.</NOTE>                             │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 621084 Output: 3390                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 21: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -R                           │                                                 │   │
│             │ │ "update_weights_from_tensor\\(" -n             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 670576 Output: 4303                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 22: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -R                           │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ "update_weights_from_tensor" -n                │ def update_weights_from_tensor(self, name,      │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ tensor: torch.Tensor):                          │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def update_weights_from_tensor(self, recv_req:  │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ success, message =                              │   │
│             │ │                                                │ self.worker.update_weights_from_tensor(recv_re… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def update_weights_from_tensor(self, recv_req:  │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ success, message =                              │   │
│             │ │                                                │ self.model_runner.update_weights_from_tensor(   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.update_weights_from_tensor_communicator =  │   │
│             │ │                                                │ _Communicator(                                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ async def update_weights_from_tensor(           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ result = (await                                 │   │
│             │ │                                                │ self.update_weights_from_tensor_communicator(o… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.update_weights_from_tensor_communicator.h… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ success, message =                              │   │
│             │ │                                                │ self.update_weights_from_tensor(recv_req)       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def update_weights_from_tensor(self, recv_req:  │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ success, message =                              │   │
│             │ │                                                │ self.tp_worker.update_weights_from_tensor(recv… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ # TODO extract common code b/t                  │   │
│             │ │                                                │ update_weights_from_distributed and             │   │
│             │ │                                                │ update_weights_from_tensor later                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def update_weights_from_tensor(self, name,      │   │
│             │ │                                                │ tensor):                                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_tensor(o… │   │
│             │ │                                                │ None)                                           │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 721006 Output: 4381                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 23: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, -1]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """TokenizerManager is a process that   │   │
│             │ │                                                │ tokenizes the text."""                          │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import asyncio                          │   │
│             │ │                                                │     17  import copy                             │   │
│             │ │                                                │     18  import dataclasses                      │   │
│             │ │                                                │     19  import logging                          │   │
│             │ │                                                │     20  import os                               │   │
│             │ │                                                │     21  import signal                           │   │
│             │ │                                                │     22  import sys                              │   │
│             │ │                                                │     23  import time                             │   │
│             │ │                                                │     24  import uuid                             │   │
│             │ │                                                │     25  from typing import Any, Awaitable,      │   │
│             │ │                                                │ Dict, Generic, List, Optional, Tuple, TypeVar,  │   │
│             │ │                                                │ Union                                           │   │
│             │ │                                                │     26                                          │   │
│             │ │                                                │     27  import fastapi                          │   │
│             │ │                                                │     28  import uvloop                           │   │
│             │ │                                                │     29  import zmq                              │   │
│             │ │                                                │     30  import zmq.asyncio                      │   │
│             │ │                                                │     31  from fastapi import BackgroundTasks     │   │
│             │ │                                                │     32                                          │   │
│             │ │                                                │     33  from sglang.srt.aio_rwlock import       │   │
│             │ │                                                │ RWLock                                          │   │
│             │ │                                                │     34  from sglang.srt.configs.model_config    │   │
│             │ │                                                │ import ModelConfig                              │   │
│             │ │                                                │     35  from sglang.srt.hf_transformers_utils   │   │
│             │ │                                                │ import get_processor, get_tokenizer             │   │
│             │ │                                                │     36  from                                    │   │
│             │ │                                                │ sglang.srt.managers.image_processor import (    │   │
│             │ │                                                │     37      get_dummy_image_processor,          │   │
│             │ │                                                │     38      get_image_processor,                │   │
│             │ │                                                │     39  )                                       │   │
│             │ │                                                │     40  from sglang.srt.managers.io_struct      │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     41      AbortReq,                           │   │
│             │ │                                                │     42      BatchEmbeddingOut,                  │   │
│             │ │                                                │     43      BatchStrOut,                        │   │
│             │ │                                                │     44      BatchTokenIDOut,                    │   │
│             │ │                                                │     45      CloseSessionReqInput,               │   │
│             │ │                                                │     46      EmbeddingReqInput,                  │   │
│             │ │                                                │     47      FlushCacheReq,                      │   │
│             │ │                                                │     48      GenerateReqInput,                   │   │
│             │ │                                                │     49      GetWeightsByNameReqInput,           │   │
│             │ │                                                │     50      GetWeightsByNameReqOutput,          │   │
│             │ │                                                │     51      InitWeightsUpdateGroupReqInput,     │   │
│             │ │                                                │     52      InitWeightsUpdateGroupReqOutput,    │   │
│             │ │                                                │     53      OpenSessionReqInput,                │   │
│             │ │                                                │     54      OpenSessionReqOutput,               │   │
│             │ │                                                │     55      ProfileReq,                         │   │
│             │ │                                                │     56      SessionParams,                      │   │
│             │ │                                                │     57      TokenizedEmbeddingReqInput,         │   │
│             │ │                                                │     58      TokenizedGenerateReqInput,          │   │
│             │ │                                                │     59      UpdateWeightFromDiskReqInput,       │   │
│             │ │                                                │     60      UpdateWeightFromDiskReqOutput,      │   │
│             │ │                                                │     61                                          │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput,           │   │
│             │ │                                                │     62                                          │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqOutput,          │   │
│             │ │                                                │     63      UpdateWeightsFromTensorReqInput,    │   │
│             │ │                                                │     64      UpdateWeightsFromTensorReqOutput,   │   │
│             │ │                                                │     65  )                                       │   │
│             │ │                                                │     66  from sglang.srt.metrics.collector       │   │
│             │ │                                                │ import TokenizerMetricsCollector                │   │
│             │ │                                                │     67  from                                    │   │
│             │ │                                                │ sglang.srt.sampling.sampling_params import      │   │
│             │ │                                                │ SamplingParams                                  │   │
│             │ │                                                │     68  from sglang.srt.server_args import      │   │
│             │ │                                                │ PortArgs, ServerArgs                            │   │
│             │ │                                                │     69  from sglang.srt.utils import (          │   │
│             │ │                                                │     70      dataclass_to_string_truncated,      │   │
│             │ │                                                │     71      get_zmq_socket,                     │   │
│             │ │                                                │     72      kill_process_tree,                  │   │
│             │ │                                                │     73  )                                       │   │
│             │ │                                                │     74                                          │   │
│             │ │                                                │     75                                          │   │
│             │ │                                                │ asyncio.set_event_loop_policy(uvloop.EventLoop… │   │
│             │ │                                                │     76                                          │   │
│             │ │                                                │     77  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     78                                          │   │
│             │ │                                                │     79                                          │   │
│             │ │                                                │     80  @dataclasses.dataclass                  │   │
│             │ │                                                │     81  class ReqState:                         │   │
│             │ │                                                │     82      """Store the state a request."""    │   │
│             │ │                                                │     83                                          │   │
│             │ │                                                │     84      out_list: List                      │   │
│             │ │                                                │     85      finished: bool                      │   │
│             │ │                                                │     86      event: asyncio.Event                │   │
│             │ │                                                │     87      obj: Any                            │   │
│             │ │                                                │     88                                          │   │
│             │ │                                                │     89      # For metrics                       │   │
│             │ │                                                │     90      created_time: float                 │   │
│             │ │                                                │     91      first_token_time: Optional = None   │   │
│             │ │                                                │     92                                          │   │
│             │ │                                                │     93      # For streaming output              │   │
│             │ │                                                │     94      last_output_offset: int = 0         │   │
│             │ │                                                │     95                                          │   │
│             │ │                                                │     96                                          │   │
│             │ │                                                │     97  class TokenizerManager:                 │   │
│             │ │                                                │     98      """TokenizerManager is a process    │   │
│             │ │                                                │ that tokenizes the text."""                     │   │
│             │ │                                                │     99                                          │   │
│             │ │                                                │    100      def __init__(                       │   │
│             │ │                                                │    101          self,                           │   │
│             │ │                                                │    102          server_args: ServerArgs,        │   │
│             │ │                                                │    103          port_args: PortArgs,            │   │
│             │ │                                                │    104      ):                                  │   │
│             │ │                                                │    105          # Parse args                    │   │
│             │ │                                                │    106          self.server_args = server_args  │   │
│             │ │                                                │    107          self.enable_metrics =           │   │
│             │ │                                                │ server_args.enable_metrics                      │   │
│             │ │                                                │    108                                          │   │
│             │ │                                                │    109          # Init inter-process            │   │
│             │ │                                                │ communication                                   │   │
│             │ │                                                │    110          context =                       │   │
│             │ │                                                │ zmq.asyncio.Context(2)                          │   │
│             │ │                                                │    111          self.recv_from_detokenizer =    │   │
│             │ │                                                │ get_zmq_socket(                                 │   │
│             │ │                                                │    112              context, zmq.PULL,          │   │
│             │ │                                                │ port_args.tokenizer_ipc_name                    │   │
│             │ │                                                │    113          )                               │   │
│             │ │                                                │    114          self.send_to_scheduler =        │   │
│             │ │                                                │ get_zmq_socket(                                 │   │
│             │ │                                                │    115              context, zmq.PUSH,          │   │
│             │ │                                                │ port_args.scheduler_input_ipc_name              │   │
│             │ │                                                │    116          )                               │   │
│             │ │                                                │    117                                          │   │
│             │ │                                                │    118          # Read model args               │   │
│             │ │                                                │    119          self.model_path =               │   │
│             │ │                                                │ server_args.model_path                          │   │
│             │ │                                                │    120          self.served_model_name =        │   │
│             │ │                                                │ server_args.served_model_name                   │   │
│             │ │                                                │    121          self.model_config =             │   │
│             │ │                                                │ ModelConfig(                                    │   │
│             │ │                                                │    122              server_args.model_path,     │   │
│             │ │                                                │    123                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │    124                                          │   │
│             │ │                                                │ revision=server_args.revision,                  │   │
│             │ │                                                │    125                                          │   │
│             │ │                                                │ context_length=server_args.context_length,      │   │
│             │ │                                                │    126                                          │   │
│             │ │                                                │ model_override_args=server_args.json_model_ove… │   │
│             │ │                                                │    127                                          │   │
│             │ │                                                │ is_embedding=server_args.is_embedding,          │   │
│             │ │                                                │    128              dtype=server_args.dtype,    │   │
│             │ │                                                │    129                                          │   │
│             │ │                                                │ quantization=server_args.quantization,          │   │
│             │ │                                                │    130          )                               │   │
│             │ │                                                │    131                                          │   │
│             │ │                                                │    132          self.is_generation =            │   │
│             │ │                                                │ self.model_config.is_generation                 │   │
│             │ │                                                │    133          self.context_len =              │   │
│             │ │                                                │ self.model_config.context_len                   │   │
│             │ │                                                │    134          self.image_token_id =           │   │
│             │ │                                                │ self.model_config.image_token_id                │   │
│             │ │                                                │    135                                          │   │
│             │ │                                                │    136          # Create image processor        │   │
│             │ │                                                │ placeholder                                     │   │
│             │ │                                                │    137          self.image_processor =          │   │
│             │ │                                                │ get_dummy_image_processor()                     │   │
│             │ │                                                │    138                                          │   │
│             │ │                                                │    139          # Create tokenizer              │   │
│             │ │                                                │    140          if                              │   │
│             │ │                                                │ server_args.skip_tokenizer_init:                │   │
│             │ │                                                │    141              self.tokenizer =            │   │
│             │ │                                                │ self.processor = None                           │   │
│             │ │                                                │    142          else:                           │   │
│             │ │                                                │    143              if                          │   │
│             │ │                                                │ self.model_config.is_multimodal:                │   │
│             │ │                                                │    144                  self.processor =        │   │
│             │ │                                                │ get_processor(                                  │   │
│             │ │                                                │    145                                          │   │
│             │ │                                                │ server_args.tokenizer_path,                     │   │
│             │ │                                                │    146                                          │   │
│             │ │                                                │ tokenizer_mode=server_args.tokenizer_mode,      │   │
│             │ │                                                │    147                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │    148                  )                       │   │
│             │ │                                                │    149                  self.tokenizer =        │   │
│             │ │                                                │ self.processor.tokenizer                        │   │
│             │ │                                                │    150                                          │   │
│             │ │                                                │ os.environ["TOKENIZERS_PARALLELISM"] = "false"  │   │
│             │ │                                                │    151                                          │   │
│             │ │                                                │    152                  # We want to            │   │
│             │ │                                                │ parallelize the image pre-processing so we      │   │
│             │ │                                                │ create an executor for it                       │   │
│             │ │                                                │    153                  self.image_processor =  │   │
│             │ │                                                │ get_image_processor(                            │   │
│             │ │                                                │    154                                          │   │
│             │ │                                                │ self.model_config.hf_config, server_args,       │   │
│             │ │                                                │ self.processor                                  │   │
│             │ │                                                │    155                  )                       │   │
│             │ │                                                │    156              else:                       │   │
│             │ │                                                │    157                  self.tokenizer =        │   │
│             │ │                                                │ get_tokenizer(                                  │   │
│             │ │                                                │    158                                          │   │
│             │ │                                                │ server_args.tokenizer_path,                     │   │
│             │ │                                                │    159                                          │   │
│             │ │                                                │ tokenizer_mode=server_args.tokenizer_mode,      │   │
│             │ │                                                │    160                                          │   │
│             │ │                                                │ trust_remote_code=server_args.trust_remote_cod… │   │
│             │ │                                                │    161                  )                       │   │
│             │ │                                                │    162                                          │   │
│             │ │                                                │    163          # Store states                  │   │
│             │ │                                                │    164          self.to_create_loop = True      │   │
│             │ │                                                │    165          self.rid_to_state: Dict = {}    │   │
│             │ │                                                │    166                                          │   │
│             │ │                                                │    167          # The event to notify the       │   │
│             │ │                                                │ weight sync is finished.                        │   │
│             │ │                                                │    168          self.model_update_lock =        │   │
│             │ │                                                │ RWLock()                                        │   │
│             │ │                                                │    169          self.model_update_result:       │   │
│             │ │                                                │ Optional[Awaitable[UpdateWeightFromDiskReqOutp… │   │
│             │ │                                                │ = (                                             │   │
│             │ │                                                │    170              None                        │   │
│             │ │                                                │    171          )                               │   │
│             │ │                                                │    172          self.asyncio_tasks = set()      │   │
│             │ │                                                │    173                                          │   │
│             │ │                                                │    174          # For session info              │   │
│             │ │                                                │    175          self.session_futures = {}  #    │   │
│             │ │                                                │ session_id -> asyncio event                     │   │
│             │ │                                                │    176                                          │   │
│             │ │                                                │    177          # Others                        │   │
│             │ │                                                │    178          self.gracefully_exit = False    │   │
│             │ │                                                │    179                                          │   │
│             │ │                                                │ self.init_weights_update_group_communicator =   │   │
│             │ │                                                │ _Communicator(                                  │   │
│             │ │                                                │    180              self.send_to_scheduler,     │   │
│             │ │                                                │ server_args.dp_size                             │   │
│             │ │                                                │    181          )                               │   │
│             │ │                                                │    182                                          │   │
│             │ │                                                │ self.update_weights_from_distributed_communica… │   │
│             │ │                                                │ = _Communicator(                                │   │
│             │ │                                                │    183              self.send_to_scheduler,     │   │
│             │ │                                                │ server_args.dp_size                             │   │
│             │ │                                                │    184          )                               │   │
│             │ │                                                │    185                                          │   │
│             │ │                                                │ self.update_weights_from_tensor_communicator =  │   │
│             │ │                                                │ _Communicator(                                  │   │
│             │ │                                                │    186              self.send_to_scheduler,     │   │
│             │ │                                                │ server_args.dp_size                             │   │
│             │ │                                                │    187          )                               │   │
│             │ │                                                │    188                                          │   │
│             │ │                                                │ self.get_weights_by_name_communicator =         │   │
│             │ │                                                │ _Communicator(                                  │   │
│             │ │                                                │    189              self.send_to_scheduler,     │   │
│             │ │                                                │ server_args.dp_size                             │   │
│             │ │                                                │    190          )                               │   │
│             │ │                                                │    191                                          │   │
│             │ │                                                │    192          # Metrics                       │   │
│             │ │                                                │    193          if self.enable_metrics:         │   │
│             │ │                                                │    194              self.metrics_collector =    │   │
│             │ │                                                │ TokenizerMetricsCollector(                      │   │
│             │ │                                                │    195                  labels={                │   │
│             │ │                                                │    196                      "model_name":       │   │
│             │ │                                                │ self.server_args.served_model_name,             │   │
│             │ │                                                │    197                      # TODO: Add lora    │   │
│             │ │                                                │ name/path in the future,                        │   │
│             │ │                                                │    198                  },                      │   │
│             │ │                                                │    199              )                           │   │
│             │ │                                                │    200                                          │   │
│             │ │                                                │    201      async def generate_request(         │   │
│             │ │                                                │    202          self,                           │   │
│             │ │                                                │    203          obj: Union[GenerateReqInput,    │   │
│             │ │                                                │ EmbeddingReqInput],                             │   │
│             │ │                                                │    204          request: Optional = None,       │   │
│             │ │                                                │    205      ):                                  │   │
│             │ │                                                │    206          created_time = time.time()      │   │
│             │ │                                                │    207                                          │   │
│             │ │                                                │    208          self.auto_create_handle_loop()  │   │
│             │ │                                                │    209                                          │   │
│             │ │                                                │    210          if isinstance(obj,              │   │
│             │ │                                                │ EmbeddingReqInput) and self.is_generation:      │   │
│             │ │                                                │    211              raise ValueError(           │   │
│             │ │                                                │    212                  "This model does not    │   │
│             │ │                                                │ appear to be an embedding model by default. "   │   │
│             │ │                                                │    213                  "Please add             │   │
│             │ │                                                │ `--is-embedding` when launching the server or   │   │
│             │ │                                                │ try another model."                             │   │
│             │ │                                                │    214              )                           │   │
│             │ │                                                │    215                                          │   │
│             │ │                                                │    216                                          │   │
│             │ │                                                │ obj.normalize_batch_and_arguments()             │   │
│             │ │                                                │    217                                          │   │
│             │ │                                                │    218          if                              │   │
│             │ │                                                │ self.server_args.log_requests:                  │   │
│             │ │                                                │    219              logger.info(f"Receive:      │   │
│             │ │                                                │ obj={dataclass_to_string_truncated(obj)}")      │   │
│             │ │                                                │    220                                          │   │
│             │ │                                                │    221          async with                      │   │
│             │ │                                                │ self.model_update_lock.reader_lock:             │   │
│             │ │                                                │    222              is_single = obj.is_single   │   │
│             │ │                                                │    223              if is_single:               │   │
│             │ │                                                │    224                  tokenized_obj = await   │   │
│             │ │                                                │ self._tokenize_one_request(obj)                 │   │
│             │ │                                                │    225                                          │   │
│             │ │                                                │ self.send_to_scheduler.send_pyobj(tokenized_ob… │   │
│             │ │                                                │    226                  async for response in   │   │
│             │ │                                                │ self._wait_one_response(                        │   │
│             │ │                                                │    227                      obj, request,       │   │
│             │ │                                                │ created_time                                    │   │
│             │ │                                                │    228                  ):                      │   │
│             │ │                                                │    229                      yield response      │   │
│             │ │                                                │    230              else:                       │   │
│             │ │                                                │    231                  async for response in   │   │
│             │ │                                                │ self._handle_batch_request(                     │   │
│             │ │                                                │    232                      obj, request,       │   │
│             │ │                                                │ created_time                                    │   │
│             │ │                                                │    233                  ):                      │   │
│             │ │                                                │    234                      yield response      │   │
│             │ │                                                │    235                                          │   │
│             │ │                                                │    236      async def _tokenize_one_request(    │   │
│             │ │                                                │    237          self,                           │   │
│             │ │                                                │    238          obj: Union[GenerateReqInput,    │   │
│             │ │                                                │ EmbeddingReqInput],                             │   │
│             │ │                                                │    239      ):                                  │   │
│             │ │                                                │    240          """Tokenize one request."""     │   │
│             │ │                                                │    241          # Tokenize                      │   │
│             │ │                                                │    242          input_embeds = None             │   │
│             │ │                                                │    243          input_text = obj.text           │   │
│             │ │                                                │    244          if obj.input_embeds is not      │   │
│             │ │                                                │ None:                                           │   │
│             │ │                                                │    245              if not                      │   │
│             │ │                                                │ self.server_args.disable_radix_cache:           │   │
│             │ │                                                │    246                  raise ValueError(       │   │
│             │ │                                                │    247                      "input_embeds is    │   │
│             │ │                                                │ provided while disable_radix_cache is False. "  │   │
│             │ │                                                │    248                      "Please add         │   │
│             │ │                                                │ `--disable-radix-cache` when you launch the     │   │
│             │ │                                                │ server "                                        │   │
│             │ │                                                │    249                      "if you want to use │   │
│             │ │                                                │ input_embeds as inputs."                        │   │
│             │ │                                                │    250                  )                       │   │
│             │ │                                                │    251              input_embeds =              │   │
│             │ │                                                │ obj.input_embeds                                │   │
│             │ │                                                │    252              input_ids = obj.input_ids   │   │
│             │ │                                                │    253          elif obj.input_ids is None:     │   │
│             │ │                                                │    254              input_ids =                 │   │
│             │ │                                                │ self.tokenizer.encode(input_text)               │   │
│             │ │                                                │    255          else:                           │   │
│             │ │                                                │    256              input_ids = obj.input_ids   │   │
│             │ │                                                │    257                                          │   │
│             │ │                                                │    258          if self.is_generation:          │   │
│             │ │                                                │    259              # TODO: also support        │   │
│             │ │                                                │ getting embeddings for multimodal models        │   │
│             │ │                                                │    260              image_inputs: Dict = await  │   │
│             │ │                                                │ self.image_processor.process_images_async(      │   │
│             │ │                                                │    261                  obj.image_data,         │   │
│             │ │                                                │ input_text or input_ids, obj                    │   │
│             │ │                                                │    262              )                           │   │
│             │ │                                                │    263              if image_inputs and         │   │
│             │ │                                                │ "input_ids" in image_inputs:                    │   │
│             │ │                                                │    264                  input_ids =             │   │
│             │ │                                                │ image_inputs["input_ids"]                       │   │
│             │ │                                                │    265              return_logprob =            │   │
│             │ │                                                │ obj.return_logprob                              │   │
│             │ │                                                │    266              logprob_start_len =         │   │
│             │ │                                                │ obj.logprob_start_len                           │   │
│             │ │                                                │    267              top_logprobs_num =          │   │
│             │ │                                                │ obj.top_logprobs_num                            │   │
│             │ │                                                │    268              session_params = (          │   │
│             │ │                                                │    269                                          │   │
│             │ │                                                │ SessionParams(**obj.session_params) if          │   │
│             │ │                                                │ obj.session_params else None                    │   │
│             │ │                                                │    270              )                           │   │
│             │ │                                                │    271                                          │   │
│             │ │                                                │    272          if obj.input_ids is not None    │   │
│             │ │                                                │ and len(input_ids) >= self.context_len:         │   │
│             │ │                                                │    273              raise ValueError(           │   │
│             │ │                                                │    274                  f"The input             │   │
│             │ │                                                │ ({len(input_ids)} tokens) is longer than the "  │   │
│             │ │                                                │    275                  f"model's context       │   │
│             │ │                                                │ length ({self.context_len} tokens)."            │   │
│             │ │                                                │    276              )                           │   │
│             │ │                                                │    277                                          │   │
│             │ │                                                │    278          # Parse sampling parameters     │   │
│             │ │                                                │    279          sampling_params =               │   │
│             │ │                                                │ SamplingParams(**obj.sampling_params)           │   │
│             │ │                                                │    280                                          │   │
│             │ │                                                │ sampling_params.normalize(self.tokenizer)       │   │
│             │ │                                                │    281          sampling_params.verify()        │   │
│             │ │                                                │    282                                          │   │
│             │ │                                                │    283          # Build return object           │   │
│             │ │                                                │    284          if isinstance(obj,              │   │
│             │ │                                                │ GenerateReqInput):                              │   │
│             │ │                                                │    285              tokenized_obj =             │   │
│             │ │                                                │ TokenizedGenerateReqInput(                      │   │
│             │ │                                                │    286                  obj.rid,                │   │
│             │ │                                                │    287                  input_text,             │   │
│             │ │                                                │    288                  input_ids,              │   │
│             │ │                                                │    289                  image_inputs,           │   │
│             │ │                                                │    290                  sampling_params,        │   │
│             │ │                                                │    291                  return_logprob,         │   │
│             │ │                                                │    292                  logprob_start_len,      │   │
│             │ │                                                │    293                  top_logprobs_num,       │   │
│             │ │                                                │    294                  obj.stream,             │   │
│             │ │                                                │    295                                          │   │
│             │ │                                                │ lora_path=obj.lora_path,                        │   │
│             │ │                                                │    296                                          │   │
│             │ │                                                │ input_embeds=input_embeds,                      │   │
│             │ │                                                │    297                                          │   │
│             │ │                                                │ session_params=session_params,                  │   │
│             │ │                                                │    298              )                           │   │
│             │ │                                                │    299          elif isinstance(obj,            │   │
│             │ │                                                │ EmbeddingReqInput):                             │   │
│             │ │                                                │    300              tokenized_obj =             │   │
│             │ │                                                │ TokenizedEmbeddingReqInput(                     │   │
│             │ │                                                │    301                  obj.rid,                │   │
│             │ │                                                │    302                  input_text,             │   │
│             │ │                                                │    303                  input_ids,              │   │
│             │ │                                                │    304                  sampling_params,        │   │
│             │ │                                                │    305              )                           │   │
│             │ │                                                │    306                                          │   │
│             │ │                                                │    307          return tokenized_obj            │   │
│             │ │                                                │    308                                          │   │
│             │ │                                                │    309      async def _wait_one_response(       │   │
│             │ │                                                │    310          self,                           │   │
│             │ │                                                │    311          obj: Union[GenerateReqInput,    │   │
│             │ │                                                │ EmbeddingReqInput],                             │   │
│             │ │                                                │    312          request: Optional = None,       │   │
│             │ │                                                │    313          created_time: Optional = None,  │   │
│             │ │                                                │    314      ):                                  │   │
│             │ │                                                │    315          """Wait for the response of one │   │
│             │ │                                                │ request."""                                     │   │
│             │ │                                                │    316          event = asyncio.Event()         │   │
│             │ │                                                │    317          state = ReqState([], False,     │   │
│             │ │                                                │ event, obj, created_time=created_time)          │   │
│             │ │                                                │    318          self.rid_to_state = state       │   │
│             │ │                                                │    319                                          │   │
│             │ │                                                │    320          while True:                     │   │
│             │ │                                                │    321              try:                        │   │
│             │ │                                                │    322                  await                   │   │
│             │ │                                                │ asyncio.wait_for(state.event.wait(), timeout=4) │   │
│             │ │                                                │    323              except                      │   │
│             │ │                                                │ asyncio.TimeoutError:                           │   │
│             │ │                                                │    324                  if request is not None  │   │
│             │ │                                                │ and await request.is_disconnected():            │   │
│             │ │                                                │    325                                          │   │
│             │ │                                                │ self.abort_request(obj.rid)                     │   │
│             │ │                                                │    326                      raise               │   │
│             │ │                                                │ ValueError(f"Abort request {obj.rid}")          │   │
│             │ │                                                │    327                  continue                │   │
│             │ │                                                │    328                                          │   │
│             │ │                                                │    329              out = state.out_list[-1]    │   │
│             │ │                                                │    330                                          │   │
│             │ │                                                │    331              state.out_list = []         │   │
│             │ │                                                │    332              if state.finished:          │   │
│             │ │                                                │    333                  if                      │   │
│             │ │                                                │ self.server_args.log_requests:                  │   │
│             │ │                                                │    334                      msg = f"Finish:     │   │
│             │ │                                                │ obj={dataclass_to_string_truncated(obj)},       │   │
│             │ │                                                │ out={dataclass_to_string_truncated(out)}"       │   │
│             │ │                                                │    335                      logger.info(msg)    │   │
│             │ │                                                │    336                  del self.rid_to_state   │   │
│             │ │                                                │    337                  yield out               │   │
│             │ │                                                │    338                  break                   │   │
│             │ │                                                │    339                                          │   │
│             │ │                                                │    340              state.event.clear()         │   │
│             │ │                                                │    341                                          │   │
│             │ │                                                │    342              if obj.stream:              │   │
│             │ │                                                │    343                  yield out               │   │
│             │ │                                                │    344              else:                       │   │
│             │ │                                                │    345                  if request is not None  │   │
│             │ │                                                │ and await request.is_disconnected():            │   │
│             │ │                                                │    346                                          │   │
│             │ │                                                │ self.abort_request(obj.rid)                     │   │
│             │ │                                                │    347                      raise               │   │
│             │ │                                                │ ValueError(f"Abort request {obj.rid}")          │   │
│             │ │                                                │    348                                          │   │
│             │ │                                                │    349      async def _handle_batch_request(    │   │
│             │ │                                                │    350          self,                           │   │
│             │ │                                                │    351          obj: Union[GenerateReqInput,    │   │
│             │ │                                                │ EmbeddingReqInput],                             │   │
│             │ │                                                │    352          request: Optional = None,       │   │
│             │ │                                                │    353          created_time: Optional = None,  │   │
│             │ │                                                │    354      ):                                  │   │
│             │ │                                                │    355          batch_size = obj.batch_size     │   │
│             │ │                                                │    356                                          │   │
│             │ │                                                │    357          generators = []                 │   │
│             │ │                                                │    358          rids = []                       │   │
│             │ │                                                │    359          if getattr(obj,                 │   │
│             │ │                                                │ "parallel_sample_num", 1) == 1:                 │   │
│             │ │                                                │    360              # Send all requests         │   │
│             │ │                                                │    361              for i in range(batch_size): │   │
│             │ │                                                │    362                  tmp_obj = obj           │   │
│             │ │                                                │    363                  tokenized_obj = await   │   │
│             │ │                                                │ self._tokenize_one_request(tmp_obj)             │   │
│             │ │                                                │    364                                          │   │
│             │ │                                                │ self.send_to_scheduler.send_pyobj(tokenized_ob… │   │
│             │ │                                                │    365                  generators.append(      │   │
│             │ │                                                │    366                                          │   │
│             │ │                                                │ self._wait_one_response(tmp_obj, request,       │   │
│             │ │                                                │ created_time)                                   │   │
│             │ │                                                │    367                  )                       │   │
│             │ │                                                │    368                                          │   │
│             │ │                                                │ rids.append(tmp_obj.rid)                        │   │
│             │ │                                                │    369          else:                           │   │
│             │ │                                                │    370              # FIXME: When using batch   │   │
│             │ │                                                │ and parallel_sample_num together, the perf is   │   │
│             │ │                                                │ not optimal.                                    │   │
│             │ │                                                │    371              if batch_size > 128:        │   │
│             │ │                                                │    372                  logger.warning(         │   │
│             │ │                                                │    373                      "Sending a single   │   │
│             │ │                                                │ large batch with parallel sampling (n > 1) has  │   │
│             │ │                                                │ not been well optimized. "                      │   │
│             │ │                                                │    374                      "The performance    │   │
│             │ │                                                │ might be better if you just duplicate the       │   │
│             │ │                                                │ requests n times or use "                       │   │
│             │ │                                                │    375                      "many threads to    │   │
│             │ │                                                │ send them one by one with parallel sampling (n  │   │
│             │ │                                                │ > 1)."                                          │   │
│             │ │                                                │    376                  )                       │   │
│             │ │                                                │    377                                          │   │
│             │ │                                                │    378              # Tokenize all requests     │   │
│             │ │                                                │    379              objs = [obj for i in        │   │
│             │ │                                                │ range(batch_size)]                              │   │
│             │ │                                                │    380              tokenized_objs = await      │   │
│             │ │                                                │ asyncio.gather(                                 │   │
│             │ │                                                │    381                                          │   │
│             │ │                                                │ *(self._tokenize_one_request(obj) for obj in    │   │
│             │ │                                                │ objs)                                           │   │
│             │ │                                                │    382              )                           │   │
│             │ │                                                │    383                                          │   │
│             │ │                                                │    384              # Cache the common prefix   │   │
│             │ │                                                │ for parallel sampling                           │   │
│             │ │                                                │    385              for i in range(batch_size): │   │
│             │ │                                                │    386                  tmp_obj =               │   │
│             │ │                                                │ copy.copy(objs)                                 │   │
│             │ │                                                │    387                  tokenized_obj =         │   │
│             │ │                                                │ copy.copy(tokenized_objs)                       │   │
│             │ │                                                │    388                  tokenized_obj.rid =     │   │
│             │ │                                                │ tmp_obj.regenerate_rid()                        │   │
│             │ │                                                │    389                                          │   │
│             │ │                                                │ tokenized_obj.sampling_params =                 │   │
│             │ │                                                │ copy.copy(tokenized_obj.sampling_params)        │   │
│             │ │                                                │    390                                          │   │
│             │ │                                                │ tokenized_obj.sampling_params.max_new_tokens =  │   │
│             │ │                                                │ 0                                               │   │
│             │ │                                                │    391                  tokenized_obj.stream =  │   │
│             │ │                                                │ False                                           │   │
│             │ │                                                │    392                                          │   │
│             │ │                                                │ self.send_to_scheduler.send_pyobj(tokenized_ob… │   │
│             │ │                                                │    393                  await                   │   │
│             │ │                                                │ self._wait_one_response(                        │   │
│             │ │                                                │    394                      tmp_obj, request,   │   │
│             │ │                                                │ created_time                                    │   │
│             │ │                                                │    395                  ).__anext__()           │   │
│             │ │                                                │    396                                          │   │
│             │ │                                                │    397              # Expand requests, assign   │   │
│             │ │                                                │ new rids for them, and send them                │   │
│             │ │                                                │    398              for i in range(batch_size): │   │
│             │ │                                                │    399                  for _ in                │   │
│             │ │                                                │ range(obj.parallel_sample_num):                 │   │
│             │ │                                                │    400                      tmp_obj =           │   │
│             │ │                                                │ copy.copy(objs)                                 │   │
│             │ │                                                │    401                      tokenized_obj =     │   │
│             │ │                                                │ copy.copy(tokenized_objs)                       │   │
│             │ │                                                │    402                      tokenized_obj.rid = │   │
│             │ │                                                │ tmp_obj.regenerate_rid()                        │   │
│             │ │                                                │    403                                          │   │
│             │ │                                                │ self.send_to_scheduler.send_pyobj(tokenized_ob… │   │
│             │ │                                                │    404                      generators.append(  │   │
│             │ │                                                │    405                                          │   │
│             │ │                                                │ self._wait_one_response(tmp_obj, request,       │   │
│             │ │                                                │ created_time)                                   │   │
│             │ │                                                │    406                      )                   │   │
│             │ │                                                │    407                                          │   │
│             │ │                                                │ rids.append(tmp_obj.rid)                        │   │
│             │ │                                                │    408                                          │   │
│             │ │                                                │    409          # Wait for all requests         │   │
│             │ │                                                │    410          is_stream = hasattr(obj,        │   │
│             │ │                                                │ "stream") and obj.stream                        │   │
│             │ │                                                │    411          if not is_stream:               │   │
│             │ │                                                │    412              outputs = await             │   │
│             │ │                                                │ asyncio.gather(*(gen.__anext__() for gen in     │   │
│             │ │                                                │ generators))                                    │   │
│             │ │                                                │    413              yield outputs               │   │
│             │ │                                                │    414          else:                           │   │
│             │ │                                                │    415              rid_to_index = {rid: i for  │   │
│             │ │                                                │ i, rid in enumerate(rids)}                      │   │
│             │ │                                                │    416              task_map =                  │   │
│             │ │                                                │ {asyncio.create_task(gen.__anext__()): gen for  │   │
│             │ │                                                │ gen in generators}                              │   │
│             │ │                                                │    417              while task_map:             │   │
│             │ │                                                │    418                  done, _ = await         │   │
│             │ │                                                │ asyncio.wait(                                   │   │
│             │ │                                                │    419                      task_map.keys(),    │   │
│             │ │                                                │ return_when=asyncio.FIRST_COMPLETED             │   │
│             │ │                                                │    420                  )                       │   │
│             │ │                                                │    421                                          │   │
│             │ │                                                │    422                  for task in done:       │   │
│             │ │                                                │    423                      gen =               │   │
│             │ │                                                │ task_map.pop(task)                              │   │
│             │ │                                                │    424                      try:                │   │
│             │ │                                                │    425                          result =        │   │
│             │ │                                                │ task.result()                                   │   │
│             │ │                                                │    426                          result["index"] │   │
│             │ │                                                │ = rid_to_index[result["meta_info"]["id"]]       │   │
│             │ │                                                │    427                          yield result    │   │
│             │ │                                                │    428                          new_task =      │   │
│             │ │                                                │ asyncio.create_task(gen.__anext__())            │   │
│             │ │                                                │    429                          task_map = gen  │   │
│             │ │                                                │    430                      except              │   │
│             │ │                                                │ StopAsyncIteration:                             │   │
│             │ │                                                │    431                          pass            │   │
│             │ │                                                │    432                                          │   │
│             │ │                                                │    433      de<response clipped><NOTE>To save   │   │
│             │ │                                                │ on context only part of this file has been      │   │
│             │ │                                                │ shown to you. You should retry this tool after  │   │
│             │ │                                                │ you have searched inside the file with `grep    │   │
│             │ │                                                │ -n` in order to find the line numbers of what   │   │
│             │ │                                                │ you are looking for.</NOTE>                     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 772487 Output: 4473                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 24: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "sed -n '480,820p'                 │                 self.server_args.load_format =  │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ obj.load_format                                 │   │
│             │ │ 'restart': False}                              │                 self.model_path =               │   │
│             │ │                                                │ obj.model_path                                  │   │
│             │ │                                                │             return result.success,              │   │
│             │ │                                                │ result.message                                  │   │
│             │ │                                                │         else:  # self.server_args.dp_size > 1   │   │
│             │ │                                                │             self.model_update_tmp = []          │   │
│             │ │                                                │             result = await                      │   │
│             │ │                                                │ self.model_update_result                        │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │             all_success = all()                 │   │
│             │ │                                                │             if all_success is True:             │   │
│             │ │                                                │                 self.server_args.model_path =   │   │
│             │ │                                                │ obj.model_path                                  │   │
│             │ │                                                │                 self.server_args.load_format =  │   │
│             │ │                                                │ obj.load_format                                 │   │
│             │ │                                                │                 self.model_path =               │   │
│             │ │                                                │ obj.model_path                                  │   │
│             │ │                                                │             all_message =                       │   │
│             │ │                                                │             all_message = " |                   │   │
│             │ │                                                │ ".join(all_message)                             │   │
│             │ │                                                │             return all_success, all_message     │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def init_weights_update_group(        │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         obj: InitWeightsUpdateGroupReqInput,    │   │
│             │ │                                                │         request: Optional = None,               │   │
│             │ │                                                │     ) -> Tuple:                                 │   │
│             │ │                                                │         self.auto_create_handle_loop()          │   │
│             │ │                                                │         assert (                                │   │
│             │ │                                                │             self.server_args.dp_size == 1       │   │
│             │ │                                                │         ), "dp_size must be 1 for init          │   │
│             │ │                                                │ parameter update group"                         │   │
│             │ │                                                │         result = (await                         │   │
│             │ │                                                │ self.init_weights_update_group_communicator(ob… │   │
│             │ │                                                │         return result.success, result.message   │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def update_weights_from_distributed(  │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         obj:                                    │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput,           │   │
│             │ │                                                │         request: Optional = None,               │   │
│             │ │                                                │     ) -> Tuple:                                 │   │
│             │ │                                                │         self.auto_create_handle_loop()          │   │
│             │ │                                                │         assert (                                │   │
│             │ │                                                │             self.server_args.dp_size == 1       │   │
│             │ │                                                │         ), "dp_size must be for update weights  │   │
│             │ │                                                │ from distributed"                               │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         # This means that weight sync           │   │
│             │ │                                                │         # cannot run while requests are in      │   │
│             │ │                                                │ progress.                                       │   │
│             │ │                                                │         async with                              │   │
│             │ │                                                │ self.model_update_lock.writer_lock:             │   │
│             │ │                                                │             result = (await                     │   │
│             │ │                                                │ self.update_weights_from_distributed_communica… │   │
│             │ │                                                │             return result.success,              │   │
│             │ │                                                │ result.message                                  │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def update_weights_from_tensor(       │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         obj: UpdateWeightsFromTensorReqInput,   │   │
│             │ │                                                │         request: Optional = None,               │   │
│             │ │                                                │     ) -> Tuple:                                 │   │
│             │ │                                                │         self.auto_create_handle_loop()          │   │
│             │ │                                                │         assert (                                │   │
│             │ │                                                │             self.server_args.dp_size == 1       │   │
│             │ │                                                │         ), "dp_size must be for update weights  │   │
│             │ │                                                │ from distributed"                               │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         # This means that weight sync           │   │
│             │ │                                                │         # cannot run while requests are in      │   │
│             │ │                                                │ progress.                                       │   │
│             │ │                                                │         async with                              │   │
│             │ │                                                │ self.model_update_lock.writer_lock:             │   │
│             │ │                                                │             result = (await                     │   │
│             │ │                                                │ self.update_weights_from_tensor_communicator(o… │   │
│             │ │                                                │             return result.success,              │   │
│             │ │                                                │ result.message                                  │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def get_weights_by_name(              │   │
│             │ │                                                │         self, obj: GetWeightsByNameReqInput,    │   │
│             │ │                                                │ request: Optional = None                        │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         self.auto_create_handle_loop()          │   │
│             │ │                                                │         results = await                         │   │
│             │ │                                                │ self.get_weights_by_name_communicator(obj)      │   │
│             │ │                                                │         all_parameters =                        │   │
│             │ │                                                │         if self.server_args.dp_size == 1:       │   │
│             │ │                                                │             return all_parameters[0]            │   │
│             │ │                                                │         else:                                   │   │
│             │ │                                                │             return all_parameters               │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def open_session(                     │   │
│             │ │                                                │         self, obj: OpenSessionReqInput,         │   │
│             │ │                                                │ request: Optional = None                        │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         self.auto_create_handle_loop()          │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         if obj.session_id is None:              │   │
│             │ │                                                │             obj.session_id = uuid.uuid4().hex   │   │
│             │ │                                                │         elif obj.session_id in                  │   │
│             │ │                                                │ self.session_futures:                           │   │
│             │ │                                                │             return None                         │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         self.send_to_scheduler.send_pyobj(obj)  │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         self.session_futures = asyncio.Future() │   │
│             │ │                                                │         session_id = await self.session_futures │   │
│             │ │                                                │         del self.session_futures                │   │
│             │ │                                                │         return session_id                       │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def close_session(                    │   │
│             │ │                                                │         self, obj: CloseSessionReqInput,        │   │
│             │ │                                                │ request: Optional = None                        │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         assert not self.to_create_loop, "close  │   │
│             │ │                                                │ session should not be the first request"        │   │
│             │ │                                                │         await                                   │   │
│             │ │                                                │ self.send_to_scheduler.send_pyobj(obj)          │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     def create_abort_task(self, obj:            │   │
│             │ │                                                │ GenerateReqInput):                              │   │
│             │ │                                                │         # Abort the request if the client is    │   │
│             │ │                                                │ disconnected.                                   │   │
│             │ │                                                │         async def abort_request():              │   │
│             │ │                                                │             await asyncio.sleep(1)              │   │
│             │ │                                                │             if obj.is_single:                   │   │
│             │ │                                                │                 self.abort_request(obj.rid)     │   │
│             │ │                                                │             else:                               │   │
│             │ │                                                │                 for rid in obj.rid:             │   │
│             │ │                                                │                     self.abort_request(rid)     │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         background_tasks = BackgroundTasks()    │   │
│             │ │                                                │         background_tasks.add_task(abort_reques… │   │
│             │ │                                                │         return background_tasks                 │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     def auto_create_handle_loop(self):          │   │
│             │ │                                                │         if not self.to_create_loop:             │   │
│             │ │                                                │             return                              │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         self.to_create_loop = False             │   │
│             │ │                                                │         loop = asyncio.get_event_loop()         │   │
│             │ │                                                │         self.asyncio_tasks.add(loop.create_tas… │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         signal_handler = SignalHandler(self)    │   │
│             │ │                                                │         loop.add_signal_handler(signal.SIGTERM, │   │
│             │ │                                                │ signal_handler.signal_handler)                  │   │
│             │ │                                                │         self.asyncio_tasks.add(loop.create_tas… │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def sigterm_watchdog(self):           │   │
│             │ │                                                │         while not self.gracefully_exit:         │   │
│             │ │                                                │             await asyncio.sleep(5)              │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         # drain requests                        │   │
│             │ │                                                │         while True:                             │   │
│             │ │                                                │             remain_num_req =                    │   │
│             │ │                                                │ len(self.rid_to_state)                          │   │
│             │ │                                                │             logger.info(                        │   │
│             │ │                                                │                 f"Gracefully exiting...         │   │
│             │ │                                                │ remaining number of requests {remain_num_req}"  │   │
│             │ │                                                │             )                                   │   │
│             │ │                                                │             if remain_num_req > 0:              │   │
│             │ │                                                │                 await asyncio.sleep(5)          │   │
│             │ │                                                │             else:                               │   │
│             │ │                                                │                 break                           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         kill_process_tree(os.getpid(),          │   │
│             │ │                                                │ include_parent=True)                            │   │
│             │ │                                                │         sys.exit(0)                             │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     async def handle_loop(self):                │   │
│             │ │                                                │         """The event loop that handles          │   │
│             │ │                                                │ requests"""                                     │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         while True:                             │   │
│             │ │                                                │             recv_obj: Union[                    │   │
│             │ │                                                │                 BatchStrOut,                    │   │
│             │ │                                                │                 BatchEmbeddingOut,              │   │
│             │ │                                                │                 BatchTokenIDOut,                │   │
│             │ │                                                │                 UpdateWeightFromDiskReqOutput,  │   │
│             │ │                                                │                 UpdateWeightsFromDistributedRe… │   │
│             │ │                                                │                 GetWeightsByNameReqOutput,      │   │
│             │ │                                                │                 InitWeightsUpdateGroupReqOutpu… │   │
│             │ │                                                │             ] = await                           │   │
│             │ │                                                │ self.recv_from_detokenizer.recv_pyobj()         │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │             if isinstance(recv_obj,             │   │
│             │ │                                                │ (BatchStrOut, BatchEmbeddingOut,                │   │
│             │ │                                                │ BatchTokenIDOut)):                              │   │
│             │ │                                                │                 for i, rid in                   │   │
│             │ │                                                │ enumerate(recv_obj.rids):                       │   │
│             │ │                                                │                     state =                     │   │
│             │ │                                                │ self.rid_to_state.get(rid, None)                │   │
│             │ │                                                │                     if state is None:           │   │
│             │ │                                                │                         continue                │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                     meta_info = {               │   │
│             │ │                                                │                         "id": rid,              │   │
│             │ │                                                │                         "finish_reason":        │   │
│             │ │                                                │ recv_obj.finished_reasons,                      │   │
│             │ │                                                │                         "prompt_tokens":        │   │
│             │ │                                                │ recv_obj.prompt_tokens,                         │   │
│             │ │                                                │                     }                           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                     if getattr(state.obj,       │   │
│             │ │                                                │ "return_logprob", False):                       │   │
│             │ │                                                │                         self.convert_logprob_s… │   │
│             │ │                                                │                             meta_info,          │   │
│             │ │                                                │                             state.obj.top_logp… │   │
│             │ │                                                │                             state.obj.return_t… │   │
│             │ │                                                │                             recv_obj,           │   │
│             │ │                                                │                             i,                  │   │
│             │ │                                                │                         )                       │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                     if not isinstance(recv_obj, │   │
│             │ │                                                │ BatchEmbeddingOut):                             │   │
│             │ │                                                │                         meta_info.update(       │   │
│             │ │                                                │                             {                   │   │
│             │ │                                                │                                 "completion_to… │   │
│             │ │                                                │ recv_obj.completion_tokens,                     │   │
│             │ │                                                │                                 "cached_tokens… │   │
│             │ │                                                │ recv_obj.cached_tokens,                         │   │
│             │ │                                                │                             }                   │   │
│             │ │                                                │                         )                       │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                     if isinstance(recv_obj,     │   │
│             │ │                                                │ BatchStrOut):                                   │   │
│             │ │                                                │                         out_dict = {            │   │
│             │ │                                                │                             "text":             │   │
│             │ │                                                │ recv_obj.output_strs,                           │   │
│             │ │                                                │                             "meta_info":        │   │
│             │ │                                                │ meta_info,                                      │   │
│             │ │                                                │                         }                       │   │
│             │ │                                                │                         if                      │   │
│             │ │                                                │ self.server_args.return_token_ids:              │   │
│             │ │                                                │                             out_dict.update(    │   │
│             │ │                                                │                                 {               │   │
│             │ │                                                │                                     "input_ids… │   │
│             │ │                                                │ recv_obj.origin_input_ids,                      │   │
│             │ │                                                │                                     "output_id… │   │
│             │ │                                                │ recv_obj.output_ids,                            │   │
│             │ │                                                │                                 }               │   │
│             │ │                                                │                             )                   │   │
│             │ │                                                │                     elif isinstance(recv_obj,   │   │
│             │ │                                                │ BatchTokenIDOut):                               │   │
│             │ │                                                │                         out_dict = {            │   │
│             │ │                                                │                             "token_ids":        │   │
│             │ │                                                │ recv_obj.output_ids,                            │   │
│             │ │                                                │                             "meta_info":        │   │
│             │ │                                                │ meta_info,                                      │   │
│             │ │                                                │                         }                       │   │
│             │ │                                                │                     else:                       │   │
│             │ │                                                │                         assert                  │   │
│             │ │                                                │ isinstance(recv_obj, BatchEmbeddingOut)         │   │
│             │ │                                                │                         out_dict = {            │   │
│             │ │                                                │                             "embedding":        │   │
│             │ │                                                │ recv_obj.embeddings,                            │   │
│             │ │                                                │                             "meta_info":        │   │
│             │ │                                                │ meta_info,                                      │   │
│             │ │                                                │                         }                       │   │
│             │ │                                                │                     state.out_list.append(out_… │   │
│             │ │                                                │                     state.finished =            │   │
│             │ │                                                │ recv_obj.finished_reasons is not None           │   │
│             │ │                                                │                     state.event.set()           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                     if self.enable_metrics:     │   │
│             │ │                                                │                         completion_tokens = (   │   │
│             │ │                                                │                             recv_obj.completio… │   │
│             │ │                                                │                             if                  │   │
│             │ │                                                │ recv_obj.completion_tokens                      │   │
│             │ │                                                │                             else 0              │   │
│             │ │                                                │                         )                       │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                         if                      │   │
│             │ │                                                │ state.first_token_time is None:                 │   │
│             │ │                                                │                             state.first_token_… │   │
│             │ │                                                │ = time.time()                                   │   │
│             │ │                                                │                             self.metrics_colle… │   │
│             │ │                                                │                                 state.first_to… │   │
│             │ │                                                │ - state.created_time                            │   │
│             │ │                                                │                             )                   │   │
│             │ │                                                │                         else:                   │   │
│             │ │                                                │                             if                  │   │
│             │ │                                                │ completion_tokens >= 2:                         │   │
│             │ │                                                │                                 # Compute       │   │
│             │ │                                                │ time_per_output_token for the streaming case    │   │
│             │ │                                                │                                 self.metrics_c… │   │
│             │ │                                                │                                     (time.time… │   │
│             │ │                                                │ - state.first_token_time)                       │   │
│             │ │                                                │                                     /           │   │
│             │ │                                                │ (completion_tokens - 1)                         │   │
│             │ │                                                │                                 )               │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │                         if state.finished:      │   │
│             │ │                                                │                             self.metrics_colle… │   │
│             │ │                                                │                                 recv_obj.promp… │   │
│             │ │                                                │                             )                   │   │
│             │ │                                                │                             self.metrics_colle… │   │
│             │ │                                                │                                 completion_tok… │   │
│             │ │                                                │                             )                   │   │
│             │ │                                                │                             self.metrics_colle… │   │
│             │ │                                                │                                 time.time() -   │   │
│             │ │                                                │ state.created_time                              │   │
│             │ │                                                │                             )                   │   │
│             │ │                                                │                             # Compute           │   │
│             │ │                                                │ time_per_output_token for the non-streaming     │   │
│             │ │                                                │ case                                            │   │
│             │ │                                                │                             if not              │   │
│             │ │                                                │ state.obj.stream and completion_tokens >= 1:    │   │
│             │ │                                                │                                 self.metrics_c… │   │
│             │ │                                                │                                     (time.time… │   │
│             │ │                                                │ - state.created_time)                           │   │
│             │ │                                                │                                     /           │   │
│             │ │                                                │ completion_tokens                               │   │
│             │ │                                                │                                 )               │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ OpenSessionReqOutput):                          │   │
│             │ │                                                │                 self.session_futures.set_resul… │   │
│             │ │                                                │                     recv_obj.session_id if      │   │
│             │ │                                                │ recv_obj.success else None                      │   │
│             │ │                                                │                 )                               │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ UpdateWeightFromDiskReqOutput):                 │   │
│             │ │                                                │                 if self.server_args.dp_size ==  │   │
│             │ │                                                │ 1:                                              │   │
│             │ │                                                │                     self.model_update_result.s… │   │
│             │ │                                                │                 else:  #                        │   │
│             │ │                                                │ self.server_args.dp_size > 1                    │   │
│             │ │                                                │                     self.model_update_tmp.appe… │   │
│             │ │                                                │                     # set future if the all     │   │
│             │ │                                                │ results are recevied                            │   │
│             │ │                                                │                     if                          │   │
│             │ │                                                │ len(self.model_update_tmp) ==                   │   │
│             │ │                                                │ self.server_args.dp_size:                       │   │
│             │ │                                                │                         self.model_update_resu… │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ InitWeightsUpdateGroupReqOutput):               │   │
│             │ │                                                │                 assert (                        │   │
│             │ │                                                │                     self.server_args.dp_size == │   │
│             │ │                                                │ 1                                               │   │
│             │ │                                                │                 ), "dp_size must be 1 for init  │   │
│             │ │                                                │ parameter update group"                         │   │
│             │ │                                                │                 self.init_weights_update_group… │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqOutput):         │   │
│             │ │                                                │                 assert (                        │   │
│             │ │                                                │                     self.server_args.dp_size == │   │
│             │ │                                                │ 1                                               │   │
│             │ │                                                │                 ), "dp_size must be 1 for       │   │
│             │ │                                                │ update weights from distributed"                │   │
│             │ │                                                │                 self.update_weights_from_distr… │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ UpdateWeightsFromTensorReqOutput):              │   │
│             │ │                                                │                 assert (                        │   │
│             │ │                                                │                     self.server_args.dp_size == │   │
│             │ │                                                │ 1                                               │   │
│             │ │                                                │                 ), "dp_size must be 1 for       │   │
│             │ │                                                │ update weights from distributed"                │   │
│             │ │                                                │                 self.update_weights_from_tenso… │   │
│             │ │                                                │             elif isinstance(recv_obj,           │   │
│             │ │                                                │ GetWeightsByNameReqOutput):                     │   │
│             │ │                                                │                 self.get_weights_by_name_commu… │   │
│             │ │                                                │             else:                               │   │
│             │ │                                                │                 raise ValueError(f"Invalid      │   │
│             │ │                                                │ object: {recv_obj=}")                           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     def convert_logprob_style(                  │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         meta_info: dict,                        │   │
│             │ │                                                │         top_logprobs_num: int,                  │   │
│             │ │                                                │         return_text_in_logprobs: bool,          │   │
│             │ │                                                │         recv_obj: BatchStrOut,                  │   │
│             │ │                                                │         recv_obj_index: int,                    │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         meta_info["input_token_logprobs"] =     │   │
│             │ │                                                │ self.detokenize_logprob_tokens(                 │   │
│             │ │                                                │             recv_obj.input_token_logprobs_val,  │   │
│             │ │                                                │             recv_obj.input_token_logprobs_idx,  │   │
│             │ │                                                │             return_text_in_logprobs,            │   │
│             │ │                                                │         )                                       │   │
│             │ │                                                │         meta_info["output_token_logprobs"] =    │   │
│             │ │                                                │ self.detokenize_logprob_tokens(                 │   │
│             │ │                                                │             recv_obj.output_token_logprobs_val, │   │
│             │ │                                                │             recv_obj.output_token_logprobs_idx, │   │
│             │ │                                                │             return_text_in_logprobs,            │   │
│             │ │                                                │         )                                       │   │
│             │ │                                                │         meta_info["normalized_prompt_logprob"]  │   │
│             │ │                                                │ = recv_obj.normalized_prompt_logprob[           │   │
│             │ │                                                │             recv_obj_index                      │   │
│             │ │                                                │         ]                                       │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │         if top_logprobs_num > 0:                │   │
│             │ │                                                │             meta_info["input_top_logprobs"] =   │   │
│             │ │                                                │ self.detokenize_top_logprobs_tokens(            │   │
│             │ │                                                │                 recv_obj.input_top_logprobs_va… │   │
│             │ │                                                │                 recv_obj.input_top_logprobs_id… │   │
│             │ │                                                │                 return_text_in_logprobs,        │   │
│             │ │                                                │             )                                   │   │
│             │ │                                                │             meta_info["output_top_logprobs"] =  │   │
│             │ │                                                │ self.detokenize_top_logprobs_tokens(            │   │
│             │ │                                                │                 recv_obj.output_top_logprobs_v… │   │
│             │ │                                                │                 recv_obj.output_top_logprobs_i… │   │
│             │ │                                                │                 return_text_in_logprobs,        │   │
│             │ │                                                │             )                                   │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     def detokenize_logprob_tokens(              │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         token_logprobs_val: List,               │   │
│             │ │                                                │         token_logprobs_idx: List,               │   │
│             │ │                                                │         decode_to_text: bool,                   │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         if not decode_to_text:                  │   │
│             │ │                                                │             return [                            │   │
│             │ │                                                │                 (logprob, token_id, None)       │   │
│             │ │                                                │                 for logprob, token_id in        │   │
│             │ │                                                │ zip(token_logprobs_val, token_logprobs_idx)     │   │
│             │ │                                                │             ]                                   │   │
│             │ │                                                │         else:                                   │   │
│             │ │                                                │             assert self.tokenizer is not None   │   │
│             │ │                                                │             token_texts =                       │   │
│             │ │                                                │ self.tokenizer.batch_decode(token_logprobs_idx) │   │
│             │ │                                                │             return list(zip(token_logprobs_val, │   │
│             │ │                                                │ token_logprobs_idx, token_texts))               │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     def detokenize_top_logprobs_tokens(         │   │
│             │ │                                                │         self,                                   │   │
│             │ │                                                │         token_logprobs_val: List,               │   │
│             │ │                                                │         token_logprobs_idx: List,               │   │
│             │ │                                                │         decode_to_text: bool,                   │   │
│             │ │                                                │     ):                                          │   │
│             │ │                                                │         # TODO: The current implementation only │   │
│             │ │                                                │ batches the detokenization for top-k tokens per │   │
│             │ │                                                │ single position.                                │   │
│             │ │                                                │         # We should batch all top-k tokens in   │   │
│             │ │                                                │ all positions.                                  │   │
│             │ │                                                │         ret = []                                │   │
│             │ │                                                │         for i in                                │   │
│             │ │                                                │ range(len(token_logprobs_val)):                 │   │
│             │ │                                                │             if token_logprobs_val:              │   │
│             │ │                                                │                 ret.append(                     │   │
│             │ │                                                │                     self.detokenize_logprob_to… │   │
│             │ │                                                │                         token_logprobs_val,     │   │
│             │ │                                                │ token_logprobs_idx, decode_to_text              │   │
│             │ │                                                │                     )                           │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 828777 Output: 4550                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 25: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │   1460          success, message =              │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ self.tp_worker.update_weights_from_disk(recv_r… │   │
│             │ │ 'view_range': [1460, 1505]}                    │   1461          if success:                     │   │
│             │ │                                                │   1462              flash_cache_success =       │   │
│             │ │                                                │ self.flush_cache()                              │   │
│             │ │                                                │   1463              assert flash_cache_success, │   │
│             │ │                                                │ "Cache flush failed after updating weights"     │   │
│             │ │                                                │   1464          else:                           │   │
│             │ │                                                │   1465              logger.error(message)       │   │
│             │ │                                                │   1466          return success, message         │   │
│             │ │                                                │   1467                                          │   │
│             │ │                                                │   1468      def init_weights_update_group(self, │   │
│             │ │                                                │ recv_req: InitWeightsUpdateGroupReqInput):      │   │
│             │ │                                                │   1469          """Initialize the online model  │   │
│             │ │                                                │ parameter update group."""                      │   │
│             │ │                                                │   1470          success, message =              │   │
│             │ │                                                │ self.tp_worker.init_weights_update_group(recv_… │   │
│             │ │                                                │   1471          return success, message         │   │
│             │ │                                                │   1472                                          │   │
│             │ │                                                │   1473      def                                 │   │
│             │ │                                                │ update_weights_from_distributed(                │   │
│             │ │                                                │   1474          self, recv_req:                 │   │
│             │ │                                                │ UpdateWeightsFromDistributedReqInput            │   │
│             │ │                                                │   1475      ):                                  │   │
│             │ │                                                │   1476          """Update the online model      │   │
│             │ │                                                │ parameter."""                                   │   │
│             │ │                                                │   1477          success, message =              │   │
│             │ │                                                │ self.tp_worker.update_weights_from_distributed… │   │
│             │ │                                                │   1478          if success:                     │   │
│             │ │                                                │   1479              flash_cache_success =       │   │
│             │ │                                                │ self.flush_cache()                              │   │
│             │ │                                                │   1480              assert flash_cache_success, │   │
│             │ │                                                │ "Cache flush failed after updating weights"     │   │
│             │ │                                                │   1481          else:                           │   │
│             │ │                                                │   1482              logger.error(message)       │   │
│             │ │                                                │   1483          return success, message         │   │
│             │ │                                                │   1484                                          │   │
│             │ │                                                │   1485      def                                 │   │
│             │ │                                                │ update_weights_from_tensor(self, recv_req:      │   │
│             │ │                                                │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │   1486          """Update the online model      │   │
│             │ │                                                │ parameter from tensors."""                      │   │
│             │ │                                                │   1487          success, message =              │   │
│             │ │                                                │ self.tp_worker.update_weights_from_tensor(recv… │   │
│             │ │                                                │   1488          # TODO extract common code b/t  │   │
│             │ │                                                │ update_weights_from_distributed and             │   │
│             │ │                                                │ update_weights_from_tensor later                │   │
│             │ │                                                │   1489          if success:                     │   │
│             │ │                                                │   1490              flash_cache_success =       │   │
│             │ │                                                │ self.flush_cache()                              │   │
│             │ │                                                │   1491              assert flash_cache_success, │   │
│             │ │                                                │ "Cache flush failed after updating weights"     │   │
│             │ │                                                │   1492          else:                           │   │
│             │ │                                                │   1493              logger.error(message)       │   │
│             │ │                                                │   1494          return success, message         │   │
│             │ │                                                │   1495                                          │   │
│             │ │                                                │   1496      def get_weights_by_name(self,       │   │
│             │ │                                                │ recv_req: GetWeightsByNameReqInput):            │   │
│             │ │                                                │   1497          parameter =                     │   │
│             │ │                                                │ self.tp_worker.get_weights_by_name(recv_req)    │   │
│             │ │                                                │   1498          return parameter                │   │
│             │ │                                                │   1499                                          │   │
│             │ │                                                │   1500      def start_profile(self) -> None:    │   │
│             │ │                                                │   1501          if self.profiler is None:       │   │
│             │ │                                                │   1502              raise                       │   │
│             │ │                                                │ RuntimeError("Profiler is not enabled.")        │   │
│             │ │                                                │   1503          self.profiler.start()           │   │
│             │ │                                                │   1504                                          │   │
│             │ │                                                │   1505      def stop_profile(self) -> None:     │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 887867 Output: 4645                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 26: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "grep -n 'torch.zeros\\|fill_'     │                                                 │   │
│             │ │ vllm/model_executor/layers/fused_moe/moe_alig… │                                                 │   │
│             │ │ benchmarks/kernels/benchmark_moe_align_block_… │                                                 │   │
│             │ │ || true", 'restart': False}                    │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 947734 Output: 5028                                                                             │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 27: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n "torch.zeros\\|fill_" -R  │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ self.seq_len_fill_value = (                     │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.model_runner.attn_backend.get_cuda_graph_… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.encoder_len_fill_value = 0                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.input_ids =                                │   │
│             │ │                                                │ torch.zeros((self.max_num_token,),              │   │
│             │ │                                                │ dtype=torch.int32)                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.req_pool_indices =                         │   │
│             │ │                                                │ torch.zeros((self.max_bs,), dtype=torch.int32)  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ (self.max_bs,), self.seq_len_fill_value,        │   │
│             │ │                                                │ dtype=torch.int32                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.out_cache_loc =                            │   │
│             │ │                                                │ torch.zeros((self.max_num_token,),              │   │
│             │ │                                                │ dtype=torch.int32)                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.positions =                                │   │
│             │ │                                                │ torch.zeros((self.max_num_token,),              │   │
│             │ │                                                │ dtype=torch.int64)                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.mrope_positions = torch.zeros((3,          │   │
│             │ │                                                │ self.max_bs), dtype=torch.int32)                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ (self.max_bs,), self.encoder_len_fill_value,    │   │
│             │ │                                                │ dtype=torch.int32                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.gathered_buffer = torch.zeros(             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.seq_lens.fill_(1)                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ ret.gathered_buffer = torch.zeros(              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ extend_start_loc =                              │   │
│             │ │                                                │ torch.zeros_like(extend_seq_lens)               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ server_args.chunked_prefill_size = -1           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ server_args.chunked_prefill_size = -1           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ param.data.fill_(loaded_weight.item())          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def _prefill_one_req(                           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ req.fill_ids = req.fill_ids[:                   │   │
│             │ │                                                │ len(req.prefix_indices) + req.extend_input_len] │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self._prefill_one_req(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self._prefill_one_req(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ req.fill_ids = req.fill_ids[:trunc_len]         │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self._prefill_one_req(0, trunc_len, 0)          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self._prefill_one_req(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ req.fill_ids = req.fill_ids[:                   │   │
│             │ │                                                │ len(req.prefix_indices) + trunc_len]            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self._prefill_one_req(prefix_len, trunc_len, 0) │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.max_prefill_tokens =                       │   │
│             │ │                                                │ server_args.max_prefill_tokens                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.max_prefill_tokens,                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.max_prefill_tokens,                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ f"max_prefill_tokens={self.max_prefill_tokens}, │   │
│             │ │                                                │ "                                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ server_args.chunked_prefill_size is not None    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size =                     │   │
│             │ │                                                │ server_args.chunked_prefill_size                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ if self.chunked_prefill_size <= 0:  # -1 means  │   │
│             │ │                                                │ disable                                         │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size = None                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size is not None and       │   │
│             │ │                                                │ server_args.enable_mixed_chunk                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def log_prefill_stats(self, adder,              │   │
│             │ │                                                │ can_run_list, running_bs, has_being_chunked):   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.max_prefill_tokens,                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size,                      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ if self.server_args.prefill_only_one_req:       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.log_prefill_stats(adder, can_run_list,     │   │
│             │ │                                                │ running_bs, has_being_chunked)                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ input_token_logprobs_idx = req.fill_ids[        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ len(req.fill_ids)                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ + 1 : len(req.fill_ids)                         │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ input_token_logprobs_idx = [req.fill_ids[0]] +  │   │
│             │ │                                                │ input_token_logprobs_idx                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ req.fill_ids[                                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ len(req.fill_ids)                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ - req.last_update_decode_tokens :               │   │
│             │ │                                                │ len(req.fill_ids)                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.fill_ids = None  # fill_ids =              │   │
│             │ │                                                │ origin_input_ids + output_ids                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.fill_ids = self.origin_input_ids +         │   │
│             │ │                                                │ self.output_ids                                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.extend_input_len = len(self.fill_ids) -    │   │
│             │ │                                                │ len(self.prefix_indices)                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.fill_ids = self.origin_input_ids +         │   │
│             │ │                                                │ self.output_ids                                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ input_len = len(self.fill_ids)                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ return self.fill_ids[:max_prefix_len]           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.out_cache_loc = torch.zeros(0,             │   │
│             │ │                                                │ dtype=torch.int32).to(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.encoder_out_cache_loc = torch.zeros(0,     │   │
│             │ │                                                │ dtype=torch.int32).to(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ input_ids = [r.fill_ids for r in reqs]          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ pre_len, seq_len = len(req.prefix_indices),     │   │
│             │ │                                                │ len(req.fill_ids)                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ req.fill_ids = req.origin_input_ids +           │   │
│             │ │                                                │ req.output_ids                                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ seg_indptr = torch.zeros((bs + 1,),             │   │
│             │ │                                                │ dtype=torch.int32, device="cuda")               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ return torch.zeros(batch_size, vocab_size,      │   │
│             │ │                                                │ dtype=torch.bool, device=device)                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def fill_vocab_mask(self, vocab_mask:           │   │
│             │ │                                                │ torch.Tensor, idx: int) -> None:                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ vocab_mask.fill_(1)                             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ vocab_mask.scatter_(0, tokens,                  │   │
│             │ │                                                │ torch.zeros_like(tokens, dtype=torch.bool))     │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ logits.masked_fill_(vocab_mask, float("-inf"))  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def fill_vocab_mask(self, vocab_mask:           │   │
│             │ │                                                │ torch.Tensor, idx: int) -> None:                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.matcher.fill_next_token_bitmask(vocab_mas… │   │
│             │ │                                                │ idx)                                            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ token_id_len = len(req.fill_ids)                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.req_to_token = torch.zeros(                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ token_ids = req.fill_ids                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ chunked_prefill_size: Optional = None           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ max_prefill_tokens: int = 16384                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_only_one_req: bool = False              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ if self.chunked_prefill_size is None:           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size = 2048                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size = 8192                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.chunked_prefill_size =                     │   │
│             │ │                                                │ self.chunked_prefill_size // 2                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ f"DP attention is enabled. The chunked prefill  │   │
│             │ │                                                │ size is adjusted to {self.chunked_prefill_size} │   │
│             │ │                                                │ to avoid MoE kernel issues. "                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ default=ServerArgs.chunked_prefill_size,        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ default=ServerArgs.max_prefill_tokens,          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ default=ServerArgs.prefill_only_one_req,        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cuda_graph_start_loc = torch.zeros(        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def get_cuda_graph_seq_len_fill_value(self):    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def get_cuda_graph_seq_len_fill_value(self):    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sglang.srt.layers.attention.triton_ops.prefill… │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_seq_len_q = extend_prefix_lens          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ per_req_query_redudant[:, prefill_seq_len_q:,   │   │
│             │ │                                                │ :] = per_req_query                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ output = per_req_out_redudant                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers:                               │   │
│             │ │                                                │ List[BatchPrefillWithPagedKVCacheWrapper]       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ torch.zeros((max_bs + 1,), dtype=torch.int32,   │   │
│             │ │                                                │ device=model_runner.device)                     │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ torch.zeros((max_bs + 1,), dtype=torch.int32,   │   │
│             │ │                                                │ device=model_runner.device)                     │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrapper_ragged = (                 │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_paged = []                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_verify = []               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_paged.append(             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_verify.append(            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_cuda_graph_metadata = {}           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers=self.prefill_wrappers_paged,   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_paged, False, False       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers=self.prefill_wrappers_verify,  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_verify, False, False      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers=self.prefill_wrappers_paged,   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrappers_paged, use_ragged,        │   │
│             │ │                                                │ extend_no_prefix                                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ cuda_graph_kv_indices = torch.zeros(            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cuda_graph_custom_mask = torch.zeros(      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers = []                           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers.append(                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers=prefill_wrappers,              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_cuda_graph_metadata =              │   │
│             │ │                                                │ prefill_wrappers                                │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.forward_metadata =                         │   │
│             │ │                                                │ PrefillMetadata(prefill_wrappers, False, False) │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers=self.prefill_cuda_graph_metad… │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ def get_cuda_graph_seq_len_fill_value(self):    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrapper_paged =                         │   │
│             │ │                                                │ self.forward_metadata.prefill_wrappers[         │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ o = prefill_wrapper_paged.forward(              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ o1, s1 =                                        │   │
│             │ │                                                │ self.prefill_wrapper_ragged.forward_return_lse( │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ o2, s2 =                                        │   │
│             │ │                                                │ prefill_wrapper_paged.forward_return_lse(       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ kv_start_idx = torch.zeros_like(encoder_lens)   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrapper_ragged =                   │   │
│             │ │                                                │ attn_backend.prefill_wrapper_ragged             │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers:                               │   │
│             │ │                                                │ List[BatchPrefillWithPagedKVCacheWrapper],      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers:                               │   │
│             │ │                                                │ List[BatchPrefillWithPagedKVCacheWrapper],      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrapper_ragged,                    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers[0],                            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers:                               │   │
│             │ │                                                │ List[BatchPrefillWithPagedKVCacheWrapper],      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrapper_ragged,                    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers,                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers:                               │   │
│             │ │                                                │ List[BatchPrefillWithPagedKVCacheWrapper],      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ kv_start_idx = torch.zeros_like(encoder_lens)   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.prefill_wrapper_ragged,                    │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ prefill_wrappers,                               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ start_loc =                                     │   │
│             │ │                                                │ torch.zeros_like(forward_batch.seq_lens,        │   │
│             │ │                                                │ dtype=torch.int32)                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ start = torch.zeros_like(pruned_lens)           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ batch_next_token_ids =                          │   │
│             │ │                                                │ torch.zeros_like(batch_next_token_ids)          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ return                                          │   │
│             │ │                                                │ torch.zeros_like(probs_sort).scatter_(-1,       │   │
│             │ │                                                │ probs_idx, probs_sort)                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ param[loaded_weight.shape[0] :].data.fill_(0)   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ output_parallel.masked_fill_(input_mask.unsque… │   │
│             │ │                                                │ 0)                                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ seg_indptr = torch.zeros(num_experts + 1,       │   │
│             │ │                                                │ device=topk_ids.device, dtype=torch.int64)      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ m_num_tiles_indptr = torch.zeros(batch_size +   │   │
│             │ │                                                │ 1, device=a.device, dtype=torch.int64)          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sorted_ids.fill_(topk_ids.numel())              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ group_mask = torch.zeros_like(group_scores)  #  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ group_mask = torch.zeros_like(group_scores)  #  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.weight =                                   │   │
│             │ │                                                │ nn.Parameter(torch.zeros(hidden_size))          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ gathered_weights =                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ gathered_weights =                              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ logits.index_fill_(-1,                          │   │
│             │ │                                                │ self.dummy_token_indices, -torch.inf)           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.gate = nn.Parameter(torch.zeros(1))        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.gate = nn.Parameter(torch.zeros(1))        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cross_attn_attn_gate =                     │   │
│             │ │                                                │ torch.nn.Parameter(torch.zeros(1))              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cross_attn_mlp_gate =                      │   │
│             │ │                                                │ torch.nn.Parameter(torch.zeros(1))              │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ batched_images = torch.zeros(                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ batched_ar_mask = torch.zeros(                  │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ cross_attention_states_flat = torch.zeros(      │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sglang.srt.layers.attention.triton_ops.prefill… │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.linear_penalties = torch.zeros(            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ grammar.fill_vocab_mask(self.vocab_mask, i)     │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ lhs = torch.empty((bs1, *shape),                │   │
│             │ │                                                │ device=device).fill_(default)                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ rhs = torch.empty((bs2, *shape),                │   │
│             │ │                                                │ device=device).fill_(default)                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.stop_token_penalties = torch.zeros(        │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ fill_value=float("-inf"),                       │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.len_output_tokens = torch.zeros(           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cached_counts = torch.zeros(               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ self.cached_counts = torch.zeros(               │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ data = torch.zeros(1, device=device)            │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ torch.frombuffer(metadata_buffer,               │   │
│             │ │                                                │ dtype=torch.uint8).fill_(0)                     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1008080 Output: 5115                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 28: Completed                                                                                  │
│ LLM         │ 💬 Planned file modifications:                                                                         │
│ Response    │ 1) Update UpdateWeightsFromTensorReqInput in io_struct.py to use serialized bytes instead of (name,    │
│             │ tensor).                                                                                               │
│             │ 2) Add MultiprocessingSerializer in utils.py for efficient tensor dict serialization.                  │
│             │ 3) Modify server.Engine.update_weights_from_tensor to serialize tensors using                          │
│             │ MultiprocessingSerializer.                                                                             │
│             │ 4) Update TpModelWorker.update_weights_from_tensor to pass serialized bytes to ModelRunner.            │
│             │ 5) Change ModelRunner.update_weights_from_tensor to accept serialized bytes and load multiple named    │
│             │ tensors.                                                                                               │
│             │                                                                                                        │
│             │ I'll now create a benchmarking script under .bench_scratch to compare pickle vs torch.save             │
│             │ serialization performance for large tensors, then execute it.                                          │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'create', 'file_text': 'import     │                                                 │   │
│             │ │ os\nimport sys\nimport time\nfrom io import    │                                                 │   │
│             │ │ BytesIO\nimport pickle\n\nimport torch\n\n#    │                                                 │   │
│             │ │ Simple benchmark for serializing a dict\n#     │                                                 │   │
│             │ │ Compares Python pickle vs torch.save\n\ndef    │                                                 │   │
│             │ │ bench_once(shape=(2048, 2048),                 │                                                 │   │
│             │ │ dtype=torch.float16, repeats=3):\n             │                                                 │   │
│             │ │ named_tensors = {\n        "w":                │                                                 │   │
│             │ │ torch.randn(*shape, dtype=dtype),\n            │                                                 │   │
│             │ │ "b": torch.randn(shape[0], dtype=dtype),\n     │                                                 │   │
│             │ │ }\n\n    # pickle baseline\n    t0 =           │                                                 │   │
│             │ │ time.time()\n    for _ in range(repeats):\n    │                                                 │   │
│             │ │ data = pickle.dumps(named_tensors)\n           │                                                 │   │
│             │ │ obj = pickle.loads(data)\n    t_pickle =       │                                                 │   │
│             │ │ time.time() - t0\n\n    #                      │                                                 │   │
│             │ │ torch.save/torch.load\n    t0 = time.time()\n  │                                                 │   │
│             │ │ for _ in range(repeats):\n        buf =        │                                                 │   │
│             │ │ BytesIO()\n        torch.save(named_tensors,   │                                                 │   │
│             │ │ buf)\n        buf.seek(0)\n        obj =       │                                                 │   │
│             │ │ torch.load(buf, map_location="cpu")\n          │                                                 │   │
│             │ │ t_torch = time.time() - t0\n\n    return       │                                                 │   │
│             │ │ t_pickle, t_torch\n\n\ndef main():\n    shape  │                                                 │   │
│             │ │ = (2048, 2048)\n    if len(sys.argv) >= 3:\n   │                                                 │   │
│             │ │ shape = (int(sys.argv[1]), int(sys.argv[2]))\n │                                                 │   │
│             │ │ repeats = int(os.environ.get("REPEATS",        │                                                 │   │
│             │ │ "3"))\n\n    t_pickle, t_torch =               │                                                 │   │
│             │ │ bench_once(shape=shape, repeats=repeats)\n     │                                                 │   │
│             │ │ size_mb = (shape[0] * shape[1] * 2 + shape[0]  │                                                 │   │
│             │ │ * 2) / (1024 ** 2)  # float16 bytes\n\n        │                                                 │   │
│             │ │ print(f"Tensor total size: ~{size_mb:.2f} MB;  │                                                 │   │
│             │ │ repeats={repeats}")\n    print(f"pickle        │                                                 │   │
│             │ │ total: {t_pickle:.4f}s; per iter:              │                                                 │   │
│             │ │ {t_pickle/repeats:.4f}s")\n                    │                                                 │   │
│             │ │ print(f"torch.save total: {t_torch:.4f}s; per  │                                                 │   │
│             │ │ iter: {t_torch/repeats:.4f}s")\n\n\nif         │                                                 │   │
│             │ │ __name__ == "__main__":\n    main()\n',        │                                                 │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │                                                 │   │
│             │ │ '', 'path':                                    │                                                 │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │                                                 │   │
│             │ │ 'view_range': []}                              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1078955 Output: 7937                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 29: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'mkdir -p                          │ ok                                              │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ && echo ok', 'restart': False}                 │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1152833 Output: 8003                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 30: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'create', 'file_text': 'import     │ File created successfully at:                   │   │
│             │ │ os\nimport sys\nimport time\nfrom io import    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ BytesIO\nimport pickle\n\nimport torch\n\n#    │                                                 │   │
│             │ │ Simple benchmark for serializing a dict\n#     │                                                 │   │
│             │ │ Compares Python pickle vs torch.save\n\ndef    │                                                 │   │
│             │ │ bench_once(shape=(2048, 2048),                 │                                                 │   │
│             │ │ dtype=torch.float16, repeats=3):\n             │                                                 │   │
│             │ │ named_tensors = {\n        "w":                │                                                 │   │
│             │ │ torch.randn(*shape, dtype=dtype),\n            │                                                 │   │
│             │ │ "b": torch.randn(shape[0], dtype=dtype),\n     │                                                 │   │
│             │ │ }\n\n    # pickle baseline\n    t0 =           │                                                 │   │
│             │ │ time.time()\n    for _ in range(repeats):\n    │                                                 │   │
│             │ │ data = pickle.dumps(named_tensors)\n           │                                                 │   │
│             │ │ obj = pickle.loads(data)\n    t_pickle =       │                                                 │   │
│             │ │ time.time() - t0\n\n    #                      │                                                 │   │
│             │ │ torch.save/torch.load\n    t0 = time.time()\n  │                                                 │   │
│             │ │ for _ in range(repeats):\n        buf =        │                                                 │   │
│             │ │ BytesIO()\n        torch.save(named_tensors,   │                                                 │   │
│             │ │ buf)\n        buf.seek(0)\n        obj =       │                                                 │   │
│             │ │ torch.load(buf, map_location="cpu")\n          │                                                 │   │
│             │ │ t_torch = time.time() - t0\n\n    return       │                                                 │   │
│             │ │ t_pickle, t_torch\n\n\ndef main():\n    shape  │                                                 │   │
│             │ │ = (2048, 2048)\n    if len(sys.argv) >= 3:\n   │                                                 │   │
│             │ │ shape = (int(sys.argv[1]), int(sys.argv[2]))\n │                                                 │   │
│             │ │ repeats = int(os.environ.get("REPEATS",        │                                                 │   │
│             │ │ "3"))\n\n    t_pickle, t_torch =               │                                                 │   │
│             │ │ bench_once(shape=shape, repeats=repeats)\n     │                                                 │   │
│             │ │ size_mb = (shape[0] * shape[1] * 2 + shape[0]  │                                                 │   │
│             │ │ * 2) / (1024 ** 2)  # float16 bytes\n\n        │                                                 │   │
│             │ │ print(f"Tensor total size: ~{size_mb:.2f} MB;  │                                                 │   │
│             │ │ repeats={repeats}")\n    print(f"pickle        │                                                 │   │
│             │ │ total: {t_pickle:.4f}s; per iter:              │                                                 │   │
│             │ │ {t_pickle/repeats:.4f}s")\n                    │                                                 │   │
│             │ │ print(f"torch.save total: {t_torch:.4f}s; per  │                                                 │   │
│             │ │ iter: {t_torch/repeats:.4f}s")\n\n\nif         │                                                 │   │
│             │ │ __name__ == "__main__":\n    main()\n',        │                                                 │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │                                                 │   │
│             │ │ '', 'path':                                    │                                                 │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │                                                 │   │
│             │ │ 'view_range': []}                              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1226789 Output: 8579                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 31: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'python                            │ Tensor total size: ~8.00 MB; repeats=3          │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ pickle    total: 0.0389s; per iter: 0.0130s     │   │
│             │ │ 'restart': False}                              │ torch.save total: 0.0143s; per iter: 0.0048s    │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1301382 Output: 8643                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 32: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str':                   │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '@dataclass\nclass                             │ has been edited. Here's the result of running   │   │
│             │ │ UpdateWeightsFromTensorReqInput:\n             │ `cat -n` on a snippet of                        │   │
│             │ │ serialized_named_tensors: bytes  # indeed      │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ Dict\n', 'old_str': '@dataclass\nclass         │    423      success: bool                       │   │
│             │ │ UpdateWeightsFromTensorReqInput:\n    name:    │    424      message: str                        │   │
│             │ │ str\n    tensor: torch.Tensor\n', 'path':      │    425                                          │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │    426                                          │   │
│             │ │ 'view_range': []}                              │    427  @dataclass                              │   │
│             │ │                                                │    428  class UpdateWeightsFromTensorReqInput:  │   │
│             │ │                                                │    429      serialized_named_tensors: bytes  #  │   │
│             │ │                                                │ indeed Dict                                     │   │
│             │ │                                                │    430                                          │   │
│             │ │                                                │    431                                          │   │
│             │ │                                                │    432  @dataclass                              │   │
│             │ │                                                │    433  class UpdateWeightsFromTensorReqOutput: │   │
│             │ │                                                │    434      success: bool                       │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1376202 Output: 8790                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 33: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'insert', 'file_text': '',         │ The file                                        │   │
│             │ │ 'insert_line': 744, 'new_str': '\nclass        │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ MultiprocessingSerializer:\n    """Efficient   │ has been edited. Here's the result of running   │   │
│             │ │ serializer for transferring named tensors      │ `cat -n` on a snippet of the edited file:       │   │
│             │ │ between processes.\n\n    Uses                 │    741          data =                          │   │
│             │ │ torch.save/torch.load to handle tensors        │ pickle.loads(serialized_data)                   │   │
│             │ │ efficiently and reduce Python\n    pickle      │    742          return data                     │   │
│             │ │ overhead compared to generic pickle.dumps for  │    743                                          │   │
│             │ │ large tensors.\n    """\n\n    @staticmethod\n │    744                                          │   │
│             │ │ def dumps(named_tensors: Dict) -> bytes:\n     │    745                                          │   │
│             │ │ buf = BytesIO()\n        # torch.save is       │    746  class MultiprocessingSerializer:        │   │
│             │ │ generally faster and more tensor-aware than    │    747      """Efficient serializer for         │   │
│             │ │ pickle.dumps\n                                 │ transferring named tensors between processes.   │   │
│             │ │ torch.save(named_tensors, buf)\n        return │    748                                          │   │
│             │ │ buf.getvalue()\n\n    @staticmethod\n    def   │    749      Uses torch.save/torch.load to       │   │
│             │ │ loads(buf: bytes) -> Dict:\n        bio =      │ handle tensors efficiently and reduce Python    │   │
│             │ │ BytesIO(buf)\n        # Load to CPU by         │    750      pickle overhead compared to generic │   │
│             │ │ default; the consumer can move to appropriate  │ pickle.dumps for large tensors.                 │   │
│             │ │ device\n        return torch.load(bio,         │    751      """                                 │   │
│             │ │ map_location="cpu")\n', 'old_str': '', 'path': │    752                                          │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │    753      @staticmethod                       │   │
│             │ │ 'view_range': [732, 760]}                      │    754      def dumps(named_tensors: Dict) ->   │   │
│             │ │                                                │ bytes:                                          │   │
│             │ │                                                │    755          buf = BytesIO()                 │   │
│             │ │                                                │    756          # torch.save is generally       │   │
│             │ │                                                │ faster and more tensor-aware than pickle.dumps  │   │
│             │ │                                                │    757          torch.save(named_tensors, buf)  │   │
│             │ │                                                │    758          return buf.getvalue()           │   │
│             │ │                                                │    759                                          │   │
│             │ │                                                │    760      @staticmethod                       │   │
│             │ │                                                │    761      def loads(buf: bytes) -> Dict:      │   │
│             │ │                                                │    762          bio = BytesIO(buf)              │   │
│             │ │                                                │    763          # Load to CPU by default; the   │   │
│             │ │                                                │ consumer can move to appropriate device         │   │
│             │ │                                                │    764          return torch.load(bio,          │   │
│             │ │                                                │ map_location="cpu")                             │   │
│             │ │                                                │    765                                          │   │
│             │ │                                                │    766  step_counter = 0                        │   │
│             │ │                                                │    767                                          │   │
│             │ │                                                │    768                                          │   │
│             │ │                                                │    769  def pytorch_profile(name, func, *args,  │   │
│             │ │                                                │ data_size=-1):                                  │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected (correct indentation, no duplicate     │   │
│             │ │                                                │ lines, etc). Edit the file again if necessary.  │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1451439 Output: 9058                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 34: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str': 'from             │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ sglang.srt.utils import                        │ has been edited. Here's the result of running   │   │
│             │ │ MultiprocessingSerializer, broadcast_pyobj,    │ `cat -n` on a snippet of                        │   │
│             │ │ set_random_seed', 'old_str': 'from             │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ sglang.srt.utils import broadcast_pyobj,       │     29  from sglang.srt.managers.schedule_batch │   │
│             │ │ set_random_seed', 'path':                      │ import ModelWorkerBatch,                        │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ global_server_args_dict                         │   │
│             │ │ 'view_range': []}                              │     30  from                                    │   │
│             │ │                                                │ sglang.srt.model_executor.forward_batch_info    │   │
│             │ │                                                │ import ForwardBatch                             │   │
│             │ │                                                │     31  from                                    │   │
│             │ │                                                │ sglang.srt.model_executor.model_runner import   │   │
│             │ │                                                │ ModelRunner                                     │   │
│             │ │                                                │     32  from sglang.srt.server_args import      │   │
│             │ │                                                │ ServerArgs                                      │   │
│             │ │                                                │     33  from sglang.srt.utils import            │   │
│             │ │                                                │ MultiprocessingSerializer, broadcast_pyobj,     │   │
│             │ │                                                │ set_random_seed                                 │   │
│             │ │                                                │     34                                          │   │
│             │ │                                                │     35  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     36                                          │   │
│             │ │                                                │     37                                          │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1527397 Output: 9250                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 35: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str': '        success, │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ message =                                      │ has been edited. Here's the result of running   │   │
│             │ │ self.model_runner.update_weights_from_tensor(… │ `cat -n` on a snippet of                        │   │
│             │ │ recv_req.serialized_named_tensors\n        )', │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ 'old_str': '        success, message =         │    195          )                               │   │
│             │ │ self.model_runner.update_weights_from_tensor(… │    196          return success, message         │   │
│             │ │ recv_req.name, recv_req.tensor\n        )',    │    197                                          │   │
│             │ │ 'path':                                        │    198      def                                 │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ update_weights_from_tensor(self, recv_req:      │   │
│             │ │ 'view_range': []}                              │ UpdateWeightsFromTensorReqInput):               │   │
│             │ │                                                │    199          success, message =              │   │
│             │ │                                                │ self.model_runner.update_weights_from_tensor(   │   │
│             │ │                                                │    200                                          │   │
│             │ │                                                │ recv_req.serialized_named_tensors               │   │
│             │ │                                                │    201          )                               │   │
│             │ │                                                │    202          return success, message         │   │
│             │ │                                                │    203                                          │   │
│             │ │                                                │    204      def get_weights_by_name(self,       │   │
│             │ │                                                │ recv_req: GetWeightsByNameReqInput):            │   │
│             │ │                                                │    205          parameter =                     │   │
│             │ │                                                │ self.model_runner.get_weights_by_name(          │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1603871 Output: 9392                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 36: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str': '    def          │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ update_weights_from_tensor(self,               │ has been edited. Here's the result of running   │   │
│             │ │ serialized_named_tensors: bytes):\n            │ `cat -n` on a snippet of                        │   │
│             │ │ from sglang.srt.utils import                   │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ MultiprocessingSerializer\n                    │    427              )                           │   │
│             │ │ named_tensors =                                │    428              logger.error(error_msg)     │   │
│             │ │ MultiprocessingSerializer.loads(serialized_na… │    429              return False, error_msg     │   │
│             │ │ # named_tensors: Dict\n                        │    430                                          │   │
│             │ │ self.model.load_weights(list(named_tensors.it… │    431      def                                 │   │
│             │ │ return True, "Success"  # TODO error           │ update_weights_from_tensor(self,                │   │
│             │ │ handling', 'old_str': '    def                 │ serialized_named_tensors: bytes):               │   │
│             │ │ update_weights_from_tensor(self, name, tensor: │    432          from sglang.srt.utils import    │   │
│             │ │ torch.Tensor):\n                               │ MultiprocessingSerializer                       │   │
│             │ │ self.model.load_weights([(name, tensor)])\n    │    433          named_tensors =                 │   │
│             │ │ return True, "Success"  # TODO error           │ MultiprocessingSerializer.loads(serialized_nam… │   │
│             │ │ handling', 'path':                             │    434          # named_tensors: Dict           │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │    435                                          │   │
│             │ │ 'view_range': [430, 436]}                      │ self.model.load_weights(list(named_tensors.ite… │   │
│             │ │                                                │    436          return True, "Success"  # TODO  │   │
│             │ │                                                │ error handling                                  │   │
│             │ │                                                │    437                                          │   │
│             │ │                                                │    438      def get_weights_by_name(            │   │
│             │ │                                                │    439          self, name: str, truncate_size: │   │
│             │ │                                                │ int = 100                                       │   │
│             │ │                                                │    440      ) -> Optional:                      │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1680754 Output: 9613                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 37: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str': '                 │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ prepare_model_and_tokenizer,\n                 │ has been edited. Here's the result of running   │   │
│             │ │ set_prometheus_multiproc_dir,\n                │ `cat -n` on a snippet of                        │   │
│             │ │ set_ulimit,\n    MultiprocessingSerializer,',  │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ 'old_str': '    prepare_model_and_tokenizer,\n │     85      delete_directory,                   │   │
│             │ │ set_prometheus_multiproc_dir,\n                │     86      is_port_available,                  │   │
│             │ │ set_ulimit,', 'path':                          │     87      kill_process_tree,                  │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │     88      maybe_set_triton_cache_manager,     │   │
│             │ │ 'view_range': [80, 92]}                        │     89      prepare_model_and_tokenizer,        │   │
│             │ │                                                │     90      set_prometheus_multiproc_dir,       │   │
│             │ │                                                │     91      set_ulimit,                         │   │
│             │ │                                                │     92      MultiprocessingSerializer,          │   │
│             │ │                                                │     93  )                                       │   │
│             │ │                                                │     94  from sglang.utils import                │   │
│             │ │                                                │ get_exception_traceback                         │   │
│             │ │                                                │     95  from sglang.version import __version__  │   │
│             │ │                                                │     96                                          │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1758226 Output: 9822                                                                            │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 38: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'str_replace', 'file_text': '',    │ The file                                        │   │
│             │ │ 'insert_line': 1, 'new_str': '                 │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ """Update weights from tensor(s).\n\n          │ has been edited. Here's the result of running   │   │
│             │ │ This method preserves the public API by        │ `cat -n` on a snippet of                        │   │
│             │ │ accepting (name, tensor),\n        but         │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ internally serializes a dict to bytes for      │    875                                          │   │
│             │ │ efficient IPC.\n        """\n        payload = │ tokenizer_manager.update_weights_from_distribu… │   │
│             │ │ {name: tensor}\n        obj =                  │ None)                                           │   │
│             │ │ UpdateWeightsFromTensorReqInput(\n             │    876          )                               │   │
│             │ │ serialized_named_tensors=MultiprocessingSeria… │    877                                          │   │
│             │ │ )\n        loop = asyncio.get_event_loop()\n   │    878      def                                 │   │
│             │ │ return loop.run_until_complete(\n              │ update_weights_from_tensor(self, name, tensor): │   │
│             │ │ tokenizer_manager.update_weights_from_tensor(… │    879          """Update weights from          │   │
│             │ │ None)\n        )', 'old_str': '                │ tensor(s).                                      │   │
│             │ │ """Update weights from distributed             │    880                                          │   │
│             │ │ source."""\n        obj =                      │    881          This method preserves the       │   │
│             │ │ UpdateWeightsFromTensorReqInput(name=name,     │ public API by accepting (name, tensor),         │   │
│             │ │ tensor=tensor)\n        loop =                 │    882          but internally serializes a     │   │
│             │ │ asyncio.get_event_loop()\n        return       │ dict to bytes for efficient IPC.                │   │
│             │ │ loop.run_until_complete(\n                     │    883          """                             │   │
│             │ │ tokenizer_manager.update_weights_from_tensor(… │    884          payload = {name: tensor}        │   │
│             │ │ None)\n        )', 'path':                     │    885          obj =                           │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │ UpdateWeightsFromTensorReqInput(                │   │
│             │ │ 'view_range': [872, 884]}                      │    886                                          │   │
│             │ │                                                │ serialized_named_tensors=MultiprocessingSerial… │   │
│             │ │                                                │    887          )                               │   │
│             │ │                                                │    888          loop = asyncio.get_event_loop() │   │
│             │ │                                                │    889          return loop.run_until_complete( │   │
│             │ │                                                │    890                                          │   │
│             │ │                                                │ tokenizer_manager.update_weights_from_tensor(o… │   │
│             │ │                                                │ None)                                           │   │
│             │ │                                                │    891          )                               │   │
│             │ │                                                │    892                                          │   │
│             │ │                                                │    893      def get_weights_by_name(self, name, │   │
│             │ │                                                │ truncate_size=100):                             │   │
│             │ │                                                │    894          """Get weights by parameter     │   │
│             │ │                                                │ name."""                                        │   │
│             │ │                                                │    895          obj =                           │   │
│             │ │                                                │ GetWeightsByNameReqInput(name=name,             │   │
│             │ │                                                │ truncate_size=truncate_size)                    │   │
│             │ │                                                │ Review the changes and make sure they are as    │   │
│             │ │                                                │ expected. Edit the file again if necessary.     │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1836171 Output: 10089                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 39: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n "class                    │                                                 │   │
│             │ │ TpModelWorkerOverlapThread\\|run_scheduler_pr… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1914752 Output: 10374                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 40: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │                                                 │   │
│             │ │ "TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 1993673 Output: 10464                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 41: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │                                                 │   │
│             │ │ "TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | head -n 50', 'restart': False}               │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2072719 Output: 10550                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 42: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'bash -lc "grep -n                 │ tool has been restarted.                        │   │
│             │ │ \'TpModelWorkerOverlapThread\\|run_scheduler_… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | head -n 80"', 'restart': True}               │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2151886 Output: 10641                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 43: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "grep -n                           │                                                 │   │
│             │ │ 'TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | head -n 80", 'restart': False}               │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2231160 Output: 10727                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 44: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │                                                 │   │
│             │ │ "TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | head -n 200', 'restart': False}              │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2310540 Output: 10813                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 45: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │                                                 │   │
│             │ │ "TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2390026 Output: 10903                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 46: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n                           │                                                 │   │
│             │ │ "TpModelWorkerOverlapThread\\|run_scheduler_p… │                                                 │   │
│             │ │ -R                                             │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ | sed -n \'1,200p\'', 'restart': False}        │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2469622 Output: 10994                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 47: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'grep -n "run_scheduler_process"   │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ -R                                             │ sglang.srt.managers.scheduler import            │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ run_scheduler_process                           │   │
│             │ │ | head -n 50', 'restart': False}               │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ target=run_scheduler_process,                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ target=run_scheduler_process,                   │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ run_scheduler_process(                          │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ sglang.srt.managers.scheduler import            │   │
│             │ │                                                │ run_scheduler_process                           │   │
│             │ │                                                │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │                                                │ target=run_scheduler_process,                   │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2549328 Output: 11067                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 48: Completed                                                                                  │
│ Tools       │ 🔧 str_replace_based_edit_tool                                                                         │
│ str_replac… │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'view', 'file_text': '',           │ Here's the result of running `cat -n` on        │   │
│             │ │ 'insert_line': 1, 'new_str': '', 'old_str':    │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/… │   │
│             │ │ '', 'path':                                    │      1  # Copyright 2023-2024 SGLang Team       │   │
│             │ │ '/home/ubuntu/OmniPerf-Bench/perf-agents-benc… │      2  # Licensed under the Apache License,    │   │
│             │ │ 'view_range': [1, 90]}                         │ Version 2.0 (the "License");                    │   │
│             │ │                                                │      3  # you may not use this file except in   │   │
│             │ │                                                │ compliance with the License.                    │   │
│             │ │                                                │      4  # You may obtain a copy of the License  │   │
│             │ │                                                │ at                                              │   │
│             │ │                                                │      5  #                                       │   │
│             │ │                                                │      6  #                                       │   │
│             │ │                                                │ http://www.apache.org/licenses/LICENSE-2.0      │   │
│             │ │                                                │      7  #                                       │   │
│             │ │                                                │      8  # Unless required by applicable law or  │   │
│             │ │                                                │ agreed to in writing, software                  │   │
│             │ │                                                │      9  # distributed under the License is      │   │
│             │ │                                                │ distributed on an "AS IS" BASIS,                │   │
│             │ │                                                │     10  # WITHOUT WARRANTIES OR CONDITIONS OF   │   │
│             │ │                                                │ ANY KIND, either express or implied.            │   │
│             │ │                                                │     11  # See the License for the specific      │   │
│             │ │                                                │ language governing permissions and              │   │
│             │ │                                                │     12  # limitations under the License.        │   │
│             │ │                                                │     13  #                                       │   │
│             │ │                                                │ ==============================================… │   │
│             │ │                                                │     14  """A controller that dispatches         │   │
│             │ │                                                │ requests to multiple data parallel workers."""  │   │
│             │ │                                                │     15                                          │   │
│             │ │                                                │     16  import logging                          │   │
│             │ │                                                │     17  import multiprocessing as mp            │   │
│             │ │                                                │     18  import signal                           │   │
│             │ │                                                │     19  import threading                        │   │
│             │ │                                                │     20  from enum import Enum, auto             │   │
│             │ │                                                │     21                                          │   │
│             │ │                                                │     22  import psutil                           │   │
│             │ │                                                │     23  import zmq                              │   │
│             │ │                                                │     24                                          │   │
│             │ │                                                │     25  from sglang.srt.managers.io_struct      │   │
│             │ │                                                │ import (                                        │   │
│             │ │                                                │     26      TokenizedEmbeddingReqInput,         │   │
│             │ │                                                │     27      TokenizedGenerateReqInput,          │   │
│             │ │                                                │     28  )                                       │   │
│             │ │                                                │     29  from sglang.srt.managers.scheduler      │   │
│             │ │                                                │ import run_scheduler_process                    │   │
│             │ │                                                │     30  from sglang.srt.server_args import      │   │
│             │ │                                                │ PortArgs, ServerArgs                            │   │
│             │ │                                                │     31  from sglang.srt.utils import bind_port, │   │
│             │ │                                                │ configure_logger, get_zmq_socket                │   │
│             │ │                                                │     32  from sglang.utils import                │   │
│             │ │                                                │ get_exception_traceback                         │   │
│             │ │                                                │     33                                          │   │
│             │ │                                                │     34  logger = logging.getLogger(__name__)    │   │
│             │ │                                                │     35                                          │   │
│             │ │                                                │     36                                          │   │
│             │ │                                                │     37  class LoadBalanceMethod(Enum):          │   │
│             │ │                                                │     38      """Load balance method."""          │   │
│             │ │                                                │     39                                          │   │
│             │ │                                                │     40      ROUND_ROBIN = auto()                │   │
│             │ │                                                │     41      SHORTEST_QUEUE = auto()             │   │
│             │ │                                                │     42                                          │   │
│             │ │                                                │     43      @classmethod                        │   │
│             │ │                                                │     44      def from_str(cls, method: str):     │   │
│             │ │                                                │     45          method = method.upper()         │   │
│             │ │                                                │     46          try:                            │   │
│             │ │                                                │     47              return cls                  │   │
│             │ │                                                │     48          except KeyError as exc:         │   │
│             │ │                                                │     49              raise ValueError(f"Invalid  │   │
│             │ │                                                │ load balance method: {method}") from exc        │   │
│             │ │                                                │     50                                          │   │
│             │ │                                                │     51                                          │   │
│             │ │                                                │     52  class DataParallelController:           │   │
│             │ │                                                │     53      """A controller that dispatches     │   │
│             │ │                                                │ requests to multiple data parallel workers."""  │   │
│             │ │                                                │     54                                          │   │
│             │ │                                                │     55      def __init__(self, server_args,     │   │
│             │ │                                                │ port_args) -> None:                             │   │
│             │ │                                                │     56          # Parse args                    │   │
│             │ │                                                │     57          self.server_args = server_args  │   │
│             │ │                                                │     58          self.port_args = port_args      │   │
│             │ │                                                │     59          self.load_balance_method =      │   │
│             │ │                                                │ LoadBalanceMethod.from_str(                     │   │
│             │ │                                                │     60                                          │   │
│             │ │                                                │ server_args.load_balance_method                 │   │
│             │ │                                                │     61          )                               │   │
│             │ │                                                │     62                                          │   │
│             │ │                                                │     63          # Init inter-process            │   │
│             │ │                                                │ communication                                   │   │
│             │ │                                                │     64          self.context = zmq.Context(1 +  │   │
│             │ │                                                │ server_args.dp_size)                            │   │
│             │ │                                                │     65          self.recv_from_tokenizer =      │   │
│             │ │                                                │ get_zmq_socket(                                 │   │
│             │ │                                                │     66              self.context, zmq.PULL,     │   │
│             │ │                                                │ port_args.scheduler_input_ipc_name              │   │
│             │ │                                                │     67          )                               │   │
│             │ │                                                │     68                                          │   │
│             │ │                                                │     69          # Dispatch method               │   │
│             │ │                                                │     70          self.round_robin_counter = 0    │   │
│             │ │                                                │     71          dispatch_lookup = {             │   │
│             │ │                                                │     72                                          │   │
│             │ │                                                │ LoadBalanceMethod.ROUND_ROBIN:                  │   │
│             │ │                                                │ self.round_robin_scheduler,                     │   │
│             │ │                                                │     73                                          │   │
│             │ │                                                │ LoadBalanceMethod.SHORTEST_QUEUE:               │   │
│             │ │                                                │ self.shortest_queue_scheduler,                  │   │
│             │ │                                                │     74          }                               │   │
│             │ │                                                │     75          self.dispatching =              │   │
│             │ │                                                │ dispatch_lookup                                 │   │
│             │ │                                                │     76                                          │   │
│             │ │                                                │     77          # Start data parallel workers   │   │
│             │ │                                                │     78          base_gpu_id = 0                 │   │
│             │ │                                                │     79          self.workers = [None] *         │   │
│             │ │                                                │ server_args.dp_size                             │   │
│             │ │                                                │     80                                          │   │
│             │ │                                                │     81          threads = []                    │   │
│             │ │                                                │     82          sockets = []                    │   │
│             │ │                                                │     83          for dp_rank in                  │   │
│             │ │                                                │ range(server_args.dp_size):                     │   │
│             │ │                                                │     84              tmp_port_args =             │   │
│             │ │                                                │ PortArgs.init_new(server_args)                  │   │
│             │ │                                                │     85                                          │   │
│             │ │                                                │ tmp_port_args.tokenizer_ipc_name =              │   │
│             │ │                                                │ port_args.tokenizer_ipc_name                    │   │
│             │ │                                                │     86                                          │   │
│             │ │                                                │ tmp_port_args.detokenizer_ipc_name =            │   │
│             │ │                                                │ port_args.detokenizer_ipc_name                  │   │
│             │ │                                                │     87                                          │   │
│             │ │                                                │     88              if                          │   │
│             │ │                                                │ server_args.enable_dp_attention:                │   │
│             │ │                                                │     89                  # Data parallelism      │   │
│             │ │                                                │ resues the tensor parallelism group,            │   │
│             │ │                                                │     90                  # so all dp ranks       │   │
│             │ │                                                │ should use the same nccl port.                  │   │
│             │ │                                                │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2629458 Output: 11159                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 49: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'python                            │ Tensor total size: ~8.00 MB; repeats=3          │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ pickle    total: 0.0359s; per iter: 0.0120s     │   │
│             │ │ 'restart': False}                              │ torch.save total: 0.0130s; per iter: 0.0043s    │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2710732 Output: 11481                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 50: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': "git -C                            │  Optimize MoE align sum kernels performance     │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │  Committer: Ubuntu                              │   │
│             │ │ add -A && git -C                               │ <ubuntu@ip-172-31-16-122.ec2.internal>          │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ Your name and email address were configured     │   │
│             │ │ reset -q .bench_scratch || true && git -C      │ automatically based                             │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │ on your username and hostname. Please check     │   │
│             │ │ commit -m 'Optimize MoE align sum kernels      │ that they are accurate.                         │   │
│             │ │ performance'", 'restart': False}               │ You can suppress this message by setting them   │   │
│             │ │                                                │ explicitly. Run the                             │   │
│             │ │                                                │ following command and follow the instructions   │   │
│             │ │                                                │ in your editor to edit                          │   │
│             │ │                                                │ your configuration file:                        │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     git config --global --edit                  │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │ After doing this, you may fix the identity used │   │
│             │ │                                                │ for this commit with:                           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │     git commit --amend --reset-author           │   │
│             │ │                                                │                                                 │   │
│             │ │                                                │  5 files changed, 39 insertions(+), 8           │   │
│             │ │                                                │ deletions(-)                                    │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2792521 Output: 11772                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 51: Completed                                                                                  │
│ Tools       │ 🔧 bash                                                                                                │
│ bash        │ ┌────────────────────────────────────────────────┬─────────────────────────────────────────────────┐   │
│             │ │ {'command': 'bash -lc \'set -e; cd             │ done                                            │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ git diff $(git merge-base HEAD origin/HEAD ||  │                                                 │   │
│             │ │ git rev-parse HEAD~1) -- .                     │                                                 │   │
│             │ │ ":(exclude).bench_scratch" >                   │                                                 │   │
│             │ │ /home/ubuntu/OmniPerf-Bench/perf-agents-bench… │                                                 │   │
│             │ │ || true; echo done\'', 'restart': False}       │                                                 │   │
│             │ └────────────────────────────────────────────────┴─────────────────────────────────────────────────┘   │
│ Total       │ Input: 2874768 Output: 11913                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 52: Completed                                                                                  │
│ Total       │ Input: 2957168 Output: 12183                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘
OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 22 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 13 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 21 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 24 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 8 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 22 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 18 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 11 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 30 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

OpenAI API call failed: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}. Will sleep for 6 seconds and will retry.
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/retry_utils.py", line 36, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/llm_clients/openai_client.py", line 86, in _create_openai_response
    response = self.client.responses.create(**api_params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/resources/responses/responses.py", line 795, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/bench-env/lib/python3.12/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': 'No tool output found for function call call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}

┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ Status      │ ✅ Step 53: Completed                                                                                  │
│ Error       │ ❌ Error code: 400 - {'error': {'message': 'No tool output found for function call                     │
│             │ call_iONOI9nIC4NkmntFEdPOCZHR.', 'type': 'invalid_request_error', 'param': 'input', 'code': None}}     │
│ Total       │ Input: 2957168 Output: 12183                                                                           │
│ Tokens      │                                                                                                        │
└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘

============================================================
Lakeview Summary
============================================================
╭───────────────────────────── Step 1 (Lakeview) ──────────────────────────────╮
│ [👁️EXAMINE_CODE] The agent <task>Inspecting repository layout and preparing   │
│ performance benchmark setup.                                                 │
│ Running ls on the sglang_core worktree and planning a .bench_scratch timing  │
│ script to benchmark before/after changes for targeted optimization files.    │
╰──────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────── Step 2 (Lakeview) ──────────────────────────────╮
│ [👁️EXAMINE_CODE] The agent <task>The agent is examining source code.          │
│ Opening sglang_core's python/sglang/srt/managers/io_struct.py (full file)    │
│ with the edit tool to inspect implementation details potentially related to  │
│ the issue.                                                                   │
╰──────────────────────────────────────────────────────────────────────────────╯
╭───────────────────────────── Step 3 (Lakeview) ──────────────────────────────╮
│ [👁️EXAMINE_CODE] The agent <task>The agent is examining source code.          │
│ Opening sglang_core's python/sglang/srt/managers/tp_worker.py to inspect its │
│ implementation using the edit tool, viewing the full file for context        │
│ related to the issue.                                                        │
╰──────────────────────────────────────────────────────────────────────────────╯

Unexpected error: list index out of range
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/cli.py", line 212, in run
    _ = asyncio.run(agent.run(task, task_args))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/agent/agent.py", line 93, in run
    await cli_console_task
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 119, in start
    await self._print_lakeview_summary()
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 154, in _print_lakeview_summary
    lake_view_panel = await step.lake_view_panel_generator
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 306, in _create_lakeview_step_display
    lake_view_step = await self.lake_view.create_lakeview_step(agent_step)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line 205, in create_lakeview_step
    tags = await self.extract_tag_in_step(this_step_str)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line 169, in extract_tag_in_step
    tags: list = [tag.strip() for tag in matched_tags[0].split(",")]
                                              ~~~~~~~~~~~~^^^
IndexError: list index out of range

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_041_9183c23e/trajectory.json