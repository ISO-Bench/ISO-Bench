diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py
index 2f974ea9a..6cd212120 100644
--- a/python/sglang/srt/layers/attention/flashattention_backend.py
+++ b/python/sglang/srt/layers/attention/flashattention_backend.py
@@ -1131,20 +1131,20 @@ class FlashAttentionBackend(AttentionBackend):
         """
         # This is being used by normal decode and draft decode when topk == 1
         self.decode_cuda_graph_metadata = {
-            "cache_seqlens": torch.zeros(max_bs, dtype=torch.int32, device=self.device),
+            "cache_seqlens": torch.empty(max_bs, dtype=torch.int32, device=self.device),
             "cu_seqlens_q": torch.arange(
                 0, max_bs + 1, dtype=torch.int32, device=self.device
             ),
             "cu_seqlens_k": torch.zeros(
                 max_bs + 1, dtype=torch.int32, device=self.device
             ),
-            "page_table": torch.zeros(
+            "page_table": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "page_table_draft_decode": torch.zeros(
+            "page_table_draft_decode": torch.empty(
                 max_bs,
                 (self.max_context_len + self.page_size - 1) // self.page_size,
                 dtype=torch.int32,
@@ -1168,13 +1168,13 @@ class FlashAttentionBackend(AttentionBackend):
             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size
 
             self.decode_cuda_graph_local_attn_metadata = {
-                "local_query_start_loc": torch.zeros(
+                "local_query_start_loc": torch.empty(
                     max_virtual_batches + 1, dtype=torch.int32, device=self.device
                 ),
-                "local_seqused_k": torch.zeros(
+                "local_seqused_k": torch.empty(
                     max_virtual_batches, dtype=torch.int32, device=self.device
                 ),
-                "local_block_table": torch.zeros(
+                "local_block_table": torch.empty(
                     max_virtual_batches,
                     max_pages_per_block,
                     dtype=torch.int32,
@@ -1185,7 +1185,7 @@ class FlashAttentionBackend(AttentionBackend):
         # This is used by draft decode's first half of metadata when topk > 1
         if self.topk > 1:
             self.draft_decode_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1198,7 +1198,7 @@ class FlashAttentionBackend(AttentionBackend):
                 "cu_seqlens_k": torch.zeros(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1228,7 +1228,7 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs * self.topk,
                     decode_length,
                     dtype=torch.int32,
@@ -1241,7 +1241,7 @@ class FlashAttentionBackend(AttentionBackend):
             and self.speculative_num_draft_tokens > 0
         ):
             self.target_verify_metadata = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1254,7 +1254,7 @@ class FlashAttentionBackend(AttentionBackend):
                 "cu_seqlens_k": torch.zeros(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     (self.max_context_len + self.page_size - 1) // self.page_size,
                     dtype=torch.int32,
@@ -1267,7 +1267,7 @@ class FlashAttentionBackend(AttentionBackend):
 
         if self.topk > 1:
             self.target_verify_metadata_topk_normal = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs, dtype=torch.int32, device=self.device
                 ),
                 "cu_seqlens_q": torch.arange(
@@ -1280,7 +1280,7 @@ class FlashAttentionBackend(AttentionBackend):
                 "cu_seqlens_k": torch.zeros(
                     max_bs + 1, dtype=torch.int32, device=self.device
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs,
                     self.max_context_len,
                     dtype=torch.int32,
@@ -1289,7 +1289,7 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
             self.target_verify_metadata_topk_expand = {
-                "cache_seqlens": torch.zeros(
+                "cache_seqlens": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     dtype=torch.int32,
                     device=self.device,
@@ -1305,7 +1305,7 @@ class FlashAttentionBackend(AttentionBackend):
                     dtype=torch.int32,
                     device=self.device,
                 ),
-                "page_table": torch.zeros(
+                "page_table": torch.empty(
                     max_bs * self.speculative_num_draft_tokens,
                     self.speculative_num_draft_tokens,
                     dtype=torch.int32,
@@ -1314,13 +1314,13 @@ class FlashAttentionBackend(AttentionBackend):
             }
 
         self.encoder_metadata = {
-            "encoder_page_table": torch.zeros(
+            "encoder_page_table": torch.empty(
                 max_bs,
                 self.max_context_len,
                 dtype=torch.int32,
                 device=self.device,
             ),
-            "encoder_lens_int32": torch.zeros(
+            "encoder_lens_int32": torch.empty(
                 max_bs, dtype=torch.int32, device=self.device
             ),
             "encoder_cu_seqlens_k": torch.zeros(
@@ -1639,7 +1639,7 @@ class FlashAttentionBackend(AttentionBackend):
                 ]
                 page_indices //= self.page_size
                 metadata.page_table[:, :max_seq_pages].copy_(page_indices)
-                metadata.page_table[:, max_seq_pages:].fill_(0)
+
 
                 self._update_local_attn_metadata_for_replay(metadata, bs)
         elif forward_mode.is_target_verify():
