{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-05b3bf5", "model_patch": "diff --git a/python/sglang/srt/hf_transformers_utils.py b/python/sglang/srt/hf_transformers_utils.py\nindex 56d7c8a1f..e989c1466 100644\n--- a/python/sglang/srt/hf_transformers_utils.py\n+++ b/python/sglang/srt/hf_transformers_utils.py\n@@ -164,7 +164,7 @@ def get_tokenizer(\n             \"slowdown. Consider using a fast tokenizer instead.\"\n         )\n \n-    handle_additional_stop_token_ids(tokenizer)\n+    attach_additional_stop_token_ids(tokenizer)\n     return tokenizer\n \n \n@@ -184,11 +184,14 @@ def get_processor(\n         **kwargs,\n     )\n \n-    handle_additional_stop_token_ids(processor.tokenizer)\n+    attach_additional_stop_token_ids(processor.tokenizer)\n     return processor\n \n \n-def handle_additional_stop_token_ids(tokenizer):\n+def attach_additional_stop_token_ids(tokenizer):\n+    # Avoid recomputing if already attached\n+    if hasattr(tokenizer, \"additional_stop_token_ids\"):\n+        return\n     # Special handling for stop token <|eom_id|> generated by llama 3 tool use.\n     if \"<|eom_id|>\" in tokenizer.get_added_vocab():\n         tokenizer.additional_stop_token_ids = set(\ndiff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex 9ae5801cc..a3f1ca66d 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -80,7 +80,7 @@ class Sampler(nn.Module):\n \n                 if not torch.all(success):\n                     logger.warning(\"Detected errors during sampling!\")\n-                    batch_next_token_ids = torch.zeros_like(batch_next_token_ids)\n+                    batch_next_token_ids.zero_()\n             elif global_server_args_dict[\"sampling_backend\"] == \"pytorch\":\n                 # A slower fallback implementation with torch native operations.\n                 batch_next_token_ids = top_k_top_p_min_p_sampling_from_probs_torch(\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fac008d3f..e11fa03df 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -339,6 +339,12 @@ class Req:\n         if self.tokenizer is not None:\n             matched_eos |= last_token_id == self.tokenizer.eos_token_id\n \n+\n+            # Also respect additional stop token ids if attached to the tokenizer\n+            extra_ids = getattr(self.tokenizer, \"additional_stop_token_ids\", None)\n+            if extra_ids:\n+                matched_eos |= last_token_id in extra_ids\n+\n         if matched_eos and not self.sampling_params.ignore_eos:\n             self.finished_reason = FINISH_MATCHED_TOKEN(matched=last_token_id)\n             return\ndiff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\nindex c9e0f078e..a11a4f0b0 100644\n--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n@@ -34,6 +34,7 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n                     data=list(\n                         req.sampling_params.stop_token_ids\n                         | {req.tokenizer.eos_token_id}\n+                        | (getattr(req.tokenizer, \"additional_stop_token_ids\", None) or set())\n                     ),\n                     dtype=torch.int64,\n                     device=self.orchestrator.device,\ndiff --git a/python/sglang/srt/sampling/sampling_params.py b/python/sglang/srt/sampling/sampling_params.py\nindex b0863b557..3edce468c 100644\n--- a/python/sglang/srt/sampling/sampling_params.py\n+++ b/python/sglang/srt/sampling/sampling_params.py\n@@ -135,8 +135,11 @@ class SamplingParams:\n             self.stop_str_max_len = stop_str_max_len\n \n         # Process stop token ids\n-        if tokenizer and tokenizer.additional_stop_token_ids:\n-            self.stop_token_ids.update(tokenizer.additional_stop_token_ids)\n+        extra_ids = getattr(tokenizer, \"additional_stop_token_ids\", None)\n+        if extra_ids:\n+            # Additional stop token ids are checked dynamically to avoid mutating state here.\n+            # Keep self.stop_token_ids untouched for user-specified stops.\n+            pass\n \n     def to_srt_kwargs(self):\n         return {\n", "model_name_or_path": "gpt-5-2025-08-07"}
