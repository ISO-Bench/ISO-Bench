Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419
Initialising MCP tools...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Task: I've uploaded a python code repository in the directory                                                                                  â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419.                                                 â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                 â”‚
â”‚                                                                                                                                                â”‚
â”‚ <test_script>                                                                                                                                  â”‚
â”‚ # This is a performance optimization task                                                                                                      â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                            â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                    â”‚
â”‚                                                                                                                                                â”‚
â”‚ </test_script>                                                                                                                                 â”‚
â”‚                                                                                                                                                â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                       â”‚
â”‚                                                                                                                                                â”‚
â”‚ Basic guidelines:                                                                                                                              â”‚
â”‚ 1. Your task is to make changes to non-test files in the                                                                                       â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419 directory to improve the performance of the      â”‚
â”‚ <test_script>.                                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                      â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.      â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with     â”‚
â”‚ running them.                                                                                                                                  â”‚
â”‚                                                                                                                                                â”‚
â”‚ Follow these steps to improve performance:                                                                                                     â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                          â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch    â”‚
â”‚ (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch/test_opt.py) to reproduce  â”‚
â”‚ and time the example, then execute it with python <filename.py> from the repo root.                                                            â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                              â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                     â”‚
â”‚                                                                                                                                                â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                  â”‚
â”‚                                                                                                                                                â”‚
â”‚ <example_optimization_diff>                                                                                                                    â”‚
â”‚ diff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py                                       â”‚
â”‚ new file mode 100644                                                                                                                           â”‚
â”‚ index 000000000..15ef0ab6a                                                                                                                     â”‚
â”‚ --- /dev/null                                                                                                                                  â”‚
â”‚ +++ b/benchmark/benchmark_batch/benchmark_batch.py                                                                                             â”‚
â”‚ @@ -0,0 +1,193 @@                                                                                                                              â”‚
â”‚ +import concurrent.futures                                                                                                                     â”‚
â”‚ +import os                                                                                                                                     â”‚
â”‚ +import random                                                                                                                                 â”‚
â”‚ +import time                                                                                                                                   â”‚
â”‚ +from concurrent.futures import ProcessPoolExecutor                                                                                            â”‚
â”‚ +from statistics import mean                                                                                                                   â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +import requests                                                                                                                               â”‚
â”‚ +from tqdm import tqdm                                                                                                                         â”‚
â”‚ +from transformers import AutoTokenizer                                                                                                        â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +from sglang.lang.backend.runtime_endpoint import RuntimeEndpoint                                                                              â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +###############################################################################                                                               â”‚
â”‚ +# CONFIG                                                                                                                                      â”‚
â”‚ +###############################################################################                                                               â”‚
â”‚ +ENDPOINT_URL = "http://127.0.0.1:30000"                                                                                                       â”‚
â”‚ +TOKENIZER_DIR = "/models/meta-llama/Llama-3.2-3B"                                                                                             â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +# Benchmark configurations                                                                                                                    â”‚
â”‚ +NUM_REQUESTS = 10  # Total number of requests (each with BATCH_SIZE prompts)                                                                  â”‚
â”‚ +NUM_TOKENS = 32000  # Tokens per prompt                                                                                                       â”‚
â”‚ +BATCH_SIZE = 8  # Number of prompts per request                                                                                               â”‚
â”‚ +GEN_TOKENS = 0  # Tokens to generate per prompt                                                                                               â”‚
â”‚ </example_optimization_diff>                                                                                                                   â”‚
â”‚                                                                                                                                                â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                          â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                      â”‚
â”‚ Your task is to:                                                                                                                               â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                 â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                       â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                             â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                    â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                      â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                             â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                 â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                               â”‚
â”‚                                                                                                                                                â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                            â”‚
â”‚ ```bash                                                                                                                                        â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                           â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                         â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                   â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                        â”‚
â”‚                                                                                                                                                â”‚
â”‚ Target files to optimize:                                                                                                                      â”‚
â”‚ - benchmark/benchmark_batch/benchmark_batch.py                                                                                                 â”‚
â”‚ - benchmark/benchmark_batch/benchmark_tokenizer.py                                                                                             â”‚
â”‚ - python/sglang/srt/managers/tokenizer_manager.py                                                                                              â”‚
â”‚                                                                                                                                                â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                             â”‚
â”‚ The task will fail if no files are modified.                                                                                                   â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## Constraints                                                                                                                                 â”‚
â”‚ - No public API breakage                                                                                                                       â”‚
â”‚ - All TestPack checks must pass                                                                                                                â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                            â”‚
â”‚ - `benchmark/benchmark_batch/benchmark_batch.py`                                                                                               â”‚
â”‚ - `benchmark/benchmark_batch/benchmark_tokenizer.py`                                                                                           â”‚
â”‚ - `python/sglang/srt/managers/tokenizer_manager.py`                                                                                            â”‚
â”‚ - `python/sglang/srt/server_args.py`                                                                                                           â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                              â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                      â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                      â”‚
â”‚ - Tensor initialization strategies                                                                                                             â”‚
â”‚ - Kernel parameter optimization                                                                                                                â”‚
â”‚ - Buffer reuse and caching                                                                                                                     â”‚
â”‚                                                                                                                                                â”‚
â”‚ ### Human Developer's Approach:                                                                                                                â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚ Perform Batch Tokenization. (#5141)                                                                                                            â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ ### Files Modified (statistics):                                                                                                               â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚ benchmark/benchmark_batch/benchmark_batch.py     | 193 +++++++++++++++++++++++                                                                 â”‚
â”‚  benchmark/benchmark_batch/benchmark_tokenizer.py | 126 +++++++++++++++                                                                        â”‚
â”‚  python/sglang/srt/managers/tokenizer_manager.py  | 129 ++++++++++++---                                                                        â”‚
â”‚  python/sglang/srt/server_args.py                 |   6 +                                                                                      â”‚
â”‚  4 files changed, 429 insertions(+), 25 deletions(-)                                                                                           â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                              â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                          â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch/test_opt.py     â”‚
â”‚ before and after edits (do not create timing scripts outside                                                                                   â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch)                                  â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                      â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                               â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                    â”‚
â”‚ When you have made optimizations:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                        â”‚
â”‚ git add -A                                                                                                                                     â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                            â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                     â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                          â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/model_patch.diff || true                         â”‚
â”‚ finish                                                                                                                                         â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                              â”‚
â”‚                                                                                                                                                â”‚
â”‚ Model Provider: openai                                                                                                                         â”‚
â”‚ Model: gpt-5-2025-08-07                                                                                                                        â”‚
â”‚ Max Steps: 120                                                                                                                                 â”‚
â”‚ Trajectory File: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_077_f0815419/trajectory.json             â”‚
â”‚ Tools: bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                        â”‚
â”‚ Project_path: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419                                    â”‚
â”‚ Issue: I've uploaded a python code repository in the directory                                                                                 â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419.                                                 â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                 â”‚
â”‚                                                                                                                                                â”‚
â”‚ <test_script>                                                                                                                                  â”‚
â”‚ # This is a performance optimization task                                                                                                      â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                            â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                    â”‚
â”‚                                                                                                                                                â”‚
â”‚ </test_script>                                                                                                                                 â”‚
â”‚                                                                                                                                                â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                       â”‚
â”‚                                                                                                                                                â”‚
â”‚ Basic guidelines:                                                                                                                              â”‚
â”‚ 1. Your task is to make changes to non-test files in the                                                                                       â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419 directory to improve the performance of the      â”‚
â”‚ <test_script>.                                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                      â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.      â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with     â”‚
â”‚ running them.                                                                                                                                  â”‚
â”‚                                                                                                                                                â”‚
â”‚ Follow these steps to improve performance:                                                                                                     â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                          â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch    â”‚
â”‚ (e.g., /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch/test_opt.py) to reproduce  â”‚
â”‚ and time the example, then execute it with python <filename.py> from the repo root.                                                            â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                              â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                     â”‚
â”‚                                                                                                                                                â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                  â”‚
â”‚                                                                                                                                                â”‚
â”‚ <example_optimization_diff>                                                                                                                    â”‚
â”‚ diff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py                                       â”‚
â”‚ new file mode 100644                                                                                                                           â”‚
â”‚ index 000000000..15ef0ab6a                                                                                                                     â”‚
â”‚ --- /dev/null                                                                                                                                  â”‚
â”‚ +++ b/benchmark/benchmark_batch/benchmark_batch.py                                                                                             â”‚
â”‚ @@ -0,0 +1,193 @@                                                                                                                              â”‚
â”‚ +import concurrent.futures                                                                                                                     â”‚
â”‚ +import os                                                                                                                                     â”‚
â”‚ +import random                                                                                                                                 â”‚
â”‚ +import time                                                                                                                                   â”‚
â”‚ +from concurrent.futures import ProcessPoolExecutor                                                                                            â”‚
â”‚ +from statistics import mean                                                                                                                   â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +import requests                                                                                                                               â”‚
â”‚ +from tqdm import tqdm                                                                                                                         â”‚
â”‚ +from transformers import AutoTokenizer                                                                                                        â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +from sglang.lang.backend.runtime_endpoint import RuntimeEndpoint                                                                              â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +###############################################################################                                                               â”‚
â”‚ +# CONFIG                                                                                                                                      â”‚
â”‚ +###############################################################################                                                               â”‚
â”‚ +ENDPOINT_URL = "http://127.0.0.1:30000"                                                                                                       â”‚
â”‚ +TOKENIZER_DIR = "/models/meta-llama/Llama-3.2-3B"                                                                                             â”‚
â”‚ +                                                                                                                                              â”‚
â”‚ +# Benchmark configurations                                                                                                                    â”‚
â”‚ +NUM_REQUESTS = 10  # Total number of requests (each with BATCH_SIZE prompts)                                                                  â”‚
â”‚ +NUM_TOKENS = 32000  # Tokens per prompt                                                                                                       â”‚
â”‚ +BATCH_SIZE = 8  # Number of prompts per request                                                                                               â”‚
â”‚ +GEN_TOKENS = 0  # Tokens to generate per prompt                                                                                               â”‚
â”‚ </example_optimization_diff>                                                                                                                   â”‚
â”‚                                                                                                                                                â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                          â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                      â”‚
â”‚ Your task is to:                                                                                                                               â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                 â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                       â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                             â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                    â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                      â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                             â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                 â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                               â”‚
â”‚                                                                                                                                                â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                            â”‚
â”‚ ```bash                                                                                                                                        â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                           â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                         â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                   â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                        â”‚
â”‚                                                                                                                                                â”‚
â”‚ Target files to optimize:                                                                                                                      â”‚
â”‚ - benchmark/benchmark_batch/benchmark_batch.py                                                                                                 â”‚
â”‚ - benchmark/benchmark_batch/benchmark_tokenizer.py                                                                                             â”‚
â”‚ - python/sglang/srt/managers/tokenizer_manager.py                                                                                              â”‚
â”‚                                                                                                                                                â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                             â”‚
â”‚ The task will fail if no files are modified.                                                                                                   â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## Constraints                                                                                                                                 â”‚
â”‚ - No public API breakage                                                                                                                       â”‚
â”‚ - All TestPack checks must pass                                                                                                                â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                            â”‚
â”‚ - `benchmark/benchmark_batch/benchmark_batch.py`                                                                                               â”‚
â”‚ - `benchmark/benchmark_batch/benchmark_tokenizer.py`                                                                                           â”‚
â”‚ - `python/sglang/srt/managers/tokenizer_manager.py`                                                                                            â”‚
â”‚ - `python/sglang/srt/server_args.py`                                                                                                           â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                              â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                      â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                      â”‚
â”‚ - Tensor initialization strategies                                                                                                             â”‚
â”‚ - Kernel parameter optimization                                                                                                                â”‚
â”‚ - Buffer reuse and caching                                                                                                                     â”‚
â”‚                                                                                                                                                â”‚
â”‚ ### Human Developer's Approach:                                                                                                                â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚ Perform Batch Tokenization. (#5141)                                                                                                            â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ ### Files Modified (statistics):                                                                                                               â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚ benchmark/benchmark_batch/benchmark_batch.py     | 193 +++++++++++++++++++++++                                                                 â”‚
â”‚  benchmark/benchmark_batch/benchmark_tokenizer.py | 126 +++++++++++++++                                                                        â”‚
â”‚  python/sglang/srt/managers/tokenizer_manager.py  | 129 ++++++++++++---                                                                        â”‚
â”‚  python/sglang/srt/server_args.py                 |   6 +                                                                                      â”‚
â”‚  4 files changed, 429 insertions(+), 25 deletions(-)                                                                                           â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                              â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                          â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch/test_opt.py     â”‚
â”‚ before and after edits (do not create timing scripts outside                                                                                   â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/.bench_scratch)                                  â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                      â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                               â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                          â”‚
â”‚                                                                                                                                                â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                    â”‚
â”‚ When you have made optimizations:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                        â”‚
â”‚ git add -A                                                                                                                                     â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                            â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                     â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                          â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_077_f0815419/model_patch.diff || true                         â”‚
â”‚ finish                                                                                                                                         â”‚
â”‚ ```                                                                                                                                            â”‚
â”‚                                                                                                                                                â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                              â”‚
â”‚                                                                                                                                                â”‚
â”‚ Must_patch: true                                                                                                                               â”‚
â”‚ Patch_path: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_077_f0815419/model_patch.diff                 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 1: Completed                                                                                   â”‚
â”‚ LLM         â”‚ ğŸ’¬ Plan:                                                                                               â”‚
â”‚ Response    â”‚ - Inspect repo structure and the target files.                                                         â”‚
â”‚             â”‚ - Run the suggested grep to check for torch.zeros/fill_ patterns.                                      â”‚
â”‚             â”‚ - Create a timing script in .bench_scratch to measure tokenization throughput.                         â”‚
â”‚             â”‚ - Implement optimizations: add batched tokenization methods and small caches; add benchmark scripts;   â”‚
â”‚             â”‚ tweak server args.                                                                                     â”‚
â”‚             â”‚ - Re-run timing to confirm improvement.                                                                â”‚
â”‚             â”‚ - Commit changes and produce diff.                                                                     â”‚
â”‚             â”‚                                                                                                        â”‚
â”‚             â”‚ I will start by exploring the repository and running the grep to identify potential patterns.          â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 108                                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 15 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,200p'", 'restart': False}          â”‚ .                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 82 ubuntu ubuntu  4096 Nov 15 08:02  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    35 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .clang-format-ignore                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .devcontainer                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   341 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .editorconfig                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu   113 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .git                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .github                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  3650 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitignore                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu     0 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .gitmodules                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu    50 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .isort.cfg                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1369 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ .pre-commit-config.yaml                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3rdparty                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu 11346 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ LICENSE                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  1720 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Makefile                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -rw-rw-r--  1 ubuntu ubuntu  6545 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ README.md                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ assets                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 31 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmark                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docker                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x 10 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ docs                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ examples                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ python                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scripts                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  8 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-kernel                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  5 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sgl-router                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  4 ubuntu ubuntu  4096 Nov 15 07:55  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ test                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 8076 Output: 1132                                                                               â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 2: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "cd                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ && grep -n 'torch.zeros\\|fill_'               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ vllm/model_executor/layers/fused_moe/moe_aligâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ benchmarks/kernels/benchmark_moe_align_block_â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ || true", 'restart': False}                    â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 17963 Output: 1230                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 3: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  """TokenizerManager is a process that   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizes the text."""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  import asyncio                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  import copy                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  import dataclasses                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  import pickle                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  import signal                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  import sys                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24  import threading                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  import time                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  import uuid                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  from collections import deque           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28  from datetime import datetime           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29  from http import HTTPStatus             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30  from typing import (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31      Any,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32      Awaitable,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33      Deque,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34      Dict,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35      Generic,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36      List,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      Optional,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      Tuple,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      TypeVar,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40      Union,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43  import fastapi                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44  import uvloop                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45  import zmq                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46  import zmq.asyncio                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47  from fastapi import BackgroundTasks     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49  from sglang.srt.aio_rwlock import       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RWLock                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50  from sglang.srt.configs.model_config    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelConfig                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51  from sglang.srt.disaggregation.utils    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52      DisaggregationMode,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53      KVClassType,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54      TransferBackend,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      get_kv_class,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57  from sglang.srt.hf_transformers_utils   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import get_processor, get_tokenizer             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58  from sglang.srt.managers.io_struct      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      AbortReq,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60      BatchEmbeddingOut,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61      BatchMultimodalOut,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62      BatchStrOut,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63      BatchTokenIDOut,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64      CloseSessionReqInput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65      ConfigureLoggingReq,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66      EmbeddingReqInput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67      ExpertDistributionReq,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      ExpertDistributionReqOutput,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69      FlushCacheReq,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70      GenerateReqInput,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71      GetInternalStateReq,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72      GetInternalStateReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73      GetWeightsByNameReqInput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74      GetWeightsByNameReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75      HealthCheckOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76      InitWeightsUpdateGroupReqInput,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      InitWeightsUpdateGroupReqOutput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78      OpenSessionReqInput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79      OpenSessionReqOutput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80      ProfileReq,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81      ProfileReqOutput,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82      ProfileReqType,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83      ReleaseMemoryOccupationReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84      ReleaseMemoryOccupationReqOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85      ResumeMemoryOccupationReqInput,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86      ResumeMemoryOccupationReqOutput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87      SessionParams,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      TokenizedEmbeddingReqInput,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      TokenizedGenerateReqInput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90      UpdateWeightFromDiskReqInput,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      UpdateWeightFromDiskReqOutput,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqInput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      UpdateWeightsFromTensorReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      UpdateWeightsFromTensorReqOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.managers.multimodal_processor import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98      get_dummy_processor,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      get_mm_processor,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      import_processors,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102  from sglang.srt.metrics.collector       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import TokenizerMetricsCollector                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103  from                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.sampling.sampling_params import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SamplingParams                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104  from sglang.srt.server_args import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PortArgs, ServerArgs                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105  from sglang.srt.utils import (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106      dataclass_to_string_truncated,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107      get_zmq_socket,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108      kill_process_tree,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110  from sglang.utils import                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TypeBasedDispatcher, get_exception_traceback    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.set_event_loop_policy(uvloop.EventLoopâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117  @dataclasses.dataclass                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118  class ReqState:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119      """Store the state a request."""    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      out_list: List                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122      finished: bool                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      event: asyncio.Event                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      obj: Any                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      # For metrics                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127      created_time: float                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      finished_time: float = 0.0          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      first_token_time: float = 0.0       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130      last_time: float = 0.0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      last_completion_tokens: int = 1     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133      # For streaming output              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134      last_output_offset: int = 0         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137  class TokenizerManager:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138      """TokenizerManager is a process    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ that tokenizes the text."""                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140      def __init__(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142          server_args: ServerArgs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143          port_args: PortArgs,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145          # Parse args                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146          self.server_args = server_args  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147          self.enable_metrics =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_metrics                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148          self.log_requests =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_requests                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149          self.log_requests_level =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_requests_level                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151          # Init inter-process            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ communication                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152          context =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ zmq.asyncio.Context(2)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153          self.recv_from_detokenizer =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154              context, zmq.PULL,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.tokenizer_ipc_name, True              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156          self.send_to_scheduler =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157              context, zmq.PUSH,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.scheduler_input_ipc_name, True        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160          # Read model args               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161          self.model_path =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.model_path                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162          self.served_model_name =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.served_model_name                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163          self.model_config =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelConfig(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164              server_args.model_path,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ trust_remote_code=server_args.trust_remote_codâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ revision=server_args.revision,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context_length=server_args.context_length,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_override_args=server_args.json_model_oveâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_embedding=server_args.is_embedding,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_multimodal=server_args.enable_multimodaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171              dtype=server_args.dtype,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ quantization=server_args.quantization,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175          self.is_generation =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_generation                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176          self.is_image_gen =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_image_gen                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177          self.context_len =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.context_len                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178          self.image_token_id =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.image_token_id                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_multimodal:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181              import_processors()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182              _processor = get_processor( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.tokenizer_path,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_mode=server_args.tokenizer_mode,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ trust_remote_code=server_args.trust_remote_codâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ revision=server_args.revision,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187                  use_fast=not            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.disable_fast_image_processor,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190              # We want to parallelize    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the image pre-processing so we create an        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ executor for it                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191              # We create mm_processor    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for any skip_tokenizer_init to make sure we     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ still encode                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192              # images even with          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ skip_tokenizer_init=False.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193              self.mm_processor =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_mm_processor(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.hf_config, server_args,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _processor                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.skip_tokenizer_init:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198                  self.tokenizer =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.processor = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200                  self.processor =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _processor                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201                  self.tokenizer =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.processor.tokenizer                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ os.environ["TOKENIZERS_PARALLELISM"] = "false"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204              self.mm_processor =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_dummy_processor()                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.skip_tokenizer_init:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207                  self.tokenizer =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.processor = None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209                  self.tokenizer =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_tokenizer(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.tokenizer_path,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_mode=server_args.tokenizer_mode,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ trust_remote_code=server_args.trust_remote_codâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ revision=server_args.revision,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216          # Store states                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217          self.no_create_loop = False     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218          self.rid_to_state: Dict = {}    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219          self.gracefully_exit = False    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220          self.last_receive_tstamp = 0    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221          self.dump_requests_folder = ""  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # By default do not dump                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222          self.dump_requests_threshold =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1000                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223          self.dump_request_list:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ List[Tuple] = []                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224          self.log_request_metadata =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_log_request_metadata()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226          # The event to notify the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weight sync is finished.                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227          self.model_update_lock =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ RWLock()                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228          self.model_update_result:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Awaitable[UpdateWeightFromDiskReqOutpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229              None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231          self.asyncio_tasks = set()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233          # For session info              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234          self.session_futures = {}  #    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ session_id -> asyncio event                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236          # Set after scheduler is        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ initialized                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237          self.max_req_input_len = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239          # Metrics                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240          if self.enable_metrics:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241              self.metrics_collector =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerMetricsCollector(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242                  labels={                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243                      "model_name":       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.served_model_name,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244                      # TODO: Add lora    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ name/path in the future,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245                  },                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248          # Communicators                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.init_weights_update_group_communicator =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_distributed_communicaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_tensor_communicator =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_weights_by_name_communicator =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.release_memory_occupation_communicator =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.resume_memory_occupation_communicator =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267          self.start_profile_communicator â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_internal_state_communicator =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.expert_distribution_communicator =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274              self.send_to_scheduler,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277          self._result_dispatcher =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TypeBasedDispatcher(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278              [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280                      (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                          BatchStrOut,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchEmbeddingOut,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchTokenIDOut,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchMultimodalOut,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285                      ),                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_batch_output,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288                  (OpenSessionReqOutput,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_open_session_req_output),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightFromDiskReqOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_update_weights_from_disk_req_outpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ InitWeightsUpdateGroupReqOutput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.init_weights_update_group_communicator.haâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_distributed_communicaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromTensorReqOutput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_tensor_communicator.hâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GetWeightsByNameReqOutput,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_weights_by_name_communicator.handle_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReleaseMemoryOccupationReqOutput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.release_memory_occupation_communicator.haâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ResumeMemoryOccupationReqOutput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.resume_memory_occupation_communicator.hanâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318                      ProfileReqOutput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.start_profile_communicator.handle_recv,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GetInternalStateReqOutput,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_internal_state_communicator.handle_reâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ExpertDistributionReqOutput,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.expert_distribution_communicator.handle_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328                  ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                  (HealthCheckOutput,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lambda x: None),                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330              ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333          self.disaggregation_mode =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DisaggregationMode(                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disaggregation_mode            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336          self.transfer_backend =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TransferBackend(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disaggregation_transfer_backeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339          # for disaggregtion, start kv   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ boostrap server on prefill                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340          if self.disaggregation_mode ==  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DisaggregationMode.PREFILL:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341              # only start bootstrap      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server on prefill tm                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342              kv_bootstrap_server_class = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_kv_class(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                  self.transfer_backend,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KVClassType.BOOTSTRAP_SERVER                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345              self.bootstrap_server =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kv_bootstrap_server_class(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disaggregation_bootstrap_port  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349      async def generate_request(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351          obj: Union[GenerateReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352          request: Optional = None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354          created_time = time.time()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356          self.auto_create_handle_loop()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358          if isinstance(obj,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput) and self.is_generation:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                  "This model does not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ appear to be an embedding model by default. "   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361                  "Please add             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--is-embedding` when launching the server or   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ try another model."                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.normalize_batch_and_arguments()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366          if self.log_requests:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367              max_length, skip_names, _ = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.log_request_metadata                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369                  f"Receive:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj={dataclass_to_string_truncated(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_length, skip_names=skip_names)}"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372          async with                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_lock.reader_lock:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373              is_single = obj.is_single   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374              if is_single:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                  tokenized_obj = await   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._tokenize_one_request(obj)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._send_one_request(obj, tokenized_obj,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ created_time)                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377                  async for response in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._wait_one_response(obj, request):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378                      yield response      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380                  async for response in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_batch_request(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381                      obj, request,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ created_time                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382                  ):                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383                      yield response      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385      async def _tokenize_one_request(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386          self,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387          obj: Union[GenerateReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388      ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389          """Tokenize one request."""     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390          # Tokenize                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391          input_embeds = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392          input_text = obj.text           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393          if obj.input_embeds is not      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394              if not                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disable_radix_cache:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395                  raise ValueError(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396                      "input_embeds is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ provided while disable_radix_cache is False. "  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397                      "Please add         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--disable-radix-cache` when you launch the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server "                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398                      "if you want to use â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_embeds as inputs."                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400              input_embeds =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.input_embeds                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401              input_ids = obj.input_ids   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402          elif obj.input_ids is not None: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403              input_ids = obj.input_ids   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405              if self.tokenizer is None:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406                  raise ValueError(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407                      "The engine         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ initialized with skip_tokenizer_init=True       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cannot "                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408                      "accept text        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prompts. Please provide input_ids or            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ re-initialize "                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409                      "the engine with    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ skip_tokenizer_init=False."                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411              input_ids =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tokenizer.encode(input_text)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413          image_inputs: Dict = await      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mm_processor.process_mm_data_async(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414              obj.image_data, input_text  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ or input_ids, obj, self.max_req_input_len       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416          if image_inputs and "input_ids" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in image_inputs:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417              input_ids =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ image_inputs["input_ids"]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418          if self.is_generation:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419              return_logprob =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.return_logprob                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420              logprob_start_len =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.logprob_start_len                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421              top_logprobs_num =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.top_logprobs_num                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    422              token_ids_logprob =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.token_ids_logprob                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    423              session_params = (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    424                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SessionParams(**obj.session_params) if          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.session_params else None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    425              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    426                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    427          input_token_num =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(input_ids) if input_ids is not None else 0  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    428          if input_token_num >=           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.context_len:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    429              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    430                  f"The input             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ({input_token_num} tokens) is longer than the " â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    431                  f"model's context       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length ({self.context_len} tokens)."            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    432              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    433                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    434          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    435                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.sampling_params.get("max_new_tokens") is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    436              and                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.sampling_params.get("max_new_tokens") +     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_token_num                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    437              >= self.context_len         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    438          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    439              raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    440                  f"Requested token count â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ exceeds the model's maximum context length "    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    441                  f"of {self.context_len} â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens. You requested a total of "              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    442                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"{obj.sampling_params.get('max_new_tokens') +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_token_num} "                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    443                  f"tokens<response       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clipped><NOTE>To save on context only part of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this file has been shown to you. You should     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ retry this tool after you have searched inside  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the file with `grep -n` in order to find the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line numbers of what you are looking            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for.</NOTE>                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 28020 Output: 1322                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 4: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,240p'                   â”‚ # Copyright 2023-2024 SGLang Team               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ # Licensed under the Apache License, Version    â”‚   â”‚
â”‚             â”‚ â”‚ && echo '----' && sed -n '240,520p'            â”‚ 2.0 (the "License");                            â”‚   â”‚
â”‚             â”‚ â”‚ python/sglang/srt/managers/tokenizer_manager.â€¦ â”‚ # you may not use this file except in           â”‚   â”‚
â”‚             â”‚ â”‚ && echo '----' && sed -n '520,920p'            â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚ python/sglang/srt/managers/tokenizer_manager.â€¦ â”‚ # You may obtain a copy of the License at       â”‚   â”‚
â”‚             â”‚ â”‚ && echo '----' && sed -n '920,1400p'           â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚ python/sglang/srt/managers/tokenizer_manager.â€¦ â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚ && echo '----' && sed -n '1400,2200p'          â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚ python/sglang/srt/managers/tokenizer_manager.â€¦ â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ # Unless required by applicable law or agreed   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to in writing, software                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # distributed under the License is distributed  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ on an "AS IS" BASIS,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ either express or implied.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # See the License for the specific language     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ governing permissions and                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # limitations under the License.                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ """TokenizerManager is a process that tokenizes â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the text."""                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import asyncio                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import copy                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import dataclasses                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import os                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import pickle                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import signal                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import sys                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import threading                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import time                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import uuid                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from collections import deque                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from datetime import datetime                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from http import HTTPStatus                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Any,                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Awaitable,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Deque,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Dict,                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Generic,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     List,                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Optional,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Tuple,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TypeVar,                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Union,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import fastapi                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import uvloop                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import zmq.asyncio                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from fastapi import BackgroundTasks             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.aio_rwlock import RWLock        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.configs.model_config import     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ModelConfig                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.disaggregation.utils import (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DisaggregationMode,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     KVClassType,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TransferBackend,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_kv_class,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.hf_transformers_utils import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_processor, get_tokenizer                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.io_struct import (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AbortReq,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BatchEmbeddingOut,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BatchMultimodalOut,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BatchStrOut,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     BatchTokenIDOut,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     CloseSessionReqInput,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ConfigureLoggingReq,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     EmbeddingReqInput,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ExpertDistributionReq,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ExpertDistributionReqOutput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     FlushCacheReq,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     GenerateReqInput,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     GetInternalStateReq,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     GetInternalStateReqOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     GetWeightsByNameReqInput,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     GetWeightsByNameReqOutput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     HealthCheckOutput,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     InitWeightsUpdateGroupReqInput,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     InitWeightsUpdateGroupReqOutput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     OpenSessionReqInput,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     OpenSessionReqOutput,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ProfileReq,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ProfileReqOutput,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ProfileReqType,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ReleaseMemoryOccupationReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ReleaseMemoryOccupationReqOutput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ResumeMemoryOccupationReqInput,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ResumeMemoryOccupationReqOutput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     SessionParams,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TokenizedEmbeddingReqInput,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     TokenizedGenerateReqInput,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightFromDiskReqInput,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightFromDiskReqOutput,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightsFromDistributedReqInput,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightsFromDistributedReqOutput,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightsFromTensorReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     UpdateWeightsFromTensorReqOutput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.multimodal_processor   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import (                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_dummy_processor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_mm_processor,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     import_processors,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.metrics.collector import        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerMetricsCollector                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.sampling.sampling_params import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SamplingParams                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.server_args import PortArgs,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import (                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     dataclass_to_string_truncated,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     get_zmq_socket,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     kill_process_tree,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.utils import TypeBasedDispatcher,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_exception_traceback                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.set_event_loop_policy(uvloop.EventLoopâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger(__name__)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclasses.dataclass                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ReqState:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Store the state a request."""            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     out_list: List                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     finished: bool                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     event: asyncio.Event                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     obj: Any                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For metrics                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     created_time: float                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     finished_time: float = 0.0                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     first_token_time: float = 0.0               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     last_time: float = 0.0                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     last_completion_tokens: int = 1             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For streaming output                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     last_output_offset: int = 0                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class TokenizerManager:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """TokenizerManager is a process that       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizes the text."""                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         server_args: ServerArgs,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         port_args: PortArgs,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Parse args                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.server_args = server_args          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.enable_metrics =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.enable_metrics                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.log_requests =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_requests                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.log_requests_level =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.log_requests_level                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Init inter-process communication      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         context = zmq.asyncio.Context(2)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.recv_from_detokenizer =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             context, zmq.PULL,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.tokenizer_ipc_name, True              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_zmq_socket(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             context, zmq.PUSH,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_args.scheduler_input_ipc_name, True        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Read model args                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_path =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.model_path                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.served_model_name =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.served_model_name                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_config = ModelConfig(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             server_args.model_path,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             trust_remote_code=server_args.trusâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             revision=server_args.revision,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             context_length=server_args.contextâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_override_args=server_args.jsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             is_embedding=server_args.is_embeddâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             enable_multimodal=server_args.enabâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             dtype=server_args.dtype,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             quantization=server_args.quantizatâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_generation =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_generation                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_image_gen =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_image_gen                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.context_len =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.context_len                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.image_token_id =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.image_token_id                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.model_config.is_multimodal:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             import_processors()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             _processor = get_processor(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 server_args.tokenizer_path,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenizer_mode=server_args.tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 trust_remote_code=server_args.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 revision=server_args.revision,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 use_fast=not                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.disable_fast_image_processor,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # We want to parallelize the image  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pre-processing so we create an executor for it  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # We create mm_processor for any    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ skip_tokenizer_init to make sure we still       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encode                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # images even with                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ skip_tokenizer_init=False.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.mm_processor =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_mm_processor(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.model_config.hf_config,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args, _processor                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if server_args.skip_tokenizer_init: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.tokenizer = self.processor â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.processor = _processor     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.tokenizer =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.processor.tokenizer                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 os.environ["TOKENIZERS_PARALLEâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = "false"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.mm_processor =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_dummy_processor()                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if server_args.skip_tokenizer_init: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.tokenizer = self.processor â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.tokenizer = get_tokenizer( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     server_args.tokenizer_path, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     tokenizer_mode=server_argsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     trust_remote_code=server_aâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     revision=server_args.revisâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Store states                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.no_create_loop = False             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.rid_to_state: Dict = {}            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.gracefully_exit = False            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.last_receive_tstamp = 0            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dump_requests_folder = ""  # By    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default do not dump                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dump_requests_threshold = 1000     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dump_request_list: List[Tuple] =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ []                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.log_request_metadata =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_log_request_metadata()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # The event to notify the weight sync   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is finished.                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_update_lock = RWLock()       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_update_result:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Awaitable[UpdateWeightFromDiskReqOutpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             None                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.asyncio_tasks = set()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # For session info                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.session_futures = {}  # session_id â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -> asyncio event                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Set after scheduler is initialized    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_req_input_len = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Metrics                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.enable_metrics:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ----                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.enable_metrics:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.metrics_collector =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerMetricsCollector(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 labels={                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "model_name":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.served_model_name,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # TODO: Add lora name/path  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in the future,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 },                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Communicators                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.init_weights_update_group_communiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.update_weights_from_distributed_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.update_weights_from_tensor_communâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.get_weights_by_name_communicator = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.release_memory_occupation_communiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.resume_memory_occupation_communicâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = _Communicator(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.start_profile_communicator =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.get_internal_state_communicator =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.expert_distribution_communicator = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _Communicator(                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.send_to_scheduler,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dp_size                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_dispatcher =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TypeBasedDispatcher(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             [                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         BatchStrOut,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         BatchEmbeddingOut,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         BatchTokenIDOut,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         BatchMultimodalOut,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self._handle_batch_output,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (OpenSessionReqOutput,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_open_session_req_output),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     UpdateWeightFromDiskReqOutâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self._handle_update_weightâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     InitWeightsUpdateGroupReqOâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.init_weights_update_gâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     UpdateWeightsFromDistributâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.update_weights_from_dâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     UpdateWeightsFromTensorReqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.update_weights_from_tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     GetWeightsByNameReqOutput,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.get_weights_by_name_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ReleaseMemoryOccupationReqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.release_memory_occupaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ResumeMemoryOccupationReqOâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.resume_memory_occupatâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ProfileReqOutput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.start_profile_communiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     GetInternalStateReqOutput,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.get_internal_state_coâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ExpertDistributionReqOutpuâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.expert_distribution_câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (HealthCheckOutput, lambda x:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None),                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.disaggregation_mode =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DisaggregationMode(                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.server_args.disaggregation_moâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.transfer_backend =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TransferBackend(                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.server_args.disaggregation_trâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # for disaggregtion, start kv boostrap  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server on prefill                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.disaggregation_mode ==          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DisaggregationMode.PREFILL:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # only start bootstrap server on    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefill tm                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             kv_bootstrap_server_class =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_kv_class(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.transfer_backend,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KVClassType.BOOTSTRAP_SERVER                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.bootstrap_server =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kv_bootstrap_server_class(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.server_args.disaggregatioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def generate_request(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: Union[GenerateReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         created_time = time.time()              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if isinstance(obj, EmbeddingReqInput)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and self.is_generation:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "This model does not appear to  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ be an embedding model by default. "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "Please add `--is-embedding`    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ when launching the server or try another        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model."                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj.normalize_batch_and_arguments()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.log_requests:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_length, skip_names, _ =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.log_request_metadata                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"Receive:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj={dataclass_to_string_truncated(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_length, skip_names=skip_names)}"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         async with                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_lock.reader_lock:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             is_single = obj.is_single           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if is_single:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj = await           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._tokenize_one_request(obj)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._send_one_request(obj,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenized_obj, created_time)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 async for response in           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._wait_one_response(obj, request):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     yield response              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 async for response in           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._handle_batch_request(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     obj, request, created_time  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     yield response              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def _tokenize_one_request(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: Union[GenerateReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Tokenize one request."""             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Tokenize                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_embeds = None                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_text = obj.text                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.input_embeds is not None:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if not                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.disable_radix_cache:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 raise ValueError(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "input_embeds is provided   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ while disable_radix_cache is False. "           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "Please add                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--disable-radix-cache` when you launch the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server "                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "if you want to use         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_embeds as inputs."                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_embeds = obj.input_embeds     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_ids = obj.input_ids           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif obj.input_ids is not None:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_ids = obj.input_ids           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.tokenizer is None:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 raise ValueError(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "The engine initialized     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with skip_tokenizer_init=True cannot "          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "accept text prompts.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Please provide input_ids or re-initialize "     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "the engine with            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ skip_tokenizer_init=False."                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_ids =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tokenizer.encode(input_text)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         image_inputs: Dict = await              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mm_processor.process_mm_data_async(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             obj.image_data, input_text or       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_ids, obj, self.max_req_input_len          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if image_inputs and "input_ids" in      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ image_inputs:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             input_ids =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ image_inputs["input_ids"]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.is_generation:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_logprob = obj.return_logprob â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logprob_start_len =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.logprob_start_len                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             top_logprobs_num =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.top_logprobs_num                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             token_ids_logprob =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.token_ids_logprob                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             session_params = (                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 SessionParams(**obj.session_paâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if obj.session_params else None                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         input_token_num = len(input_ids) if     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_ids is not None else 0                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if input_token_num >= self.context_len: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"The input ({input_token_num}  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens) is longer than the "                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"model's context length        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ({self.context_len} tokens)."                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if (                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             obj.sampling_params.get("max_new_tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is not None                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.sampling_params.get("max_new_tokens") +     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input_token_num                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             >= self.context_len                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"Requested token count exceeds â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the model's maximum context length "            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"of {self.context_len} tokens. â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You requested a total of "                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"{obj.sampling_params.get('maâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + input_token_num} "                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"tokens: {input_token_num}     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens from the input messages and "            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"{obj.sampling_params.get('maâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens for the "                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"completion. Please reduce the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ number of tokens in the input "                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"messages or the completion to â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fit within the limit."                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Parse sampling parameters             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sampling_params =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SamplingParams(**obj.sampling_params)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sampling_params.normalize(self.tokenizâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sampling_params.verify()                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Build return object                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if isinstance(obj, GenerateReqInput):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenized_obj =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizedGenerateReqInput(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 obj.rid,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 input_text,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 input_ids,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 image_inputs,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 sampling_params,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_logprob,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 logprob_start_len,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 top_logprobs_num,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 token_ids_logprob,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 obj.stream,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 bootstrap_host=obj.bootstrap_hâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 bootstrap_room=obj.bootstrap_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 lora_path=obj.lora_path,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 input_embeds=input_embeds,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 session_params=session_params,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 custom_logit_processor=obj.cusâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_hidden_states=obj.returâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif isinstance(obj,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput):                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenized_obj =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizedEmbeddingReqInput(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 obj.rid,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 input_text,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 input_ids,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 image_inputs,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 sampling_params,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return tokenized_obj                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _send_one_request(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: Union[GenerateReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenized_obj:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Union[TokenizedGenerateReqInput,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizedEmbeddingReqInput],                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         created_time: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         state = ReqState([], False,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.Event(), obj,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ created_time=created_time)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.rid_to_state = state               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(tokeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def _wait_one_response(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: Union[GenerateReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Wait for the response of one         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request."""                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         state = self.rid_to_state               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             try:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 await                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.wait_for(state.event.wait(), timeout=4) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             except asyncio.TimeoutError:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if request is not None and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ await request.is_disconnected():                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.abort_request(obj.rid) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "Request is             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disconnected from the client side. "            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"Abort request         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {obj.rid}"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 continue                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             out = state.out_list[-1]            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.out_list = []                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if state.finished:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.log_requests:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ----                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.log_requests:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_length, skip_names,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_skip_names = self.log_request_metadata      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_config.is_multimodal_gen:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         msg = f"Finish:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj={dataclass_to_string_truncated(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_length, skip_names=skip_names)}"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         msg = f"Finish:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj={dataclass_to_string_truncated(obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_length, skip_names=skip_names)},            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out={dataclass_to_string_truncated(out,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_length, skip_names=out_skip_names)}"        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     logger.info(msg)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 del self.rid_to_state           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Check if this was an          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ abort/error created by scheduler                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ isinstance(out["meta_info"].get("finish_reasonâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dict):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     finish_reason =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out["meta_info"]["finish_reason"]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if (                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         finish_reason.get("typâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "abort"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         and                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ finish_reason.get("status_code") ==             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ HTTPStatus.BAD_REQUEST                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ):                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         raise                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ValueError(finish_reason["message"])            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 yield out                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 break                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.event.clear()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if obj.stream:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 yield out                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if request is not None and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ await request.is_disconnected():                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.abort_request(obj.rid) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "Request is             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disconnected from the client side. "            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"Abort request         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {obj.rid}"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def _handle_batch_request(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: Union[GenerateReqInput,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EmbeddingReqInput],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         created_time: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch_size = obj.batch_size             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         generators = []                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         rids = []                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if getattr(obj, "parallel_sample_num",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1) == 1:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Send all requests                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for i in range(batch_size):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tmp_obj = obj                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj = await           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._tokenize_one_request(tmp_obj)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._send_one_request(tmp_obj, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenized_obj, created_time)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 generators.append(self._wait_oâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request))                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 rids.append(tmp_obj.rid)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # FIXME: When using batch and       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parallel_sample_num together, the perf is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ optimal.                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if batch_size > 128:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 logger.warning(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "Sending a single large     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch with parallel sampling (n > 1) has not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ been well optimized. "                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "The performance might be   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ better if you just duplicate the requests n     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ times or use "                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "many threads to send them  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ one by one with parallel sampling (n > 1)."     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Tokenize all requests             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             objs = [obj for i in                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(batch_size)]                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenized_objs = await              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.gather(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 *(self._tokenize_one_request(oâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for obj in objs)                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Cache the common prefix for       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parallel sampling                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for i in range(batch_size):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tmp_obj = copy.copy(objs)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ copy.copy(tokenized_objs)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj.rid =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tmp_obj.regenerate_rid()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj.sampling_params = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ copy.copy(tokenized_obj.sampling_params)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj.sampling_params.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 0                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenized_obj.stream = False    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._send_one_request(tmp_obj, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenized_obj, created_time)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 await                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._wait_one_response(tmp_obj,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request).__anext__()                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Expand requests, assign new rids  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for them, and send them                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for i in range(batch_size):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for _ in                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(obj.parallel_sample_num):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     tmp_obj = copy.copy(objs)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     tokenized_obj =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ copy.copy(tokenized_objs)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     tokenized_obj.rid =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tmp_obj.regenerate_rid()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self._send_one_request(tmpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenized_obj, created_time)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     generators.append(self._waâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request))                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     rids.append(tmp_obj.rid)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Wait for all requests                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         is_stream = hasattr(obj, "stream") and  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.stream                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not is_stream:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             outputs = await                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.gather(*(gen.__anext__() for gen in     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ generators))                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             yield outputs                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             rid_to_index = {rid: i for i, rid   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in enumerate(rids)}                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             task_map =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {asyncio.create_task(gen.__anext__()): gen for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gen in generators}                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             while task_map:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 done, _ = await asyncio.wait(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     task_map.keys(),            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ return_when=asyncio.FIRST_COMPLETED             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for task in done:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     gen = task_map.pop(task)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     try:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         result = task.result()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         result["index"] =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ rid_to_index[result["meta_info"]["id"]]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         yield result            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         new_task =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.create_task(gen.__anext__())            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         task_map = gen          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     except StopAsyncIteration:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         pass                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def flush_cache(self):                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req = FlushCacheReq()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(req)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def abort_request(self, rid: str):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if rid not in self.rid_to_state:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         del self.rid_to_state                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req = AbortReq(rid)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(req)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def start_profile(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         output_dir: Optional = None,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         num_steps: Optional = None,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         activities: Optional[List] = None,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req = ProfileReq(                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=ProfileReqType.START_PROFILE,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             output_dir=output_dir,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             num_steps=num_steps,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             activities=activities,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             profile_id=str(time.time()),        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         result = (await                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.start_profile_communicator(req))[0]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not result.success:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise RuntimeError(result.message)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return result                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def stop_profile(self):                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req =                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ProfileReq(type=ProfileReqType.STOP_PROFILE)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(req)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ start_expert_distribution_record(self):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.expert_distribution_communicator(ExpertDiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ stop_expert_distribution_record(self):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.expert_distribution_communicator(ExpertDiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dump_expert_distribution_record(self):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.expert_distribution_communicator(ExpertDiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def update_weights_from_disk(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: UpdateWeightFromDiskReqInput,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> Tuple:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # default the load format to the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.load_format is None:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             obj.load_format =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.load_format                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.info("Start update_weights. Load â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ format=%s", obj.load_format)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if True:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Hold the lock if it is not async. â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ This means that weight sync                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # cannot run while requests are in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ progress.                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             async with                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_lock.writer_lock:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return await                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._wait_for_model_update_from_disk(obj)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def _wait_for_model_update_from_disk( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, obj: UpdateWeightFromDiskReqInput â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> Tuple:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(obj)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_update_result =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ asyncio.Future()                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.server_args.dp_size == 1:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = await                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_result                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if result.success:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.served_model_name =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.model_path                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.server_args.model_path =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.model_path                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.server_args.load_format =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.load_format                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.model_path =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.model_path                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return result.success,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result.message, result.num_paused_requests      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:  # self.server_args.dp_size > 1   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_update_tmp = []          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = await                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_result                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             all_success = all()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if all_success is True:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.server_args.model_path =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.model_path                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.server_args.load_format =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.load_format                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.model_path =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.model_path                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             all_message =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             all_message = " |                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ".join(all_message)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             all_paused_requests =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return all_success, all_message,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ all_paused_requests                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def init_weights_update_group(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: InitWeightsUpdateGroupReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> Tuple:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.server_args.dp_size == 1       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "dp_size must be 1 for init          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ parameter update group"                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         result = (await                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.init_weights_update_group_communicator(obâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return result.success, result.message   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def update_weights_from_distributed(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ UpdateWeightsFromDistributedReqInput,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> Tuple:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.server_args.dp_size == 1       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "dp_size must be for update weights  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from distributed"                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This means that weight sync           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # cannot run while requests are in      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ progress.                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         async with                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_lock.writer_lock:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = (await                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_distributed_communicaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return result.success,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result.message                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def update_weights_from_tensor(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: UpdateWeightsFromTensorReqInput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> Tuple:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.server_args.dp_size == 1       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "dp_size must be 1 for update        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights from distributed"                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This means that weight sync           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # cannot run while requests are in      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ progress.                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         async with                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_update_lock.writer_lock:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = (await                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.update_weights_from_tensor_communicator(oâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return result.success,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result.message                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def get_weights_by_name(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, obj: GetWeightsByNameReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request: Optional = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         results = await                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_weights_by_name_communicator(obj)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         all_parameters =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.server_args.dp_size == 1:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return all_parameters[0]            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return all_parameters               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def release_memory_occupation(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: ReleaseMemoryOccupationReqInput,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.release_memory_occupation_communicator(obâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def resume_memory_occupation(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         obj: ResumeMemoryOccupationReqInput,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         request: Optional = None,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.resume_memory_occupation_communicator(obj) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def open_session(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, obj: OpenSessionReqInput,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request: Optional = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.auto_create_handle_loop()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.session_id is None:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             obj.session_id = uuid.uuid4().hex   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif obj.session_id in                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.session_futures:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return None                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.send_to_scheduler.send_pyobj(obj)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.session_futures = asyncio.Future() â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         session_id = await self.session_futures â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         del self.session_futures                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return session_id                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def close_session(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, obj: CloseSessionReqInput,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request: Optional = None                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.send_to_scheduler.send_pyobj(obj)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def get_internal_state(self) ->       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Dict[Any, Any]:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req = GetInternalStateReq()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         res: List[GetInternalStateReqOutput] =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             await                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_internal_state_communicator(req)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return res[0].internal_state            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def get_log_request_metadata(self):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         max_length = None                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         skip_names = None                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         out_skip_names = None                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.log_requests:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.log_requests_level == 0:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_length = 1 << 30            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 skip_names = set(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "text",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "input_ids",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "input_embeds",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "image_data",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "audio_data",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "lora_path",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 out_skip_names = set(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     [                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "text",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "output_ids",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif self.log_requests_level == 1:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_length = 2048               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif self.log_requests_level == 2:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_length = 1 << 30            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 raise ValueError(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     f"Invalid                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --log-requests-level:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {self.log_requests_level=}"                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return max_length, skip_names,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_skip_names                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def configure_logging(self, obj:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ConfigureLoggingReq):                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.log_requests is not None:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.log_requests =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.log_requests                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.log_requests_level is not None:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.log_requests_level =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.log_requests_level                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.dump_requests_folder is not      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.dump_requests_folder =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.dump_requests_folder                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj.dump_requests_threshold is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.dump_requests_threshold =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ obj.dump_requests_threshold                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logging.info(f"Config logging: {obj=}") â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.log_request_metadata =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_log_request_metadata()                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def create_abort_task(self, obj:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GenerateReqInput):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Abort the request if the client is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disconnected.                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         async def abort_request():              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             await asyncio.sleep(1)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if obj.is_single:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.abort_request(obj.rid)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for rid in obj.rid:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.abort_request(rid)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         background_tasks = BackgroundTasks()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         background_tasks.add_task(abort_requesâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return background_tasks                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def auto_create_handle_loop(self):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.no_create_loop:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.no_create_loop = True              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         loop = asyncio.get_event_loop()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.asyncio_tasks.add(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             loop.create_task(print_exception_wâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # We cannot add signal handler when the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer manager is not in                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # the main thread due to the CPython    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ limitation.                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if threading.current_thread() is        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ threading.main_thread():                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             signal_handler =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SignalHandler(self)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             loop.add_signal_handler(signal.SIGâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ signal_handler.signal_handler)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.warning(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "Signal handler is not added    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ because the tokenizer manager is "              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "not in the main thread. This   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disables graceful shutdown of the "             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "tokenizer manager when SIGTERM â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is received."                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.asyncio_tasks.add(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             loop.create_task(print_exception_wâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def sigterm_watchdog(self):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while not self.gracefully_exit:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             await asyncio.sleep(5)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Drain requests                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             remain_num_req =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(self.rid_to_state)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 f"Gracefully exiting...         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ remaining number of requests {remain_num_req}"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if remain_num_req > 0:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 await asyncio.sleep(5)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 break                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kill_process_tree(os.getpid(),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ include_parent=True)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sys.exit(0)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def handle_loop(self):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ----                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def handle_loop(self):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """The event loop that handles          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ requests"""                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj = await                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.recv_from_detokenizer.recv_pyobj()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._result_dispatcher(recv_obj)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.last_receive_tstamp =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.time()                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _handle_batch_output(                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         recv_obj: Union[                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             BatchStrOut, BatchEmbeddingOut,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchMultimodalOut, BatchTokenIDOut             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ],                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i, rid in enumerate(recv_obj.rids): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state = self.rid_to_state.get(rid,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None)                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if state is None:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 continue                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Build meta_info and return value  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             meta_info = {                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "id": rid,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "finish_reason":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.finished_reasons,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "prompt_tokens":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.prompt_tokens,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if getattr(state.obj,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "return_logprob", False):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.convert_logprob_style(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     meta_info,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     state.obj.top_logprobs_num, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     state.obj.token_ids_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     state.obj.return_text_in_lâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     recv_obj,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     i,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if not isinstance(recv_obj,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchEmbeddingOut):                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 meta_info.update(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     {                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "completion_tokens":    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.completion_tokens,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "cached_tokens":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.cached_tokens,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     }                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if getattr(recv_obj,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "output_hidden_states", None):                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 meta_info["hidden_states"] =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.output_hidden_states                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if isinstance(recv_obj,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchStrOut):                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 out_dict = {                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "text":                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.output_strs,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "meta_info": meta_info,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 }                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif isinstance(recv_obj,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchTokenIDOut):                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.stream_output and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.obj.stream:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output_token_ids =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.output_ids[                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         state.last_output_offsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     state.last_output_offset =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(recv_obj.output_ids)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output_token_ids =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.output_ids                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 out_dict = {                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "output_ids":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ output_token_ids,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "meta_info": meta_info,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 }                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif isinstance(recv_obj,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchMultimodalOut):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 raise NotImplementedError()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 assert isinstance(recv_obj,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ BatchEmbeddingOut)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 out_dict = {                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "embedding":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.embeddings,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "meta_info": meta_info,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 }                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.finished =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.finished_reasons is not None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if state.finished:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.speculative_algorithm:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     meta_info["spec_verify_ct"] â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = recv_obj.spec_verify_ct                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 state.finished_time =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time.time()                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 meta_info["e2e_latency"] =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.finished_time - state.created_time        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.out_list.append(out_dict)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.event.set()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Log metrics and dump              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.enable_metrics and          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.obj.log_metrics:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.collect_metrics(state,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj, i)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.dump_requests_folder and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.finished and state.obj.log_metrics:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.dump_requests(state,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_dict)                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def convert_logprob_style(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         meta_info: dict,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         top_logprobs_num: int,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         token_ids_logprob: List,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return_text_in_logprobs: bool,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         recv_obj: BatchStrOut,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         recv_obj_index: int,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         meta_info["input_token_logprobs"] =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.detokenize_logprob_tokens(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.input_token_logprobs_val,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.input_token_logprobs_idx,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_text_in_logprobs,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         meta_info["output_token_logprobs"] =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.detokenize_logprob_tokens(                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.output_token_logprobs_val, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.output_token_logprobs_idx, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return_text_in_logprobs,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if top_logprobs_num > 0:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             meta_info["input_top_logprobs"] =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.detokenize_top_logprobs_tokens(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.input_top_logprobs_vaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.input_top_logprobs_idâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_text_in_logprobs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             meta_info["output_top_logprobs"] =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.detokenize_top_logprobs_tokens(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.output_top_logprobs_vâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.output_top_logprobs_iâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_text_in_logprobs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if token_ids_logprob is not None:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             meta_info["input_token_ids_logprobâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = self.detokenize_top_logprobs_tokens(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.input_token_ids_logprâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.input_token_ids_logprâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_text_in_logprobs,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             meta_info["output_token_ids_logproâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.detokenize_top_logprobs_tâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     recv_obj.output_token_ids_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     recv_obj.output_token_ids_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return_text_in_logprobs,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def detokenize_logprob_tokens(              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         token_logprobs_val: List,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         token_logprobs_idx: List,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         decode_to_text: bool,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not decode_to_text:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return [                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (logprob, token_id, None)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 for logprob, token_id in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ zip(token_logprobs_val, token_logprobs_idx)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert self.tokenizer is not None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             token_texts =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tokenizer.batch_decode(token_logprobs_idx) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return list(zip(token_logprobs_val, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_logprobs_idx, token_texts))               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def detokenize_top_logprobs_tokens(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         token_logprobs_val: List,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         token_logprobs_idx: List,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         decode_to_text: bool,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # TODO: The current implementation only â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batches the detokenization for top-k tokens per â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ single position.                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # We should batch all top-k tokens in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ all positions.                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ret = []                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i in                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(len(token_logprobs_val)):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if token_logprobs_val:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ret.append(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.detokenize_logprob_toâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         token_logprobs_val,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ token_logprobs_idx, decode_to_text              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ret.append(None)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return ret                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def collect_metrics(self, state: ReqState,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj: BatchStrOut, i: int):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         completion_tokens = (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.completion_tokens          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if getattr(recv_obj,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "completion_tokens", None)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else 0                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if state.first_token_time == 0.0:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.first_token_time =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.last_time = time.time()                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             state.last_completion_tokens =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ completion_tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.metrics_collector.observe_timâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 state.first_token_time -        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.created_time                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             num_new_tokens = completion_tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - state.last_completion_tokens                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if num_new_tokens:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 new_time = time.time()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 interval = new_time -           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.last_time                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.metrics_collector.observeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     interval,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     num_new_tokens,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 state.last_time = new_time      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 state.last_completion_tokens =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ completion_tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if state.finished:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.metrics_collector.observe_oneâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.prompt_tokens,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 completion_tokens,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 recv_obj.cached_tokens,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 state.finished_time -           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.created_time,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def dump_requests(self, state: ReqState,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_dict: dict):                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dump_request_list.append(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             (state.obj, out_dict,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ state.created_time, time.time())                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if len(self.dump_request_list) >=       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.dump_requests_threshold:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             filename = os.path.join(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.dump_requests_folder,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 datetime.now().strftime("%Y-%mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + ".pkl",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             logger.info(f"Dump                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {len(self.dump_request_list)} requests to       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {filename}")                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             to_dump = self.dump_request_list    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.dump_request_list = []         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             def background_task():              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 os.makedirs(self.dump_requestsâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ exist_ok=True)                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 with open(filename, "wb") as f: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     pickle.dump(to_dump, f)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Schedule the task to run in the   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ background without awaiting it                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             asyncio.create_task(asyncio.to_thrâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _handle_open_session_req_output(self,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.session_futures.set_result(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             recv_obj.session_id if              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj.success else None                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _handle_update_weights_from_disk_req_output(seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recv_obj):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.server_args.dp_size == 1:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_update_result.set_resulâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:  # self.server_args.dp_size > 1   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_update_tmp.append(recv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # set future if the all results are â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ recevied                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if len(self.model_update_tmp) ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.server_args.dp_size:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.model_update_result.set_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ async def print_exception_wrapper(func):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Sometimes an asyncio function does not      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ print exception.                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     We do another wrapper to handle the         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ exception.                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     try:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await func()                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     except Exception:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         traceback = get_exception_traceback()   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.error(f"TokenizerManager hit an  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ exception: {traceback}")                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kill_process_tree(os.getpid(),          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ include_parent=True)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sys.exit(1)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class SignalHandler:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(self, tokenizer_manager:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ TokenizerManager):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tokenizer_manager =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_manager                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def signal_handler(self, signum=None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ frame=None):                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         logger.warning(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"SIGTERM received. {signum=}       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {frame=}. Draining requests and shutting        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ down..."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.tokenizer_manager.gracefully_exit  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = True                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ T = TypeVar("T")                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class _Communicator(Generic[T]):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Note: The communicator now only run up   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to 1 in-flight request at any time."""          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(self, sender, fan_out: int):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._sender = sender                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._fan_out = fan_out                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_event: Optional = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_values: Optional[List[T]]  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._ready_queue: Deque = deque()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     async def __call__(self, obj):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ready_event = asyncio.Event()           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self._result_event is not None or    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ len(self._ready_queue) > 0:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._ready_queue.append(ready_eveâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             await ready_event.wait()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert self._result_event is None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert self._result_values is None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if obj:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._sender.send_pyobj(obj)        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_event = asyncio.Event()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_values = []                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         await self._result_event.wait()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         result_values = self._result_values     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_event =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._result_values = None                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if len(self._ready_queue) > 0:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._ready_queue.popleft().set()   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return result_values                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def handle_recv(self, recv_obj: T):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._result_values.append(recv_obj)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if len(self._result_values) ==          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self._fan_out:                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._result_event.set()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ----                                            â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 42901 Output: 1510                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 5: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ str_replace_based_edit_tool                                                                         â”‚
â”‚ str_replacâ€¦ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'view', 'file_text': '',           â”‚ Here's the result of running `cat -n` on        â”‚   â”‚
â”‚             â”‚ â”‚ 'insert_line': 0, 'new_str': '', 'old_str':    â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ '', 'path':                                    â”‚      1  # Copyright 2023-2024 SGLang Team       â”‚   â”‚
â”‚             â”‚ â”‚ '/home/ubuntu/OmniPerf-Bench/perf-agents-bencâ€¦ â”‚      2  # Licensed under the Apache License,    â”‚   â”‚
â”‚             â”‚ â”‚ 'view_range': [1, -1]}                         â”‚ Version 2.0 (the "License");                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      3  # you may not use this file except in   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      4  # You may obtain a copy of the License  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ at                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      5  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      6  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      7  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      8  # Unless required by applicable law or  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ agreed to in writing, software                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚      9  # distributed under the License is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ distributed on an "AS IS" BASIS,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     10  # WITHOUT WARRANTIES OR CONDITIONS OF   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ANY KIND, either express or implied.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     11  # See the License for the specific      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ language governing permissions and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     12  # limitations under the License.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     13  #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     14  """The arguments of the server."""      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     15                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     16  import argparse                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     17  import dataclasses                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     18  import json                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     19  import logging                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     20  import os                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     21  import random                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     22  import tempfile                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     23  from typing import List, Literal,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     24                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     25  from sglang.srt.hf_transformers_utils   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import check_gguf_file                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     26  from sglang.srt.reasoning_parser import â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ReasoningParser                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     27  from sglang.srt.utils import (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     28      configure_ipv6,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     29      get_amdgpu_memory_capacity,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     30      get_device,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     31      get_hpu_memory_capacity,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     32      get_nvgpu_memory_capacity,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     33      is_cuda,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     34      is_flashinfer_available,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     35      is_hip,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     36      is_port_available,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     37      is_remote_url,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     38      is_valid_ipv6_address,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     39      nullable_str,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     40  )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     41                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     42  logger = logging.getLogger(__name__)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     43                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     44                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     45  @dataclasses.dataclass                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     46  class ServerArgs:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     47      # Model and tokenizer               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     48      model_path: str                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     49      tokenizer_path: Optional = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     50      tokenizer_mode: str = "auto"        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     51      skip_tokenizer_init: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     52      load_format: str = "auto"           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     53      trust_remote_code: bool = False     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     54      dtype: str = "auto"                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     55      kv_cache_dtype: str = "auto"        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     56      quantization: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     57      quantization_param_path: Optional = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     58      context_length: Optional = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     59      device: Optional = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     60      served_model_name: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     61      chat_template: Optional = None      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     62      completion_template: Optional =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     63      is_embedding: bool = False          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     64      revision: Optional = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     65                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     66      # Port for the HTTP server          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     67      host: str = "127.0.0.1"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     68      port: int = 30000                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     69                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     70      # Memory and scheduling             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     71      mem_fraction_static: Optional =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     72      max_running_requests: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     73      max_total_tokens: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     74      chunked_prefill_size: Optional =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     75      max_prefill_tokens: int = 16384     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     76      schedule_policy: str = "fcfs"       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     77      schedule_conservativeness: float =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1.0                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     78      cpu_offload_gb: int = 0             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     79      page_size: int = 1                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     80                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     81      # Other runtime options             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     82      tp_size: int = 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     83      stream_interval: int = 1            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     84      stream_output: bool = False         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     85      random_seed: Optional = None        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     86                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ constrained_json_whitespace_pattern: Optional = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     87      watchdog_timeout: float = 300       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     88      dist_timeout: Optional = None  #    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ timeout for torch.distributed                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     89      download_dir: Optional = None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     90      base_gpu_id: int = 0                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     91      gpu_id_step: int = 1                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     92                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     93      # Logging                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     94      log_level: str = "info"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     95      log_level_http: Optional = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     96      log_requests: bool = False          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     97      log_requests_level: int = 0         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     98      show_time_cost: bool = False        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     99      enable_metrics: bool = False        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    100      decode_log_interval: int = 40       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    101                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    102      # API related                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    103      api_key: Optional = None            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    104      file_storage_path: str =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "sglang_storage"                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    105      enable_cache_report: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    106      reasoning_parser: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    107                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    108      # Data parallelism                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    109      dp_size: int = 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    110      load_balance_method: str =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "round_robin"                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    111                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    112      # Expert parallelism                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    113      ep_size: int = 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    114                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    115      # Multi-node distributed serving    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    116      dist_init_addr: Optional = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    117      nnodes: int = 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    118      node_rank: int = 0                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    119                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    120      # Model override args in JSON       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    121      json_model_override_args: str =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "{}"                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    122                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    123      # LoRA                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    124      lora_paths: Optional[List] = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    125      max_loras_per_batch: int = 8        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    126      lora_backend: str = "triton"        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    127                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    128      # Kernel backend                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    129      attention_backend: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    130      sampling_backend: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    131      grammar_backend: Optional = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    132                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    133      # Speculative decoding              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    134      speculative_algorithm: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    135      speculative_draft_model_path:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    136      speculative_num_steps: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    137      speculative_eagle_topk: Optional =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    138      speculative_num_draft_tokens:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    139                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative_accept_threshold_single: float =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1.0                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    140      speculative_accept_threshold_acc:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ float = 1.0                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    141      speculative_token_map: Optional =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    142                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    143      # Double Sparsity                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    144      enable_double_sparsity: bool =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    145      ds_channel_config_path: Optional =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    146      ds_heavy_channel_num: int = 32      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    147      ds_heavy_token_num: int = 256       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    148      ds_heavy_channel_type: str = "qk"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    149      ds_sparse_decode_threshold: int =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 4096                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    150                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    151      # Optimization/debug options        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    152      disable_radix_cache: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    153      disable_cuda_graph: bool = False    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    154      disable_cuda_graph_padding: bool =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    155      enable_nccl_nvls: bool = False      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    156      disable_outlines_disk_cache: bool = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    157      disable_custom_all_reduce: bool =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    158      enable_llama4_multimodal: Optional  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    159      disable_overlap_schedule: bool =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    160      enable_mixed_chunk: bool = False    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    161      enable_dp_attention: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    162      enable_ep_moe: bool = False         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    163      enable_deepep_moe: bool = False     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    164      deepep_mode:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Literal["auto", "normal",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "low_latency"]] = "auto"                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    165      enable_torch_compile: bool = False  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    166      torch_compile_max_bs: int = 32      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    167      cuda_graph_max_bs: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    168      cuda_graph_bs: Optional[List] =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    169      torchao_config: str = ""            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    170      enable_nan_detection: bool = False  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    171      enable_p2p_check: bool = False      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    172      triton_attention_reduce_in_fp32:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bool = False                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    173      triton_attention_num_kv_splits: int â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 8                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    174      num_continuous_decode_steps: int =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    175      delete_ckpt_after_loading: bool =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    176      enable_memory_saver: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    177      allow_auto_truncate: bool = False   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    178      enable_custom_logit_processor: bool â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = False                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    179      tool_call_parser: Optional = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    180      enable_hierarchical_cache: bool =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    181      hicache_ratio: float = 2.0          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    182      flashinfer_mla_disable_ragged: bool â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = False                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    183      warmups: Optional = None            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    184      moe_dense_tp_size: Optional = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    185      n_share_experts_fusion: int = 0     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    186      disable_chunked_prefix_cache: bool  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = False                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    187      disable_fast_image_processor: bool  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = False                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    188                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    189      # Debug tensor dumps                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    190      debug_tensor_dump_output_folder:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    191      debug_tensor_dump_input_file:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = None                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    192      debug_tensor_dump_inject: bool =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    193                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    194      # For PD disaggregation: can be     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "null" (not disaggregated), "prefill"           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (prefill-only), or "decode" (decode-only)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    195      disaggregation_mode: str = "null"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    196      disaggregation_bootstrap_port: int  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 8998                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    197      disaggregation_transfer_backend:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ str = "mooncake"                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    198      disaggregation_ib_device: Optional  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    199                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    200      def __post_init__(self):            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    201          # Expert parallelism            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    202          if self.enable_ep_moe:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    203              self.ep_size = self.tp_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    204              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    205                  f"EP MoE is enabled.    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The expert parallel size is adjusted to be the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ same as the tensor parallel                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size[{self.tp_size}]."                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    206              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    207                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    208          # Set missing default values    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    209          if self.tokenizer_path is None: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    210              self.tokenizer_path =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_path                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    211                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    212          if self.device is None:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    213              self.device = get_device()  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    214                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    215          if self.served_model_name is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    216              self.served_model_name =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.model_path                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    217                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    218          if self.random_seed is None:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    219              self.random_seed =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ random.randint(0, 1 << 30)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    220                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    221          if is_cuda():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    222              gpu_mem =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_nvgpu_memory_capacity()                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    223          elif is_hip():                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    224              gpu_mem =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_amdgpu_memory_capacity()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    225          elif self.device == "hpu":      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    226              gpu_mem =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_hpu_memory_capacity()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    227          else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    228              # GPU memory is not known   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ yet or no GPU is available.                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    229              gpu_mem = None              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    230                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    231          # Set mem fraction static,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ which depends on the tensor parallelism size    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    232          if self.mem_fraction_static is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    233              if self.tp_size >= 16:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    234                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_fraction_static = 0.79                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    235              elif self.tp_size >= 8:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    236                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_fraction_static = 0.81                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    237              elif self.tp_size >= 4:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    238                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_fraction_static = 0.85                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    239              elif self.tp_size >= 2:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    240                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_fraction_static = 0.87                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    241              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    242                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.mem_fraction_static = 0.88                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    243                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    244          # Set chunked prefill size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ which depends on the gpu memory capacity        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    245          if self.chunked_prefill_size is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    246              if gpu_mem is not None and  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gpu_mem < 25_000:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    247                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.chunked_prefill_size = 2048                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    248              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    249                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.chunked_prefill_size = 8192                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    250                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    251          assert                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.chunked_prefill_size % self.page_size == 0 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    252                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    253          assert self.moe_dense_tp_size   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in {                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    254              1,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    255              None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    256          }, f"moe_dense_tp_size only     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ support 1 and None currently"                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    257                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    258          if self.attention_backend ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "flashmla":                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    259              logger.warning(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    260                  "FlashMLA only supports â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ a page_size of 64, change page_size to 64."     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    261              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    262              self.page_size = 64         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    263          # Set cuda graph max batch size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    264          if self.cuda_graph_max_bs is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    265              # Based on detailed         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ statistics, when serving TP1/TP2 models on      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lower-end GPUs with HBM<25G, you can either     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disable cuda graph or set `cuda_graph_max_bs`   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to a very small value to reduce the memory      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ overhead of creating cuda graphs, with almost   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ no impact on performance. However, when serving â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ models with TP4 or TP8, we need to enable cuda  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph to maintain high performance. In this     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ case, we can set `cuda_graph_max_bs` to 80      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (half of the default value 160) to reduce the   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ memory overhead of creating cuda graphs.        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Looking at the logs from TP4 serving of         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ qwen2-72b, a value of 80 is sufficient and can  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reduce the memory overhead of creating cuda     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graphs on lower-end GPUs compared to the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ original 160, avoiding OOM issues.              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    266              if gpu_mem is not None and  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gpu_mem < 25_000:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    267                  if self.tp_size < 4:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    268                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.cuda_graph_max_bs = 8                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    269                  else:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    270                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.cuda_graph_max_bs = 80                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    271              else:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    272                  self.cuda_graph_max_bs  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 160                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    273                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    274          # Set kernel backends for hpu   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    275          if self.device == "hpu":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    276              self.attention_backend =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "torch_native"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    277              self.sampling_backend =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "pytorch"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    278                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    279          if self.sampling_backend is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    280              self.sampling_backend = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    281                  "flashinfer" if         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_flashinfer_available() else "pytorch"        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    282              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    283                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    284          if self.attention_backend ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "torch_native":                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    285              logger.warning(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    286                  "Cuda graph is disabled â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ because of using torch native attention         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backend"                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    287              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    288              self.disable_cuda_graph =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ True                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    289                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    290          # Choose grammar backend        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    291          if self.grammar_backend is      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    292              self.grammar_backend =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "xgrammar"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    293                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    294          # Expert parallelism            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    295          if self.enable_ep_moe:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    296              self.ep_size = self.tp_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    297              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    298                  f"EP MoE is enabled.    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ The expert parallel size is adjusted to be the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ same as the tensor parallel                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size[{self.tp_size}]."                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    299              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    300                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    301          self.enable_multimodal:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional = self.enable_llama4_multimodal        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    302                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    303          # Data parallelism attention    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    304          if self.enable_dp_attention:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    305                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.schedule_conservativeness =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.schedule_conservativeness * 0.3            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    306              assert (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    307                  self.dp_size > 1        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    308              ), "Please set a dp-size >  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1. You can use 1 < dp-size <= tp-size "         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    309              assert self.tp_size %       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.dp_size == 0                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    310              self.chunked_prefill_size = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.chunked_prefill_size // self.dp_size       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    311              logger.warning(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    312                  f"DP attention is       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enabled. The chunked prefill size is adjusted   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to {self.chunked_prefill_size} to avoid MoE     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernel issues. "                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    313              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    314                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    315          self.enable_sp_layernorm =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    316          # DeepEP MoE                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    317          if self.enable_deepep_moe:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    318              if self.deepep_mode ==      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "auto":                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    319                  assert (                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    320                      not                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_dp_attention                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    321                  ), "DeepEP MoE `auto`   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mode is not supported with DP Attention."       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    322              self.ep_size = self.tp_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    323              self.enable_sp_layernorm =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    324                  self.dp_size <          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.tp_size if self.enable_dp_attention else   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ True                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    325              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    326              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    327                  f"DeepEP MoE is         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enabled. The expert parallel size is adjusted   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to be the same as the tensor parallel           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size[{self.tp_size}]."                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    328              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    329                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    330          # Speculative Decoding          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    331          if self.speculative_algorithm   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "NEXTN":                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    332              # NEXTN shares the same     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ implementation of EAGLE                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    333              self.speculative_algorithm  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = "EAGLE"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    334                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    335          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    336              self.speculative_algorithm  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "EAGLE"                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    337              or                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_algorithm == "EAGLE3"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    338          ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    339              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_running_requests is None:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    340                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.max_running_requests = 48                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    341                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_overlap_schedule = True            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    342              logger.info(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    343                  "Overlap scheduler is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disabled because of using "                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    344                  "eagle speculative      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decoding."                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    345              )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    346                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    347              # Auto choose parameters    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    348              if                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_steps is None:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    349                  assert (                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    350                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_eagle_topk is None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    351                      and                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens is None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    352                  )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    353                  (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    354                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_steps,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    355                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_eagle_topk,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    356                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    357                  ) =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ auto_choose_speculative_params(self)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    358                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    359              if self.page_size > 1 and   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_eagle_topk > 1:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    360                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_eagle_topk = 1                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    361                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger.info("speculative_eagle_topk is changed  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to 1 when page_size > 1")                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    362                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    363              # The token generated from  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the verify step is counted.                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    364              # If sepculative_num_steps  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ >= speculative_num_draft_tokens, the additional â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens will definitely be discarded.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    365              # assert                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_steps <                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    366                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    367          # GGUF                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    368          if (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    369              self.load_format == "auto"  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ or self.load_format == "gguf"                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    370          ) and                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ check_gguf_file(self.model_path):               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    371              self.quantization =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.load_format = "gguf"                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    372                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    373          if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_remote_url(self.model_path):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    374              self.load_format = "remote" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    375                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    376          # AMD-specific Triton attention â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV splits default number                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    377          if is_hip():                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    378                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.triton_attention_num_kv_splits = 16        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    379                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    380          # PD disaggregation             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    381          if self.disaggregation_mode ==  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "prefill":                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    382              self.disable_cuda_graph =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ True                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    383              logger.warning("KV cache is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forced as chunk cache for decode server")       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    384                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_overlap_schedule = True            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    385              logger.warning("Overlap     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scheduler is disabled for prefill server")      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    386          elif self.disaggregation_mode   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "decode":                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    387              self.disable_radix_cache =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ True                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    388              logger.warning("Cuda graph  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is disabled for prefill server")                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    389                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_overlap_schedule = True            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    390              logger.warning("Overlap     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scheduler is disabled for decode server")       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    391                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    392                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ os.environ["SGLANG_ENABLE_TORCH_COMPILE"] = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    393              "1" if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.enable_torch_compile else "0"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    394          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    395          # Set env var before grammar    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ backends init                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    396                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ os.environ["SGLANG_DISABLE_OUTLINES_DISK_CACHEâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    397              "1" if                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_outlines_disk_cache else "0"       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    398          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    399                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    400      @staticmethod                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    401      def add_cli_args(parser:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ argparse.ArgumentParser):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    402          # Model and port args           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    403          parser.add_argument(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    404              "--model-path",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    405              type=str,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    406              help="The path of the model â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights. This can be a local folder or a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Hugging Face repo ID.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    407              required=True,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    408          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    409          parser.add_argument(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    410              "--tokenizer-path",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    411              type=str,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    412                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.tokenizer_path,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    413              help="The path of the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer.",                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    414          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    415          parser.add_argument(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    416              "--host", type=str,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.host, help="The host of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    417          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    418          parser.add_argument(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    419              "--port", type=int,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.port, help="The port of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    420          )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚    421          parser.add_argume<response      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clipped><NOTE>To save on context only part of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this file has been shown to you. You should     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ retry this tool after you have searched inside  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the file with `grep -n` in order to find the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line numbers of what you are looking            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for.</NOTE>                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 67380 Output: 2175                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 6: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n "def add_cli_args" -n     â”‚ 401:    def add_cli_args(parser:                â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ argparse.ArgumentParser):                       â”‚   â”‚
â”‚             â”‚ â”‚ && sed -n \'400,860p\'                         â”‚     @staticmethod                               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚     def add_cli_args(parser:                    â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ argparse.ArgumentParser):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Model and port args                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--model-path",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the model         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights. This can be a local folder or a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Hugging Face repo ID.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             required=True,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tokenizer-path",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tokenizer_path,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the tokenizer.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--host", type=str,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.host, help="The host of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--port", type=int,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.port, help="The port of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server."                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tokenizer-mode",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tokenizer_mode,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "slow"],           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Tokenizer mode. 'auto' will   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use the fast "                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "tokenizer if available, and 'slow' â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ will "                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "always use the slow tokenizer.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--skip-tokenizer-init",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="If set, skip init tokenizer   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and pass input_ids in generate request",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--load-format",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.load_format,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=[                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "auto",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "pt",                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "safetensors",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "npcache",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "dummy",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "sharded_state",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "gguf",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "bitsandbytes",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "layered",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "remote",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The format of the model       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights to load. "                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"auto" will try to load the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights in the safetensors format '             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "and fall back to the pytorch bin   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ format if safetensors format "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "is not available. "                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"pt" will load the weights in the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pytorch bin format. '                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"safetensors" will load the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights in the safetensors format. '            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"npcache" will load the weights in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pytorch format and store '                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "a numpy cache to speed up the      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ loading. "                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"dummy" will initialize the        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights with random values, '                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "which is mainly for profiling."    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"gguf" will load the weights in    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the gguf format. '                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"bitsandbytes" will load the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights using bitsandbytes '                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "quantization."                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '"layered" loads weights layer by   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer so that one can quantize a '              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "layer before loading another to    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ make the peak memory envelope "                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "smaller.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--trust-remote-code",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Whether or not to allow for   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ custom models defined on the Hub in their own   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ modeling files.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dtype",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.dtype,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "half", "float16", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "bfloat16", "float", "float32"],                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Data type for model weights   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and activations.\n\n"                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "auto" will use FP16 precision   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for FP32 and FP16 models, and '                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "BF16 precision for BF16 models.\n" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "half" for FP16. Recommended for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AWQ quantization.\n'                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float16" is the same as         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "half".\n'                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "bfloat16" for a balance between â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ precision and range.\n'                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float" is shorthand for FP32    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ precision.\n'                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             '* "float32" for FP32 precision.',  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--kv-cache-dtype",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.kv_cache_dtype,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["auto", "fp8_e5m2",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "fp8_e4m3"],                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help='Data type for kv cache        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ storage. "auto" will use model data type.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "fp8_e5m2" and "fp8_e4m3" is supported for CUDA â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 11.8+.',                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--quantization",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.quantization,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=[                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "awq",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "fp8",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "gptq",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "marlin",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "gptq_marlin",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "awq_marlin",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "bitsandbytes",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "gguf",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "modelopt",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "modelopt_fp4",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "w8a8_int8",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "w8a8_fp8",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "moe_wna16",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The quantization method.",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--quantization-param-path",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=nullable_str,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Path to the JSON file         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ containing the KV cache "                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "scaling factors. This should       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ generally be supplied, when "                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "KV cache dtype is FP8. Otherwise,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV cache scaling factors "                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "default to 1.0, which may cause    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ accuracy issues. ",                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--context-length",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.context_length,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The model's maximum context   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length. Defaults to None (will use the value    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from the model's config.json instead).",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--device",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The device to use ('cuda',    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 'xpu', 'hpu', 'cpu'). Defaults to               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ auto-detection if not specified.",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--served-model-name",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.served_model_naâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Override the model name       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ returned by the v1/models endpoint in OpenAI    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ API server.",                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--chat-template",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.chat_template,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The buliltin chat template    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ name or the path of the chat template file.     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ This is only used for OpenAI-compatible API     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server.",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--completion-template",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.completion_tempâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The buliltin completion       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ template name or the path of the completion     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ template file. This is only used for            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ OpenAI-compatible API server. only for code     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ completion currently.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--is-embedding",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Whether to use a CausalLM as  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ an embedding model.",                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--revision",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The specific model version to â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use. It can be a branch "                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "name, a tag name, or a commit id.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ If unspecified, will use "                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "the default version.",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Memory and scheduling                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--mem-fraction-static",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.mem_fraction_stâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The fraction of the memory    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ used for static allocation (model weights and   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV cache memory pool). Use a smaller value if   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you see out-of-memory errors.",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-running-requests",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.max_running_reqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of running â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ requests.",                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-total-tokens",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.max_total_tokenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in the memory pool. If not specified, it will   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ be automatically calculated based on the memory â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ usage fraction. "                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "This option is typically used for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ development and debugging purposes.",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--chunked-prefill-size",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.chunked_prefillâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in a chunk for the chunked prefill. Setting     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this to -1 means disabling chunked prefill.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-prefill-tokens",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.max_prefill_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The maximum number of tokens  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in a prefill batch. The real bound will be the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ maximum of this value and the model's maximum   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context length.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--schedule-policy",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.schedule_policy, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["lpm", "random", "fcfs",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "dfs-weight"],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The scheduling policy of the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ requests.",                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--schedule-conservativeness",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.schedule_conserâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="How conservative the schedule â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ policy is. A larger value means more            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ conservative scheduling. Use a larger value if  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you see requests being retracted frequently.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--cpu-offload-gb",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.cpu_offload_gb,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="How many GBs of RAM to        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reserve for CPU offloading.",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--page-size",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.page_size,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of tokens in a     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ page.",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Other runtime options                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tensor-parallel-size",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tp-size",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tp_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The tensor parallelism        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size.",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--stream-interval",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.stream_interval, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The interval (or buffer size) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for streaming in terms of the token length. A   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ smaller value makes streaming smoother, while a â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ larger value makes the throughput higher",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--stream-output",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Whether to output as a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sequence of disjoint segments.",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--random-seed",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.random_seed,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The random seed.",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--constrained-json-whitespace-patâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.constrained_jsoâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help=r"Regex pattern for syntactic  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ whitespaces allowed in JSON constrained output. â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ For example, to allow the model generate        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ consecutive whitespaces, set the pattern to     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [\n\t ]*",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--watchdog-timeout",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.watchdog_timeouâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set watchdog timeout in       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seconds. If a forward batch takes longer than   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this, the server will crash to prevent          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hanging.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dist-timeout",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.dist_timeout,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set timeout for               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.distributed initialization.",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--download-dir",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.download_dir,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Model download directory for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ huggingface.",                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--base-gpu-id",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.base_gpu_id,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The base GPU ID to start      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allocating GPUs from. Useful when running       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multiple instances on the same machine.",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--gpu-id-step",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.gpu_id_step,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The delta between consecutive â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GPU IDs that are used. For example, setting it  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to 2 will use GPU 0,2,4,...",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Logging                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-level",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.log_level,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The logging level of all      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ loggers.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-level-http",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.log_level_http,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The logging level of HTTP     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server. If not set, reuse --log-level by        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-requests",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Log metadata, inputs, outputs â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of all requests. The verbosity is decided by    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --log-requests-level",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--log-requests-level",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="0: Log metadata. 1. Log       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata and partial input/output. 2. Log every â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input/output.",                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=[0, 1, 2],                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--show-time-cost",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Show time cost of custom      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ marks.",                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-metrics",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable log prometheus         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metrics.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--decode-log-interval",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.decode_log_inteâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The log interval of decode    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch.",                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # API related                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--api-key",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.api_key,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set API key of the server. It â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is also used in the OpenAI API compatible       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server.",                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--file-storage-path",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.file_storage_paâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the file storage  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in backend.",                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-cache-report",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Return number of cached       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens in usage.prompt_tokens_details for each  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ openai request.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--reasoning-parser",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=list(ReasoningParser.Detecâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.reasoning_parseâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help=f"Specify the parser for       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reasoning models, supported parsers are:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ {list(ReasoningParser.DetectorMap.keys())}.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Data parallelism                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--data-parallel-size",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dp-size",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.dp_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The data parallelism size.",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--load-balance-method",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.load_balance_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The load balancing strategy   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for data parallelism.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=[                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "round_robin",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "shortest_queue",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ],                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Expert parallelism                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--expert-parallel-size",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ep-size",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ep_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The expert parallelism        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size.",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Multi-node distributed serving        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--dist-init-addr",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--nccl-init-addr",  # For backward â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compatbility. This will be removed in the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ future.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The host address for          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ initializing distributed backend (e.g.,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `192.168.0.2:25000`).",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--nnodes", type=int,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.nnodes, help="The number of  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ nodes."                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--node-rank", type=int,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default=ServerArgs.node_rank, help="The node    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ rank."                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Model override args                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--json-model-override-args",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="A dictionary in JSON string   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ format used to override default model           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configurations.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.json_model_overâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # LoRA                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--lora-paths",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nargs="*",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=None,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action=LoRAPathAction,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The list of LoRA adapters.    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ You can provide a list of either path in str or â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ renamed path in the format {name}={path}.",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--max-loras-per-batch",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=8,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Maximum number of adapters    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for a running batch, include base-only          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ request.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--lora-backend",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default="triton",                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Choose the kernel backend for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi-LoRA serving.",                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Kernel backend                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--attention-backend",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["flashinfer", "triton",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "torch_native", "fa3", "flashmla"],             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.attention_backeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Choose the kernels for        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention layers.",                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--sampling-backend",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["flashinfer", "pytorch"],  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.sampling_backenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Choose the kernels for        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sampling layers.",                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--grammar-backend",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["xgrammar", "outlines",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "llguidance", "none"],                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.grammar_backend, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Choose the backend for        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ grammar-guided decoding.",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-flashinfer-mla",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action=DeprecatedAction,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="--enable-flashinfer-mla is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deprecated. Please use '--attention-backend     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flashinfer' instead.",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-flashmla",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action=DeprecatedAction,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="--enable-flashmla is          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deprecated. Please use '--attention-backend     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flashmla' instead.",                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--flashinfer-mla-disable-ragged",  â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 97779 Output: 2303                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 7: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '860,1720p'                â”‚             "--flashinfer-mla-disable-ragged",  â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚             help="Not using ragged prefill      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ wrapper when running flashinfer mla",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Speculative decoding                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-algorithm",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["EAGLE", "EAGLE3",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "NEXTN"],                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Speculative algorithm.",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-draft-model-path",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the draft model   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ weights. This can be a local folder or a        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Hugging Face repo ID.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-num-steps",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of steps sampled   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from draft model in Speculative Decoding.",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-eagle-topk",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of tokens sampled  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from the draft model in eagle2 each step.",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_eagâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-num-draft-tokens",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of tokens sampled  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from the draft model in Speculative Decoding.", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-accept-threshold-siâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Accept a draft token if its   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ probability in the target model is greater than â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this threshold.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_accâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-accept-threshold-acâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The accept probability of a   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ draft token is raised from its target           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ probability p to min(1, p / threshold_acc).",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_accâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--speculative-token-map",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the draft model's â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ small vocab table.",                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.speculative_tokâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Double Sparsity                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-double-sparsity",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable double sparsity        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention",                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ds-channel-config-path",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ds_channel_confâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The path of the double        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sparsity channel config",                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ds-heavy-channel-num",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ds_heavy_channeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of heavy channels  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in double sparsity attention",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ds-heavy-token-num",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ds_heavy_token_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of heavy tokens in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ double sparsity attention",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ds-heavy-channel-type",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ds_heavy_channeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The type of heavy channels in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ double sparsity attention",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--ds-sparse-decode-threshold",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.ds_sparse_decodâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The type of heavy channels in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ double sparsity attention",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Optimization/debug options            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-radix-cache",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable RadixAttention for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefix caching.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-cuda-graph",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable cuda graph.",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-cuda-graph-padding",     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable cuda graph when       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ padding is needed. Still uses cuda graph when   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ padding is not needed.",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-nccl-nvls",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable NCCL NVLS for prefill  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ heavy requests when available.",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-outlines-disk-cache",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable disk cache of         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ outlines to avoid possible crashes related to   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ file system or high concurrency.",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-custom-all-reduce",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable the custom all-reduce â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernel and fall back to NCCL.",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-llama4-multimodal",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.enable_llama4_mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable the multimodal         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ functionality for Llama-4.",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-overlap-schedule",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable the overlap           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scheduler, which overlaps the CPU scheduler     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with GPU model worker.",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-mixed-chunk",             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enabling mixing prefill and   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decode in a batch when using chunked prefill.", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-dp-attention",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enabling data parallelism for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention and tensor parallelism for FFN. The   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dp size should be equal to the tp size.         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Currently only DeepSeek-V2 is supported.",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-ep-moe",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enabling expert parallelism   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for moe. The ep size is equal to the tp size.", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-torch-compile",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Optimize the model with       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.compile. Experimental feature.",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--torch-compile-max-bs",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.torch_compile_mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set the maximum batch size    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ when using torch compile.",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--cuda-graph-max-bs",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.cuda_graph_max_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set the maximum batch size    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for cuda graph.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--cuda-graph-bs",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             nargs="+",                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Set the list of batch sizes   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for cuda graph.",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--torchao-config",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.torchao_config,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Optimize the model with       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torchao. Experimental feature. Current choices  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ are: int8dq, int8wo, int4wo-<group_size>,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fp8wo, fp8dq-per_tensor, fp8dq-per_row",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-nan-detection",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable the NaN detection for  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ debugging purposes.",                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-p2p-check",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable P2P check for GPU      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ access, otherwise the p2p access is allowed by  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--triton-attention-reduce-in-fp32â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Cast the intermidiate         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention results to fp32 to avoid possible     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ crashes related to fp16."                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "This only affects Triton attention â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernels.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--triton-attention-num-kv-splits", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.triton_attentioâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of KV splits in    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash decoding Triton kernel. Larger value is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ better in longer context scenarios. The default â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ value is 8.",                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--num-continuous-decode-steps",    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.num_continuous_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Run multiple continuous       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decoding steps to reduce scheduling overhead. " â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "This can potentially increase      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ throughput but may also increase                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ time-to-first-token latency. "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "The default value is 1, meaning    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ only run one decoding step at a time.",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--delete-ckpt-after-loading",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Delete the model checkpoint   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ after loading the model.",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-memory-saver",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Allow saving memory using     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ release_memory_occupation and                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ resume_memory_occupation",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--allow-auto-truncate",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Allow automatically           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ truncating requests that exceed the maximum     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ input length instead of returning an error.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-custom-logit-processor",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable users to pass custom   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logit processors to the server (disabled by     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default for security)",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--tool-call-parser",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["qwen25", "mistral",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "llama3", "deepseekv3"],                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.tool_call_parseâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Specify the parser for        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ handling tool-call interactions. Options        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ include: 'qwen25', 'mistral', and 'llama3'.",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-hierarchical-cache",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enable hierarchical cache",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--hicache-ratio",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=float,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             required=False,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.hicache_ratio,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The ratio of the size of host â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV cache memory pool to the size of device      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pool.",                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--enable-deepep-moe",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Enabling DeepEP MoE           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ implementation for EP MoE.",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--moe-dense-tp-size",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.moe_dense_tp_siâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="TP size for MoE dense MLP     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers. This flag is useful when, with large TP â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ size, there are errors caused by weights in MLP â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers having dimension smaller than the min    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dimension GEMM supports.",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--deepep-mode",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["normal", "low_latency",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "auto"],                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default="auto",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Select the mode when enable   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepEP MoE, could be `normal`, `low_latency` or â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `auto`. Default is `auto`, which means          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `low_latency` for decode batch and `normal` for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefill batch.",                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--n-share-experts-fusion",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The number of shared_experts  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ need to be replicated to fuse with normal       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ experts in deepseek v3/r1, "                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "set it to tp_size can get best     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ optimized performace.",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-chunked-prefix-cache",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Disable chunked prefix cache  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ feature for deepseek, which should save         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ overhead for short sequences.",                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disable-fast-image-processor",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             action="store_true",                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Adopt base image processor    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ instead of fast image processor.",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Server warmups                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--warmups",                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             required=False,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Specify custom warmup         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ functions (csv) to run before server starts eg. â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --warmups=warmup_name1,warmup_name2 "           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "will run the functions             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `warmup_name1` and `warmup_name2` specified in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ warmup.py before the server starts listening    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for requests",                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Debug tensor dumps                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--debug-tensor-dump-output-folderâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.debug_tensor_duâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The output folder for dumping â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensors.",                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--debug-tensor-dump-input-file",   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.debug_tensor_duâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The input filename for        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dumping tensors",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--debug-tensor-dump-inject",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.debug_tensor_duâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Inject the outputs from jax   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ as the input of every layer.",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Disaggregation                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disaggregation-mode",            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default="null",                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             choices=["null", "prefill",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "decode"],                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help='Only used for PD              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disaggregation. "prefill" for prefill-only      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server, and "decode" for decode-only server. If â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not specified, it is not PD disaggregated',     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disaggregation-bootstrap-port",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=int,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.disaggregation_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="Bootstrap server port on the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefill server. Default is 8998.",              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disaggregation-transfer-backendâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.disaggregation_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The backend for               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disaggregation transfer. Default is mooncake.", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         parser.add_argument(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "--disaggregation-ib-device",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             type=str,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             default=ServerArgs.disaggregation_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             help="The ib device for             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ disaggregation transfer. Default is None, it    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ will be detected automatically if using the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mooncake backend.",                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @classmethod                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def from_cli_args(cls, args:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ argparse.Namespace):                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         args.tp_size =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.tensor_parallel_size                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         args.dp_size = args.data_parallel_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         args.ep_size =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ args.expert_parallel_size                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         attrs =                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return cls(**{attr: getattr(args, attr) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for attr in attrs})                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def url(self):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if is_valid_ipv6_address(self.host):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"http://[{self.host}]:{self.port}"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ f"http://{self.host}:{self.port}"               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def check_server_args(self):                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.tp_size % self.nnodes == 0     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "tp_size must be divisible by number â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of nodes"                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert not (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.dp_size > 1 and self.nnodes != â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1 and not self.enable_dp_attention              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "multi-node data parallel is not     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported unless dp attention!"                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.max_loras_per_batch > 0        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # FIXME                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and (self.lora_paths is None or     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_cuda_graph)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and (self.lora_paths is None or     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.disable_radix_cache)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "compatibility of lora and cuda      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ graph and radix attention is in progress"       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert self.base_gpu_id >= 0,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "base_gpu_id must be non-negative"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert self.gpu_id_step >= 1,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "gpu_id_step must be positive"                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if isinstance(self.lora_paths, list):   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             lora_paths = self.lora_paths        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.lora_paths = {}                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             for lora_path in lora_paths:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if "=" in lora_path:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     name, path =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora_path.split("=", 1)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.lora_paths = path      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.lora_paths = lora_path â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def prepare_server_args(argv: List) ->          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs:                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Prepare the server arguments from the       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ command line arguments.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Args:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         args: The command line arguments.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Typically, it should be `sys.argv[1:]`          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             to ensure compatibility with        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `parse_args` when no arguments are passed.      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Returns:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         The server arguments.                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     parser = argparse.ArgumentParser()          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ServerArgs.add_cli_args(parser)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     raw_args = parser.parse_args(argv)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     server_args =                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs.from_cli_args(raw_args)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return server_args                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ZMQ_TCP_PORT_DELTA = 233                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclasses.dataclass                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class PortArgs:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # The ipc filename for tokenizer to receive â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ inputs from detokenizer (zmq)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_ipc_name: str                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # The ipc filename for scheduler (rank 0)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to receive inputs from tokenizer (zmq)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     scheduler_input_ipc_name: str               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # The ipc filename for detokenizer to       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ receive inputs from scheduler (zmq)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     detokenizer_ipc_name: str                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # The port for nccl initialization          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (torch.dist)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     nccl_port: int                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # The ipc filename for rpc call between     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Engine and Scheduler                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     rpc_ipc_name: str                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @staticmethod                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_new(server_args, dp_rank: Optional â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = None) -> "PortArgs":                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         port = server_args.port +               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ random.randint(100, 1000)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         while True:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if is_port_available(port):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 break                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if port < 60000:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 port += 42                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 port -= 43                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not server_args.enable_dp_attention: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Normal case, use IPC within a     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ single node                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return PortArgs(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenizer_ipc_name=f"ipc://{teâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 scheduler_input_ipc_name=f"ipcâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 detokenizer_ipc_name=f"ipc://{â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 nccl_port=port,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 rpc_ipc_name=f"ipc://{tempfileâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # DP attention. Use TCP + port to   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ handle both single-node and multi-node.         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if server_args.nnodes == 1 and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dist_init_addr is None:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dist_init_addr = ("127.0.0.1",  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.port + ZMQ_TCP_PORT_DELTA)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dist_init_addr.startswith("["):  #  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ipv6 address                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 port_num, host =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ configure_ipv6(server_args.dist_init_addr)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dist_init_addr = (host,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ str(port_num))                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dist_init_addr =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ server_args.dist_init_addr.split(":")           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 len(dist_init_addr) == 2        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ), "please provide --dist-init-addr â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ as host:port of head node"                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             dist_init_host, dist_init_port =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dist_init_addr                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             port_base = int(dist_init_port) + 1 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if dp_rank is None:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 scheduler_input_port = (        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     port_base + 3               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # TokenizerManager to        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DataParallelController                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 scheduler_input_port =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ port_base + 3 + 1 + dp_rank                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return PortArgs(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 tokenizer_ipc_name=f"tcp://{diâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 scheduler_input_ipc_name=f"tcpâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 detokenizer_ipc_name=f"tcp://{â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1}",                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 nccl_port=port,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 rpc_ipc_name=f"tcp://{dist_iniâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 2}",                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class LoRAPathAction(argparse.Action):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __call__(self, parser, namespace,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ values, option_string=None):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         setattr(namespace, self.dest, {})       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for lora_path in values:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if "=" in lora_path:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 name, path =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora_path.split("=", 1)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 getattr(namespace, self.dest) = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ path                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 getattr(namespace, self.dest) = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora_path                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class DeprecatedAction(argparse.Action):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(self, option_strings, dest,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ nargs=0, **kwargs):                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         super(DeprecatedAction, self).__init__( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             option_strings, dest, nargs=nargs,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ **kwargs                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __call__(self, parser, namespace,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ values, option_string=None):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         raise ValueError(self.help)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def auto_choose_speculative_params(self:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ServerArgs):                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Automatically choose the parameters for     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative decoding.                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     You can tune them on your own models and    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prompts with                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ scripts/playground/bench_speculative.py         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if self.decrypted_config_file:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config_path =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decrypted_config_file                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config_path =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ os.path.join(self.model_path, "config.json")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if not os.path.exists(config_path):         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         raise ValueError(f"{config_path} is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ found.")                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     config = json.load(open(config_path))       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     arch = config.get("architectures",          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["Unknown"])[0]                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if arch in ["LlamaForCausalLM"]:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # The default value for llama           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return (5, 4, 8)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     elif arch in ["DeepseekV3ForCausalLM",      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "DeepseekV2ForCausalLM"]:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # The default value for deepseek        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return (5, 4, 8)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     elif arch in ["Grok1ForCausalLM",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "Grok1VForCausalLM"]:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return (5, 4, 8)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # The default value for all other       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ models                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return (5, 4, 8)                        â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 131973 Output: 2376                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 8: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "ls -la                            â”‚ total 124                                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ drwxrwxr-x 31 ubuntu ubuntu 4096 Nov 15 07:55 . â”‚   â”‚
â”‚             â”‚ â”‚ | sed -n '1,200p'", 'restart': False}          â”‚ drwxrwxr-x 15 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ..                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bench_in_batch_prefix                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ benchmark_vllm_060                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ blog_v0_2                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ deepseek_v3                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dspy                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ generative_agents                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ gsm8k                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hellaswag                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hicache                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ json_decode_regex                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ json_jump_forward                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ json_schema                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  9 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kernels                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ line_retrieval                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ llava_bench                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ llm_judge                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ long_json_decode                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ lora                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mmlu                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mmmu                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mtbench                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi_chain_reasoning                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi_document_qa                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ multi_turn_chat                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ react                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  3 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ reasoning_benchmark                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tip_suggestion                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tree_of_thought_deep                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ drwxrwxr-x  2 ubuntu ubuntu 4096 Nov 15 07:55   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tree_of_thought_v0                              â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 170619 Output: 2575                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 9: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,220p'                   â”‚ # Copyright 2023-2024 SGLang Team               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ # Licensed under the Apache License, Version    â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ 2.0 (the "License");                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # you may not use this file except in           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # You may obtain a copy of the License at       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Unless required by applicable law or agreed   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to in writing, software                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # distributed under the License is distributed  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ on an "AS IS" BASIS,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ either express or implied.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # See the License for the specific language     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ governing permissions and                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # limitations under the License.                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import json                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import logging                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import math                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import os                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from enum import IntEnum, auto                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import List, Optional, Set, Union   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import torch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from transformers import PretrainedConfig       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.hf_transformers_utils import    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_config, get_context_length                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.layers.quantization import      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ QUANTIZATION_METHODS                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import get_bool_env_var,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_hip                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ logger = logging.getLogger(__name__)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class AttentionArch(IntEnum):                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     MLA = auto()                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     MHA = auto()                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class ModelConfig:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_path: str,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         trust_remote_code: bool = True,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         revision: Optional = None,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         context_length: Optional = None,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_override_args: Optional = None,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         is_embedding: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         enable_multimodal: Optional = None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         dtype: str = "auto",                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         quantization: Optional = None,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         override_config_file: Optional = None,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> None:                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_path = model_path            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.revision = revision                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.quantization = quantization        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Parse args                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.maybe_pull_model_tokenizer_from_râ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_override_args =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ json.loads(model_override_args)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kwargs = {}                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if override_config_file and             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ override_config_file.strip():                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             kwargs["_configuration_file"] =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ override_config_file.strip()                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.hf_config = get_config(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.model_path,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             trust_remote_code=trust_remote_codâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             revision=revision,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_override_args=self.model_oveâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             **kwargs,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.hf_text_config =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_hf_text_config(self.hf_config)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.attention_chunk_size = getattr(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_text_config,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "attention_chunk_size", None                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if enable_multimodal is None:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.hf_config.architectures[0]  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "Llama4ForConditionalGeneration":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 enable_multimodal = False       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 logger.info(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "Multimodal is disabled for â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Llama4. To enable it, set                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ --enable-llama4-multimodal."                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 enable_multimodal = True        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Check model type                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_generation =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_generation_model(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.architectures,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_embedding                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_multimodal = enable_multimodal  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and is_multimodal_model(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.architectures        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_multimodal_gen =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enable_multimodal and is_multimodal_gen_model(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.architectures        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_image_gen = enable_multimodal   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and is_image_gen_model(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.architectures        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_audio_model = enable_multimodal â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and is_audio_model(                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.architectures        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.is_encoder_decoder =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is_encoder_decoder_model(self.hf_config.architâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.dtype =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _get_and_verify_dtype(self.hf_text_config,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Derive context length                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         derived_context_len =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_context_length(self.hf_text_config)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if context_length is not None:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if context_length >                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ derived_context_len:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if get_bool_env_var(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "SGLANG_ALLOW_OVERWRITE_LOâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ default="True"                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ):                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     logger.warning(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"Warning:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ User-specified context_length                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ({context_length}) is greater than the derived  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context_length ({derived_context_len}). "       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"This may lead to      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ incorrect model outputs or CUDA errors."        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.context_len =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context_length                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     raise ValueError(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"User-specified        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context_length ({context_length}) is greater    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ than the derived context_length                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ({derived_context_len}). "                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"This may lead to      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ incorrect model outputs or CUDA errors. Note    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ that the derived context_length may differ from â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_position_embeddings in the model's config.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         f"To allow overriding   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this maximum, set the env var                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.context_len =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ context_length                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.context_len =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ derived_context_len                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Unify the config keys for             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ hf_text_config                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.head_dim = getattr(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_text_config,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "head_dim",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_text_config.hidden_size //  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.num_attention_heads,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # FIXME: temporary special judge for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MLA architecture                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if (                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "DeepseekV2ForCausalLM" in          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.architectures                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             or "DeepseekV3ForCausalLM" in       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.architectures                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             or "DeepseekV3ForCausalLMNextN" in  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.architectures                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.head_dim = 256                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_arch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch.MLA                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.kv_lora_rank =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.kv_lora_rank                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.qk_nope_head_dim =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.qk_nope_head_dim                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.qk_rope_head_dim =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.qk_rope_head_dim                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.v_head_dim =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.v_head_dim                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Handle rope scaling with yarn     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.scaling = 1 /                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ math.sqrt(self.qk_nope_head_dim +               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.qk_rope_head_dim)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.hf_config.rope_scaling:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mscale_all_dim =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.rope_scaling.get(                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "mscale_all_dim", False     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 scaling_factor =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.rope_scaling["factor"]           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mscale =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ yarn_get_mscale(scaling_factor,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ float(mscale_all_dim))                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.scaling = self.scaling *   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ mscale * mscale                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif "MiniCPM3ForCausalLM" in           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.architectures:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.head_dim = 128                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_arch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch.MLA                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.kv_lora_rank =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.kv_lora_rank                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.qk_rope_head_dim =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.qk_rope_head_dim                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif "DeepseekVL2ForCausalLM" in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.architectures:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.head_dim = 256                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_arch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch.MLA                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.kv_lora_rank =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.kv_lora_rank                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.qk_rope_head_dim =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.qk_rope_head_dim            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_arch =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch.MHA                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.num_attention_heads =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.num_attention_heads         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.num_key_value_heads = getattr(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_text_config,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "num_key_value_heads", None                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # for Dbrx and MPT models               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.hf_config.model_type in         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["dbrx", "mpt"]:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.num_key_value_heads = getattr( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.hf_config.attn_config,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "kv_n_heads", None                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.num_key_value_heads is None:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.num_key_value_heads =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.num_attention_heads                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.hidden_size =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.hidden_size                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.num_hidden_layers =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.num_hidden_layers           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.vocab_size =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_text_config.vocab_size                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Verify quantization                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self._verify_quantization()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Cache attributes                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.hf_eos_token_id =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.get_hf_eos_token_id()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.image_token_id =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ getattr(self.hf_config, "image_token_id", None) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # adapted from                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/vllm-project/vllm/blob/mainâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def get_total_num_kv_heads(self) -> int:    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Returns the total number of KV       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ heads."""                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # For GPTBigCode & Falcon:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # NOTE: for falcon, when                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ new_decoder_architecture is True, the           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # multi_query flag is ignored and we    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use n_head_kv for the number of                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # KV heads.                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         falcon_model_types = ["falcon",         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "RefinedWeb", "RefinedWebModel"]                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         new_decoder_arch_falcon = (             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_config.model_type in        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ falcon_model_types                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and getattr(self.hf_config,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "new_decoder_architecture", False)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not new_decoder_arch_falcon and      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ getattr(                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.hf_text_config, "multi_query", â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ False                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Multi-query attention, only one   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ KV head.                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Currently, tensor parallelism is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not supported in this case.                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # For DBRX and MPT                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.hf_config.model_type in         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["mpt"]:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if "kv_n_heads" in                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.attn_config:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.attn_config["kv_n_heads"]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.hf_config.num_attention_heads              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.hf_config.model_type in         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["dbrx"]:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return getattr(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.hf_config.attn_config,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "kv_n_heads",                   â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 210301 Output: 4827                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 10: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,220p'                   â”‚ # Copyright 2023-2024 SGLang Team               â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ # Licensed under the Apache License, Version    â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ 2.0 (the "License");                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # you may not use this file except in           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compliance with the License.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # You may obtain a copy of the License at       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ http://www.apache.org/licenses/LICENSE-2.0      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Unless required by applicable law or agreed   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to in writing, software                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # distributed under the License is distributed  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ on an "AS IS" BASIS,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ either express or implied.                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # See the License for the specific language     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ governing permissions and                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # limitations under the License.                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ==============================================â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ """Utilities for Huggingface Transformers."""   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import contextlib                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import os                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import warnings                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from pathlib import Path                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import Dict, Optional, Type, Union  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from huggingface_hub import snapshot_download   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from transformers import (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AutoConfig,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AutoProcessor,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     AutoTokenizer,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     PretrainedConfig,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     PreTrainedTokenizer,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     PreTrainedTokenizerFast,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from transformers.models.auto.modeling_auto     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.configs import (                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ChatGLMConfig,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DbrxConfig,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DeepseekVL2Config,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ExaoneConfig,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     MultiModalityConfig,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ )                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.connector import                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ create_remote_connector                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.utils import is_remote_url      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _CONFIG_REGISTRY: Dict[str,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Type[PretrainedConfig]] = {                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ChatGLMConfig.model_type: ChatGLMConfig,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DbrxConfig.model_type: DbrxConfig,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ExaoneConfig.model_type: ExaoneConfig,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     DeepseekVL2Config.model_type:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ DeepseekVL2Config,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     MultiModalityConfig.model_type:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MultiModalityConfig,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ }                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for name, cls in _CONFIG_REGISTRY.items():      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     with contextlib.suppress(ValueError):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         AutoConfig.register(name, cls)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def download_from_hf(model_path: str):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if os.path.exists(model_path):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return model_path                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return snapshot_download(model_path,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ allow_patterns=["*.json", "*.bin", "*.model"])  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def get_config(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model: str,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     trust_remote_code: bool,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     revision: Optional = None,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model_override_args: Optional = None,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     **kwargs,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     is_gguf = check_gguf_file(model)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if is_gguf:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kwargs["gguf_file"] = model             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model = Path(model).parent              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     config = AutoConfig.from_pretrained(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ trust_remote_code=trust_remote_code,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ revision=revision, **kwargs                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # FIXME: Pour contents of janus-pro's       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ langauge_config to first-level                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if isinstance(model, str) and               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model.lower().startswith("deepseek-ai/janus-prâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert hasattr(config,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "language_config")                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for key, val in                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config.language_config.__dict__.items():        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             setattr(config, key, val)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         setattr(config, "architectures",        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ["MultiModalityCausalLM"])                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if config.model_type in _CONFIG_REGISTRY:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config_class = _CONFIG_REGISTRY         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config =                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config_class.from_pretrained(model,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ revision=revision)                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # NOTE(HandH1998): Qwen2VL requires     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `_name_or_path` attribute in `config`.          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         setattr(config, "_name_or_path", model) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if model_override_args:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config.update(model_override_args)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Special architecture mapping check for    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ GGUF models                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if is_gguf:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if config.model_type not in             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MODEL_FOR_CAUSAL_LM_MAPPING_NAMES:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise RuntimeError(f"Can't get gguf â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ config for {config.model_type}.")               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_type =                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ MODEL_FOR_CAUSAL_LM_MAPPING_NAMES               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         config.update({"architectures": })      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return config                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Models don't use the same configuration key   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for determining the maximum                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # context length.  Store them here so we can    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sanely check them.                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # NOTE: The ordering here is important. Some    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ models have two of these and we                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # have a preference for which value gets used.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ CONTEXT_LENGTH_KEYS = [                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "max_sequence_length",                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "seq_length",                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "max_seq_len",                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "model_max_length",                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     "max_position_embeddings",                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ]                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def get_context_length(config):                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Get the context length of a model from a â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ huggingface model configs."""                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     text_config = config                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     rope_scaling = getattr(text_config,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "rope_scaling", None)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if rope_scaling:                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         rope_scaling_factor =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ rope_scaling.get("factor", 1)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if "original_max_position_embeddings"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in rope_scaling:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             rope_scaling_factor = 1             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if rope_scaling.get("rope_type", None)  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == "llama3":                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             rope_scaling_factor = 1             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         rope_scaling_factor = 1                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     for key in CONTEXT_LENGTH_KEYS:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         val = getattr(text_config, key, None)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if val is not None:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return int(rope_scaling_factor *    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ val)                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return 2048                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # A fast LLaMA tokenizer with the pre-processed â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `tokenizer.json` file.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _FAST_LLAMA_TOKENIZER =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "hf-internal-testing/llama-tokenizer"           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def get_tokenizer(                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_name: str,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     *args,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_mode: str = "auto",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     trust_remote_code: bool = False,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_revision: Optional = None,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     **kwargs,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ) -> Union[PreTrainedTokenizer,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PreTrainedTokenizerFast]:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Gets a tokenizer for the given model     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ name via Huggingface."""                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if tokenizer_mode == "slow":                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if kwargs.get("use_fast", False):       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise ValueError("Cannot use the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ fast tokenizer in slow tokenizer mode.")        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kwargs["use_fast"] = False              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     is_gguf = check_gguf_file(tokenizer_name)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if is_gguf:                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kwargs["gguf_file"] = tokenizer_name    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_name =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Path(tokenizer_name).parent                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if is_remote_url(tokenizer_name):           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # BaseConnector implements __del__() to â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ clean up the local dir.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Since config files need to exist all  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the time, so we DO NOT use                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # with statement to avoid closing the   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ client.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         client =                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ create_remote_connector(tokenizer_name)         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         client.pull_files(ignore_pattern=["*.pâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "*.safetensors", "*.bin"])                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_name = client.get_local_dir() â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     try:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer =                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AutoTokenizer.from_pretrained(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenizer_name,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             *args,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             trust_remote_code=trust_remote_codâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             tokenizer_revision=tokenizer_revisâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             clean_up_tokenization_spaces=False, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             **kwargs,                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     except TypeError as e:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # The LLaMA tokenizer causes a protobuf â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ error in some environments.                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         err_msg = (                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "Failed to load the tokenizer. If   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ you are using a LLaMA V1 model "                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             f"consider using                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ '{_FAST_LLAMA_TOKENIZER}' instead of the "      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "original tokenizer."               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         raise RuntimeError(err_msg) from e      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     except ValueError as e:                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # If the error pertains to the          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer class not existing or not             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # currently being imported, suggest     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ using the --trust-remote-code flag.             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not trust_remote_code and (          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "does not exist or is not currently â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ imported." in str(e)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             or "requires you to execute the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer file" in str(e)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             err_msg = (                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "Failed to load the tokenizer.  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ If the tokenizer is a custom "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "tokenizer not yet available in â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the HuggingFace transformers "                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "library, consider setting      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `trust_remote_code=True` in LLM "               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "or using the                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `--trust-remote-code` flag in the CLI."         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise RuntimeError(err_msg) from e  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             raise e                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if not isinstance(tokenizer,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ PreTrainedTokenizerFast):                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         warnings.warn(                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "Using a slow tokenizer. This might â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cause a significant "                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "slowdown. Consider using a fast    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer instead."                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     attach_additional_stop_token_ids(tokenizer) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return tokenizer                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def get_processor(                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_name: str,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     *args,                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_mode: str = "auto",               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     trust_remote_code: bool = False,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokenizer_revision: Optional = None,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     use_fast: Optional = True,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     **kwargs,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 254330 Output: 4904                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 11: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ğŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '220,640p'                 â”‚ ):                                              â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚     # pop 'revision' from kwargs if present.    â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚     revision = kwargs.pop("revision",           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer_revision)                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     config = AutoConfig.from_pretrained(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_name,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         trust_remote_code=trust_remote_code,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         revision=revision,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         **kwargs,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # fix: for Qwen2-VL model, inject default   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 'size' if not provided.                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if config.model_type in {"qwen2_vl"}:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if "size" not in kwargs:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             kwargs["size"] = {"shortest_edge":  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3136, "longest_edge": 1003520}                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if config.model_type not in {"llava",       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "clip"}:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         kwargs["use_fast"] = use_fast           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     processor = AutoProcessor.from_pretrained(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer_name,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         *args,                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         trust_remote_code=trust_remote_code,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         revision=revision,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         **kwargs,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     attach_additional_stop_token_ids(processorâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return processor                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attach_additional_stop_token_ids(tokenizer):    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Special handling for stop token           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ <|eom_id|> generated by llama 3 tool use.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if "<|eom_id|>" in                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokenizer.get_added_vocab():                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer.additional_stop_token_ids =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set(                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             [tokenizer.get_added_vocab()["<|eoâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     else:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         tokenizer.additional_stop_token_ids =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def check_gguf_file(model: Union) -> bool:      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Check if the file is a GGUF model."""    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     model = Path(model)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if not model.is_file():                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return False                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     elif model.suffix == ".gguf":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return True                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     with open(model, "rb") as f:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         header = f.read(4)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return header == b"GGUF"                    â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 300268 Output: 4979                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 12: Completed                                                                                  â”‚
â”‚ Error       â”‚ âŒ Unterminated string starting at: line 1 column 33 (char 32)                                         â”‚
â”‚ Total       â”‚ Input: 300268 Output: 4979                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

============================================================
Lakeview Summary
============================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ğŸ‘ï¸EXAMINE_CODE] The agent <task>Exploring repository structure.              â”‚
â”‚ Listing sglang_core worktree to locate target files, preparing to grep for   â”‚
â”‚ torch.zeros/fill_ occurrences related to tokenization throughput issues.     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ğŸ‘ï¸EXAMINE_CODE] The agent <task>Examining source code.                       â”‚
â”‚ Running grep to find torch.zeros or fill_ occurrences in                     â”‚
â”‚ moe_align_block_size.py and its benchmark to locate problematic tensor       â”‚
â”‚ initialization patterns.                                                     â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ğŸ‘ï¸EXAMINE_CODE] The agent <task>Examining source code.                       â”‚
â”‚ Viewing sglang/srt/managers/tokenizer_manager.py to understand tokenization  â”‚
â”‚ manager implementation for potential batching and caching optimizations      â”‚
â”‚ impacting tokenization throughput.                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ğŸ‘ï¸EXAMINE_CODE] The agent <task>Examining source code.                       â”‚
â”‚ Viewing tokenizer_manager.py to analyze tokenization logic and spot          â”‚
â”‚ opportunities for batching or caching related to the performance issue.      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Unexpected error: list index out of range
Traceback (most recent call last):
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/cli.py", line 212, in run
    _ = asyncio.run(agent.run(task, task_args))
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 194, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/agent/agent.py", line 93, in run
    await cli_console_task
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 119, in start
    await self._print_lakeview_summary()
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 154, in _print_lakeview_summary
    lake_view_panel = await step.lake_view_panel_generator
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/cli/simple_console.py", line 306, in _create_lakeview_step_display
    lake_view_step = await self.lake_view.create_lakeview_step(agent_step)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line 205, in create_lakeview_step
    tags = await self.extract_tag_in_step(this_step_str)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/OmniPerf-Bench/third-party/trae-agent/trae_agent/utils/lake_view.py", line 169, in extract_tag_in_step
    tags: list = [tag.strip() for tag in matched_tags[0].split(",")]
                                              ~~~~~~~~~~~~^^^
IndexError: list index out of range

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-bd68ff67/sglang_077_f0815419/trajectory.json