{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-0f75b90", "model_patch": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex f200a367b..76ff496c7 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1131,20 +1131,20 @@ class FlashAttentionBackend(AttentionBackend):\n         \"\"\"\n         # This is being used by normal decode and draft decode when topk == 1\n         self.decode_cuda_graph_metadata = {\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cache_seqlens\": torch.empty(max_bs, dtype=torch.int32, device=self.device),\n             \"cu_seqlens_q\": torch.arange(\n                 0, max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n             \"cu_seqlens_k\": torch.zeros(\n                 max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-            \"page_table\": torch.zeros(\n+            \"page_table\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"page_table_draft_decode\": torch.zeros(\n+            \"page_table_draft_decode\": torch.empty(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n@@ -1165,19 +1165,18 @@ class FlashAttentionBackend(AttentionBackend):\n             max_virtual_batches = max_bs * (\n                 (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             )\n-            max_blocks_per_seq = (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size\n \n             self.decode_cuda_graph_local_attn_metadata = {\n-                \"local_query_start_loc\": torch.zeros(\n+                \"local_query_start_loc\": torch.empty(\n                     max_virtual_batches + 1, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_seqused_k\": torch.zeros(\n+                \"local_seqused_k\": torch.empty(\n                     max_virtual_batches, dtype=torch.int32, device=self.device\n                 ),\n-                \"local_block_table\": torch.zeros(\n+                \"local_block_table\": torch.empty(\n                     max_virtual_batches,\n-                    max_blocks_per_seq * max_pages_per_block,\n+                    max_pages_per_block,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),\n", "model_name_or_path": "gpt-5-2025-08-07"}
