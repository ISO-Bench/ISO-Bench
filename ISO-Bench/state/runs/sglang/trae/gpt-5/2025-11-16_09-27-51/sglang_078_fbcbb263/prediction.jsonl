{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-2fce449", "model_patch": "diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex 181ac7eef..e621f4496 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -22,7 +22,7 @@ BaseTokenToKVPool maps a token location to its KV cache data.\n \"\"\"\n \n import logging\n-from typing import List, Tuple, Union\n+from typing import List, Tuple, Union, TYPE_CHECKING\n \n import torch\n \n@@ -126,11 +126,16 @@ class BaseTokenToKVPool:\n         select_index = self.free_slots[:need_size]\n         self.free_slots = self.free_slots[need_size:]\n \n-        return select_index.to(self.device, non_blocking=True)\n+        # Avoid unnecessary device transfer when already on CPU\n+        if isinstance(self.device, str) and self.device.startswith(\"cuda\"):\n+            return select_index.to(self.device, non_blocking=True)\n+        return select_index\n \n     def free(self, free_index: torch.Tensor):\n         if self.is_not_in_free_group:\n-            self.free_slots = torch.concat((self.free_slots, free_index.cpu()))\n+            if free_index.device.type != \"cpu\":\n+                free_index = free_index.cpu()\n+            self.free_slots = torch.concat((self.free_slots, free_index))\n         else:\n             self.free_group.append(free_index)\n \n@@ -141,7 +146,11 @@ class BaseTokenToKVPool:\n     def free_group_end(self):\n         self.is_not_in_free_group = True\n         if self.free_group:\n-            self.free(torch.concat(self.free_group))\n+            free_tensor = torch.concat(self.free_group)\n+            if free_tensor.device.type != \"cpu\":\n+                free_tensor = free_tensor.cpu()\n+            self.free_slots = torch.concat((self.free_slots, free_tensor))\n+        self.free_group = []\n \n     def clear(self):\n         # The padded slot 0 is used for writing dummy outputs from padded tokens.\n@@ -221,18 +230,17 @@ class MHATokenToKVPool(BaseTokenToKVPool):\n         cache_v: torch.Tensor,\n     ):\n         layer_id = layer.layer_id\n-        copy_two_array(\n-            loc,\n-            self.k_buffer[layer_id],\n-            cache_k,\n-            self.v_buffer[layer_id],\n-            cache_v,\n-            self.dtype,\n-            self.store_dtype,\n-        )\n+        if cache_k.dtype != self.dtype:\n+            cache_k = cache_k.to(self.dtype)\n+            cache_v = cache_v.to(self.dtype)\n+        if self.store_dtype != self.dtype:\n+            self.k_buffer[layer_id][loc] = cache_k.view(self.store_dtype)\n+            self.v_buffer[layer_id][loc] = cache_v.view(self.store_dtype)\n+        else:\n+            self.k_buffer[layer_id][loc] = cache_k\n+            self.v_buffer[layer_id][loc] = cache_v\n \n \n-@torch.compile(dynamic=True)\n def copy_two_array(loc, dst_1, src_1, dst_2, src_2, dtype, store_dtype):\n     dst_1[loc] = src_1.to(dtype).view(store_dtype)\n     dst_2[loc] = src_2.to(dtype).view(store_dtype)\n", "model_name_or_path": "gpt-5-2025-08-07"}
