{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-b04df75", "model_patch": "diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py\nindex 0e1640fcf..020daf739 100644\n--- a/python/sglang/srt/layers/quantization/fp8_utils.py\n+++ b/python/sglang/srt/layers/quantization/fp8_utils.py\n@@ -332,27 +332,13 @@ def block_quant_to_tensor_quant(\n     assert n_tiles == x_s.shape[0]\n     assert k_tiles == x_s.shape[1]\n \n+    # Vectorized dequant: expand block scales to element-wise scale and multiply\n     x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [\n-        [\n-            x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            for i in range(k_tiles)\n-        ]\n-        for j in range(n_tiles)\n-    ]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n+    x_scale_repeat = x_s.repeat_interleave(block_n, dim=0).repeat_interleave(block_k, dim=1)\n+    x_dq_block = x_dq_block * x_scale_repeat[:n, :k]\n \n     x_q_tensor, scale = (\n-        scaled_fp8_quant(x_dq_block)\n-        if _is_cuda\n-        else input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n+        scaled_fp8_quant(x_dq_block) if _is_cuda else input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n     )\n     return x_q_tensor, scale\n \n@@ -375,21 +361,10 @@ def block_quant_dequant(\n     assert n_tiles == x_s.shape[0]\n     assert k_tiles == x_s.shape[1]\n \n-    x_dq_block = torch.empty_like(x_q_block, dtype=dtype)\n-\n-    for j in range(n_tiles):\n-        for i in range(k_tiles):\n-            x_q_block_tile = x_q_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile = x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile[:, :] = x_q_block_tile.to(torch.float32) * x_s[j][i]\n-\n-    return x_dq_block\n+    # Vectorized dequant: expand block scales and multiply in float32, then cast\n+    x_scale_repeat = x_s.repeat_interleave(block_n, dim=0).repeat_interleave(block_k, dim=1)\n+    x_dq_block = x_q_block.to(torch.float32) * x_scale_repeat[:n, :k]\n+    return x_dq_block.to(dtype)\n \n \n def channel_quant_to_tensor_quant(\n@@ -436,9 +411,10 @@ def _apply_fallback_scaled_mm(\n     output = _process_scaled_mm_output(output, input_2d_shape, output_shape)\n     x_scale = torch.narrow(x_scale, 0, 0, input_2d_shape[0])\n \n-    output = output * x_scale * weight_scale.t()\n+    output.mul_(x_scale)\n+    output.mul_(weight_scale.t())\n     if bias is not None:\n-        output = output + bias\n+        output.add_(bias)\n     return output.to(dtype=input_dtype)\n \n \n", "model_name_or_path": "gpt-5-2025-08-07"}
