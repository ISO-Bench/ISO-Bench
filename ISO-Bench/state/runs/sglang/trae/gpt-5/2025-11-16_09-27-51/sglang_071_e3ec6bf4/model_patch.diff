diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py
index 0e1640fcf..020daf739 100644
--- a/python/sglang/srt/layers/quantization/fp8_utils.py
+++ b/python/sglang/srt/layers/quantization/fp8_utils.py
@@ -332,27 +332,13 @@ def block_quant_to_tensor_quant(
     assert n_tiles == x_s.shape[0]
     assert k_tiles == x_s.shape[1]
 
+    # Vectorized dequant: expand block scales to element-wise scale and multiply
     x_dq_block = x_q_block.to(torch.float32)
-
-    x_dq_block_tiles = [
-        [
-            x_dq_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            for i in range(k_tiles)
-        ]
-        for j in range(n_tiles)
-    ]
-
-    for i in range(k_tiles):
-        for j in range(n_tiles):
-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]
+    x_scale_repeat = x_s.repeat_interleave(block_n, dim=0).repeat_interleave(block_k, dim=1)
+    x_dq_block = x_dq_block * x_scale_repeat[:n, :k]
 
     x_q_tensor, scale = (
-        scaled_fp8_quant(x_dq_block)
-        if _is_cuda
-        else input_to_float8(x_dq_block, dtype=x_q_block.dtype)
+        scaled_fp8_quant(x_dq_block) if _is_cuda else input_to_float8(x_dq_block, dtype=x_q_block.dtype)
     )
     return x_q_tensor, scale
 
@@ -375,21 +361,10 @@ def block_quant_dequant(
     assert n_tiles == x_s.shape[0]
     assert k_tiles == x_s.shape[1]
 
-    x_dq_block = torch.empty_like(x_q_block, dtype=dtype)
-
-    for j in range(n_tiles):
-        for i in range(k_tiles):
-            x_q_block_tile = x_q_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            x_dq_block_tile = x_dq_block[
-                j * block_n : min((j + 1) * block_n, n),
-                i * block_k : min((i + 1) * block_k, k),
-            ]
-            x_dq_block_tile[:, :] = x_q_block_tile.to(torch.float32) * x_s[j][i]
-
-    return x_dq_block
+    # Vectorized dequant: expand block scales and multiply in float32, then cast
+    x_scale_repeat = x_s.repeat_interleave(block_n, dim=0).repeat_interleave(block_k, dim=1)
+    x_dq_block = x_q_block.to(torch.float32) * x_scale_repeat[:n, :k]
+    return x_dq_block.to(dtype)
 
 
 def channel_quant_to_tensor_quant(
@@ -436,9 +411,10 @@ def _apply_fallback_scaled_mm(
     output = _process_scaled_mm_output(output, input_2d_shape, output_shape)
     x_scale = torch.narrow(x_scale, 0, 0, input_2d_shape[0])
 
-    output = output * x_scale * weight_scale.t()
+    output.mul_(x_scale)
+    output.mul_(weight_scale.t())
     if bias is not None:
-        output = output + bias
+        output.add_(bias)
     return output.to(dtype=input_dtype)
 
 
