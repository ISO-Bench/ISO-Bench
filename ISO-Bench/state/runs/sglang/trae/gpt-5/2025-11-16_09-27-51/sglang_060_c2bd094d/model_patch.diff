diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py
index 6d6c432f8..daa510018 100644
--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py
+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py
@@ -6,14 +6,10 @@ import triton
 import triton.language as tl
 
 from sglang.srt.distributed import get_tensor_model_parallel_rank
-from sglang.srt.layers.quantization.fp8_kernel import per_token_group_quant_fp8
 from sglang.srt.utils import is_cuda
 
 _is_cuda = is_cuda()
-if _is_cuda:
-    from sglang.srt.layers.quantization.fp8_kernel import (
-        sglang_per_token_group_quant_fp8,
-    )
+# Lazy import quantization kernels inside functions to avoid heavy package imports at module load time.
 logger = logging.getLogger(__name__)
 
 
@@ -42,7 +38,7 @@ def deepep_compute_src2dst_triton_kernel(
 
 def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)
 
     # Find offet
@@ -147,7 +143,7 @@ def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):
 
 def run_moe_ep_preproess(topk_ids: torch.Tensor, num_experts: int):
     reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)
-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)
     src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)
 
     compute_seg_indptr_triton_kernel[(num_experts,)](
@@ -515,9 +511,10 @@ def grouped_gemm_triton(
         assert len(block_shape) == 2
         block_n, block_k = block_shape[0], block_shape[1]
         if _is_cuda:
-            a, scale_a = sglang_per_token_group_quant_fp8(a, block_k)
+            from sglang.srt.layers.quantization.fp8_kernel import sglang_per_token_group_quant_fp8 as _ptgq
         else:
-            a, scale_a = per_token_group_quant_fp8(a, block_k)
+            from sglang.srt.layers.quantization.fp8_kernel import per_token_group_quant_fp8 as _ptgq
+        a, scale_a = _ptgq(a, block_k)
 
         assert triton.cdiv(a.shape[-1], block_k) == scale_a.shape[-1]
         assert triton.cdiv(b.shape[-2], block_n) == scale_b.shape[-2]
@@ -531,7 +528,8 @@ def grouped_gemm_triton(
         "BLOCK_SIZE_K": 128,
     }
 
-    m_num_tiles_indptr = torch.zeros(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr = torch.empty(batch_size + 1, device=a.device, dtype=torch.int64)
+    m_num_tiles_indptr[0] = 0
     compute_m_num_tiles_indptr[(1,)](
         m_num_tiles_indptr, seg_indptr, batch_size, config["BLOCK_SIZE_M"]
     )
diff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py
index a9b443a75..404cb00c0 100644
--- a/python/sglang/srt/layers/moe/ep_moe/layer.py
+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py
@@ -851,14 +851,13 @@ class DeepEPMoE(EPMoE):
             self.grouped_gemm_runner = GroupedGemmRunner(
                 hidden_states.device, use_flashinfer=False  # TODO: use flashinfer
             )
-        seg_indptr_cur_rank = torch.cat(
-            [
-                torch.zeros(
-                    1, device=tokens_per_expert.device, dtype=tokens_per_expert.dtype
-                ),
-                torch.cumsum(tokens_per_expert, dim=0),
-            ]
+        seg_indptr_cur_rank = torch.empty(
+            tokens_per_expert.shape[0] + 1,
+            device=tokens_per_expert.device,
+            dtype=tokens_per_expert.dtype,
         )
+        seg_indptr_cur_rank[0] = 0
+        seg_indptr_cur_rank[1:] = torch.cumsum(tokens_per_expert, dim=0)
         reorder_topk_ids = torch.repeat_interleave(tokens_per_expert)
         if self.activation_scheme == "dynamic" and not self.use_block_quant:
             max_value = (
diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
index c91ccd633..c06bf431f 100644
--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
@@ -472,7 +472,7 @@ class DeepEPDispatcher:
         batch_size = indices.shape[0]
         multihot_routing_map = torch.zeros(
             (batch_size, self.num_local_experts),
-            dtype=torch.long,
+            dtype=torch.bool,
             device=indices.device,
         )
 
