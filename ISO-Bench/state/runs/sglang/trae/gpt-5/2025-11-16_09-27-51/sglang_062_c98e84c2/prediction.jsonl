{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9c064bf", "model_patch": "diff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex ad7f0a1f3..97af8e5b6 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -34,16 +34,16 @@ class Sampler(nn.Module):\n         logits = logits.contiguous()\n         logits.div_(sampling_info.temperatures)\n         probs = torch.softmax(logits, dim=-1)\n-        logits = None\n         del logits\n \n-        if torch.any(torch.isnan(probs)):\n+        if torch.isnan(probs).any():\n             logger.warning(\"Detected errors during sampling! NaN in the probability.\")\n-            probs = torch.where(\n-                torch.isnan(probs), torch.full_like(probs, 1e-10), probs\n-            )\n+            probs.nan_to_num_(nan=1e-10)\n \n-        if global_server_args_dict[\"sampling_backend\"] == \"flashinfer\":\n+        if sampling_info.top_ks.max().item() <= 1:\n+            # Use fast greedy path when all requests are greedy\n+            batch_next_token_ids = torch.argmax(probs, -1)\n+        elif global_server_args_dict[\"sampling_backend\"] == \"flashinfer\":\n             max_top_k_round, batch_size = 32, probs.shape[0]\n             uniform_samples = torch.rand(\n                 (max_top_k_round, batch_size), device=probs.device\n@@ -85,6 +85,10 @@ def top_k_top_p_min_p_sampling_from_probs_torch(\n     top_ps: torch.Tensor,\n     min_ps: torch.Tensor,\n ):\n+    # Fast path for greedy sampling when applicable\n+    if top_ks.max().item() <= 1 and torch.all(top_ps >= 1.0) and torch.all(min_ps <= 0):\n+        return torch.argmax(probs, dim=-1)\n+\n     \"\"\"A top-k, top-p and min-p sampling implementation with native pytorch operations.\"\"\"\n     probs_sort, probs_idx = probs.sort(dim=-1, descending=True)\n     probs_sum = torch.cumsum(probs_sort, dim=-1)\ndiff --git a/test/srt/test_bench_serving.py b/test/srt/test_bench_serving.py\nindex 056483487..6955d4917 100644\n--- a/test/srt/test_bench_serving.py\n+++ b/test/srt/test_bench_serving.py\n@@ -27,11 +27,11 @@ class TestBenchServing(unittest.TestCase):\n             model=DEFAULT_MODEL_NAME_FOR_TEST,\n             num_prompts=200,\n             request_rate=float(\"inf\"),\n+            other_server_args=[\"--max-running-requests\", \"10\"],\n             dataset_name=\"sharegpt\",\n             random_input_len=None,\n             random_output_len=None,\n             disable_stream=True,\n-            other_server_args=[\"--max-running-requests\", \"10\"],\n         )\n \n         if is_in_ci():\ndiff --git a/test/srt/test_pytorch_sampling_backend.py b/test/srt/test_pytorch_sampling_backend.py\nindex ddd744149..57544e63f 100644\n--- a/test/srt/test_pytorch_sampling_backend.py\n+++ b/test/srt/test_pytorch_sampling_backend.py\n@@ -27,6 +27,22 @@ class TestPyTorchSamplingBackend(unittest.TestCase):\n     def tearDownClass(cls):\n         kill_child_process(cls.process.pid)\n \n+    def test_greedy_shortcut_torch(self):\n+        import torch\n+        from sglang.srt.layers.sampler import top_k_top_p_min_p_sampling_from_probs_torch\n+\n+        torch.manual_seed(0)\n+        bs, vocab = 8, 1024\n+        logits = torch.randn((bs, vocab), dtype=torch.float32)\n+        probs = torch.softmax(logits, dim=-1)\n+        top_ks = torch.ones((bs,), dtype=torch.int)\n+        top_ps = torch.ones((bs,), dtype=torch.float32)\n+        min_ps = torch.zeros((bs,), dtype=torch.float32)\n+\n+        out = top_k_top_p_min_p_sampling_from_probs_torch(probs, top_ks, top_ps, min_ps)\n+        argmax = torch.argmax(probs, dim=-1)\n+        assert torch.equal(out, argmax)\n+\n     def test_mmlu(self):\n         args = SimpleNamespace(\n             base_url=self.base_url,\n", "model_name_or_path": "gpt-5-2025-08-07"}
