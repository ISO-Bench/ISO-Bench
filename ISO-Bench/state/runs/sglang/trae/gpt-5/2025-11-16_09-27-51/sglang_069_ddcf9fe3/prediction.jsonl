{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-6252ade", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\nindex 608f9bab0..79cdcc033 100644\n--- a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n+++ b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n@@ -74,6 +74,7 @@ def _fwd_kernel(\n     BLOCK_M: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     USE_CUSTOM_MASK: tl.constexpr,\n+    SKIP_PREFIX_CUSTOM_MASK: tl.constexpr,\n     STORE_TRANSPOSE: tl.constexpr,\n ):\n     cur_seq = tl.program_id(0)\n@@ -142,7 +143,7 @@ def _fwd_kernel(\n             K_Buffer + offs_buf_k, mask=(mask_n[None, :]) & (mask_d[:, None]), other=0.0\n         )\n \n-        qk = tl.dot(q.to(k.dtype), k)\n+        qk = tl.dot(q.to(k.dtype), k, out_dtype=tl.float32)\n         if BLOCK_DPE > 0:\n             offs_kpe = (\n                 offs_kv_loc[None, :] * stride_buf_kbs\n@@ -154,13 +155,13 @@ def _fwd_kernel(\n                 mask=mask_n[None, :],\n                 other=0.0,\n             )\n-            qk += tl.dot(qpe.to(kpe.dtype), kpe)\n+            qk += tl.dot(qpe.to(kpe.dtype), kpe, out_dtype=tl.float32)\n         qk *= sm_scale\n \n         if logit_cap > 0:\n             qk = logit_cap * tanh(qk / logit_cap)\n \n-        if USE_CUSTOM_MASK:\n+        if USE_CUSTOM_MASK and not SKIP_PREFIX_CUSTOM_MASK:\n             custom_mask = tl.load(\n                 mask_ptr\n                 + cur_seq_mask_start_idx\n@@ -222,7 +223,7 @@ def _fwd_kernel(\n                 mask=mask_n[None, :],\n                 other=0.0,\n             )\n-            qk += tl.dot(qpe, kpe)\n+            qk += tl.dot(qpe, kpe, out_dtype=tl.float32)\n \n         qk *= sm_scale\n \n@@ -302,6 +303,7 @@ def extend_attention_fwd(\n     max_len_extend,\n     sm_scale=None,\n     logit_cap=0.0,\n+    skip_prefix_custom_mask: bool = True,\n ):\n     \"\"\"\n     q_extend, k_extend, v_extend, o_extend: contiguous tensors\n@@ -398,6 +400,7 @@ def extend_attention_fwd(\n         Lq=Lq,\n         Lv=Lv,\n         USE_CUSTOM_MASK=USE_CUSTOM_MASK,\n+        SKIP_PREFIX_CUSTOM_MASK=skip_prefix_custom_mask,\n         STORE_TRANSPOSE=is_hip_,\n         num_warps=num_warps,\n         num_stages=num_stages,\n", "model_name_or_path": "gpt-5-2025-08-07"}
