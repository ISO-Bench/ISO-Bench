diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
index 100fa57fb..6ffa42f9f 100644
--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py
@@ -311,7 +311,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):
                 BLOCK_SIZE=512,
             )
         else:
-            output = torch.zeros(
+            output = torch.empty(
                 (0, hidden_states.shape[1]),
                 device=hidden_states.device,
                 dtype=hidden_states.dtype,
diff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py
index 96a13a999..47c67d40a 100644
--- a/python/sglang/srt/model_executor/forward_batch_info.py
+++ b/python/sglang/srt/model_executor/forward_batch_info.py
@@ -280,7 +280,7 @@ class ForwardBatch:
             ).to(device, non_blocking=True)
 
             sum_len = sum(batch.global_num_tokens)
-            ret.gathered_buffer = torch.zeros(
+            ret.gathered_buffer = torch.empty(
                 (sum_len, model_runner.model_config.hidden_size),
                 dtype=model_runner.dtype,
                 device=device,
@@ -515,7 +515,8 @@ def compute_position_torch(
         ],
         axis=0,
     )
-    extend_start_loc = torch.zeros_like(extend_seq_lens)
+    extend_start_loc = torch.empty_like(extend_seq_lens)
+    extend_start_loc[0] = 0
     extend_start_loc[1:] = torch.cumsum(extend_seq_lens[:-1], dim=0)
     return positions.to(torch.int64), extend_start_loc
 
