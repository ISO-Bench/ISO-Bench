{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-c4336b2", "model_patch": "diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 1d1cf3688..58e77f286 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -72,6 +72,7 @@ from sglang.srt.utils import (\n     configure_logger,\n     crash_on_warnings,\n     get_zmq_socket,\n+    gpu_proc_affinity,\n     kill_parent_process,\n     set_random_seed,\n     suppress_other_loggers,\n@@ -1393,6 +1394,8 @@ def run_scheduler_process(\n     dp_rank: Optional[int],\n     pipe_writer,\n ):\n+    # set cpu affinity to this gpu process\n+    gpu_proc_affinity(server_args.tp_size, server_args.nnodes, gpu_id)\n     # [For Router] if env var \"DP_RANK\" exist, set dp_rank to the value of the env var\n     if dp_rank is None and \"DP_RANK\" in os.environ:\n         dp_rank = int(os.environ[\"DP_RANK\"])\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex e947d1a92..1ccab7cf6 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -53,10 +53,112 @@ from triton.runtime.cache import (\n     default_dump_dir,\n     default_override_dir,\n )\n+from functools import lru_cache\n \n logger = logging.getLogger(__name__)\n \n \n+@lru_cache(maxsize=None)\n+def _parse_numa_cpu_map() -> List[List[int]]:\n+    \"\"\"Parse NUMA node CPU lists from sysfs.\n+\n+    Returns a list where each element is the sorted list of CPU ids for a node.\n+    If parsing fails or the system has no NUMA info, returns an empty list.\n+    \"\"\"\n+    nodes_dir = \"/sys/devices/system/node\"\n+    if not os.path.isdir(nodes_dir):\n+        return []\n+    cpu_map: List[List[int]] = []\n+    try:\n+        for name in sorted(os.listdir(nodes_dir)):\n+            if not name.startswith(\"node\"):\n+                continue\n+            path = os.path.join(nodes_dir, name, \"cpulist\")\n+            if not os.path.exists(path):\n+                continue\n+            with open(path, \"r\") as f:\n+                cpulist = f.read().strip()\n+            if not cpulist:\n+                cpu_map.append([])\n+                continue\n+            # Parse format like: \"0-7,16-23\"\n+            cpus: List[int] = []\n+            for part in cpulist.split(\",\"):\n+                if \"-\" in part:\n+                    a, b = part.split(\"-\")\n+                    cpus.extend(range(int(a), int(b) + 1))\n+                else:\n+                    cpus.append(int(part))\n+            cpu_map.append(sorted(cpus))\n+        return cpu_map\n+    except Exception:\n+        return []\n+\n+\n+def _available_cpus() -> List[int]:\n+    try:\n+        allowed = os.sched_getaffinity(0)\n+        cpus = sorted(list(allowed))\n+        if cpus:\n+            return cpus\n+    except Exception:\n+        pass\n+    count = psutil.cpu_count(logical=True) or os.cpu_count() or 1\n+    return list(range(count))\n+\n+\n+def _chunk_list(lst: List[int], num_chunks: int) -> List[List[int]]:\n+    num_chunks = max(1, min(num_chunks, len(lst)))\n+    base, rem = divmod(len(lst), num_chunks)\n+    out: List[List[int]] = []\n+    start = 0\n+    for i in range(num_chunks):\n+        size = base + (1 if i < rem else 0)\n+        out.append(lst[start : start + size])\n+        start += size\n+    return out\n+\n+\n+def gpu_proc_affinity(tp_size: int, nnodes: int, gpu_id: int) -> None:\n+    \"\"\"\n+    Pin the current process to a subset of CPU cores to reduce context switching\n+    and improve cache/NUMA locality. Heuristic mapping with multi-socket support.\n+\n+    - Prefer binding to the CPUs of a NUMA node when available\n+    - Otherwise, partition the allowed CPUs across local GPUs\n+    \"\"\"\n+    try:\n+        cpus_allowed = _available_cpus()\n+        if not cpus_allowed:\n+            return\n+\n+        # Try NUMA-aware assignment first\n+        numa_map = _parse_numa_cpu_map()\n+        selected: List[int] = []\n+        if numa_map:\n+            node_idx = gpu_id % len(numa_map)\n+            node_cpus = [c for c in numa_map[node_idx] if c in cpus_allowed]\n+            if node_cpus:\n+                selected = node_cpus\n+\n+        # Fallback: split CPUs evenly across GPUs/TP ranks on this host\n+        if not selected:\n+            try:\n+                local_gpus = torch.cuda.device_count() if torch.cuda.is_available() else tp_size\n+            except Exception:\n+                local_gpus = tp_size or 1\n+            groups = max(1, local_gpus)\n+            chunks = _chunk_list(cpus_allowed, groups)\n+            selected = chunks[gpu_id % len(chunks)] if chunks else cpus_allowed\n+\n+        if selected:\n+            os.sched_setaffinity(0, set(selected))\n+            logger.info(\"Set CPU affinity (pid=%d) to %s\", os.getpid(), selected)\n+    except Exception as e:\n+        # Best-effort only\n+        logger.debug(\"Failed to set CPU affinity: %s\", e)\n+\n+\n show_time_cost = False\n time_infos = {}\n \n@@ -192,6 +294,7 @@ def get_available_gpu_memory(device, gpu_id, distributed=False):\n     return free_gpu_memory / (1 << 30)\n \n \n+@lru_cache(maxsize=None)\n def is_pin_memory_available() -> bool:\n     return torch.cuda.is_available()\n \n@@ -711,7 +814,7 @@ def broadcast_pyobj(\n         tensor_data = torch.empty(size, dtype=torch.uint8)\n         dist.broadcast(tensor_data, src=0, group=dist_group)\n \n-        serialized_data = bytes(tensor_data.cpu().numpy())\n+        serialized_data = tensor_data.cpu().numpy().tobytes()\n         data = pickle.loads(serialized_data)\n         return data\n \n@@ -764,6 +867,11 @@ def get_zmq_socket(context: zmq.Context, socket_type: zmq.SocketType, endpoint:\n         buf_size = -1\n \n     socket = context.socket(socket_type)\n+    # Avoid blocking on close; best-effort fire-and-forget behavior for IPC\n+    try:\n+        socket.setsockopt(zmq.LINGER, 0)\n+    except Exception:\n+        pass\n     if socket_type == zmq.PUSH:\n         socket.setsockopt(zmq.SNDHWM, 0)\n         socket.setsockopt(zmq.SNDBUF, buf_size)\n@@ -793,6 +901,7 @@ def dump_to_file(dirpath, name, value):\n     np.save(output_filename, value)\n \n \n+@lru_cache(maxsize=None)\n def is_triton_3():\n     return triton.__version__.startswith(\"3.\")\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
