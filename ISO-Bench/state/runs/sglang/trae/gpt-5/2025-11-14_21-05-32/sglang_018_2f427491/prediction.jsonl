{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-d818966", "model_patch": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex 8895e6be6..703137fe1 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -49,10 +49,6 @@ def fused_topk_native(\n         hidden_states.shape[0] == gating_output.shape[0]\n     ), f\"Number of tokens mismatch, {hidden_states.shape=} vs {gating_output.shape=}\"\n     M, _ = hidden_states.shape\n-    topk_weights = torch.empty(\n-        M, topk, dtype=torch.float32, device=hidden_states.device\n-    )\n-    topk_ids = torch.empty(M, topk, dtype=torch.int32, device=hidden_states.device)\n     topk_weights = F.softmax(gating_output.float(), dim=-1)\n     topk_weights, topk_ids = torch.topk(topk_weights, topk, dim=-1)\n     if renormalize:\n@@ -146,7 +142,8 @@ def grouped_topk(\n \n     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n-    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    if num_token_non_padded is not None:\n+        _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n     return topk_weights, topk_ids\n \n \n@@ -210,7 +207,8 @@ def biased_grouped_topk_impl(\n \n     topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n     topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n-    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    if num_token_non_padded is not None:\n+        _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n     return topk_weights, topk_ids\n \n \n@@ -264,6 +262,8 @@ def biased_grouped_topk(\n         # TODO merge into kernel for this branch\n         topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n         # TODO will fuse this into kernel, thus use slow manual operation now\n+        if num_token_non_padded is None:\n+            return topk_weights, topk_ids\n         torch.compile(\n             _mask_topk_ids_padded_region, dynamic=True, backend=get_compiler_backend()\n         )(topk_ids, num_token_non_padded)\n", "model_name_or_path": "gpt-5-2025-08-07"}
