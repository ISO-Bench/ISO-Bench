{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-ceba0ce", "model_patch": "diff --git a/python/sglang/srt/disaggregation/mooncake/memory_pool.py b/python/sglang/srt/disaggregation/mooncake/memory_pool.py\nindex 6e8edaf92..e555a1b8f 100644\n--- a/python/sglang/srt/disaggregation/mooncake/memory_pool.py\n+++ b/python/sglang/srt/disaggregation/mooncake/memory_pool.py\n@@ -11,15 +11,21 @@ from torch.cuda.memory import CUDAPluggableAllocator\n class MooncakeNVLinkAllocator:\n     _instances: Dict[torch.device, CUDAPluggableAllocator] = {}\n     _lock: Final = threading.Lock()\n+    _so_path: Optional[str] = None\n \n     @classmethod\n     def _get_so_path(cls) -> str:\n         \"\"\"Dynamically locate hook.so in the mooncake package installation\"\"\"\n+        # Fast-path: cached result\n+        if cls._so_path is not None:\n+            return cls._so_path\n+\n         try:\n             # Attempt to locate package resource\n             with resources.path(\"mooncake\", \"hook.so\") as so_path:\n                 if so_path.exists():\n-                    return str(so_path)\n+                    cls._so_path = str(so_path)\n+                    return cls._so_path\n         except (ImportError, FileNotFoundError, TypeError):\n             pass\n \n@@ -30,11 +36,14 @@ class MooncakeNVLinkAllocator:\n             base_path = os.path.dirname(os.path.abspath(mooncake.__file__))\n             so_path = os.path.join(base_path, \"hook.so\")\n             if os.path.exists(so_path):\n-                return so_path\n+                cls._so_path = so_path\n+                return cls._so_path\n         except (ImportError, FileNotFoundError, TypeError):\n-            raise ImportError(\n-                \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL require mooncake-transfer-engine >= 0.3.3.post2.\"\n-            )\n+            pass\n+\n+        raise ImportError(\n+            \"SGLANG_MOONCAKE_CUSTOM_MEM_POOL require mooncake-transfer-engine >= 0.3.3.post2.\"\n+        )\n \n     @classmethod\n     def get_allocator(cls, device: torch.device) -> CUDAPluggableAllocator:\ndiff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex c01807f1b..3bb8a014e 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -303,7 +303,7 @@ class MHATokenToKVPool(KVCache):\n                 # [size, head_num, head_dim] for each layer\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.k_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -311,7 +311,7 @@ class MHATokenToKVPool(KVCache):\n                     for _ in range(self.layer_num)\n                 ]\n                 self.v_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (self.size + self.page_size, self.head_num, self.head_dim),\n                         dtype=self.store_dtype,\n                         device=self.device,\n@@ -620,7 +620,7 @@ class MLATokenToKVPool(KVCache):\n             ):\n                 # The padded slot 0 is used for writing dummy outputs from padded tokens.\n                 self.kv_buffer = [\n-                    torch.zeros(\n+                    torch.empty(\n                         (size + page_size, 1, kv_lora_rank + qk_rope_head_dim),\n                         dtype=self.store_dtype,\n                         device=device,\n@@ -758,13 +758,13 @@ class DoubleSparseTokenToKVPool(KVCache):\n         with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):\n             # [size, head_num, head_dim] for each layer\n             self.k_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\n             ]\n             self.v_buffer = [\n-                torch.zeros(\n+                torch.empty(\n                     (size + page_size, head_num, head_dim), dtype=dtype, device=device\n                 )\n                 for _ in range(layer_num)\ndiff --git a/scripts/ci_install_dependency.sh b/scripts/ci_install_dependency.sh\nindex 922c886c4..85b497b6d 100755\n--- a/scripts/ci_install_dependency.sh\n+++ b/scripts/ci_install_dependency.sh\n@@ -23,7 +23,7 @@ pip install -e \"python[dev]\"\n pip list\n \n # Install additional dependencies\n-pip install mooncake-transfer-engine==0.3.2.post1 nvidia-cuda-nvrtc-cu12\n+pip install mooncake-transfer-engine==0.3.3.post2 nvidia-cuda-nvrtc-cu12\n \n # For lmms_evals evaluating MMMU\n git clone --branch v0.3.3 --depth 1 https://github.com/EvolvingLMMs-Lab/lmms-eval.git\n", "model_name_or_path": "gpt-5-2025-08-07"}
