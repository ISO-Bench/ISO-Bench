{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-5347567", "model_patch": "diff --git a/python/sglang/srt/managers/mm_utils.py b/python/sglang/srt/managers/mm_utils.py\nindex 13ca29c54..1c35fa840 100644\n--- a/python/sglang/srt/managers/mm_utils.py\n+++ b/python/sglang/srt/managers/mm_utils.py\n@@ -433,7 +433,8 @@ def embed_mm_inputs(\n                 device=input_ids.device,\n             )\n             # calculate per request items length offset\n-            items_size = torch.zeros(len(mm_inputs_list) + 1, dtype=int)\n+            items_size = torch.empty(len(mm_inputs_list) + 1, dtype=int)\n+            items_size[0] = 0\n             items_offsets = []\n             for i, mm_inputs in enumerate(mm_inputs_list):\n                 mm_items = [\n@@ -591,7 +592,7 @@ def get_multimodal_data_bounds(\n     valid_mm_data_nums = min(len(data_start_tokens_cpu), len(data_end_tokens_cpu))\n \n     if valid_mm_data_nums == 0:\n-        return torch.zeros((0, 2), device=input_ids.device)\n+        return torch.empty((0, 2), device=input_ids.device)\n \n     # Filter out pairs where start_token >= end_token\n     valid_pairs = []\n@@ -602,7 +603,7 @@ def get_multimodal_data_bounds(\n             valid_pairs.append((start_token + 1, end_token - 1))\n \n     if not valid_pairs:\n-        return torch.zeros((0, 2), device=input_ids.device)\n+        return torch.empty((0, 2), device=input_ids.device)\n \n     # Convert valid pairs to tensor\n     valid_pairs_tensor = torch.as_tensor(valid_pairs, device=input_ids.device)\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex ad8bcf119..1a92356bd 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -1096,14 +1096,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n", "model_name_or_path": "gpt-5-2025-08-07"}
