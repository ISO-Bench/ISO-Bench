{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-42a2d82", "model_patch": "diff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex 7a03e162f..ac6b1fb6f 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -260,7 +260,7 @@ def correctness_test(\n \n     # Decode\n     output_ids = [input_ids[i] + [next_token_ids[i]] for i in range(len(input_ids))]\n-    for _ in range(bench_args.output_len[0]):\n+    for _ in range(bench_args.output_len[0] - 1):\n         next_token_ids, _ = decode(next_token_ids, batch, model_runner)\n         for i in range(len(reqs)):\n             output_ids[i].append(next_token_ids[i])\n@@ -311,7 +311,7 @@ def latency_test_run_once(\n \n     # Decode\n     decode_latencies = []\n-    for i in range(output_len):\n+    for i in range(output_len - 1):\n         torch.cuda.synchronize()\n         tic = time.time()\n         next_token_ids, _ = decode(next_token_ids, batch, model_runner)\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2ab041726..2d36983a3 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -681,7 +681,9 @@ class ScheduleBatch:\n                 for r in self.reqs\n             ]\n         else:\n-            self.sampling_info.penalizer_orchestrator.cumulate_input_tokens(input_ids)\n+            orchestrator = self.sampling_info.penalizer_orchestrator\n+            if any(p.is_prepared() for p in orchestrator.penalizers.values()):\n+                orchestrator.cumulate_input_tokens(input_ids)\n \n         self.input_ids = torch.tensor(input_ids, dtype=torch.int32, device=\"cuda\")\n         self.seq_lens.add_(1)\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex fe9afc9f3..5e18e7b8a 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -560,9 +560,11 @@ class ModelTpServer:\n                 logits_output = self.model_runner.forward(batch)\n                 next_token_ids = self.model_runner.sample(logits_output, batch)\n \n-                batch.sampling_info.penalizer_orchestrator.cumulate_output_tokens(\n-                    next_token_ids\n-                )\n+                orchestrator = batch.sampling_info.penalizer_orchestrator\n+                if any(p.is_prepared() for p in orchestrator.penalizers.values()):\n+                    orchestrator.cumulate_output_tokens(\n+                        next_token_ids\n+                    )\n \n                 # Move logprobs to cpu\n                 if logits_output.next_token_logprobs is not None:\n@@ -752,9 +754,11 @@ class ModelTpServer:\n         # Forward and sample the next tokens\n         logits_output = self.model_runner.forward(batch)\n         next_token_ids = self.model_runner.sample(logits_output, batch)\n-        batch.sampling_info.penalizer_orchestrator.cumulate_output_tokens(\n-            next_token_ids\n-        )\n+        orchestrator = batch.sampling_info.penalizer_orchestrator\n+        if any(p.is_prepared() for p in orchestrator.penalizers.values()):\n+            orchestrator.cumulate_output_tokens(\n+                next_token_ids\n+            )\n \n         # Move logprobs to cpu\n         if logits_output.next_token_logprobs is not None:\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 4815fbc56..7406964a6 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -140,7 +140,8 @@ class InputMetadata:\n     def compute_extend_infos(self, batch: ScheduleBatch):\n         self.extend_seq_lens = torch.tensor(batch.extend_lens_cpu, device=\"cuda\")\n         self.extend_prefix_lens = torch.tensor(batch.prefix_lens_cpu, device=\"cuda\")\n-        self.extend_start_loc = torch.zeros_like(self.extend_seq_lens)\n+        self.extend_start_loc = torch.empty_like(self.extend_seq_lens)\n+        self.extend_start_loc[0] = 0\n         self.extend_start_loc[1:] = torch.cumsum(self.extend_seq_lens[:-1], dim=0)\n         self.extend_no_prefix = all(x == 0 for x in batch.prefix_lens_cpu)\n         self.extend_seq_lens_cpu = batch.extend_lens_cpu\ndiff --git a/scripts/playground/reference_hf.py b/scripts/playground/reference_hf.py\nindex 1eb7b0dd2..9af7cb66c 100644\n--- a/scripts/playground/reference_hf.py\n+++ b/scripts/playground/reference_hf.py\n@@ -81,7 +81,7 @@ def synthetic_tokens(args):\n \n     for p in prompts:\n         input_ids = p\n-        for i in range(output_len + 1):\n+        for i in range(output_len):\n             prefill_logits = m.forward(torch.tensor([input_ids], device=\"cuda\")).logits[\n                 0\n             ][-1]\n", "model_name_or_path": "gpt-5-2025-08-07"}
