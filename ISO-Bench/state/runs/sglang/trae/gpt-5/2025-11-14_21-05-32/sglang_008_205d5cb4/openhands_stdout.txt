Changed working directory to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4
Initialising MCP tools...
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Task Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [1mTask:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4.     â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py                         â”‚
â”‚ index 2f974ea9a..a626ff0d8 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/layers/attention/flashattention_backend.py                                                                                               â”‚
â”‚ +++ b/python/sglang/srt/layers/attention/flashattention_backend.py                                                                                               â”‚
â”‚ @@ -1434,19 +1434,7 @@ class FlashAttentionBackend(AttentionBackend):                                                                                            â”‚
â”‚                  self.decode_cuda_graph_metadata = metadata                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚                  if self.attention_chunk_size is not None:                                                                                                       â”‚
â”‚ -                    metadata.local_attn_metadata = FlashAttentionMetadata.LocalAttentionMetadata(                                                               â”‚
â”‚ -                        local_query_start_loc=self.decode_cuda_graph_local_attn_metadata[                                                                       â”‚
â”‚ -                            "local_query_start_loc"                                                                                                             â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_seqused_k=self.decode_cuda_graph_local_attn_metadata[                                                                             â”‚
â”‚ -                            "local_seqused_k"                                                                                                                   â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_block_table=self.decode_cuda_graph_local_attn_metadata[                                                                           â”‚
â”‚ -                            "local_block_table"                                                                                                                 â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_max_query_len=1,                                                                                                                  â”‚
â”‚ -                        local_max_seq_len=1,                                                                                                                    â”‚
â”‚ -                    )                                                                                                                                           â”‚
â”‚ +                    self._update_local_attn_metadata_for_capture(metadata, batch_size)                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚          elif forward_mode.is_target_verify():                                                                                                                   â”‚
â”‚              if self.topk <= 1:                                                                                                                                  â”‚
â”‚ @@ -1807,6 +1795,62 @@ class FlashAttentionBackend(AttentionBackend):                                                                                            â”‚
â”‚          )                                                                                                                                                       â”‚
â”‚          metadata.local_attn_metadata = local_metadata                                                                                                           â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ +    def _update_local_attn_metadata_for_capture(                                                                                                                â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/layers/attention/flashattention_backend.py                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/layers/attention/flashattention_backend.py`                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ perf: Optimize local attention memory allocation in FlashAttentionBackend (#6356)                                                                                â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ .../srt/layers/attention/flashattention_backend.py | 70 ++++++++++++++++++----                                                                                   â”‚
â”‚  1 file changed, 57 insertions(+), 13 deletions(-)                                                                                                               â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mModel Provider:[0m openai                                                                                                                                           â”‚
â”‚ [1mModel:[0m gpt-5-2025-08-07                                                                                                                                          â”‚
â”‚ [1mMax Steps:[0m 120                                                                                                                                                   â”‚
â”‚ [1mTrajectory File:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_008_205d5cb4/trajectory.json                               â”‚
â”‚ [1mTools:[0m bash, str_replace_based_edit_tool, sequentialthinking, task_done                                                                                          â”‚
â”‚ [1mProject_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4                                                      â”‚
â”‚ [1mIssue:[0m I've uploaded a python code repository in the directory /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4.    â”‚
â”‚ Consider the following test script showing an example usage of the repository:                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <test_script>                                                                                                                                                    â”‚
â”‚ # This is a performance optimization task                                                                                                                        â”‚
â”‚ # The specific operations to optimize are in the files listed below                                                                                              â”‚
â”‚ # Focus on performance improvements in the target functions                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ </test_script>                                                                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Can you help me implement the necessary changes to the repository so that the runtime of the <test_script> is optimized?                                         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Basic guidelines:                                                                                                                                                â”‚
â”‚ 1. Your task is to make changes to non-test files in the /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4 directory â”‚
â”‚ to improve the performance of the <test_script>.                                                                                                                 â”‚
â”‚ 2. Make changes while ensuring the repository is functionally equivalent to the original.                                                                        â”‚
â”‚ 3. Do not overoptimize for just the specific inputs in <test_script>. Make general performance improvements for the usage scenario shown.                        â”‚
â”‚ 4. You may need to rebuild the repo for your changes to take effect before testing. Some rebuilds may take time to run, so be patient with running them.         â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Follow these steps to improve performance:                                                                                                                       â”‚
â”‚ 1. As a first step, explore the repository structure.                                                                                                            â”‚
â”‚ 2. Create a script ONLY inside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch (e.g.,               â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch/test_opt.py) to reproduce and time the example,     â”‚
â”‚ then execute it with python <filename.py> from the repo root.                                                                                                    â”‚
â”‚ 3. Edit the source code of the repository to improve performance.                                                                                                â”‚
â”‚ 4. Rebuild and rerun your script to confirm that performance has improved.                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Here is an example of the kind of optimizations that have been shown to improve performance in this codebase:                                                    â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ <example_optimization_diff>                                                                                                                                      â”‚
â”‚ diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py                         â”‚
â”‚ index 2f974ea9a..a626ff0d8 100644                                                                                                                                â”‚
â”‚ --- a/python/sglang/srt/layers/attention/flashattention_backend.py                                                                                               â”‚
â”‚ +++ b/python/sglang/srt/layers/attention/flashattention_backend.py                                                                                               â”‚
â”‚ @@ -1434,19 +1434,7 @@ class FlashAttentionBackend(AttentionBackend):                                                                                            â”‚
â”‚                  self.decode_cuda_graph_metadata = metadata                                                                                                      â”‚
â”‚                                                                                                                                                                  â”‚
â”‚                  if self.attention_chunk_size is not None:                                                                                                       â”‚
â”‚ -                    metadata.local_attn_metadata = FlashAttentionMetadata.LocalAttentionMetadata(                                                               â”‚
â”‚ -                        local_query_start_loc=self.decode_cuda_graph_local_attn_metadata[                                                                       â”‚
â”‚ -                            "local_query_start_loc"                                                                                                             â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_seqused_k=self.decode_cuda_graph_local_attn_metadata[                                                                             â”‚
â”‚ -                            "local_seqused_k"                                                                                                                   â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_block_table=self.decode_cuda_graph_local_attn_metadata[                                                                           â”‚
â”‚ -                            "local_block_table"                                                                                                                 â”‚
â”‚ -                        ],                                                                                                                                      â”‚
â”‚ -                        local_max_query_len=1,                                                                                                                  â”‚
â”‚ -                        local_max_seq_len=1,                                                                                                                    â”‚
â”‚ -                    )                                                                                                                                           â”‚
â”‚ +                    self._update_local_attn_metadata_for_capture(metadata, batch_size)                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚          elif forward_mode.is_target_verify():                                                                                                                   â”‚
â”‚              if self.topk <= 1:                                                                                                                                  â”‚
â”‚ @@ -1807,6 +1795,62 @@ class FlashAttentionBackend(AttentionBackend):                                                                                            â”‚
â”‚          )                                                                                                                                                       â”‚
â”‚          metadata.local_attn_metadata = local_metadata                                                                                                           â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ +    def _update_local_attn_metadata_for_capture(                                                                                                                â”‚
â”‚ </example_optimization_diff>                                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: The above diff is an EXAMPLE of optimizations that were successful in a different context.                                                            â”‚
â”‚ These changes have NOT been applied to your codebase yet.                                                                                                        â”‚
â”‚ Your task is to:                                                                                                                                                 â”‚
â”‚ 1. Understand the optimization pattern shown (e.g., torch.zeros â†’ torch.empty)                                                                                   â”‚
â”‚ 2. Look at the CURRENT code in the target files                                                                                                                  â”‚
â”‚ 3. Find places where you can apply SIMILAR optimizations                                                                                                         â”‚
â”‚ 4. MAKE THE CHANGES yourself using str_replace_editor                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ The codebase you're working with is at the BASE commit - it does NOT have these optimizations yet.                                                               â”‚
â”‚ You need to IMPLEMENT similar optimizations yourself.                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ HERE'S WHAT YOU NEED TO DO:                                                                                                                                      â”‚
â”‚ 1. The files CURRENTLY contain torch.zeros() calls that need optimization                                                                                        â”‚
â”‚ 2. You need to CHANGE torch.zeros to torch.empty where appropriate                                                                                               â”‚
â”‚ 3. You need to REMOVE .fill_() operations that are unnecessary                                                                                                   â”‚
â”‚ 4. These are NEW changes you're making - not already in the code                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START WITH THIS COMMAND to see what needs changing:                                                                                                              â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ grep -n 'torch.zeros\|fill_' vllm/model_executor/layers/fused_moe/moe_align_block_size.py benchmarks/kernels/benchmark_moe_align_block_size.py                   â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ CRITICAL: You MUST make actual code changes. Look for patterns like:                                                                                             â”‚
â”‚ - Analyze the target files for performance bottlenecks                                                                                                           â”‚
â”‚ - Look for unnecessary memory allocations or initializations                                                                                                     â”‚
â”‚ - Consider more efficient algorithms or data structures                                                                                                          â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ Target files to optimize:                                                                                                                                        â”‚
â”‚ - python/sglang/srt/layers/attention/flashattention_backend.py                                                                                                   â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ IMPORTANT: You MUST make actual code changes to at least one file.                                                                                               â”‚
â”‚ The task will fail if no files are modified.                                                                                                                     â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Constraints                                                                                                                                                   â”‚
â”‚ - No public API breakage                                                                                                                                         â”‚
â”‚ - All TestPack checks must pass                                                                                                                                  â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## Target Files (ONLY modify these)                                                                                                                              â”‚
â”‚ - `python/sglang/srt/layers/attention/flashattention_backend.py`                                                                                                 â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## SPECIFIC OPTIMIZATION TARGETS:                                                                                                                                â”‚
â”‚ Based on the human commit analysis, focus on these areas:                                                                                                        â”‚
â”‚ - Memory allocation patterns (torch.zeros vs torch.empty)                                                                                                        â”‚
â”‚ - Tensor initialization strategies                                                                                                                               â”‚
â”‚ - Kernel parameter optimization                                                                                                                                  â”‚
â”‚ - Buffer reuse and caching                                                                                                                                       â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Human Developer's Approach:                                                                                                                                  â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ perf: Optimize local attention memory allocation in FlashAttentionBackend (#6356)                                                                                â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ### Files Modified (statistics):                                                                                                                                 â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚ .../srt/layers/attention/flashattention_backend.py | 70 ++++++++++++++++++----                                                                                   â”‚
â”‚  1 file changed, 57 insertions(+), 13 deletions(-)                                                                                                               â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## IMMEDIATE ACTION REQUIREMENTS:                                                                                                                                â”‚
â”‚ 1. Start editing files by iteration 3                                                                                                                            â”‚
â”‚ 2. Create and run /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch/test_opt.py before and after      â”‚
â”‚ edits (do not create timing scripts outside /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/.bench_scratch)        â”‚
â”‚ 3. Make at least 3 concrete optimizations                                                                                                                        â”‚
â”‚ 4. Commit changes by iteration 8                                                                                                                                 â”‚
â”‚ 5. Use finish command by iteration 10                                                                                                                            â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ ## TASK COMPLETION COMMAND:                                                                                                                                      â”‚
â”‚ When you have made optimizations:                                                                                                                                â”‚
â”‚ ```bash                                                                                                                                                          â”‚
â”‚ git add -A                                                                                                                                                       â”‚
â”‚ git reset -q .bench_scratch || true                                                                                                                              â”‚
â”‚ git commit -m 'Optimize MoE align sum kernels performance'                                                                                                       â”‚
â”‚ git diff $(git merge-base HEAD origin/HEAD || git rev-parse HEAD~1) -- . ':(exclude).bench_scratch' >                                                            â”‚
â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-bench/.work/worktrees/sglang_core/sglang_008_205d5cb4/model_patch.diff || true                                           â”‚
â”‚ finish                                                                                                                                                           â”‚
â”‚ ```                                                                                                                                                              â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ START IMPLEMENTING IMMEDIATELY. NO MORE ANALYSIS.                                                                                                                â”‚
â”‚                                                                                                                                                                  â”‚
â”‚ [1mMust_patch:[0m true                                                                                                                                                 â”‚
â”‚ [1mPatch_path:[0m /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_008_205d5cb4/model_patch.diff                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 1: Completed                                                                                   â”‚
â”‚ LLM         â”‚ ðŸ’¬ Plan:                                                                                               â”‚
â”‚ Response    â”‚ - Inspect the target file for torch.zeros/fill_ usage.                                                 â”‚
â”‚             â”‚ - Create a timing script in .bench_scratch to import and minimally exercise the module.                â”‚
â”‚             â”‚ - Apply optimizations in python/sglang/srt/layers/attention/flashattention_backend.py (replace         â”‚
â”‚             â”‚ zeros->empty, remove unnecessary fill_, introduce metadata reuse helper if applicable).                â”‚
â”‚             â”‚ - Run timing script before/after changes.                                                              â”‚
â”‚             â”‚ - Commit and generate patch.                                                                           â”‚
â”‚             â”‚                                                                                                        â”‚
â”‚             â”‚ Now, Iâ€™ll search the target file for torch.zeros/fill_ and open it.                                    â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1,200p'                   â”‚ from __future__ import annotations              â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ from dataclasses import dataclass               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from typing import TYPE_CHECKING, Optional,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Union                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import numpy as np                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import torch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.configs.model_config import     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.layers.attention.base_attn_backend   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import AttentionBackend                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.managers.schedule_batch import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sglang.srt.model_executor.forward_batch_info    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ForwardBatch, ForwardMode                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sglang.srt.speculative.eagle_utils import  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleDraftInput, EagleVerifyInput               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if TYPE_CHECKING:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     from sglang.srt.layers.radix_attention      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import RadixAttention                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     from sglang.srt.model_executor.model_runner â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ import ModelRunner                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sgl_kernel import merge_state_v2           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ from sgl_kernel.flash_attn import               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash_attn_varlen_func, flash_attn_with_kvcache â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @dataclass                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class FlashAttentionMetadata:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Metadata to be init once in the model    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward pass,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     each layer's forward pass can reuse the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     For each init metadata function, we will    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ try set up them in below order                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Sequence lengths for the forward batch    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     cache_seqlens_int32: torch.Tensor = None    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Maximum sequence length for query         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_seq_len_q: int = 1                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Maximum sequence length for key           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_seq_len_k: int = 0                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Cumulative sequence lengths for query     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     cu_seqlens_q: torch.Tensor = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Cumulative sequence lengths for key       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     cu_seqlens_k: torch.Tensor = None           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Window size (typically used by Gemma)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     window_size: tuple = (-1, -1)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Page table, the index of KV Cache         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Tables/Blocks                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     page_table: torch.Tensor = None             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Encoder metadata                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Cumulative sequence lengths for encoder   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ key                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     encoder_cu_seqlens_k: torch.Tensor = None   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Maximum sequence length for encoder key   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     encoder_max_seq_len_k: int = 0              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Sequence lengths for the forward batch    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     encoder_lens_int32: torch.Tensor = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Page table for the encoder                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     encoder_page_table: torch.Tensor = None     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     @dataclass                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     class LocalAttentionMetadata:               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_query_start_loc: torch.Tensor =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None  # cu_seqlens_q for local attention        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_seqused_k: torch.Tensor = None  # â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sequence lengths for local attention            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_block_table: torch.Tensor = None  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # block table for local attention               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_max_query_len: int = 0  # max     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ query length for local attention                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_max_seq_len: int = 0  # max       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sequence length for local attention             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     local_attn_metadata:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[LocalAttentionMetadata] = None         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Copied from:                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ https://github.com/houseroad/vllm/blob/4e45bfcâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Take in `query_start_loc_np` and              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `seq_lens_np` and break the sequences into      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # local attention blocks, where each block is   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ passed to the attention kernel                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # as an independent local ("virtual") batch     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ item.                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # For example, if are performing a chunked      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prefill a batch of 3 sequences:                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   q_seqlens  = [4, 10, 5]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   kv_seqlens = [6, 17, 9]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Then normally for regular attention we would  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ compute with an attention mask                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #  for batch idx 0 (q_seqlens = 4, kv_seqlens = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 6) like:                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   batch idx: 0 (q_seqlens = 4, kv_seqlens =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 6)                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        k_toks >   0 1 2 3 4 5                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        q_toks v  _____________                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               0 | 1 1 1                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               1 | 1 1 1 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               2 | 1 1 1 1 1                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               3 | 1 1 1 1 1 1                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # for local attention (with attn_chunk_size =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 4) we would compute with an                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #  attention mask like:                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   batch idx: 0  (q_seqlens = 4, kv_seqlens =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 6, attn_chunk_size = 4)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        k_toks >   0 1 2 3 4 5                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        q_toks v  _____________                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               0 | 1 1 1                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               1 | 1 1 1 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               2 |         1                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               3 |         1 1                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # We can simulate this mask using standard      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash-attention by breaking the                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #  sequences into local ("virtual") batches,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ where each local batch item is a                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #  local attention block, so in this case batch â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ idx 0 would be broken up into:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   local-batch idx: 0 (q_seqlens = 2,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kv_seqlens = 4)  (batch 0)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        k_toks >   0 1 2 3                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        q_toks v  _____________                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               0 | 1 1 1                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               1 | 1 1 1 1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   local-batch idx: 1 (q_seqlens = 2,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ kv_seqlens = 2) (batch 0)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        k_toks >   4 5                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #        q_toks v  _____________                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               2 | 1                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #               3 | 1 1                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # e.g. if we have:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   attn_chunk_size = 4                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   query_start_loc_np = [0, 4, 14, 19]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (q_seqlens = [4, 10, 5])                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # Then this function would return:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #                           __b0__              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ______b1______  __b2__ < orig batch indices     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   q_seqlens_local    = [   2,  2,  1,  4,  4, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1,  4,  1]                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   cu_seqlens_q_local = [0, 4,  6, 10, 14, 18, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 19, 23, 24]                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   seqlens_k_local    = [   4,  2,  4,  4,  4, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1,  4,  1]                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ #   block_table_local  : shape                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def make_local_attention_virtual_batches(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     attn_chunk_size: int,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     query_start_loc_np: np.ndarray,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seq_lens_np: np.ndarray,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_table: torch.Tensor,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     page_size: int = 0,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ) -> tuple:                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Take in `query_start_loc_np` and            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ `seq_lens_np` and break the sequences into      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     local attention blocks, where each block is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ passed to the attention kernel                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     as an independent local ("virtual") batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ item.                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Args:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         attn_chunk_size: Size of local          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention chunks                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         query_start_loc_np: Cumulative sum of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ query lengths (numpy array)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens_np: Sequence lengths (numpy    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ array)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         block_table: Block table for KV cache   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         page_size: Size of each page in the KV  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Returns:                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_q_local: Query sequence lengths â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for local attention                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q_local: Cumulative sum of   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ query sequence lengths for local attention      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_k_local: Key sequence lengths   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for local attention                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         block_table_local: Block table for      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local attention                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Adjust attention_chunk_size based on the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ actual sequence length                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # to avoid index out of bounds errors       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     max_seq_len = seq_lens_np.max()             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     effective_chunk_size = min(attn_chunk_size, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_seq_len)                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Make sure effective_chunk_size is         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ divisible by page_size                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     effective_chunk_size =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (effective_chunk_size // page_size) * page_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     if effective_chunk_size < page_size:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         effective_chunk_size = page_size        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     attn_chunk_size = effective_chunk_size      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     q_seqlens = query_start_loc_np[1:] -        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ query_start_loc_np[:-1]                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     actual_batch_size = seq_lens_np.shape[0]    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Handle if we are starting in the middle   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ of a local attention block,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  we assume q_seqlens > 0 (for all         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ elements), for each batch idx we compute        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  the number of tokens that are not in the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ first local attention block and                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  then we can simply use a cdiv for the    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ rest.                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For example if we have:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   attn_chunk_size = 4                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   q_seqlens = [4, 10, 5]                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   k_seqlens = [6, 17, 9]                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Then we would get:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   new_tokens_in_first_block = [2, 1, 4]   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   local_blocks = [2, 4, 2]                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     q_tokens_in_first_block = np.minimum(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         attn_chunk_size - ((seq_lens_np -       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ q_seqlens) % attn_chunk_size), q_seqlens        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ).astype(np.int32)                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     tokens_in_last_block = attn_chunk_size +    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (seq_lens_np % -attn_chunk_size)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     local_blocks = 1 + cdiv(q_seqlens -         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ q_tokens_in_first_block, attn_chunk_size)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Once we know the number of local blocks   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ we can compute the request spans                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  for each batch idx, we can figure out    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ the number of "virtual" requests we             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  have to make,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For the above example we would get:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   seqlens_q_local = [2, 2, 1, 4, 4, 1, 4, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1]                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # First Get batched arange. (E.g., [2, 4,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2] -> [0, 1, 0, 1, 2, 3, 0, 1])                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   (TODO: max a utility to share this code â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ with _prepare_inputs)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # arange step 1. [2, 4, 2] -> [2, 6, 8]     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     cu_num_blocks = np.cumsum(local_blocks)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     virtual_batches = cu_num_blocks[-1]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # arange step 2. [2, 6, 8] -> [0, 0, 2, 2,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2, 2, 6, 6]                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_offsets = np.repeat(cu_num_blocks -   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_blocks, local_blocks)                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # arange step 3. [0, 1, 0, 1, 2, 3, 0, 1]   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     arange = np.arange(virtual_batches,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=np.int32) - block_offsets                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # also compute reverse arange (i.e. [1, 0,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 3, 2, 1, 0, 1, 0])                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     rarange = np.repeat(local_blocks,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_blocks) - arange - 1                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Then we can compute the seqlens_q_local,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ handling the fact that the                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  first and last blocks could be partial   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seqlens_q_local = np.repeat(q_seqlens -     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ q_tokens_in_first_block, local_blocks)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # set the first block since this may be a   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ partial block                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seqlens_q_local = q_tokens_in_first_block   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # set the remaining blocks                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seqlens_q_local = np.minimum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_q_local - attn_chunk_size *     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (arange - 1), attn_chunk_size                   â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 8068 Output: 960                                                                                â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 2: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '200,800p'                 â”‚         seqlens_q_local - attn_chunk_size *     â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ (arange - 1), attn_chunk_size                   â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # convert from q_seqlens to cu_seqlens_q    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     cu_seqlens_q_local =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ np.pad(np.cumsum(seqlens_q_local), (1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0)).astype(np.int32)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # compute the seqlens_k_local,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  basically a full local attention block   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for all but the last block in each              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #  batch                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For our example this will be:             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   seqlens_k_local = [4, 2, 4, 4, 4, 1, 4, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1]                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seqlens_k_local =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ np.full(cu_num_blocks[-1], attn_chunk_size,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=np.int32)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     seqlens_k_local = tokens_in_last_block      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     k_seqstarts_absolute =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ np.repeat(seq_lens_np, local_blocks) - (        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         rarange * attn_chunk_size +             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ np.repeat(tokens_in_last_block, local_blocks)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For the example the local attention       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ blocks start at:                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #                           _b0_            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ _____b1_____  _b2_                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   k_seqstarts_absolute = [0, 4, 4, 8, 12, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 16, 4, 8]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_starts = k_seqstarts_absolute //      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ page_size                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     assert attn_chunk_size % page_size == 0, (  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         f"attn_chunk_size {attn_chunk_size} is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not "                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         f"divisible by page_size {page_size}"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     pages_per_local_batch = attn_chunk_size //  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ page_size                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Create a block_table for the local        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention blocks                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # For out example if we have a block-table  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ like (assuming page_size=2):                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   block_table = [                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 9],  < batch 0                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [10, 11, 12, 13, 14, 15, 16, 17, 18,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 19],  < batch 1                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [20, 21, 22, 23, 24, 25, 26, 27, 28,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 29],  < batch 2                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   ]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Then for the local batches we would want  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ a block-table like                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   block_table_local = [                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [  0,  1 ], < local-batch 0, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0, starting from k[0])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [  2,  3 ], < local-batch 1, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0, starting from k[4])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 12, 13 ], < local-batch 2, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1, starting from k[4])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 14, 15 ], < local-batch 3, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1, starting from k[8])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 16, 17 ], < local-batch 4, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1, starting from k[12])                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 18, 19 ], < local-batch 5, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1, starting from k[16])                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 22, 23 ], < local-batch 6, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2, starting from k[4])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #     [ 24, 25 ], < local-batch 7, (batch   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 2, starting from k[8])                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     #   ]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_indices = np.broadcast_to(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         np.arange(pages_per_local_batch,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=np.int32),                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         (virtual_batches,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pages_per_local_batch),                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) + np.expand_dims(block_starts, axis=1)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # Ensure block_indices doesn't exceed       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ block_table dimensions                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # This is a critical safety check that      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prevents index out of bounds errors             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # when dealing with large sequences (>8192  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tokens) or when the block_table                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     # dimensions are smaller than what would be â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ needed for the full attention chunk size.       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_indices =                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ block_indices.flatten().clip(max=block_table.sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - 1)                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     batch_indices = np.repeat(                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         np.arange(actual_batch_size,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=np.int32),                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_blocks * pages_per_local_batch,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     block_table_local = block_table.view(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         virtual_batches, -1                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     )                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return seqlens_q_local, cu_seqlens_q_local, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seqlens_k_local, block_table_local              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def cdiv(a: int, b: int) -> int:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """Ceiling division."""                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return -(a // -b)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # TODO(hebiao064): remove this once we have a   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ better way to handle the merge_state_v2         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.compile issue                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ @torch._dynamo.disable()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ def merge_state_v2_wrapper(o, s_a, o_exp, s_b): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     return merge_state_v2(o, s_a, o_exp, s_b)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class FlashAttentionBackend(AttentionBackend):  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """FlashAttention backend implementation.   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Note about the init:                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - If no spec decoding                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         - FlashAttentionBackend will be init    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ once when the server starts.                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - If spec decoding                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         - FlashAttentionBackend will be init    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ once for the target worker                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         - FlashAttentionMultiStepBackend will   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ be once for the draft worker                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             - It will spawn num_steps           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionBackend for the draft worker      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     Note about CUDA Graph:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - We only support CUDA Graph for Decode     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (Normal Decode and Draft Decode) and Target     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Verify.                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - We don't support CUDA Graph for Extend    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and Draft Extend.                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - When server init, init_cuda_graph_state   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ will be called first and then                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init_cuda_graph_capture will be called.         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     - For each forward batch,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init_replay_cuda_graph will be called first and â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ then replay the graph.                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     """                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         model_runner: ModelRunner,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         skip_prefill: bool = False,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         speculative_step_id=0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         topk=0,                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         speculative_num_steps=0,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         super().__init__()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         assert not (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_runner.sliding_window_size is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.model_config.is_encoder_decoder    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ), "Sliding window and cross attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ are not supported together"                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata = None                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # extra metadata for handling           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative decoding topk > 1, extended draft   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decode and verify                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata_spec_decode_expaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata = None                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.max_context_len =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.model_config.context_len           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.device = model_runner.device       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.decode_cuda_graph_metadata = {}    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.target_verify_metadata = {}        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.req_to_token =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.req_to_token_pool.req_to_token     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.kv_cache_dtype =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.kv_cache_dtype                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.kv_cache_dtype_str =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.kv_cache_dtype         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.page_size = model_runner.page_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.use_mla =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.model_config.attention_arch ==     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ AttentionArch.MLA                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.skip_prefill = skip_prefill        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.topk =                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model_runner.server_args.speculative_eagle_topk â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ or 0                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.speculative_num_steps =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative_num_steps                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.speculative_num_draft_tokens = (   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_runner.server_args.speculatiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.speculative_step_id =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative_step_id                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Local attention settings              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.attention_chunk_size = (           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             model_runner.attention_chunk_size   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if hasattr(model_runner,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "attention_chunk_size")                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else None                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_forward_metadata(self,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch: ForwardBatch):                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Initialize forward metadata hence    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ all layers in the forward pass can reuse it.""" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata = FlashAttentionMetadata()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_in_batch =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         batch_size = forward_batch.batch_size   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device = seqlens_in_batch.device        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode.is_decode_or_idle(): â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Draft Decode                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if forward_batch.spec_info is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.topk <= 1:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         seqlens_in_batch +      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (self.speculative_step_id + 1)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ).to(torch.int32)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item() + (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_step_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_q =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         0, batch_size + 1,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=device                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         torch.cumsum(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             metadata.cache_seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (1, 0),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         forward_batch.req_poolâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (seqlens_in_batch).to(torch.int32)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_q =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.topk                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_q =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         0,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         batch_size * self.topk  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1,                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         step=self.topk,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         device=device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         torch.cumsum(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             metadata.cache_seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (1, 0),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         forward_batch.req_poolâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     decode_length =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_step_id + 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cache_seqlâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = torch.full(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (seqlens_in_batch.numeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ * self.topk,),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         decode_length,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         device=device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.max_seq_leâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.max_seq_leâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = self.speculative_step_id + 1                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cu_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = torch.arange(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         0,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata_expand.cache_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1,                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         device=device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cu_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = torch.arange(                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         0,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata_expand.cache_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ * decode_length + 1,                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         step=decode_length,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         device=device,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_loc =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.out_cache_loc.view(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_num_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -1                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ).T.contiguous()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.page_table  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cache_loc[:,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :decode_length].contiguous().to(torch.int32)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.forward_metadata_specâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Normal Decode                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seqlens_in_batch.to(torch.int32)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0, batch_size + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=device                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(seqlens_in_baâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32), (1, 0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.req_pool_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # TODO: we need to test this part   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for llama 4 eagle case                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self._init_local_attn_metadata(metâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode.is_target_verify():  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.topk <= 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.seq_lens +    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ).to(torch.int32)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k = (      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.seq_lens_cpuâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     +                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     batch_size *                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.speculative_num_draftâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=device,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata.cache_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (1, 0),                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.req_pool_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._init_local_attn_metadataâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens.to(torch.int32)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     batch_size *                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     step=self.speculative_num_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=device,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata.cache_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (1, 0),                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.req_pool_indâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.max_seq_len_q = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cu_seqlens_q =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.seq_lens.numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ * self.speculative_num_draft_tokens             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     + 1,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=device,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # create expand page table      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 offsets = torch.arange(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.speculative_num_draftâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=device                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ).unsqueeze(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # shape: (1,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cols = offsets.expand(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.seq_lens.numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -1                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ) +                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens.unsqueeze(1)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cum_len =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             forward_batch.seq_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + self.speculative_num_draft_tokens             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ).repeat_interleave(seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dim=0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (1, 0),                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )[:-1]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mask_extraction_indices = (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cols.repeat_interleave(selâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     + cum_len[:, None]          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ).view(1, -1)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mask =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.spec_info.custom_mask[            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask_extraction_indices     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ].view(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # (bsz * draft_num,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ draft_num)                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # shift table indices to avoid  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ padding                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # non_masked_page_table [[8, 9, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10],   mask (display with int format) [[1, 0,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0],                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 #                        [8, 9, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10],                                   [1, 1,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0],                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 #                        [8, 9, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10]]                                   [1, 0,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1]]                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # if masked with padding [[8,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0, 0],   our mask without padding       [[8, 9, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10],                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 #                        [8, 9, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 0],                                    [8, 9,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10],                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 #                        [8, 0, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 10]]                                   [8, 10,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 9]]                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # note here cache_seqlens_int32 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is [1, 2, 2] so extra page indices will be      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ignored in each row                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 col_indices = offsets.expand(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask.shape[0],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Build keys: if an entry is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ valid (mask==True), keep its original index;    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # if not, add                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens so that it    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sorts after all valid entries.                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 keys = torch.where(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask, col_indices,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ col_indices + self.speculative_num_draft_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 _, sort_order =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.sort(keys, dim=1)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 non_masked_page_table = (       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.req_to_tokenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         forward_batch.req_poolâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     .gather(1, cols)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     .repeat_interleave(self.spâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # (bsz, draft_num)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.page_table =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ non_masked_page_table.gather(1, sort_order)     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cache_seqlens_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = mask.sum(dim=1).to(torch.int32)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cu_seqlens_k =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata_expand.cache_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (1, 0),                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.max_seq_len_k = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cache_seqlâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.forward_metadata_spec_decâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode.is_extend_or_draft_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.cache_seqlens_int32 =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seqlens_in_batch.to(torch.int32)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.max_seq_len_k =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item()         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.cu_seqlens_k =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 torch.cumsum(seqlens_in_batch,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32), (1, 0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.page_table =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 forward_batch.req_pool_indices, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 any(forward_batch.extend_prefiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 or forward_batch.forward_mode   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == ForwardMode.DRAFT_EXTEND                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 extend_seq_lens =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.extend_seq_lens                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max(forward_batch.extend_seq_lens_cpu)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(extend_seq_leâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32), (1, 0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cu_seqlens_k                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Setup local attention if enabled  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if forward_batch.forward_mode ==    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ ForwardMode.EXTEND:                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._init_local_attn_metadataâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Encoder metadata for cross attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if forward_batch.encoder_lens is not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert (                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 forward_batch.encoder_lens.numâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ == 1                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ), "Only encoder size 1 is          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported for now"                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_lens_int32 =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.encoder_lens.to(torch.int32)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 torch.cumsum(metadata.encoder_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (1, 0),                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.encoder_lens_int32.max().item()        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 forward_batch.req_pool_indices, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : metadata.encoder_max_seq_len_k                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Currently only support            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.encoder_lens.numel() == 1         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.page_table =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.req_to_token_pool.req_to_token[   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 forward_batch.req_pool_indices, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.encoder_max_seq_len_k  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.encoder_max_seq_lâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Convert the page table to a strided   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ format which is needed by FA3 API               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.page_size > 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.strided_indices =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 0,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.page_table.shape[1], self.page_size,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.page_table = (             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table[:,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.strided_indices] // self.page_size         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata = metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_extend(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         q: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         v: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         layer: RadixAttention,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch: ForwardBatch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         save_kv_cache=True,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # For multi-head latent attention       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         q_rope: Optional = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k_rope: Optional = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if k is not None:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert v is not None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if save_kv_cache:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_loc = (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.out_cache_loc â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if not                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.is_cross_attention                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     else                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.encoder_out_cache_loc             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if not self.use_mla:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.token_to_kv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         layer, cache_loc, k, v, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.k_scale, layer.v_scale                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.token_to_kv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         layer,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cache_loc,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k_rope,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Use precomputed metadata across all   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata = self.forward_metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Calculate window size (can be moved   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to metadata if layer properties don't change)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # we don't do layer.sliding_window_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - 1 since in                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model.get_attention_sliding_window_size() we    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ already - 1                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # here is two side inclusive            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         window_size = (                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             (layer.sliding_window_size, 0)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if layer.sliding_window_size is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None and layer.sliding_window_size > -1         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else (-1, -1)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k_descale, v_descale = None, None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # only use kv scaling if: 1) fp8 kv is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ explicitly enabled, 2) RadixAttention           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # has corresponding quantization method â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ so that layer.k_scale is not None               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.kv_cache_dtype_str != "auto"    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and layer.k_scale is not None:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             descale_shape =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (forward_batch.batch_size, layer.tp_k_head_num) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             k_descale =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.k_scale.expand(descale_shape)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             v_descale =                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_scale.expand(descale_shape)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             q = q.to(self.kv_cache_dtype)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         causal = not layer.is_cross_attention   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Check if we should use local          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         use_local_attn = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_chunk_size is not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and metadata.local_attn_metadata is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and (hasattr(layer, "use_irope")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and layer.use_irope)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # We do cascade attention for Target    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Verify with topk > 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         use_cascade_attn = (                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             forward_batch.forward_mode.is_targâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and self.topk > 1                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Get the appropriate page table based  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ on whether we're using local attention          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if use_local_attn:                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_metadata =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.local_attn_metadata                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             page_table =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata.local_block_table                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata.local_query_start_loc            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cache_seqlens =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata.local_seqused_k                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seqlen_q =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata.local_max_query_len              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seqlen_k =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata.local_max_seq_len                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             page_table = metadata.page_table    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cu_seqlens_q                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cache_seqlens =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cache_seqlens_int32                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seqlen_q =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_q                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seqlen_k =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_k =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cu_seqlens_k                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Use Flash Attention for prefill       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not self.use_mla:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Do multi-head attention           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             key_cache, value_cache =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.token_to_kv_pool.get_kv_buffer(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 layer.layer_id                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             key_cache = key_cache.view(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1, self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.head_dim             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             value_cache = value_cache.view(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1, self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_v_head_num, layer.head_dim             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if layer.is_cross_attention:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_table =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.encoder_page_table                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_seqlens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.encoder_lens_int32                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_k =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.encoder_cu_seqlens_k                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 window_size = (-1, -1)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = flash_attn_with_kvcache(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q=q.contiguous().view(-1,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_cache=key_cache,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 v_cache=value_cache,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_table=page_table,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_seqlens=cache_seqlens,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_q=cu_seqlens_q,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_k_new=cu_seqlens_k   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if not use_local_attn else None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_seqlen_q=max_seqlen_q,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 softmax_scale=layer.scaling,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 causal=False if                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use_cascade_attn else causal,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 window_size=window_size,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 softcap=layer.logit_cap,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_descale=k_descale,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 v_descale=v_descale,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_softmax_lse=use_cascadeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if use_cascade_attn:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o, softmax_lse, *rest = result  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o_expand, softmax_lse_expand,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ *rest_expand = flash_attn_with_kvcache(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q.contiguous().view(-1,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=key_cache,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=value_cache,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=self.forward_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=self.forwardâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=self.forward_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=self.forwâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=self.forward_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=False,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     window_size=window_size,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return_softmax_lse=True,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o, _ = merge_state_v2_wrapper(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_lse.T.contiguous(), â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o_expand,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_lse_expand.T.contiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o = result                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if (                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 not                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ global_server_args_dict["disable_chunked_prefiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 and                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.attn_attend_prefix_cache is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 and not                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode.is_target_verify()   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 and not                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.forward_mode.is_draft_extend()    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ):                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Do multi-head attention with  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ chunked prefix cache                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.attn_attend_prefix_cache:         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # MHA for chunked prefix kv â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache when running model with MLA               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     assert                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.prefix_chunk_idx is not None      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     assert                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.prefix_chunk_cu_seq_lens is not   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     assert                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.prefix_chunk_max_seq_lens is not  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     chunk_idx =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.prefix_chunk_idx                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     assert chunk_idx >= 0       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output, lse, *rest =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash_attn_varlen_func(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         q=q.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k=k.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         v=v.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.v_head_dim),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cu_seqlens_q=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cu_seqlens_k=forward_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         max_seqlen_q=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         max_seqlen_k=forward_bâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         softmax_scale=layer.scâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         causal=False,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         return_softmax_lse=Truâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # MHA for extend part of    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sequence without attending prefix kv cache      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     output, lse, *rest =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash_attn_varlen_func(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         q=q.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k=k.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         v=v.view(-1,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.v_head_dim),         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cu_seqlens_q=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cu_seqlens_k=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         max_seqlen_q=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 19785 Output: 1040                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 3: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '800,1600p'                â”‚                         max_seqlen_q=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                         max_seqlen_k=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                         softmax_scale=layer.scâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         causal=True,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         return_softmax_lse=Truâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return output, lse              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Do absorbed multi-latent      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 kv_cache =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.token_to_kv_pool.get_key_buffer(â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_rope = kv_cache[:, :,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim :]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 c_kv = kv_cache[:, :, :         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_rope_cache = k_rope.view(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     layer.tp_k_head_num,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     layer.head_dim -            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 c_kv_cache = c_kv.view(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1, self.page_size,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_v_head_num, layer.v_head_dim           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if q_rope is not None:          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q_nope = q.view(-1,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.v_head_dim)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q_rope = q_rope.view(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         -1,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim -           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q_all =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ q.contiguous().view(-1, layer.tp_q_head_num,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.head_dim)                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q_nope = q_all[:, :, :      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q_rope = q_all[:, :,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim :]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 result =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash_attn_with_kvcache(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q_rope,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=k_rope_cache,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=c_kv_cache,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     qv=q_nope,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=page_table,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=cache_seqlenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=cu_seqlens_q,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=cu_seqlenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if not use_local_attn else None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=max_seqlen_q,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=False if             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use_cascade_attn else causal,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return_softmax_lse=use_casâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if use_cascade_attn:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o, softmax_lse, *rest =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o_expand,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ softmax_lse_expand, *rest_expand = (            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         flash_attn_with_kvcachâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             q=q_rope,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             k_cache=k_rope_cacâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             v_cache=c_kv_cache, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             qv=q_nope,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             page_table=self.foâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cache_seqlens=selfâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cu_seqlens_q=self.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cu_seqlens_k_new=sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             max_seqlen_q=self.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             softmax_scale=layeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             causal=False,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             window_size=windowâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             softcap=layer.logiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             k_descale=k_descalâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             v_descale=v_descalâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             return_softmax_lseâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o, _ =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ merge_state_v2_wrapper(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         o,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         softmax_lse.T.contiguoâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         o_expand,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         softmax_lse_expand.T.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o = result                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return o.view(-1, layer.tp_q_head_num * â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def forward_decode(                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         q: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         v: torch.Tensor,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         layer: RadixAttention,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_batch: ForwardBatch,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         save_kv_cache=True,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # For multi-head latent attention       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         q_rope: Optional = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k_rope: Optional = None,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ) -> torch.Tensor:                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if k is not None:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             assert v is not None                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if save_kv_cache:                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_loc = (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.out_cache_loc â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     if not                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.is_cross_attention                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     else                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.encoder_out_cache_loc             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if not self.use_mla:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.token_to_kv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         layer, cache_loc, k, v, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.k_scale, layer.v_scale                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     forward_batch.token_to_kv_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         layer,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cache_loc,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         k_rope,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Use precomputed metadata across all   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layers                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata = self.forward_metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_attn_metadata = getattr(metadata, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ "local_attn_metadata", None)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         use_local_attn = (                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_chunk_size is not    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and local_attn_metadata is not None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and (hasattr(layer, "use_irope")    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and layer.use_irope)                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # We do cascade attention for Draft     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Decode with topk > 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         use_cascade_attn = self.topk > 1        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Calculate window size (can be moved   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ to metadata if layer properties don't change)   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # we don't do layer.sliding_window_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - 1 since in                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ model.get_attention_sliding_window_size() we    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ already - 1                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # here is two side inclusive            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         window_size = (                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             (layer.sliding_window_size, 0)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if layer.sliding_window_size is not â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None and layer.sliding_window_size > -1         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else (-1, -1)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         causal = not layer.is_cross_attention   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k_descale, v_descale = None, None       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # only use kv scaling if: 1) fp8 kv is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ explicitly enabled, 2) RadixAttention           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # has corresponding quantization method â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ so that layer.k_scale is not None               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.kv_cache_dtype_str != "auto":   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if layer.k_scale is not None:       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 descale_shape =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (forward_batch.batch_size, layer.tp_k_head_num) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_descale =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.k_scale.expand(descale_shape)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 v_descale =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_scale.expand(descale_shape)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             q = q.to(self.kv_cache_dtype)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if not self.use_mla:                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Do multi-head attention           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             key_cache, value_cache =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.token_to_kv_pool.get_kv_buffer(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 layer.layer_id                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             key_cache = key_cache.view(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1, self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_k_head_num, layer.head_dim             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             value_cache = value_cache.view(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1, self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_v_head_num, layer.head_dim             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if layer.is_cross_attention:        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Always use non-chunked logic  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for cross-attention                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o = flash_attn_with_kvcache(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q.contiguous().view(-1,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=key_cache,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=value_cache,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=metadata.encodeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=metadata.encâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=metadata.cu_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=metadata.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=1,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=False,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     window_size=(-1, -1),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             elif use_local_attn:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Use chunked (local) attention â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batching for self-attention                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o = flash_attn_with_kvcache(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q.contiguous().view(-1,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim),           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=key_cache,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=value_cache,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=local_attn_metaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=local_attn_mâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=local_attn_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=None,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=local_attn_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=True,                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     window_size=(-1, -1),       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_table =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.page_table                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_seqlens =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cache_seqlens_int32                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_k =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cu_seqlens_k                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_seqlen_q =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_q                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_reshaped =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ q.contiguous().view(                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1, layer.tp_q_head_num,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.head_dim                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Default: single-token         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self-attention                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 result =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ flash_attn_with_kvcache(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q_reshaped,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=key_cache,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=value_cache,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=page_table,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=cache_seqlenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=metadata.cu_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=cu_seqlenâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=max_seqlen_q,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=False if             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use_cascade_attn else causal,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     window_size=window_size,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return_softmax_lse=use_casâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if use_cascade_attn:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o, softmax_lse, *rest =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ result                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o_expand,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ softmax_lse_expand, *rest_expand = (            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         flash_attn_with_kvcachâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             q=q_reshaped,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             k_cache=key_cache,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             v_cache=value_cachâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             page_table=self.foâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cache_seqlens=selfâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cu_seqlens_q=self.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             cu_seqlens_k_new=sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             max_seqlen_q=self.â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             softmax_scale=layeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             causal=False,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             window_size=windowâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             softcap=layer.logiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             k_descale=k_descalâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             v_descale=v_descalâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             return_softmax_lseâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o, _ = merge_state_v2(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         o,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         softmax_lse.T.contiguoâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         o_expand,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         softmax_lse_expand.T.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o = result                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         else:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Do absorbed multi-latent          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             kv_cache =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.token_to_kv_pool.get_key_buffer(â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             k_rope = kv_cache[:, :,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim :]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             c_kv = kv_cache[:, :, :             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             k_rope_cache = k_rope.view(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1,                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.page_size,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 layer.tp_k_head_num,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 layer.head_dim -                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim,                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             c_kv_cache = c_kv.view(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 -1, self.page_size,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_v_head_num, layer.v_head_dim           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if q_rope is not None:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_nope = q.view(-1,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.v_head_dim)          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_rope = q_rope.view(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1, layer.tp_q_head_num,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.head_dim - layer.v_head_dim               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_all = q.contiguous().view(-1, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.tp_q_head_num, layer.head_dim)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_nope = q_all[:, :, :          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q_rope = q_all[:, :,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim :]                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seqlen_q =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_q                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             result = flash_attn_with_kvcache(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 q=q_rope,                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_cache=k_rope_cache,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 v_cache=c_kv_cache,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 qv=q_nope,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_table=metadata.page_table, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cache_seqlens=metadata.cache_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_q=metadata.cu_seqleâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cu_seqlens_k_new=metadata.cu_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_seqlen_q=max_seqlen_q,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 softmax_scale=layer.scaling,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 causal=False if                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ use_cascade_attn else causal,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 softcap=layer.logit_cap,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 k_descale=k_descale,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 v_descale=v_descale,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 return_softmax_lse=use_cascadeâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ # softmax_lse is needed for merge states        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if use_cascade_attn:                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o, softmax_lse, *rest = result  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o_expand, softmax_lse_expand,   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ *rest_expand = flash_attn_with_kvcache(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     q=q_rope,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_cache=k_rope_cache,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_cache=c_kv_cache,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     qv=q_nope,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table=self.forward_meâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_seqlens=self.forwardâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_q=self.forward_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cu_seqlens_k_new=self.forwâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seqlen_q=self.forward_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_scale=layer.scalinâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     causal=False,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     window_size=window_size,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softcap=layer.logit_cap,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     k_descale=k_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     v_descale=v_descale,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     return_softmax_lse=True,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o, _ = merge_state_v2(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_lse.T.contiguous(), â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     o_expand,                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     softmax_lse_expand.T.contiâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 o = result                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return o.view(-1, layer.tp_q_head_num * â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ layer.v_head_dim)                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_cuda_graph_state(self, max_bs:     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int):                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Initialize CUDA graph state for the  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention backend.                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         Args:                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_bs (int): Maximum batch size to â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ support in CUDA graphs                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         This creates fixed-size tensors that    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ will be reused during CUDA graph replay         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         to avoid memory allocations.            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This is being used by normal decode   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ and draft decode when topk == 1                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.decode_cuda_graph_metadata = {     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "cache_seqlens":                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(max_bs, dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device),                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "cu_seqlens_q": torch.arange(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 0, max_bs + 1,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "cu_seqlens_k": torch.zeros(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs + 1, dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "page_table": torch.zeros(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (self.max_context_len +         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size - 1) // self.page_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dtype=torch.int32,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 device=self.device,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "page_table_draft_decode":          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (self.max_context_len +         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size - 1) // self.page_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dtype=torch.int32,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 device=self.device,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "strided_indices": torch.arange(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 0, self.max_context_len,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size, device=self.device              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         }                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Only allocate local attention buffers â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ if local attention is enabled                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This prevents OOM errors when local   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention is not being used                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.attention_chunk_size is not     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Estimate maximum sizes for local  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention metadata                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_seq_len = self.max_context_len  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             page_size = self.page_size or 1     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             attn_chunk_size =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.attention_chunk_size                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_virtual_batches = max_bs * (    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 (max_seq_len + attn_chunk_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ - 1) // attn_chunk_size                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             max_pages_per_block =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (attn_chunk_size + page_size - 1) // page_size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.decode_cuda_graph_local_attn_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = {                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "local_query_start_loc":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_virtual_batches + 1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "local_seqused_k": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_virtual_batches,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "local_block_table":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_virtual_batches,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_pages_per_block,        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This is used by draft decode's first  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ half of metadata when topk > 1                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.topk > 1:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.draft_decode_metadata_topk_noâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = {                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cache_seqlens": torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs, dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_q": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs * self.topk + 1,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     step=self.topk,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_k": torch.zeros(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs + 1,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "page_table": torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.max_context_len,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # This is used by draft decode's    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ second half of metadata when topk > 1           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             decode_length =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_step_id + 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.draft_decode_metadata_topk_exâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = {                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cache_seqlens": torch.full(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (max_bs * self.topk,),      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     decode_length,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_q": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs * self.topk + 1,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_k": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs * self.topk *        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ decode_length + 1,                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     step=decode_length,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "page_table": torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs * self.topk,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     decode_length,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if (                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.speculative_num_draft_tokens   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ is not None                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             and                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens > 0           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ):                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.target_verify_metadata = {     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cache_seqlens": torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs, dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_q": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     step=self.speculative_num_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_k": torch.zeros(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs + 1,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "page_table": torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (self.max_context_len +     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size - 1) // self.page_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "strided_indices":              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0, self.max_context_len,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size, device=self.device              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.topk > 1:                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.target_verify_metadata_topk_nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = {                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cache_seqlens": torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs, dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_q": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     step=self.speculative_num_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_k": torch.zeros(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs + 1,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=self.device           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "page_table": torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.max_context_len,       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.target_verify_metadata_topk_eâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = {                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cache_seqlens": torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_k": torch.zeros(    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "cu_seqlens_q": torch.arange(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "page_table": torch.zeros(      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_bs *                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.speculative_num_draftâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=self.device,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             }                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.encoder_metadata = {               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "encoder_page_table": torch.zeros(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.max_context_len,           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 dtype=torch.int32,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 device=self.device,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "encoder_lens_int32": torch.zeros(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs, dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "encoder_cu_seqlens_k":             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_bs + 1, dtype=torch.int32,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ),                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         }                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init_forward_metadata_capture_cuda_graph(       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         bs: int,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         num_tokens: int,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req_pool_indices: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens: torch.Tensor,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         encoder_lens: Optional,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_mode: ForwardMode,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         spec_info:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Union[EagleDraftInput,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleVerifyInput]],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Initialize forward metadata for      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ capturing CUDA graph."""                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata = FlashAttentionMetadata()     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # metadata_expand is needed for Spec    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Decoding when top k > 1                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata_expand =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata()                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device = seq_lens.device                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if forward_mode.is_decode_or_idle():    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if spec_info is not None:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Draft Decode                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.topk <= 1:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # When topk = 1, we use the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ normal decode metadata                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = self.decode_cuda_graph_metadata[              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "cache_seqlens"         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ][:bs]                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.max().item() + (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_step_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_q =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata[                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "cu_seqlens_q"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ][: bs + 1]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         torch.cumsum(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             metadata.cache_seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ),                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (1, 0),                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata[                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "page_table_draft_decoâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.decode_cuda_graph_metâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # When top k > 1, we need   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ two specific draft decode metadata, and then    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ merge states                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # 1. The first half of      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for prefix tokens                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.draft_decode_metaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_q =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.topk                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.max().item()                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_q =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.draft_decode_metadata_topk_normal[         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "cu_seqlens_q"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ][: bs + 1]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.draft_decode_metadata_topk_normal[         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "cu_seqlens_k"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ][: bs + 1]                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.draft_decode_metadata_topk_normal[         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "page_table"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # 2. The second half of     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for draft tokens (per_batch_num_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = topk)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cache_seqlâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.draft_decode_metaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             : bs * self.topk    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.max_seq_leâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.max_seq_leâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_step_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )  # , do this in replay    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cu_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.draft_decode_metaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             : bs * self.topk +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cu_seqlensâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.draft_decode_metaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             : bs * self.topk +  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ]                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.page_table  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = self.draft_decode_metadata_topk_expand[       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         "page_table"            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ][: bs * self.topk]         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.draft_decode_metadataâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.draft_decode_metadataâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Normal Decode                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Get sequence information      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.to(torch.int32)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 batch_size = len(seq_lens)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 device = seq_lens.device        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(seq_lens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32), (1, 0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Precompute maximum sequence   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.max().item()                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Precompute page table         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata["page_table"][  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req_pool_indices, :         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Precompute cumulative         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sequence lengths                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0, batch_size + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=torch.int32, device=device                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.decode_cuda_graph_metadata â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.attention_chunk_size is â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ not None:                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.local_attn_metadaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ =                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata.LocalAttentionMetadata(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         local_query_start_loc=â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             "local_query_startâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ],                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         local_seqused_k=self.dâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             "local_seqused_k"   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ],                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         local_block_table=selfâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             "local_block_table" â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ],                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         local_max_query_len=1,  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         local_max_seq_len=1,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif forward_mode.is_target_verify():   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.topk <= 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata[                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cache_seqlens"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][:bs]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (seq_lens +                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens).to(torch.inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k = (      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     seq_lens.max().item() +     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.arange(                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     bs *                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.speculative_num_draftâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     device=device,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata["cu_seqlens_k"][    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     : (bs + 1)                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata["page_table"][      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req_pool_indices, :         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.target_verify_metadata =   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # When topk > 1, we need two    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ specific target verify metadata, and then merge â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ states                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # 1. The first half of metadata â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for prefix tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_normal[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cache_seqlens"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][:bs]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_q =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # metadata.max_seq_len_k =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch.seq_lens_cpu.max().item(), do     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ this in replay                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_q =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_normal[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cu_seqlens_q"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][: bs + 1]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k =         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_normal[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cu_seqlens_k"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][: bs + 1]                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_normal[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "page_table"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # 2. The second half of         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for draft tokens (per_batch_num_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = topk)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cache_seqlens_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.target_verify_metadatâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         : bs *                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.max_seq_len_q = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cu_seqlens_q =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_expand[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cu_seqlens_q"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][: bs *                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1]          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cu_seqlens_k =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_expand[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "cu_seqlens_k"              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][: bs *                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens + 1]          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.page_table =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_expand[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     "page_table"                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ][: bs *                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens]              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.target_verify_metadata_toâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.target_verify_metadata_toâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if encoder_lens is not None:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             encoder_bs = encoder_lens.numel()   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_lens_int32 =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_metadata["encoder_lens_int32"][    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 :encoder_bs                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_cu_seqlens_k =     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_metadata[                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 "encoder_cu_seqlens_k"          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ][: (encoder_bs + 1)]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_page_table =       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.encoder_metadata["encoder_page_table"][    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req_pool_indices, :             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata = metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata_spec_decode_expaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ init_forward_metadata_replay_cuda_graph(        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self,                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         bs: int,                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req_pool_indices: torch.Tensor,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens: torch.Tensor,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens_sum: int,                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         encoder_lens: Optional,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         forward_mode: ForwardMode,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         spec_info:                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ Optional[Union[EagleDraftInput,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ EagleVerifyInput]],                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens_cpu: Optional,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         out_cache_loc: torch.Tensor = None,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Initialize forward metadata for      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ replaying CUDA graph."""                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens = seq_lens[:bs]                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens_cpu = seq_lens_cpu[:bs]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         req_pool_indices =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ req_pool_indices[:bs]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device = seq_lens.device                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata = None                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata_expand = None                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if forward_mode.is_decode_or_idle():    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if spec_info is not None:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Draft Decode                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 if self.topk <= 1:              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # When topk = 1, we use the â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ normal decode metadata                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (seq_lens +             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (self.speculative_step_id + 1)).to(torch.int32) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_cpu.max().item() + (                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_step_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + 1                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k[1:].â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         torch.cumsum(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             metadata.cache_seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     max_seq_pages = (           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata.max_seq_len_k  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + self.page_size - 1                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ) // self.page_size         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_indices =              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_to_token[                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         req_pool_indices[:,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None],                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.decode_cuda_graphâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             :max_seq_pages      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ],                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_indices //=            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table[:,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :max_seq_pages].copy_(page_indices)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 else:                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # When top k > 1, we need   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ two specific draft decode metadata, and then    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ merge states                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # 1. The first half of      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for prefix tokens                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.draft_decode_metadata_topk_normal          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cache_seqlens_intâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # metadata.max_seq_len_q =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.topk, already set in capture               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_cpu.max().item()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # metadata.cu_seqlens_q     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ already set in capture                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.cu_seqlens_k[1:].â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         torch.cumsum(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             metadata.cache_seqâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 37724 Output: 1121                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 4: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "sed -n '1600,2800p'               â”‚                         )                       â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     page_table =                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_to_token[                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         req_pool_indices, :     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ]                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.page_table[:, :    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k].copy_(page_table)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     # 2. The second half of     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for draft tokens (per_batch_num_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = topk)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.draft_decode_metadata_topk_expand          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     decode_length =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_step_id + 1                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cache_loc =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ out_cache_loc.view(                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         self.speculative_num_sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ -1                                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ).T.contiguous()            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.page_tableâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache_loc.shape[0]].copy_(                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         cache_loc[:,            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :decode_length].contiguous().to(torch.int32)    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # TODO: Handle local attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for draft decode when llama4 eagle is  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ supported                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Normal Decode                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_len =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_cpu.max().item()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_len                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32 =  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.to(torch.int32)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # Optimize cumulative sequence  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length calculation                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k[1:].copyâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(seq_lens,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_seq_pages = (               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k +    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size - 1                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ) // self.page_size             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_indices =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_to_token[                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req_pool_indices[:, None],  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.decode_cuda_graph_metâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         None, :                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ],                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_indices //= self.page_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table[:,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :max_seq_pages].copy_(page_indices)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table[:,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_seq_pages:].fill_(0)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self._update_local_attn_metadaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bs)                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         elif forward_mode.is_target_verify():   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             if self.topk <= 1:                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (seq_lens +                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens).to(torch.inâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k = (      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     seq_lens_cpu.max().item() + â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k[1:].copyâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(metadata.cachâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 max_seq_pages = (               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.max_seq_len_k +    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.page_size - 1                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ) // self.page_size             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_indices =                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.req_to_token[                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req_pool_indices[:, None],  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.decode_cuda_graph_metâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_indices //= self.page_size â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table[:,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :max_seq_pages].copy_(page_indices)             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             else:                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # When topk > 1, we need two    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ specific target verify metadata, and then merge â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ states                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # 1. The first half of metadata â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ for prefix tokens                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata =                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_normal         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cache_seqlens_int32.câ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # metadata.max_seq_len_q =      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens, already set  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ in capture                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.max_seq_len_k =        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens_cpu.max().item()                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # metadata.cu_seqlens_q already â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ set in capture                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.cu_seqlens_k[1:].copyâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(metadata.cachâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 page_table = self.req_to_token[ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     req_pool_indices, :         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ]                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.page_table[:, :        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k].copy_(page_table)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # 2. The second half of         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata for draft tokens (per_batch_num_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = topk)                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand =               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.target_verify_metadata_topk_expand         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # metadata_expand.max_seq_len_q â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = 1, already set in capture                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # metadata_expand.cu_seqlens_q  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ already set in capture                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 offsets = torch.arange(         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.speculative_num_draftâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=device                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ).unsqueeze(                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     0                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # shape: (1,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens)              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cols =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ offsets.expand(seq_lens.numel(), -1) +          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ seq_lens.unsqueeze(1)                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 cum_len =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.nn.functional.pad(                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         (                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                             seq_lens +          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         ).repeat_interleave(seâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dim=0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     ),                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     (1, 0),                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )[:-1]                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mask_extraction_indices = (     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     cols.repeat_interleave(selâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     + cum_len[:, None]          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ).view(1, -1)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 # avoid extracting padded seq   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ indices which will be out of boundary           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mask_extraction_indices[        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     :,                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_info.positions.numel() *                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens :             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ].fill_(0)                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 mask =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ spec_info.custom_mask.view(                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     -1,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # (bsz * draft_num,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ draft_num)                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 col_indices = offsets.expand(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask.shape[0],              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.speculative_num_draft_tokens               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 keys = torch.where(             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask, col_indices,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ col_indices + self.speculative_num_draft_tokens â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 _, sort_order =                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.sort(keys, dim=1)                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 non_masked_page_table = (       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     self.req_to_token           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     .gather(1, cols)            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     .repeat_interleave(self.spâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0)                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )  # (bsz, draft_num)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.page_table.copâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     non_masked_page_table.gathâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ sort_order)                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cache_seqlens_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     mask.sum(dim=1).to(torch.iâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.cu_seqlens_k[1â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     torch.cumsum(               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         metadata_expand.cache_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dim=0,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                         dtype=torch.int32,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     )                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata_expand.max_seq_len_k = â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ (                                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata_expand.cache_seqlâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if encoder_lens is not None:            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Only support encoder size 1 for   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ now                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_max_seq_len_k =    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ encoder_lens[0]                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_lens_int32.copy_(â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_cu_seqlens_k[1:].â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 torch.cumsum(metadata.encoder_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dim=0, dtype=torch.int32)                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.encoder_page_table[:, :    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.encoder_max_seq_len_k].copy_(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 self.req_to_token               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             # Update the regular page table     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             page_table = self.req_to_token[     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 req_pool_indices,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 metadata.encoder_max_seq_len_k  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ : (                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     metadata.encoder_max_seq_lâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ + metadata.max_seq_len_k                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 ),                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             ]                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.page_table[:, :            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.max_seq_len_k].copy_(page_table)       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata = metadata        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.forward_metadata_spec_decode_expaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ = metadata_expand                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_cuda_graph_seq_len_fill_value(self):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Get the fill value for sequence      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ length in CUDA graph."""                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         return 0                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _init_local_attn_metadata(self,         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata: FlashAttentionMetadata, device):      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Centralized utility to initialize    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_attn_metadata if chunked attention is     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ enabled."""                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.attention_chunk_size is None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.local_attn_metadata = None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q = metadata.cu_seqlens_q    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cache_seqlens_int32 =                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cache_seqlens_int32                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         page_table = metadata.page_table        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if cu_seqlens_q is None or              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache_seqlens_int32 is None or page_table is    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             metadata.local_attn_metadata = None â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q_np =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cu_seqlens_q.cpu().numpy()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seq_lens_np =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cache_seqlens_int32.cpu().numpy()               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         (                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seqlens_q_local_np,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q_local_np,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seqlens_k_local_np,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             block_table_local,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ) =                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ make_local_attention_virtual_batches(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_chunk_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q_np,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seq_lens_np,                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             page_table,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.page_size,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_metadata =                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ FlashAttentionMetadata.LocalAttentionMetadata(  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_query_start_loc=torch.from_nâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_seqused_k=torch.from_numpy(sâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_block_table=block_table_locaâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_max_query_len=int(seqlens_q_â€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             local_max_seq_len=int(seqlens_k_loâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         metadata.local_attn_metadata =          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ local_metadata                                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def _update_local_attn_metadata_for_replay( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, metadata: FlashAttentionMetadata, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ bs: int                                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         """Update preallocated local attention  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata in-place before CUDA graph replay."""  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if self.attention_chunk_size is None:   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             return                              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Access preallocated buffers           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_q_buf =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_local_attn_metadata[     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "local_query_start_loc"             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_k_buf =                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_local_attn_metadata["loâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_block_buf =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_local_attn_metadata[     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             "local_block_table"                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ]                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q =                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ self.decode_cuda_graph_metadata["cu_seqlens_q"] â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Create a modified version for local   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ attention that only processes the last token    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This mimics the normal decode pattern â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q = torch.arange(            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             bs + 1, device=cu_seqlens_q.device, â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ dtype=cu_seqlens_q.dtype                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens =                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.cache_seqlens_int32[:bs]               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Slice the page_table to match the     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch size and actual sequence length           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # This serves three important purposes: â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # 1. Ensures we only process the actual â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ batch size (bs) and not the maximum batch size  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # 2. Limits the sequence length to      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ prevent processing padding tokens or garbage    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ values                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # 3. Prevents zeros in the block table  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ which can cause garbage output during replay    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         #                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Without this slicing, the             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ pre-allocated page_table may contain zeros or   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ invalid indices                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # beyond the actual sequence length,    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ leading to incorrect attention calculations     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         max_seq_len = int(seqlens.max().item()) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         sliced_page_table =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ metadata.page_table[:bs, :max_seq_len]          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q_np =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ cu_seqlens_q.cpu().numpy()                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_np = seqlens.cpu().numpy()      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         (                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seqlens_q_local_np,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q_local_np,              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seqlens_k_local_np,                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             block_table_local,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         ) =                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ make_local_attention_virtual_batches(           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attention_chunk_size,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             cu_seqlens_q_np,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             seqlens_np,                         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             sliced_page_table,                  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.page_size,                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         )                                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Convert back to tensors               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         device = local_q_buf.device             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         cu_seqlens_q_local =                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.from_numpy(cu_seqlens_q_local_np).to(devâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         seqlens_k_local =                       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.from_numpy(seqlens_k_local_np).to(device) â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         block_table_local =                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ block_table_local.to(device)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # Get sizes                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         q_len = cu_seqlens_q_local.shape[0]     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         k_len = seqlens_k_local.shape[0]        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         b0, b1 = block_table_local.shape        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         # In-place updates into preallocated    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ tensors and zero out the unused space           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_q_buf[:q_len].copy_(cu_seqlens_qâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_q_buf.fill_(0)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_k_buf[:k_len].copy_(seqlens_k_loâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_k_buf.fill_(0)                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_block_buf[:b0,                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ :b1].copy_(block_table_local)                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_block_buf.fill_(0)                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         local_block_buf[:b0, b1:].fill_(0)      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         if metadata.local_attn_metadata is not  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ None:                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             lam = metadata.local_attn_metadata  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             lam.local_max_query_len =           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int(seqlens_q_local_np.max())                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             lam.local_max_seq_len =             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int(seqlens_k_local_np.max())                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ class FlashAttentionMultiStepBackend:           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def __init__(                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self, model_runner: ModelRunner, topk:  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ int, speculative_num_steps: int                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     ):                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.model_runner = model_runner        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.topk = topk                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.speculative_num_steps =            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ speculative_num_steps                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         self.attn_backends = []                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i in                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(self.speculative_num_steps):              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attn_backends.append(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 FlashAttentionBackend(          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     model_runner,               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     speculative_step_id=i,      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     topk=self.topk,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                     speculative_num_steps=selfâ€¦ â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                 )                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             )                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚     def init_forward_metadata(self,             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ forward_batch: ForwardBatch):                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚         for i in                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ range(self.speculative_num_steps - 1):          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚             self.attn_backends[3m.init_forward_meâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def init_cuda_graph_state(self, max_bs: [0m    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mint):[0m                                           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i in [0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(self.speculative_num_steps):[0m              â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.attn_backends[0m[3m.init_cuda_graphâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minit_forward_metadata_capture_cuda_graph([0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self,[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        forward_batch: ForwardBatch,[0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    ):[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        assert forward_batch.spec_info is not [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mNone[0m                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        assert [0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3misinstance(forward_batch.spec_info, [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mEagleDraftInput)[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i in [0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(self.speculative_num_steps - 1):[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.attn_backends[0m[3m.init_forward_meâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.batch_size,[0m       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.batch_size * [0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mself.topk,[0m                                      â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.req_pool_indices,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.seq_lens,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                encoder_lens=forward_batch.encâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_mode=ForwardMode.DECODâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                spec_info=forward_batch.spec_iâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    def [0m                                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3minit_forward_metadata_replay_cuda_graph([0m        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        self, forward_batch: ForwardBatch, bs: [0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mint[0m                                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m    ):[0m                                          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        assert forward_batch.spec_info is not [0m  â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mNone[0m                                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        assert [0m                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3misinstance(forward_batch.spec_info, [0m            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mEagleDraftInput)[0m                                â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m        for i in [0m                               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3mrange(self.speculative_num_steps - 1):[0m          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            self.attn_backends[0m[3m.init_forward_meâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                bs,[0m                             â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.req_pool_indices,[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.seq_lens,[0m         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_batch.seq_lens_sum,[0m     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                encoder_lens=forward_batch.encâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                forward_mode=ForwardMode.DECODâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                spec_info=forward_batch.spec_iâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                seq_lens_cpu=forward_batch.seqâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m                out_cache_loc=forward_batch.ouâ€¦[0m â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ [3m            )[0m                                   â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 62719 Output: 1203                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 5: Completed                                                                                   â”‚
â”‚ Total       â”‚ Input: 90990 Output: 5299                                                                              â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 6: Completed                                                                                   â”‚
â”‚ Total       â”‚ Input: 119352 Output: 9395                                                                             â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 7: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ "decode_cuda_graph_metadata\[\\"cu_seqlens_k\â€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ -n                                             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 147805 Output: 11092                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 8: Completed                                                                                   â”‚
â”‚ Total       â”‚ Input: 177997 Output: 15188                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 9: Completed                                                                                   â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': 'grep -n                           â”‚ 389:                                            â”‚   â”‚
â”‚             â”‚ â”‚ "torch\\.zeros\\|fill_\\|torch\\.full\\|torchâ€¦ â”‚ metadata_expand.cache_seqlens_int32 =           â”‚   â”‚
â”‚             â”‚ â”‚ /home/ubuntu/OmniPerf-Bench/perf-agents-benchâ€¦ â”‚ torch.full(                                     â”‚   â”‚
â”‚             â”‚ â”‚ 'restart': False}                              â”‚ 1134:            "cache_seqlens":               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(max_bs, dtype=torch.int32,          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ device=self.device),                            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1138:            "cu_seqlens_k": torch.zeros(   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1141:            "page_table": torch.zeros(     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1147:            "page_table_draft_decode":     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1171:                "local_query_start_loc":   â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1174:                "local_seqused_k":         â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1177:                "local_block_table":       â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1188:                "cache_seqlens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1198:                "cu_seqlens_k":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1201:                "page_table": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1212:                "cache_seqlens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.full(                                     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1231:                "page_table": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1244:                "cache_seqlens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1254:                "cu_seqlens_k":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1257:                "page_table": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1270:                "cache_seqlens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1280:                "cu_seqlens_k":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1283:                "page_table": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1292:                "cache_seqlens":           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1297:                "cu_seqlens_k":            â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1308:                "page_table": torch.zeros( â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1317:            "encoder_page_table":          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1323:            "encoder_lens_int32":          â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1326:            "encoder_cu_seqlens_k":        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ torch.zeros(                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1642:                metadata.page_table[:,     â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ max_seq_pages:].fill_(0)                        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1710:                ].fill_(0)                 â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1769:    def                                    â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ get_cuda_graph_seq_len_fill_value(self):        â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1871:        local_q_buf.fill_(0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1873:        local_k_buf.fill_(0)               â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1875:        local_block_buf.fill_(0)           â”‚   â”‚
â”‚             â”‚ â”‚                                                â”‚ 1876:        local_block_buf[:b0, b1:].fill_(0) â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 208280 Output: 15795                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 10: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 239641 Output: 19891                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 11: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 271093 Output: 23987                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 12: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 302636 Output: 28083                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 13: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 334270 Output: 32179                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 14: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 365995 Output: 36275                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 15: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 397811 Output: 40371                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 16: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 429718 Output: 44467                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 17: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 461716 Output: 48563                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 18: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 493805 Output: 52659                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 19: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 525985 Output: 56755                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 20: Completed                                                                                  â”‚
â”‚ Total       â”‚ Input: 558256 Output: 60851                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 21: Completed                                                                                  â”‚
â”‚ Tools       â”‚ ðŸ”§ bash                                                                                                â”‚
â”‚ bash        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚             â”‚ â”‚ {'command': "python - << 'PY'\nimport sys,     â”‚ python version 3.12.3 (main, Aug 14 2025,       â”‚   â”‚
â”‚             â”‚ â”‚ pkgutil\nprint('python version',               â”‚ 17:47:21) [GCC 13.3.0]                          â”‚   â”‚
â”‚             â”‚ â”‚ sys.version)\ntry:\n    import sgl_kernel\n    â”‚ sgl_kernel import failed:                       â”‚   â”‚
â”‚             â”‚ â”‚ print('sgl_kernel imported OK:',               â”‚ ModuleNotFoundError("No module named            â”‚   â”‚
â”‚             â”‚ â”‚ sgl_kernel.__spec__)\nexcept Exception as e:\n â”‚ 'sgl_kernel'")                                  â”‚   â”‚
â”‚             â”‚ â”‚ print('sgl_kernel import failed:',             â”‚                                                 â”‚   â”‚
â”‚             â”‚ â”‚ repr(e))\nPY", 'restart': False}               â”‚                                                 â”‚   â”‚
â”‚             â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Total       â”‚ Input: 590618 Output: 62351                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Status      â”‚ âœ… Step 22: Completed                                                                                  â”‚
â”‚ Error       â”‚ âŒ Unterminated string starting at: line 1 column 33 (char 32)                                         â”‚
â”‚ Total       â”‚ Input: 590618 Output: 62351                                                                            â”‚
â”‚ Tokens      â”‚                                                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

============================================================
[1mLakeview Summary[0m
============================================================
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 1 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mThe agent opens python/sglang/srt/layers/attention/flashattention_backend.py[0m â”‚
â”‚ [3mto find torch.zeros and fill_ usages, planning replacements (zeros->empty, [0m  â”‚
â”‚ [3mremove fill_) before benchmarking and patching.[0m                              â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 2 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt opens python/sglang/srt/layers/attention/flashattention_backend.py (lines[0m â”‚
â”‚ [3m200â€“800) to inspect its implementation for patterns related to the suspected[0m â”‚
â”‚ [3mperformance issue.[0m                                                           â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 3 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt views lines 800â€“1600 of [0m                                                  â”‚
â”‚ [3mpython/sglang/srt/layers/attention/flashattention_backend.py to search for [0m  â”‚
â”‚ [3mtorch.zeros and fill_ usages for planned performance-related changes.[0m        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 4 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt views lines 1600â€“2800 of [0m                                                 â”‚
â”‚ [3mpython/sglang/srt/layers/attention/flashattention_backend.py, scanning for [0m  â”‚
â”‚ [3mtorch.zeros and fill_ patterns to inform planned performance changes.[0m        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 7 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt greps flashattention_backend.py for [0m                                      â”‚
â”‚ [3mdecode_cuda_graph_metadata["cu_seqlens_k"] occurrences to locate relevant [0m   â”‚
â”‚ [3msections for potential optimization and code changes.[0m                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 9 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [ðŸ‘ï¸EXAMINE_CODE] The agent [1m<task>The agent is examining source code.[0m          â”‚
â”‚ [3mIt greps flashattention_backend.py for torch.zeros, fill_, torch.full, and [0m  â”‚
â”‚ [3mtorch.empty usages to locate memory allocation patterns for planned [0m         â”‚
â”‚ [3mperformance optimizations.[0m                                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Step 21 (Lakeview) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ [â‰ï¸OUTLIER] The agent [1m<task>The agent is verifying the environment and [0m       â”‚
â”‚ [1mdependencies.[0m                                                                â”‚
â”‚ [3mIt runs a Python snippet to print Python version and attempt importing [0m      â”‚
â”‚ [3msgl_kernel, ensuring the module is importable in the current environment.[0m    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

============================================================
[1mExecution Summary[0m
============================================================
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task             â”‚ I've uploaded a python code           â”‚
â”‚                  â”‚ repository in the dire...             â”‚
â”‚ Success          â”‚ âŒ No                                 â”‚
â”‚ Steps            â”‚ 22                                    â”‚
â”‚ Execution Time   â”‚ 2075.92s                              â”‚
â”‚ Total Tokens     â”‚ 652969                                â”‚
â”‚ Input Tokens     â”‚ 590618                                â”‚
â”‚ Output Tokens    â”‚ 62351                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trajectory saved to: /home/ubuntu/OmniPerf-Bench/perf-agents-bench/state/runs/sglang_core-ae58875a/sglang_008_205d5cb4/trajectory.json