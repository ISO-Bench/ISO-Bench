{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-73fa2d4", "model_patch": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 30a009c2e..944f35679 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -31,8 +31,8 @@ class PolicyScheduler:\n         max_total_num_tokens,\n         tree_cache,\n     ):\n-        if tree_cache.disable and policy == \"lpm\":\n-            # LMP is meaningless when the tree cache is disabled.\n+        if tree_cache.disable and policy in [\"lpm\", \"dfs-weight\"]:\n+            # LPM and DFS-weight are meaningless when the tree cache is disabled.\n             policy = \"fcfs\"\n \n         self.policy = policy\n@@ -157,6 +157,9 @@ class PrefillAdder:\n \n     @contextmanager\n     def _lock_node(self, last_node):\n+        if self.tree_cache.disable:\n+            yield None\n+            return\n         try:\n             delta = self.tree_cache.inc_lock_ref(last_node)\n             self.rem_total_tokens += delta\n@@ -187,7 +190,8 @@ class PrefillAdder:\n             ):\n                 # Non-chunked prefill\n                 self.can_run_list.append(req)\n-                self.tree_cache.inc_lock_ref(req.last_node)\n+                if not self.tree_cache.disable:\n+                    self.tree_cache.inc_lock_ref(req.last_node)\n                 self._prefill_one_req(\n                     prefix_len, input_tokens, req.sampling_params.max_new_tokens\n                 )\n@@ -201,7 +205,8 @@ class PrefillAdder:\n                 req.input_ids = req.input_ids[: len(req.prefix_indices) + trunc_len]\n                 self.can_run_list.append(req)\n                 self.new_inflight_req = req\n-                self.tree_cache.inc_lock_ref(req.last_node)\n+                if not self.tree_cache.disable:\n+                    self.tree_cache.inc_lock_ref(req.last_node)\n                 self._prefill_one_req(prefix_len, trunc_len, 0)\n \n         return True\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2489abd5d..2e693febd 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -363,13 +363,13 @@ class ScheduleBatch:\n         out_cache_loc = self.token_to_kv_pool.alloc(num_tokens)\n \n         if out_cache_loc is None:\n-            if self.tree_cache is not None:\n+            if self.tree_cache is not None and not getattr(self.tree_cache, \"disable\", False):\n                 self.tree_cache.evict(num_tokens, self.token_to_kv_pool.free)\n                 out_cache_loc = self.token_to_kv_pool.alloc(num_tokens)\n \n             if out_cache_loc is None:\n                 logger.error(\"Prefill out of memory. Try to lower your batch size.\")\n-                if self.tree_cache is not None:\n+                if self.tree_cache is not None and not getattr(self.tree_cache, \"disable\", False):\n                     self.tree_cache.pretty_print()\n                 exit(1)\n \ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 0228073c7..26494c01b 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -271,7 +271,8 @@ class ModelTpServer:\n \n     def print_stats(self):\n         num_used = self.max_total_num_tokens - (\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n+            self.token_to_kv_pool.available_size()\n+            + (0 if getattr(self.tree_cache, \"disable\", False) else self.tree_cache.evictable_size())\n         )\n         throughput = self.num_generated_tokens / (time.time() - self.last_stats_tic)\n         self.num_generated_tokens = 0\n@@ -288,7 +289,8 @@ class ModelTpServer:\n     def check_memory(self):\n         crash = os.getenv(\"CI\", \"false\") == \"true\"\n         available_size = (\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size()\n+            self.token_to_kv_pool.available_size()\n+            + (0 if getattr(self.tree_cache, \"disable\", False) else self.tree_cache.evictable_size())\n         )\n         if available_size != self.max_total_num_tokens:\n             warnings.warn(\n@@ -387,7 +389,8 @@ class ModelTpServer:\n \n         adder = PrefillAdder(\n             self.tree_cache,\n-            self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size(),\n+            self.token_to_kv_pool.available_size()\n+            + (0 if getattr(self.tree_cache, \"disable\", False) else self.tree_cache.evictable_size()),\n             self.max_prefill_tokens,\n             self.chunked_prefill_size,\n         )\ndiff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py\nindex c23812049..1b7edefb8 100644\n--- a/python/sglang/srt/mem_cache/radix_cache.py\n+++ b/python/sglang/srt/mem_cache/radix_cache.py\n@@ -169,6 +169,8 @@ class RadixCache(BasePrefixCache):\n                 heapq.heappush(leaves, x.parent)\n \n     def inc_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 0:\n@@ -179,6 +181,8 @@ class RadixCache(BasePrefixCache):\n         return delta\n \n     def dec_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 1:\n", "model_name_or_path": "gpt-5-2025-08-07"}
