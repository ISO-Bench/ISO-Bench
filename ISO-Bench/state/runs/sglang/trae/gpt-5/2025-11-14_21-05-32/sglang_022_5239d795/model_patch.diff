diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index 26c5e617a..db15d71eb 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -1607,31 +1607,31 @@ class DeepseekV2ForCausalLM(nn.Module):
                     "up_proj.weight",
                     "up_proj.weight_scale_inv",
                 ]
-            names_to_remove = []
+            names_to_remove = set()
             for moe_layer in tqdm(
                 range(
                     self.config.first_k_dense_replace,
                     self.config.num_hidden_layers,
                     self.config.moe_layer_freq,
                 ),
-                desc=f"Cloning {self.n_share_experts_fusion} "
-                "replicas of the shared expert into MoE",
+                desc=f"Cloning {self.n_share_experts_fusion} replicas of the shared expert into MoE",
+                disable=True,
             ):
+                base_layer = f"model.layers.{moe_layer}.mlp"
+                shared_prefix = f"{base_layer}.shared_experts."
+                experts_prefix = f"{base_layer}.experts."
                 for num_repeat in range(self.n_share_experts_fusion):
+                    expert_id = self.config.n_routed_experts + num_repeat
+                    expert_prefix = f"{experts_prefix}{expert_id}."
                     for suffix in suffix_list:
-                        shared_expert_weight_name = (
-                            f"model.layers.{moe_layer}.mlp.shared_experts.{suffix}"
-                        )
+                        shared_expert_weight_name = f"{shared_prefix}{suffix}"
                         weights_list.append(
                             (
-                                f"model.layers.{moe_layer}."
-                                f"mlp.experts."
-                                f"{self.config.n_routed_experts + num_repeat}"
-                                f".{suffix}",
-                                weights_dict[shared_expert_weight_name].clone(),
+                                f"{expert_prefix}{suffix}",
+                                weights_dict[shared_expert_weight_name],
                             )
                         )
-                        names_to_remove += [shared_expert_weight_name]
+                        names_to_remove.add(shared_expert_weight_name)
             weights = [w for w in weights_list if w[0] not in names_to_remove]
 
         # Params for weights, fp8 weight scales, fp8 activation scales
