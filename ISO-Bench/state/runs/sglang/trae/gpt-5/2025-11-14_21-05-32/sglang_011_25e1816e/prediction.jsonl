{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-a53fe42", "model_patch": "diff --git a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\nindex f1ee5d40e..e5e999747 100644\n--- a/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n+++ b/sgl-kernel/csrc/allreduce/trt_reduce_internal.cu\n@@ -183,7 +183,9 @@ __inline__ __device__ void block_barrier(\n     }\n   }\n \n-  __syncthreads();\n+  if constexpr (start || need_fence) {\n+    __syncthreads();\n+  }\n }\n \n template <typename T, int RANKS_PER_NODE, bool COPY_INPUT = true>\n@@ -224,13 +226,14 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n \n   // The source pointers. Distributed round-robin for the different warps.\n   auto peer_comm_buffer_ptrs = params.peer_comm_buffer_ptrs->ptrs;\n-  T* local_shared_buffer = reinterpret_cast<T*>(peer_comm_buffer_ptrs[params.local_rank]);\n+  T* __restrict__ local_shared_buffer = reinterpret_cast<T*>(peer_comm_buffer_ptrs[params.local_rank]);\n   // Start and end offsets of the thread\n   size_t chunk_start = bidx * params.elts_per_block + tidx * NUM_ELTS;\n-  size_t chunk_end = std::min((bidx + 1) * params.elts_per_block, params.elts_per_rank);\n+  size_t chunk_end = min((bidx + 1) * params.elts_per_block, params.elts_per_rank);\n+  T* __restrict__ local_output_buffer = reinterpret_cast<T*>(params.local_output_buffer_ptr);\n \n   if constexpr (COPY_INPUT) {\n-    T const* local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n+    T const* __restrict__ local_input_buffer = reinterpret_cast<T const*>(params.local_input_buffer_ptr);\n     // Copy from local buffer to shareable buffer\n     for (size_t iter_offset = chunk_start; iter_offset < chunk_end; iter_offset += blockDim.x * NUM_ELTS) {\n       *reinterpret_cast<int4*>(&local_shared_buffer[iter_offset]) =\n@@ -243,25 +246,23 @@ static __global__ void __launch_bounds__(512, 1) oneShotAllReduceKernel(AllReduc\n \n   // Each block accumulates the values from the different GPUs on the same node.\n   for (size_t iter_offset = chunk_start; iter_offset < chunk_end; iter_offset += blockDim.x * NUM_ELTS) {\n-    // Iterate over the different ranks/devices on the node to load the values.\n-    PackedStruct vals[RANKS_PER_NODE];\n-#pragma unroll\n-    for (int ii = 0; ii < RANKS_PER_NODE; ++ii) {\n-      vals[ii].packed = *reinterpret_cast<int4 const*>(&((T*)peer_comm_buffer_ptrs[ii])[iter_offset]);\n-    }\n-\n-    // Sum the values from the different ranks.\n+    // Sum the values from the different ranks in a fixed order to ensure determinism.\n     PackedStruct sums;\n-    sums.packed = {0, 0, 0, 0};\n+    sums.packed = make_int4(0, 0, 0, 0);\n #pragma unroll\n     for (int rank = 0; rank < RANKS_PER_NODE; ++rank) {\n       // Always reduce from rank 0 to ensure stable reduce order.\n-      sums.packed = add128b(sums, vals[rank]);\n+      PackedStruct val;\n+      val.packed = *reinterpret_cast<int4 const*>(&((T*)peer_comm_buffer_ptrs[rank])[iter_offset]);\n+      sums.packed = add128b(sums, val);\n     }\n \n     // Store to the destination buffer.\n-    *reinterpret_cast<int4*>(&reinterpret_cast<T*>(params.local_output_buffer_ptr)[iter_offset]) = sums.packed;\n+    *reinterpret_cast<int4*>(&local_output_buffer[iter_offset]) = sums.packed;\n   }\n+  // Signal completion for corresponding blocks on other GPUs. No fence needed here.\n+  block_barrier<false>(\n+      params.peer_barrier_ptrs_out, params.barrier_flag, params.local_rank, RANKS_PER_NODE, tidx, bidx, grid_size);\n }\n \n template <typename T, int RANKS_PER_NODE, bool COPY_INPUT = true>\n", "model_name_or_path": "gpt-5-2025-08-07"}
