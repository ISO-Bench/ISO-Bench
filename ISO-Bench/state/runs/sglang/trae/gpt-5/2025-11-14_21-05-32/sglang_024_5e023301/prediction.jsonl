{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-fa7d7fd", "model_patch": "diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py\nindex aeab9d48d..b72099f92 100644\n--- a/python/sglang/srt/layers/quantization/fp8_utils.py\n+++ b/python/sglang/srt/layers/quantization/fp8_utils.py\n@@ -146,7 +146,7 @@ def apply_w8a8_block_fp8_linear(\n         q_input, x_scale = per_token_group_quant_fp8(\n             input_2d, block_size[1], column_major_scales=False\n         )\n-        output = torch.zeros(\n+        output = torch.empty(\n             [q_input.shape[0], weight.shape[0]],\n             dtype=input.dtype,\n             device=q_input.device,\n@@ -210,22 +210,11 @@ def block_quant_to_tensor_quant(\n     assert n_tiles == x_s.shape[0]\n     assert k_tiles == x_s.shape[1]\n \n-    x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [\n-        [\n-            x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            for i in range(k_tiles)\n-        ]\n-        for j in range(n_tiles)\n-    ]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n+    # Vectorized blockwise dequantization via Kronecker product to avoid Python loops\n+    tile = torch.ones(block_n, block_k, dtype=torch.float32, device=x_s.device)\n+    scale_full = torch.kron(x_s.to(torch.float32), tile)\n+    scale_full = scale_full[:n, :k]\n+    x_dq_block = x_q_block.to(torch.float32) * scale_full\n \n     x_q_tensor, scale = (\n         scaled_fp8_quant(x_dq_block)\n@@ -248,6 +237,26 @@ def channel_quant_to_tensor_quant(\n     return x_q_tensor, scale\n \n \n+def block_quant_dequant(\n+    x_q_block: torch.Tensor,\n+    x_s: torch.Tensor,\n+    block_size: List[int],\n+    dtype: torch.dtype,\n+) -> torch.Tensor:\n+    \"\"\"Convert block-wise quantized tensor to dequantized tensor with target dtype.\"\"\"\n+    block_n, block_k = block_size[0], block_size[1]\n+    n, k = x_q_block.shape\n+    n_tiles = (n + block_n - 1) // block_n\n+    k_tiles = (k + block_k - 1) // block_k\n+    assert n_tiles == x_s.shape[0]\n+    assert k_tiles == x_s.shape[1]\n+\n+    tile = torch.ones(block_n, block_k, dtype=torch.float32, device=x_s.device)\n+    scale_full = torch.kron(x_s.to(torch.float32), tile)[:n, :k]\n+    x_dq = x_q_block.to(torch.float32) * scale_full\n+    return x_dq.to(dtype)\n+\n+\n def _process_scaled_mm_output(output, input_2d_shape, output_shape):\n     if type(output) is tuple and len(output) == 2:\n         output = output[0]\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 339aaad6b..82555d7e0 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -113,6 +113,14 @@ expert_distribution_recorder = ExpertDistributionRecorder()\n logger = logging.getLogger(__name__)\n \n \n+\n+\n+def _use_bmm_bf16_fallback() -> bool:\n+    try:\n+        return os.getenv(\"SGLANG_DSV3_BMM_FALLBACK_BF16\", \"0\") == \"1\"\n+    except Exception:\n+        return False\n+\n class AttnForwardMethod(IntEnum):\n     # Use multi-head attention\n     MHA = auto()\n@@ -763,13 +771,19 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 self.w_kc.to(torch.bfloat16) * self.w_scale,\n             )\n         elif self.w_kc.dtype == torch.float8_e4m3fn:\n-            q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(\n-                q_nope.transpose(0, 1),\n-                zero_allocator.allocate(1),\n-            )\n-            q_nope_out = bmm_fp8(\n-                q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16\n-            )\n+            if _use_bmm_bf16_fallback():\n+                q_nope_out = torch.bmm(\n+                    q_nope.to(torch.bfloat16).transpose(0, 1),\n+                    self.w_kc.to(torch.bfloat16) * self.w_scale,\n+                )\n+            else:\n+                q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(\n+                    q_nope.transpose(0, 1),\n+                    zero_allocator.allocate(1),\n+                )\n+                q_nope_out = bmm_fp8(\n+                    q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16\n+                )\n         else:\n             q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)\n \n@@ -810,17 +824,23 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 self.w_vc.to(torch.bfloat16) * self.w_scale,\n             )\n         elif self.w_vc.dtype == torch.float8_e4m3fn:\n-            attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(\n-                attn_output.transpose(0, 1),\n-                zero_allocator.allocate(1),\n-            )\n-            attn_bmm_output = bmm_fp8(\n-                attn_output_val,\n-                self.w_vc,\n-                attn_output_scale,\n-                self.w_scale,\n-                torch.bfloat16,\n-            )\n+            if _use_bmm_bf16_fallback():\n+                attn_bmm_output = torch.bmm(\n+                    attn_output.to(torch.bfloat16).transpose(0, 1),\n+                    self.w_vc.to(torch.bfloat16) * self.w_scale,\n+                )\n+            else:\n+                attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(\n+                    attn_output.transpose(0, 1),\n+                    zero_allocator.allocate(1),\n+                )\n+                attn_bmm_output = bmm_fp8(\n+                    attn_output_val,\n+                    self.w_vc,\n+                    attn_output_scale,\n+                    self.w_scale,\n+                    torch.bfloat16,\n+                )\n         else:\n             attn_bmm_output = torch.bmm(attn_output.transpose(0, 1), self.w_vc)\n         attn_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)\n@@ -862,14 +882,20 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 self.w_kc.to(torch.bfloat16) * self.w_scale,\n             )\n         elif self.w_kc.dtype == torch.float8_e4m3fn:\n-            q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(\n-                q_nope.transpose(0, 1),\n-                zero_allocator.allocate(1),\n-                dtype=torch.float8_e4m3fn,\n-            )\n-            q_nope_out = bmm_fp8(\n-                q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16\n-            )\n+            if _use_bmm_bf16_fallback():\n+                q_nope_out = torch.bmm(\n+                    q_nope.to(torch.bfloat16).transpose(0, 1),\n+                    self.w_kc.to(torch.bfloat16) * self.w_scale,\n+                )\n+            else:\n+                q_nope_val, q_nope_scale = per_tensor_quant_mla_fp8(\n+                    q_nope.transpose(0, 1),\n+                    zero_allocator.allocate(1),\n+                    dtype=torch.float8_e4m3fn,\n+                )\n+                q_nope_out = bmm_fp8(\n+                    q_nope_val, self.w_kc, q_nope_scale, self.w_scale, torch.bfloat16\n+                )\n         else:\n             q_nope_out = torch.bmm(q_nope.transpose(0, 1), self.w_kc)\n         q_input[..., : self.kv_lora_rank] = q_nope_out.transpose(0, 1)\n@@ -958,18 +984,24 @@ class DeepseekV2AttentionMLA(nn.Module):\n                 self.w_vc.to(torch.bfloat16) * self.w_scale,\n             )\n         elif self.w_vc.dtype == torch.float8_e4m3fn:\n-            attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(\n-                attn_output.transpose(0, 1),\n-                zero_allocator.allocate(1),\n-                dtype=torch.float8_e4m3fn,\n-            )\n-            attn_bmm_output = bmm_fp8(\n-                attn_output_val,\n-                self.w_vc,\n-                attn_output_scale,\n-                self.w_scale,\n-                torch.bfloat16,\n-            )\n+            if _use_bmm_bf16_fallback():\n+                attn_bmm_output = torch.bmm(\n+                    attn_output.to(torch.bfloat16).transpose(0, 1),\n+                    self.w_vc.to(torch.bfloat16) * self.w_scale,\n+                )\n+            else:\n+                attn_output_val, attn_output_scale = per_tensor_quant_mla_fp8(\n+                    attn_output.transpose(0, 1),\n+                    zero_allocator.allocate(1),\n+                    dtype=torch.float8_e4m3fn,\n+                )\n+                attn_bmm_output = bmm_fp8(\n+                    attn_output_val,\n+                    self.w_vc,\n+                    attn_output_scale,\n+                    self.w_scale,\n+                    torch.bfloat16,\n+                )\n         else:\n             attn_bmm_output = torch.bmm(attn_output.transpose(0, 1), self.w_vc)\n         attn_output = attn_bmm_output.transpose(0, 1).flatten(1, 2)\n", "model_name_or_path": "gpt-5-2025-08-07"}
