{"instance_id": "/home/ubuntu/OmniPerf-Bench/sglan__repo-9f2c956", "model_patch": "diff --git a/docs/backend/server_arguments.md b/docs/backend/server_arguments.md\nindex 36c44b57e..50b888cbf 100644\n--- a/docs/backend/server_arguments.md\n+++ b/docs/backend/server_arguments.md\n@@ -221,3 +221,4 @@ Please consult the documentation below and [server_args.py](https://github.com/s\n | `triton_attention_num_kv_splits` | Use to adjust the number of KV splits in triton kernels. | `8` |\n | `flashinfer_mla_disable_ragged` | Disable the use of the [ragged prefill](https://github.com/flashinfer-ai/flashinfer/blob/5751fc68f109877f6e0fc54f674cdcdef361af56/docs/tutorials/kv_layout.rst#L26) wrapper for the FlashInfer MLA attention backend. Ragged prefill increases throughput by computing MHA instead of paged MLA when there is no prefix match. Only use it when FlashInfer is being used as the MLA backend. | `False` |\n | `disable_chunked_prefix_cache` | Disable the use of chunked prefix cache for DeepSeek models. Only use it when FA3 is attention backend. | `False` |\n+| `enable_dp_lm_head` | Enable vocabulary parallel across the attention TP group to avoid all-gather across DP groups, optimizing performance under DP attention. | `False` |\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 4958c6d04..9e7cabba0 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -177,7 +177,7 @@ class LogitsMetadata:\n         else:\n             dp_local_start_pos = cumtokens[dp_rank - 1]\n         dp_local_num_tokens = self.global_num_tokens_for_logprob_gpu[dp_rank]\n-        gathered_buffer = torch.zeros(\n+        gathered_buffer = torch.empty(\n             (\n                 sum(self.global_num_tokens_for_logprob_cpu),\n                 hidden_states.shape[1],\n@@ -424,11 +424,10 @@ class LogitsProcessor(nn.Module):\n         \"\"\"\n         if self.do_tensor_parallel_all_gather_dp_attn:\n             logits_metadata.compute_dp_attention_metadata(hidden_states)\n-            hidden_states, local_hidden_states = (\n-                logits_metadata.gathered_buffer,\n-                hidden_states.clone(),\n-            )\n-            dp_gather_replicate(hidden_states, local_hidden_states, logits_metadata)\n+            local_seq_len = hidden_states.shape[0]\n+            src_hidden_states = hidden_states\n+            hidden_states = logits_metadata.gathered_buffer\n+            dp_gather_replicate(hidden_states, src_hidden_states, logits_metadata)\n \n         if hasattr(lm_head, \"weight\"):\n             logits = torch.matmul(\n@@ -447,7 +446,7 @@ class LogitsProcessor(nn.Module):\n         if self.do_tensor_parallel_all_gather_dp_attn:\n             logits, global_logits = (\n                 torch.empty(\n-                    (local_hidden_states.shape[0], logits.shape[1]),\n+                    (local_seq_len, logits.shape[1]),\n                     device=logits.device,\n                     dtype=logits.dtype,\n                 ),\ndiff --git a/python/sglang/srt/layers/vocab_parallel_embedding.py b/python/sglang/srt/layers/vocab_parallel_embedding.py\nindex ebc148feb..c97c6505f 100644\n--- a/python/sglang/srt/layers/vocab_parallel_embedding.py\n+++ b/python/sglang/srt/layers/vocab_parallel_embedding.py\n@@ -463,7 +463,8 @@ class VocabParallelEmbedding(torch.nn.Module):\n         if not self.use_presharded_weights:\n             loaded_weight = loaded_weight.narrow(output_dim, start_idx, shard_size)\n         param[: loaded_weight.shape[0]].data.copy_(loaded_weight)\n-        param[loaded_weight.shape[0] :].data.fill_(0)\n+        if param.shape[0] > loaded_weight.shape[0]:\n+            param[loaded_weight.shape[0] :].data.zero_()\n \n     def forward(self, input_):\n         if self.tp_size > 1:\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex a797a7f3a..ae9933c50 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -75,6 +75,7 @@ global_server_args_dict = {\n     \"enable_deepep_moe\": ServerArgs.enable_deepep_moe,\n     \"enable_dp_attention\": ServerArgs.enable_dp_attention,\n     \"enable_ep_moe\": ServerArgs.enable_ep_moe,\n+    \"enable_dp_lm_head\": ServerArgs.enable_dp_lm_head,\n     \"enable_nan_detection\": ServerArgs.enable_nan_detection,\n     \"flashinfer_mla_disable_ragged\": ServerArgs.flashinfer_mla_disable_ragged,\n     \"max_micro_batch_size\": ServerArgs.max_micro_batch_size,\n@@ -1024,14 +1025,14 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):\n         )\n \n         if not decoder_out_cache_loc:\n-            self.out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\n             self.out_cache_loc = torch.cat(decoder_out_cache_loc)\n \n         if not encoder_out_cache_loc:\n-            self.encoder_out_cache_loc = torch.zeros(0, dtype=torch.int64).to(\n+            self.encoder_out_cache_loc = torch.empty(0, dtype=torch.int64).to(\n                 self.device, non_blocking=True\n             )\n         else:\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 2e3b6c4df..9c604776a 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -159,6 +159,7 @@ class ServerArgs:\n     disable_overlap_schedule: bool = False\n     enable_mixed_chunk: bool = False\n     enable_dp_attention: bool = False\n+    enable_dp_lm_head: bool = False\n     enable_ep_moe: bool = False\n     enable_deepep_moe: bool = False\n     deepep_mode: Optional[Literal[\"auto\", \"normal\", \"low_latency\"]] = \"auto\"\n", "model_name_or_path": "gpt-5-2025-08-07"}
